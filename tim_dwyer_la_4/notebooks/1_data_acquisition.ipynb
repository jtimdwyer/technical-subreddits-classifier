{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape from Reddit \n",
    "\n",
    "_By Tim Dwyer_ \n",
    "\n",
    "\n",
    "We're going to scrape posts from the `all` page on Reddit. Here we use the official Reddit API. To do this I registered a scripting app with Reddit to obtain the OAuth credentials. This information isn't present in this repository but if you went and registered your own app, you could use this code for the same purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "import json\n",
    "import time\n",
    "from calendar import monthrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_token(app_id, secret, user, pw):\n",
    "    client_auth = requests.auth.HTTPBasicAuth(app_id, secret)\n",
    "    post_data = {\"grant_type\": \"password\", \"username\": user, \"password\": pw}\n",
    "    headers = {\"User-Agent\": f\"data science class project .1 u/{user}\"}\n",
    "    \n",
    "    response = requests.post(\"https://www.reddit.com/api/v1/access_token\",\n",
    "                         auth=client_auth, data=post_data, headers=headers)\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def print_time():\n",
    "    cur_time = time.localtime()\n",
    "    cur_hour, cur_min, cur_sec = cur_time.tm_hour, cur_time.tm_min, cur_time.tm_sec\n",
    "    return print(cur_hour, cur_min, cur_sec)\n",
    "\n",
    "def reddit_scrape(subreddit, app_id, secret, user, pw, pages=5, save_to_json=True, verbose=True, timer=True):\n",
    "    after = None\n",
    "    posts = []\n",
    "    params = {}\n",
    "    ten_percentile = 0\n",
    "    prev_query_time = 0\n",
    "    start_time = time.time()\n",
    "    response = get_new_token(app_id, secret, user, pw)\n",
    "    user_agent = \"data science class project 0.1 u/timdwyer_app\"\n",
    "    my_headers = {\n",
    "        \"Authorization\": f\"{response['token_type']} {response['access_token']}\",\n",
    "        \"User-Agent\": user_agent\n",
    "     }\n",
    "    \n",
    "    if timer:\n",
    "        print_time()\n",
    "        \n",
    "    for page in range(pages):\n",
    "        if after:\n",
    "            params['after'] = after\n",
    "\n",
    "        time_elapsed = time.time() - prev_query_time\n",
    "        if time_elapsed <= 1:\n",
    "            time.sleep(1 - time_elapsed)\n",
    "\n",
    "        token_time = time.time() - start_time\n",
    "        if token_time >= 3600:\n",
    "                response = get_new_token(app_id, secret, user, pw)\n",
    "                my_headers = {\n",
    "                    \"Authorization\": f\"{response['token_type']} {response['access_token']}\",\n",
    "                    \"User-Agent\": user_agent\n",
    "                }\n",
    "\n",
    "        res = requests.get(f'https://reddit.com/r/{subreddit}.json', params=params, headers=my_headers)\n",
    "        prev_query_time = time.time()\n",
    "\n",
    "        cur_page = res.json()\n",
    "        posts.extend(cur_page['data'].get('children'))\n",
    "        after = cur_page['data']['after']\n",
    "\n",
    "\n",
    "        if verbose and page/pages > ten_percentile:\n",
    "            print_time()\n",
    "            print(page)\n",
    "            ten_percentile += .1\n",
    "\n",
    "    if timer:\n",
    "        print_time()\n",
    "\n",
    "\n",
    "    if save_to_json:\n",
    "        with open(f'./data/json/{subreddit}.json', 'w+') as file:\n",
    "            json.dump(posts, file)\n",
    "    else:\n",
    "        return posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list `info` contains my `app_id, app_secret, user_name, password` in sequence. This cell will not run as written since the file with this information is not contained in this repository as it is not for sharing. If you would like to test that this code runs you should create your own reddit app, and pass your secret and id along. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "with open('./reddit_login.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        info.append(line.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_scrape('all', *info, pages=500, verbose=False, timer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./data/json/all.json') as file:\n",
    "    threads = json.load(file)\n",
    "    print(len(set( thread['data']['title'] for thread in threads )), len(threads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape from Pushshift API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up with fewer unique posts in than it seems at first `all`. Due to a constraint in how results are returned by the Reddit API, we can only get this many from `all` at any given time, and the limit is about $1,000$ on other subreddits.\n",
    "\n",
    "One way to get around this would be to use the Reddit Search feature to create new listings that we could associate with specific time intervals. Instead we'll shift gears and use the [Pushshift API](https://pushshift.io/). This is a third party API which allows us to search Reddit threads by time stamp without having to go through the extra step above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_scrape_push(subreddit, num_days, start_date, save_to_json=True, verbose=True, timer=True):   \n",
    "#   start_date = '2018_01_01_00_00_00' should be formatted like this as input.\n",
    "    pattern = '%Y_%m_%d_%H_%M_%S'\n",
    "    \n",
    "    after = int(time.mktime(time.strptime(start_date, pattern)))\n",
    "    before = after + 3600\n",
    "    posts = []\n",
    "    params = {}\n",
    "    \n",
    "    ten_percentile = 0\n",
    "    prev_query_time = 0\n",
    "    \n",
    "    pushshift_url = 'https://api.pushshift.io/reddit/search/submission/'\n",
    "    params = {\n",
    "        'subreddit':subreddit,\n",
    "        'after':after,\n",
    "        'before':before,\n",
    "        'size':500\n",
    "         }\n",
    "\n",
    "    if timer:\n",
    "        print_time()\n",
    "    \n",
    "    for day in range(num_days):\n",
    "        for hour in range(24):\n",
    "            time_elapsed = time.time() - prev_query_time\n",
    "            if time_elapsed <= 1:\n",
    "                time.sleep(1 - time_elapsed)\n",
    "\n",
    "            res = requests.get(pushshift_url, params=params)\n",
    "            prev_query_time = time.time()\n",
    "\n",
    "            cur_page = res.json()\n",
    "            posts.extend(cur_page['data'])\n",
    "\n",
    "            params['after'] = params['before']\n",
    "            params['before'] = params['before'] + 3600\n",
    "\n",
    "        if verbose and day/num_days > ten_percentile:\n",
    "            print_time()\n",
    "            print(day)\n",
    "            ten_percentile += .1\n",
    "\n",
    "        time.sleep(1)\n",
    "    \n",
    "    if timer:\n",
    "        print_time()\n",
    "\n",
    "    if save_to_json:\n",
    "        with open(f'./data/json/pushshift_{subreddit}_{start_date}.json', 'w+') as file:\n",
    "            json.dump(posts, file)\n",
    "    else:\n",
    "        return posts\n",
    "\n",
    "\n",
    "def year_scrape_push(subreddit, year):\n",
    "    months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    days_in_month = {month:monthrange(year, int(month))[1] for month in months}\n",
    "    \n",
    "    for month in months:\n",
    "            reddit_scrape_push(subreddit, days_in_month[month], f'{year}_{month}_01_00_00_00', \n",
    "                       save_to_json=True, verbose=False, timer=False)\n",
    "            \n",
    "    posts = []\n",
    "    for month in months:\n",
    "        start_date = f'{year}_{month}_01_00_00_00'\n",
    "        with open(f'./data/json/pushshift_{subreddit}_{start_date}.json', 'r') as file:\n",
    "            months_posts = json.load(file)\n",
    "            posts.extend(months_posts)\n",
    "    \n",
    "    with open(f'./data/json/pushshift_{subreddit}_{year}.json', 'w+') as file:\n",
    "        json.dump(posts, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\n",
    "    'math',\n",
    "    'learnmath',\n",
    "    \n",
    "    'python',\n",
    "    'learnpython',\n",
    "    \n",
    "    'datascience',\n",
    "    'learnmachinelearning',\n",
    "]\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    year_scrape_push(subreddit, 2017)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
