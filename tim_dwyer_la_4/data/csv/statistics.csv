title,num_comments,score,over_18,locked,stickied,subreddit,created_utc,is_self,selftext
Brier score calculation: 2 methods should yield same result,0,1,False,False,False,statistics,1483270680,True,[removed]
just imagine,0,0,False,False,False,statistics,1483284209,False,
2 questions on Q-Q plots,0,0,False,False,False,statistics,1483284404,True,[deleted]
Thoughts on my tracking of a Wrestling Video Game this year?,0,3,False,False,False,statistics,1483287401,False,
Who says Statisticians can't have a little fun?,4,14,False,False,False,statistics,1483290250,False,
Good book on distances,4,3,False,False,False,statistics,1483291733,True,"Happy new year! Do you know a good book on the topic of distances/divergences between two statistical distributions (e.g. the KL divergence, Hellinger distance) or two vectors (e.g. Euclidean distance, Mahalanobis distances)? I am looking for a book that would group these measure by properties (e.g. ultrametric, circumeuclidean, etc.).
Cheers,"
Statistics REUs for this summer?,0,1,False,False,False,statistics,1483300671,True,[removed]
How does one get better at statistics?,0,1,False,False,False,statistics,1483314062,True,[removed]
Statistics Land,1,16,False,False,False,statistics,1483318959,False,
Books / Problems for Statistics Undergrad Wanting to Enter Tech,0,1,False,False,False,statistics,1483327282,True,[removed]
Careers in Statistics,12,14,False,False,False,statistics,1483332341,True,"I'm not sure if this is the right sub for this question, but I'm looking for some possible careers that involve stats. I'm looking to focus on statistics in college (not sure where yet) and I was wondering what possible careers would come from this focus and how much college it would take to be successful. I'm very interested in the field of statistics. I know it's a pretty general question but any answers will help out a lot. 
"
"What is a good book (or site) I can use that lists popular general statistics about people, places, and things in different categories?",2,0,False,False,False,statistics,1483333059,True,[deleted]
Which area is the best to learn for data science?,0,1,False,False,False,statistics,1483359857,True,[removed]
"""radial basis functions fitting"". Does this make any sense?",9,10,False,False,False,statistics,1483362245,True,"Happy new year everyone.

In the field were I work (compartmental modelling of drugs), a group came up with a method to fit data, that gained quite some popularity, which they called ""radial basis functions fitting"". To me, instead, it doesn't make sense and it works just by accident. I wanted the opinion of someone with better understanding of the underlying theory. 

Basically, we have a system which response is y(t) = k1* x(t)+k2* conv(x(t), exp(-k3*t)), where t is time, y is the output value that we measure, x is the input (a function of time) and conv is the convolution operator. Ks are the 3 parameters we need to fit. Actually what we really want to measure is B = k1+k2/k3 -1. 

What we used to do was to have x(t), measure y(t) and fit the 3 k minimizing the chi square in a non-linear fit. Sadly, this fit generally had lots of noise.

The proposed ""radial basis function"" method works by computing theta_i (t)= conv(x(t), exp(-k3_i*t)) for a multiple k3 values, indexed by i, finely sampled over the whole range of values that k3 might assume in a physical situation. Then, for each *i*, they do a linear fit of k1_i and k2_i fitting y(t) = k1_i *x(t) + k2_i * theta_i(t), and save the chi-squared. Then, they keep the triplet of k1, k2 and k3 for the theta_i that gave the minimum chi squared. 

Now, to me this is absolutely unneeded. The only reason that it gives improvements over the standard ""usual"" fit is that, as x(t) is approximately a linear slowly varying function, k2 and k3 are strongly correlated and therefore our k1+k2/k3 -1 final parameter is almost not influenced by the correct fit of k2 and k3. 

Any thoughts? Has anyone ever heard of this method applied in other settings?"
Can someone help with SPSS?,0,1,False,False,False,statistics,1483363806,True,[removed]
Categorical data in regression,0,1,False,False,False,statistics,1483366874,True,[deleted]
Calculating Odds ratios,5,1,False,False,False,statistics,1483371652,False,
Anyone knows how to compute this problem in Rstudio?,0,1,False,False,False,statistics,1483379366,True,[removed]
Is there a MAXimum amount of data/observations for which t tests can be performed?,12,4,False,False,False,statistics,1483384866,True,"I am attempting to use a paired t-test on 14k observations. Plots of the data suggest that there shouldn't be a difference yet the alternative hypothesis is being supported by the data at p<2.2e-16.
Is this a symptom of the large data set or am I being too caviler with my data.  

** 
Data is student performance records (grades A-E/1-5) before and after semester on a test. p is as above the t~=-30
df~= 14000
meanofdiffs ~= -.22
CI ~= -Inf to -.20"
Comparing frequencies between 2 samples with 2 categorical variables,0,1,False,False,False,statistics,1483388625,True,[removed]
Can anyone suggest a textbook that focuses on sample size and power calculations in clinical research that have problem sets? (x-post /r/epidemiology),0,1,False,False,False,statistics,1483389136,False,[deleted]
Interview prep/ all of statistics in a week?,15,47,False,False,False,statistics,1483390039,True,"As the title says. I have a job interview coming up that I need to be well prepared for and after being in a pretty dumbed down finance job for 3 years, I need to quickly refamiliarize myself pretty much all of statistics. I know I need coverage of all the basic theory/applications that might be covered in a PhD first year (probability, glms, gams, categorical data, regularization, survival, resampling, testing) but every interviewer at this company is allowed to cover whatever he or she finds interesting so I am looking for breadth of coverage over depth. I have 1-2 weeks to study.

Are there any textbooks you recommend for reading straight through or any alternative study strategies? I have several textbooks that I love for their coverage in particular areas but do not think are very useful for rapid review (Casella and Berger, Agresti). Maybe someone has qual prep notes they could share? Thanks for any help!

ETA: I've ordered Wasserman and started reading Shalizi's online text. Thanks for the great suggestions!

"
Any Bayesian clustering tutorials using PyMC3?,1,4,False,False,False,statistics,1483392003,True,[deleted]
Can anyone suggest a textbook that focuses on sample size and power calculation in clincal research with problem sets? (x-post /r/epidemiology),3,3,False,False,False,statistics,1483400496,True,"I am trying to put together an independent study to fulfill some of my PhD program electives requirements on how to learn to calculate sample sizes for different type of research studies, mainly focusing on clinical trials and/or psychometric studies (I know two different fields, but my current job merges both concepts). Both my MPH program and my PhD Advanced Epi course only gave me a 1-2 hour lecture giving an overview of sample size calculation and power calculation, which left me not knowing much about the topic. Thus, I figure I put together an independent study to focus more into this topic. 

I went to my library and found a few textbooks on the topic, but unfortunately, none of them have problem sets for me to apply my learning. In addition, as part of the independent study, I have to be ""assessed"" somehow to show that I have mastered the concept. I did a quick search on amazon and saw a few textbooks on sample size calculation and power in clinical research, but unsure if any of them include problem sets. Ideally, I would like to demonstrate the concept discussed in each chapter using SAS. The problem set would reinforce the material for me.  

Being on a doctorate student salary, I would rather not purchase random textbooks and figure out which would fulfill my needs. Can anyone recommend a textbook with problem sets that focuses on this topic?

[X-post](https://www.reddit.com/r/epidemiology/comments/5lkmw2/textbook_recommendation_to_learn_sample_size/?utm_content=title&utm_medium=front&utm_source=reddit&utm_name=epidemiology)"
Remote statistics jobs,2,8,False,False,False,statistics,1483402819,True,"Hi all, are there any specific areas/jobs/industries/applications of stats that are particularly well-suited to remote work? Thanks in advance. "
approximating the integration with sum in bayesian inference,0,1,False,False,False,statistics,1483405691,False,
Do Masters programs in statistics expect undergrad research experience on an application?,8,7,False,False,False,statistics,1483428770,True,"I'm a stats undergrad. It seems to me that there are far less opportunities to be a research assistant in a field like statistics than in a field like biology. I know graduate programs in many sciences expect research experience, but is this also true for stats?"
Students of Statistics - Professional Help for Your Assignments,0,1,False,False,False,statistics,1483437441,False,
[HELP] I don't know what statistical test to use,1,0,False,False,False,statistics,1483443414,True,[removed]
Statistics Homework Help service- A complete guide on statistics concepts!,0,0,False,False,False,statistics,1483443462,False,
Predicting daily athlete performance: extremely dismal R-squared,27,8,False,False,False,statistics,1483457247,True,"I am using multiple regression to predict individual daily fantasy baseball points for the 2012 MLB season. 

The results have been underwhelming (that's an understatement): the dataset I have contains all of the features you would think would be relevant for predicting daily player performance, yet I have achieved an R-squared of 0.056 and the standard error of the model exceeds the mean of the dependent variable by a lot. 

The variables you would intuit as being significant turned out to be significant, which is good. But at this point I feel like I've exhausted all of my modeling tricks.

This leads me to suspect that either a) the data is of poor quality (doesn't seem likely, given the source) or b) daily fantasy points might as well be a stochastic process for baseball.

For those who have built models trying to predict these types of seemingly random outcomes -especially if you've built models for predicting fantasy points - how much success have you had from a modeling standpoint? Even if your model seems to perform poorly, like mine, does it translate into an effective betting strategy?"
North Carolina has less election integrity than North Korea ... or not.,5,3,False,False,False,statistics,1483460028,False,
"How many times to repeat to get a ""real"" value?",19,5,False,False,False,statistics,1483461726,True,"How can you figure out how many times you should test something for it to give a ""real""(closer as possible to reality) value? 
For example,how many times would I have to flip a coin to say its probability was 50-50 (let's not consider falling standing up) or a dice and say the probability is 1/6 per side? 
Is there a way to calculate this given all the possible outcomes? Thanks"
"Statistics as an ""additional skill"" on resumes?",15,3,False,False,False,statistics,1483464377,True,[deleted]
Can you guys help me with this probably easy multiple choice question?,0,1,False,False,False,statistics,1483468681,True,[removed]
Please convince me of the value of basis statistics,0,1,False,False,False,statistics,1483470785,True,[deleted]
Please convince me of the value of basic statistics,6,0,False,False,False,statistics,1483471465,True,[removed]
"I'd like to apply to some top master's programs, however they all require the GRE exam and I'm a horrible test taker. Also, funding?",1,1,False,False,False,statistics,1483472137,False,
Understanding mixture models and expectation-maximization (using baseball statistics),0,27,False,False,False,statistics,1483475378,False,
NSDUH survey + PSPP,2,2,False,False,False,statistics,1483477944,True,"Hi, I wonder if someone tried to work on NSDUH surveys. They provide files about 300mb and I'm unable to work properly with PSPP. Never happened to me before, although is true that I usually use small datasets.

I'm using Windows 7 64 bit with PSPP in 64 bit too, 8 GB Ram.

Is this normal, can I do any tweak?

What I basically want is to extract everything about Marijuana and play with that data."
Grouping similar risk events together,0,1,False,False,False,statistics,1483482682,True,[deleted]
What are the chances of my brother and me eating lunch at the same place?,8,0,False,False,False,statistics,1483489007,True,[removed]
Skewed data in logistic regression,4,0,False,False,False,statistics,1483498169,True,"I'm currently analysing sets of IVs to build a logistic regression model and I have questionnaire responses from respondents who are 16 to 26 years old (among other demographics) that I would like to use to predict the probability of them participating in civic activities  (yes, no; 0, 1). The age data do not follow a normal distribution on a histogram or on a Q-Q plot (moderate-ish positive skew).

The question is: Is this likely due to the effect of 'top and tailing' the respondent's age and has created an artificial floor/ceiling to my data. Will including this in my model be problematic? Are there any references I could search that discuss this specific problem with skewed?

Many thanks for your help."
Engineer who want to enter master in statistics,0,1,False,False,False,statistics,1483506941,True,[removed]
[Q] Plotting signed values with huge range,22,7,False,False,False,statistics,1483536193,True,"I hope questions about visualization are in-topic as well.

I need to visualize a heatmap of a matrix whose values vary over a huge range. These values are signed.

How should I transform the data to improve the visualization? Does it make sense to take the log preserving the signs and be careful with values too close to 0? It should work, but is there a better way?"
Question in regards to Chi-Squared,0,0,False,False,False,statistics,1483536979,True,[removed]
CV of a grouped Car accident claims,0,1,False,False,False,statistics,1483543268,True,[removed]
[Academic] Political Science and Psychology Survey ~8 mins,1,0,False,False,False,statistics,1483546635,False,
Determining whether one set of data impacts another type of data over a period of time.,4,9,False,False,False,statistics,1483573456,True,"Hi everyone, I was hoping I could leverage the wisdom of /r/statistics for a little problem that I'm not used to dealing with.

I've been given a couple of data sets. The first set consists of tracking survey data that measures customer satisfaction (on a 1 to 10 scale) across a number of issues, as well as overall satisfaction. Included in that data is the specific day in which the customer used the service.

The second is a reported number of incidents that the company experienced, broken down by the type of incident and the amount of delay it caused to the service. This is also catalogued by days.

I've been tasked in determining whether the incidents as recorded in the 2nd data set has any impact on customer satisfaction as recorded by the 1st data set. At first I thought this would be straightforward: With the survey data, I was planning on setting up a data set with the mean ratings on a weekly or bi-weekly basis (There's not enough data on a daily basis), along with a tally of the number of incidents and the sum of delays for each type of incident for the same time periods. Then, I'd run correlations or a regression to determine what types of incidents, if any, impact customer satisfaction. I'd also look at whether the total number of incidents (across categories) and their delays have any impact on satisfaction.

But now, I'm starting to doubt myself. Is the data I'm creating now considered time-series data? If not, great - I can go on from here.

But if it should be considered as time-series data, I'm a little bit out of my element. What type of analysis should I run using R? Here is what the data looks like for the [number of incidents](http://i.imgur.com/qyb2gUI.jpg) and the [mean satisfaction rating](http://i.imgur.com/dV6uVNQ.jpg). It doesn't appear to me that there's much of a relationship with the data and the time dimension - would I be correct in thinking that I'm being a worrywart?"
I'm confused about a regression result in a journal paper.,0,1,False,False,False,statistics,1483574319,True,[deleted]
A question about a regression result in a journal paper.,2,1,False,False,False,statistics,1483574950,True,[deleted]
Outlier detection for many time series?,4,3,False,False,False,statistics,1483599071,True,"I have several thousand people and how much they use a product each week over the last couple years and I want to build a model to help identify if the newest week is an outlier relative to their past behavior. 

The ideal output I want to get is the probability of the newest value being >= the observed value. So, I know that means I need to use Bayesian methods in order to get poster predictive distributions. What I'm struggling with is figuring out what type of model I should use/how to set it up properly.

I could, of course, just build a separate linear model for each person, but that seems too crude, error-prone, and inefficient, especially since I'll be retraining the models every week as new data comes in.

I was thinking maybe a Bayesian multilevel model might do the trick? I've only used those for cross-sectional data before, so I'm unsure how I should apply it to time series data (I'm not really too experienced with time series in general).

It doesn't have to be a particularly sophisticated model as it is just going to be for basic QC purposes; the only real requirement is a resulting probability that incorporates each individual's history that's reasonably accurate.

Any advice/resources/suggestions would be greatly appreciated. I can use either SAS or R to build the models, so bonus points if you can direct me to code or packages/PROCs to use.

Thank you!"
Was 2016 especially dangerous for celebrities? An empirical analysis.,13,88,False,False,False,statistics,1483610552,False,
Help to analysis a set of chemistry data,0,1,False,False,False,statistics,1483621006,True,[removed]
"Not every x has a y and vice versa but each has a time value, how to show correlation?",3,1,False,False,False,statistics,1483622612,True,"Hi, I have a dataset that looks like this

T   X   Y

1,   0,   null   

2,   1,   2

3,   null,     4

4,   2,   null

I know the t value for every x and y, but I usually don't have both an x and y for a given time. I understand how I could show r^2 for x and t or y and t, but not sure how I would go about proving x and y are correlated. "
[Academic] Measuring Passive-Aggression in the General Population,2,0,False,False,False,statistics,1483623760,True,[removed]
[academic] Humour types and psychosocial adjustment (all welcome),0,0,False,False,False,statistics,1483627558,False,[deleted]
Statistics for an experiment?,0,1,False,False,False,statistics,1483629561,True,[removed]
Setting Realistic Goals (xpost from /r/Fitness),1,1,False,False,False,statistics,1483631915,False,
Looking to divide 3k URLs with a heavily positively skewed traffic distribution into six groups of 500 who have minimal seasonal variation (further explanation + image in text),0,4,False,False,False,statistics,1483646081,True,"Obligatory caveat: Stats neophyte, R neophyte, trying to learn more.

I have daily traffic data for 3k URLs for the entirety of 2016. There is a broad cyclical seasonal trend that the majority of them share, but the extent to which this seasonality is expressed differs. There is a [tremendous positive skew](http://imgur.com/8N9Rf7W) to the distribution of traffic in the data (here the x axis shows the total amount of traffic).

I want to create six groups of 500 URLs for testing purposes and want to ensure that the variance in seasonality of each group is minimized so I can draw conclusions without running the rest for a year, but I don't really know how to proceed. I initially tried to do so by randomly assigning each URL to one of six groups, but since the skew is so nuts I don't really know how to test to ensure that the seasonality of my resultant groups is significantly different. [Here](http://imgur.com/gQLaS79) is a time series showing the seasonality of each randomly generated group--the y axis is the % difference of the month relative to the month before it, which is why January isn't present.

To the eye test, this looks like the same general trend is occurring, but there seems to be significant variation at certain points. Anyway, I'm hoping to do something reproducible so I don't want to just trust my gut.

I'm basically looking to apply the methodology found [here](https://codeascraft.com/2016/10/25/seo-title-tag-optimization/) for my own purposes. In this post, they indicated that they used t-tests to reinforce the assumption that the test groups aren't meaningfully different, but due to my data's skew, I don't think t-tests will tell me anything. I read about transforming the data using log transformation but [even after that](http://imgur.com/rG9dyi4) the distribution wasn't remotely normal. After that I read about Box-Cox transformations but that got so confusing I couldn't make heads or tails of it--trying to execute it in R assumed I already had a linear model, and so far as I can tell I only have one variable.

Anyway, I'm really banging my head against a wall at this point. I would seriously appreciate any pointers you have.

**Edit:** Added a graph I neglected to link to"
Can you use a multilevel model to estimate a quantity for any level other than the bottom one?,7,1,False,False,False,statistics,1483646082,True,"Consider two similar models with different response variables.  Suppose we want to predict the annual budget for a few high school, and we also want to predict students' SAT scores for the same high schools.  We make the completely unrealistic assumption that they're uncorrelated because this discussion doesn't deal with multivariate models.

Estimating SAT scores fits nicely into the multilevel modeling framework because they work at the (lower) student level, and information on student performance is correlated within the same classroom.  However, what about estimating the school's budget (at the upper level)?  Can you use multilevel modeling to estimate it, or would you have to do some kind of aggregation?"
Statistics for an experimental verification of a design model?,0,1,False,False,False,statistics,1483666993,True,[removed]
Win-Vector Blog: Why Do Decision Trees Work? (intuitive argument using Hoeffding's inequality),1,0,False,False,False,statistics,1483680247,False,
Nate Silver: Make College Football Great Again,6,9,False,False,False,statistics,1483687884,False,
Outlier detection method comparison. Are review score Outliers Mistakes / attention grabbers or Truth? • /r/AskGames,0,2,False,False,False,statistics,1483731858,False,
"Eight 8-sided dice; rolling 1, 2, 3, 4, 5, 6, 7, 8",13,18,False,False,False,statistics,1483739133,True,"I rolled eight 8-sided dice and got one of each number on my first roll. It was neat.  

I was wondering how I would calculate the probability of that happening; my intuition tells me it's (8^(8)/8) but I'd like to find out for sure. Would it be the same as rolling eight ones/exactly four ones and four twos/any specific combination?"
Don't understand this SE calculation,3,1,False,False,False,statistics,1483753634,True,"In this study, of women with either breast cancer (ja +s) or not (nej -s), their vitamin D levels were measured:

http://prntscr.com/dsgo2y

I am then to use <30 as a reference group and calculate the odds ratio and then 95% CI for >75.

This is how they do it:

http://prntscr.com/dsgolf

And this is the CI calculation:

http://prntscr.com/dsgop0

What I don't understand is the SE calculation here, the root(1/n) sum. I can't find it done this way anywhere? Hoping for some help here! Thanks. "
Yahtzee,3,2,False,False,False,statistics,1483761122,True,"I hope this is a fair question for this sub, Can someone help me work out these odds? My wife and I were playing tonight and she rolled 5 of a kind on the first roll once, and then on her next roll she did it again...WTF? What are the odds of doing that?"
How realistic is it for one to become a self employed Statistician?,16,32,False,False,False,statistics,1483789077,True,"Would this option be available in the foreseeable future for instance, if it exists?"
Coursework Advice,3,2,False,False,False,statistics,1483802214,True,"I'm in my last semester of a masters in Public Policy and I've chosen to focus on data management and analysis. I am currently signed up to take a reproductive justice class, in addition to a big data class, and a database management class. While I've heard phenomenal things about the repro justice class, I don't think it will get me a job. I could drop that class and instead take a statistics and data science class that covers C and fortran. Would a background in these two languages be useful for someone interested in data analyst jobs?"
[X-Post /r/Python] For data science: Check out the Feather package. Feather is a language agnostic format for data frames in Python and R.,2,63,False,False,False,statistics,1483812428,False,
Help with simple probability problem,0,1,False,False,False,statistics,1483818699,True,[removed]
GRE Score for Masters in Statistics,11,2,False,False,False,statistics,1483824832,True,[deleted]
Help me understand this? Not well educated in statistics,1,1,False,False,False,statistics,1483829038,True,[removed]
Statistics Book for CS Junior,0,1,False,False,False,statistics,1483837980,True,[removed]
"Simulating variables with abitrary distributions,covariance, and missing entries",0,1,False,False,False,statistics,1483847772,True,[removed]
Probability or Statistics,0,1,False,False,False,statistics,1483852424,True,[removed]
Emergency Crash Course - multivariate statistics,0,1,False,False,False,statistics,1483885356,True,[removed]
Logistic Regression: State and Private Facility Comparison,0,1,False,False,False,statistics,1483892773,True,[removed]
State and Private Facilities Comparison: Logistic Regression,5,6,False,False,False,statistics,1483894882,True,"I've been tasked with comparing a binary outcome between private and state types of a particular facility. I'm new to logistic regression modeling--my experience and education were predominantly continuous data--and I am hoping to get some advice.

There was a previous evaluation done by my department approximately 6 years ago. They used two logistic regression models, one for private facilities and one for state facilities. They also used the percentage of time spent at each facility type as an independent variable. I decided using the percentage of time didn't make sense so decided to use days spent at each facility type to see if it would explain the outcome. My main issue is whether to use two logistic regression models, one for days spent at each facility type, or if I should put days spent at each facility type in one logistic model. It makes sense to me conceptually to put two independent variables each measuring the days each participant spent at each facility (Approximately half of the participants spent time at each type) instead of a separate logistic regression model for each one but I wasn't confident in my decision. Any thoughts from people who know what they are doing? ha"
Significant Differences between 8th and 9th Edition of Devore's Probability and Statistics textbook?,3,1,False,False,False,statistics,1483901833,True,"I am enrolling in an applied statistics course that has the 9th Edition of Devore's ""Probability and Statistics for Engineering and the Sciences. I was able to find a copy of the 8th edition of the same book for a much better deal, and I am just wondering if it is similar enough to still be used.

I found this online which compares the table of contents and outlines some general differences between the two : http://www.picktextbook.com/statistics/probability-statistics-engineering-sciences.shtml

It explains that the primary differences are the ""elimination of the rejection region approach"" and that ""hypothesis testing is now based on p-values"". Obviously, I haven't taken the course yet so I'm not sure if these changes are significant enough to warrant the 9th edition purchase?

Thank you"
Online forecasting competition (macroeconomics),3,16,False,False,False,statistics,1483907029,False,
Log transformed data question,13,8,False,False,False,statistics,1483933934,True,"So if my variable isn't normally distributed I have to do a log transform on it when using any tests that assume normality (e.g. t-test, ANOVA, etc.).  I understand the purpose is to use a certain log base (base 10, base 2, etc.) depending on the type of variable that ""forces"" the data to show a more linear relationship.  I also read that after you perform your statistics on log transformed data you need to convert it back to derive meaning from the numbers.  

But when I do the log transform what do the log transformed values represent?  How are different log bases more or less appropriate for more different types of variables?  "
The Unique Study Benefits of Online Excel Homework Help Service,0,1,False,False,False,statistics,1483956398,False,
StatCrunch Assignment Help by Statisticshelpdesk Is a Complete Package,0,1,False,False,False,statistics,1483959994,False,
Data Research Assignment Help Service For Accurate Data Analysis,0,1,False,False,False,statistics,1483964105,False,
Online Biostatistics Help Service from Statisticshelpdesk.com Is a Savior for Students,0,1,False,False,False,statistics,1483966474,False,
Recommendations for survey design and analysis books (evaluation)?,3,2,False,False,False,statistics,1483972986,True,"Hi all, I'm looking for a survey design and analysis reference, preferable with an angle towards intervention evaluation or similar in the public health realm. Bonus points for discussion of cultural/community and ethical issues.

I've started a new role in evaluation and just looking for something to ground myself in what the norms are to make sure I'm not thinking too outside the box as I settle in."
Big data could help economists avoid any more Michael Fish moments,0,0,False,False,False,statistics,1483976561,False,
Nine Recognitions for Statistics in 2016,0,1,False,False,False,statistics,1483987080,False,[deleted]
Causal inference using Random Forests,1,22,False,False,False,statistics,1483987113,False,[deleted]
I used Bayesian Mixed Effects model to grade College Football teams,6,42,False,False,False,statistics,1483987285,False,
"Obituary: John Aitchison, 1926 - 2016",0,2,False,False,False,statistics,1483987306,False,
Simple question but I feel I'm missing something,4,1,False,False,False,statistics,1483987607,True,[removed]
R or Python for data management and analysis?,1,1,False,False,False,statistics,1484000762,True,[deleted]
Resources for investigating a degree in statistical/data analysis?,2,1,False,False,False,statistics,1484005627,True,"I have an Associate's degree in Health Information Management (everything to do with handling medical records) and I've found that the most interesting part of the field is data analysis and the performance improvement of which it's an integral part. I'd like to pursue this further with a Bachelor's degree, but I'm unsure where to start or which institutions have degrees that would most closely align with my interests.

In this academic world of hundreds of for-profit ""universities"" scamming people out of tens of thousands of dollars for worthless degrees I'm very hesitant to just start googling and see where it leads me. Does anyone here know of a resource I can use to find reputable and rigorous degrees where I can really get into the nitty-gritty of data/statistical analysis and performance improvement? Does anybody have success stories/warnings about particular programs? Thanks in advance for the help."
Dependent Data with Small Sample Size,0,1,False,False,False,statistics,1484012251,True,"I am looking for a good analysis option when you are collecting data from a small group (let's say n = 10) on multiple occasions across a handful of Conditions where not every participant is measured in every condition and some participants are measured on multiple occasions in the same condition. For the sake of argument let's assume there are 20 total observations taken on your group of n = 10, across 8 conditions. The dependent variable is a 'happiness score' measured on a 1-10 scale. 

MLM is not a good option due to small sample size
Repeated-measures GLM not a good option because the data are so messy. 

I did not collect the data and my first response would be to have a better data collection methodology, but here I am. 

I truly appreciate any thoughts you all may have of this. 
"
Evaluation of models,0,1,False,False,False,statistics,1484017634,True,[removed]
"I'm teaching my students about target populations, sampling, and generalizability this week.",4,2,False,False,False,statistics,1484017771,False,
Government Statisticians: How's the work?,10,8,False,False,False,statistics,1484033513,True,"A quick search shows a ton of positions open for survey statisticians. I was hoping to hear from anyone working for the government, state or local. How does the day to day compare to industry or academia? 

To clarify: Looking to hear about experiences in work requiring at least a masters*"
Household disposable income income and inequality in the UK - Office for National Statistics,1,9,False,False,False,statistics,1484050638,False,
Problems with randomized controlled trials (or any bounded statistical analysis) and thinking more seriously about story time,8,14,False,False,False,statistics,1484059616,False,
Evaluating a Policy Change Over Time using Propensity Score Matching: yes or no?,0,1,False,False,False,statistics,1484063993,True,[removed]
Problem with using Commander in R,8,5,False,False,False,statistics,1484064170,True,"I haven't seen this issue anywhere else online. 

I installed R Commander on the fully updated 3.2.2 version of R and I already have Quartz installed. However when I try to run it I receive this message:

Error : .onAttach failed in attachNamespace() for 'Rcmdr', details: 

  call: structure(.External(""dotTclObjv"", objv, PACKAGE = ""tcltk""), class = ""tclObj"") 

  error: [tcl] invalid command name ""image""

Anyone know how to fix this?"
Would statistics become useless if we (relatively easily) knew all parameters of populations?,8,6,False,False,False,statistics,1484067596,True,"If one day we were able to easily figure out all parameters of a population within a reasonable time frame, would statistics become useless?"
electrical engineer want to study statistics,0,1,False,False,False,statistics,1484068697,True,[removed]
Level of Linear Algebra necessary in Biostatistics?,11,2,False,False,False,statistics,1484069058,True,"I recently graduated with a BA in Public Health and am hoping to apply for MPH/MS programs in Biostatistics this Fall.  In the interim, I have been working towards completing the minimum necessary prerequisites for most programs (i.e. Calc I-III, Linear Algebra, computer programming).  As it stands, I am enrolled in Calc II this semester and need to take Calc III and Linear Algebra. 

For those in the Biostatistics field, I'm wondering if anyone can provide clarification regarding the level of Linear Algebra required.  I may have the chance to take ""Introduction to Linear Algebra"" (non-proofs based) this semester, and Calc III over the summer, enabling me to apply to programs in the Fall.  The proofs based Linear Algebra course at my school lists Calc III as a prerequisite, and therefore I would either be applying much later (January) this cycle or would have to wait an entire year and apply the next cycle.  While I understand proofs based Linear Algebra would be more ideal, I'm also limited by time constraints.  I'm also more interested in applied, rather than theoretical, statistical work. 

Therefore my main question is: would the Introduction to Linear Algebra course suffice for most programs?  I'm asking primarily on an administrative level, but would also be curious to know the type of Linear Algebra background needed for the field more generally, and if it's acceptable to learn the theory/proofs after applications are sent out and before starting the graduate program.  Thank you!"
How do I calculate the probability of a node failing in a network - given predecessor nodes?,9,2,False,False,False,statistics,1484078380,True,"Hi r/statistics,

I think I do have a simple question -- that unfortunately exceeds my brain capacity.

My **scenario** is the following:

1. We have a network of nodes and directed arcs.
2. Each node as an inherent (independent) probability of failure (let's say: p=0.1).
3. At the same time, each node has a *higher* probability of failure if any of the *predecessor* nodes has failed. Then the node fails with probability p=0.3.

**How do I calculate the (absolute) probability of failure for the last node (assuming we have one single final node)?** E.g. the last nodes would then fail with probability 0.6 and would be working with probability 1 - 0.6=0.4. (Numbers totally pulled out my *****)

Is there any formula one could create for simple networks of different shapes?

Many thanks for any formula attempts, pointers, advice or hints to software libraries that might help (ideally Python)!

--sabertoothedhedgehog"
A Brief Introduction to Graphical Models and Bayesian Networks,1,31,False,False,False,statistics,1484085133,False,
Lightweight R Package for Bayesian AB Testing,5,11,False,False,False,statistics,1484097086,False,
Bayesian approach to parameter estimation using contaminated data,5,4,False,False,False,statistics,1484113046,True,"Hi,

I'm new to Bayesian statistics and parameter estimation. I'm used to frequentist maximum likelihood.

I have the following problem that I'd like help with.

There are two collections from which marbles can be drawn, C1 and C2. Each marble has a colour and a weight associated with it.

Given N marbles, I would like to estimate the weight distribution for marbles from C1 by itself. The problem is that the N marbles will have some from C1 and some from C2.

Assume that I know there are twice as many marbles in C1 than in C2.

How can this problem be worked out in a bayesian way? These are my thoughts: There is a probability P(m|C1) and P(m|C2), for the probability that a marble 'm' came from C1 or C2. I would like to use ALL the marbles in the set N to estimate the weight distribution of marbles from C1, and feel that I should somehow weight them using P(m|C1).

Any ideas? I've tried to look up this problem using google, but I can't find anything and I'm not sure what terms to search since I'm new to Bayesian methods.

Thank you"
StatCrunch Assignment Help,0,1,False,False,False,statistics,1484117991,False,
Students of Statistics - Professional Help for Your Assignments,0,1,False,False,False,statistics,1484129208,False,
Can I normally distribute a total dollar amount over a given number of quarters in excel?,6,0,False,False,False,statistics,1484141696,True,How could I take say $1 Million dollars and normally distribute that amount over 5 quarters so that I could see total $ spend in each quarter?
Help understanding,0,1,False,False,False,statistics,1484146728,True,[removed]
Probability question,17,9,False,False,False,statistics,1484156676,True,[deleted]
Online resources for statistics?,7,3,False,False,False,statistics,1484168065,True,"Hello, I am retaking a probability and statistics class this semester and college. I struggled last time I took it because I could not understand the instructor and did not put in the necessary time to do well. Does anyone know of any online resources which is helpful for beginners learning this nearly impossible subject of math? "
Tutorial: Introduction to Forecasting with ARIMA in R,1,12,False,False,False,statistics,1484169394,False,
tl;dr Bayesian A/B Testing with Python,1,16,False,False,False,statistics,1484169788,False,
Basic sampling question regarding balancing a sample to meet target distributions.,4,2,False,False,False,statistics,1484170353,True,I have raw data from a survey of people and I need to balance the sample to reflect that 67% of the responses should be of men and 33% of women. I also need to ensure that ages of the participants are distributed equally. Is there a way in python/ r to quickly recalculate the frequency based on balancing these two variables?
Seeking advice regarding a Masters in Biostatistics degree.,9,13,False,False,False,statistics,1484170506,True,"I am a 34-year-old stay-at-home mom of 3 kids. I graduated 11.5 years ago with a B.S. in Finance and am starting to realize that my kids will all be in school within 3 years and I want to prepare myself to enter the workforce again, although I would like to try something other than Finance (I worked 3 years post-graduation for the air force and then quit to raise my children). I took 2 stats classes in college and loved them and I have always been interested in research and the medical field. My googling led me to biostatistics a few months ago and right now it is something I really feel like I want to pursue. I have talked with the University and would only need to fulfill about 4-5 prerequisites to be eligible for the program. My questions are as follows:

1. Other than completing a few online tutorials giving an introduction to R, I don't have any programming experience. My husband is a software engineer and has explained a few things along the way, but I am mostly starting from scratch. One of the prerequisites that I will need to take is a computer science course. Will this give me a good enough start, or will I be shooting myself in the foot to start into a master's program with only that amount of programming experience?

2. Is biostats a lucrative career field? For the most part I want to challenge myself and enjoy what I do, but I also don't want to spend nearly $30k on a master's degree that won't pay for itself in the end. How difficult is it to obtain a job following graduation? What are some ways I can gain some experience along the way? How much could I expect to make after completing my Master's Degree? 

3. Are the majority of biostats jobs full time, in house positions? Or are there positions available for part time and/or telecommuting? Again, I have 3 kids (ages 9, 6, 2), so I am still trying to decide if this would be a wise career choice. 

4. Am I crazy? It has been nearly 12 years since I went to college and/or took a math class. Four of the prerequisites that I will need to take are math classes (Calc 1, Calc 2, Multivariate Calc, and Matrix Theory), but am I going to be eaten alive if I pursue this?"
What is the probability a professional Philadelphia sports teams wins a championship in any given year?,3,4,False,False,False,statistics,1484183447,True,"0% haha. Funny. Okay now that jokes are out of the way....

Philadelphia has 4 professional sports teams. 3 leagues have 30 teams and 1 has 32 teams in the league. Assuming that all teams have an even chance at winning the championship, what is the probability a Philadelphia team will win in any given year? 

Also,  is it possible to figure out... what is the expected amount of years it would take for at least 1 team to win?"
Multiple Linear Regression for non linear data?,3,4,False,False,False,statistics,1484188529,True,"Hell /r/statistics,

I am trying to fit 3D data using regression. My independent variables have a non linear relationship to the dependent variable (they form a non linear surface). 

I read that ""The term “linear” in linear regression means that the regression function is linear in thecoefficients α and β_j. It is not required that the X_i appear as linear terms in the regression
function."" 
[PDF from university of Michigan] (https://www.google.de/url?sa=t&source=web&rct=j&url=http://www.stat.lsa.umich.edu/~kshedden/Courses/Stat401/Notes/401-multreg.pdf&ved=0ahUKEwjo0bDrybvRAhWqqlQKHa0ADasQFggaMAA&usg=AFQjCNFzfJ_qKQ6Z0x7ISuiHJq3SDzZqfg)


Is that true? If yes, can I use multiple linear regression techniques to fit non linear data sets?

To give more concrete example: my 2 independent variables are pressure and temperature, my dependent variable is the output of a sensor. I would like to predict the sensor output based on the input temperature and applied pressure.

Thanks."
How to become great at statistics?,24,16,False,False,False,statistics,1484208980,True,"Having been good at statistics, I always wondered: How to become ***great*** at statistics?

Not just good, but actually really great. I mean professorial-level statistics.

Any ideas?"
How Online Statistical Simulation Assignment Help May Help Students,0,1,False,False,False,statistics,1484217680,False,
throwing away all Gibbs samples after approximation,0,4,False,False,False,statistics,1484221960,False,
Doing a statistic project about how amount of computers/consoles affect the amount of pets i will post a link to the results,0,1,False,False,False,statistics,1484223536,True,[removed]
Doing a statistic project about math so i will tell you more if you ask i will post a link to results,0,1,False,False,False,statistics,1484224185,True,[removed]
Got training budget :D now where do I go?,1,5,False,False,False,statistics,1484233245,True,"Hey guys, Looks like my company might be willing to spend money sending me somewhere internationally for machine-learning/analytics training (currently in SA).

Does anyone know of any good courses or conferences or training modules that could be cool to go to? I'd be most interested in very technical/in-depth that looks at Machine learning algorithms or clustering or something a bit more advanced. I already have a masters degree in Statistics, so I'm pretty down with the basics of traditional statistical models.

Otherwise, I'm gonna be looking online for some good advanced courses to do and I'll just get a week's free leave to work on them.

Thanks!"
Question regarding multi level analysis,8,3,False,False,False,statistics,1484242263,True,"For example the model:
SCOREij= Y00 + Y01 A +  β1j B + U0j + εij
β1j=γ10+u1j   with u1j≠0
With A being a micro level variable als B a macro level variable

Now, I don't quite understand how (if?) you can deduce from the model which variable you have to place in the ""fixed"" or the ""random"" box in SPSS.
Help would be much appreciated!

"
Statistiker/Nationalekonom sökes till Medlingsinstitutet,0,1,False,False,False,statistics,1484244070,False,
Possible application of ordinal logistic regression?,0,1,False,False,False,statistics,1484254508,True,[deleted]
MCMC Convergence with a Tricky Distribution [xpost from /r/dataisbeautiful,14,60,False,False,False,statistics,1484258161,False,
Can I use the results of poor linear regression results of individual costs to accurately predict aggregate costs?,1,1,False,False,False,statistics,1484260129,True,"I am trying to predict the costs for a series of projects. If we can predict projects that go over or under their budget before it happens, it can greatly help with our financial stability. We have the final project costs as well as starting estimates and some other characteristics about the project.

Essentially, I am modeling the following using a simple linear regression:  

    Final Project Cost ~ Estimated Project Costs+Predictive Project Indicator 1..N

or alternatively to adjust for ""auto-correlation"" between estimated and final costs:

    (Final Project Cost -Estimated Project Costs) ~ Estimated Project Costs + Predictive Project Indicator 1..N

On the individual project level, this model does slightly worse than the estimate project costs alone when using the RMSE metric. Also the model has a low r^2  of .10 - .15.


However, using a training-test split sample, the test dataset results show that sum(predicted individual project costs) matches much closer to the sum(Final individual project costs) compared to sum(Estimated individual project costs). The training sample as expected showed a perfect match in sums.

It would be a nice consolation prize if we could predict things on aggregate level accurately. However, I am not convinced that what I am seeing is trust worthy. I don't understand why such a poor model on the individual level does so well on the aggregate.

Can you provide any insight on what is going on?

Thanks!!!!"
Expected Correlation Calculation,2,1,False,False,False,statistics,1484281026,True,"Suppose that we four random variables, $A$, $B$, $C$, $D$ that have the same standard deviation. Here, $A$, $B$, $C$, and $D$ are chosen randomly and independently of each other. Define $E = -A-B+kC+(2-k)D$ and $F = -A-B+(2-k)C+kD$, where $k$ is a real constant. Find the set of values of $k$ such that the variables $E$ and $F$ have zero correlation. 

The application of this for my research is that A, B, C, D are subject scores of students, and I would like to investigate the relation of $C$ and $D$ - whether students strong in $C$ are weak in $D$ (relative to their overall performance), or vice versa. My intention is that if $C$ and $D$ are random (once the overall performance is taken into account), the correlation of indicator variables $E$ and $F$ must be zero. 

Upon experimenting with large randomly-generated data sets of Microsoft Excel, I am led to believe that the possible values of $k$ are $1 \pm \sqrt{2}$, but I have no idea how to prove that these are the possible values of $k$. "
Should I do preliminary analyses on scales or on the items before making them into scales?,0,1,False,False,False,statistics,1484297999,True,[removed]
Binomial distribution with uncertainty about π,8,12,False,False,False,statistics,1484326922,True,"If I have *n* trials each with success probability *p*, then the number of total successes is distributed along the lines of Bin(*np*, *np*(1-*p*)).

But what if *p* is also affected by uncertainty? (I.e. if *p* was estimated.) Obviously, the expected value will also be *np*, but what about the variance?

Do I have to use, for example, the Beta distribution to model such uncertainty?

For example, a political poll may report that 40% of likely voters will vote for a candidate and there are *n* = 1000 likely voters. Obviously, the expected value of the votes for the candidate will be 0.4·1000 = 400 votes, but the variance will only be 0.4·0.6·1000 = 240 votes if we assume *p* = 0.4 to be certain. Real political polls, however, always include a [margin of error](https://en.wikipedia.org/wiki/Margin_of_error#Definition). How can this margin of error be included in estimating the variance of a binomial-like series with *n* trials?"
Mathematics Discord Server,0,1,False,False,False,statistics,1484326922,True,[removed]
Creating an equation using variables in a survival analysis with time-dependent covariates,2,5,False,False,False,statistics,1484328911,True,[deleted]
Treating interval data as continuous.,2,2,False,False,False,statistics,1484330693,True,[deleted]
"Statistically, the 13th is more likely to be a Friday than any other day of the week",16,52,False,False,False,statistics,1484335406,False,
What is the interpretation of this observation?,3,1,False,False,False,statistics,1484338860,True,"I have some identities that are distributed in a 2D space. Each identity has the properties X, Y, latitude, and longitude. Each dataset on average has 75 of these identities. For a particular dataset, when I plot X as a function of Y, I get no correlation. When I group the identities (say into 10 groups) based on their spatial proximity (using latitude and longitude data) and plot average X and Y for all groups, I get a strong positive correlation.

Why do I see such behavior? Is this a well-known property of datasets? How can I justify this from a statistical point of view in my research paper? Are there any other problems that show similar behavior to which I can refer?"
Determining even distribution,0,1,False,False,False,statistics,1484343187,True,[removed]
What are the statistical odds that I will (1) outlive my brother who is 5 years older than me and (2) outlive my youngest cousin who is 14 years younger? Assume average and identical lifestyle.,3,0,False,False,False,statistics,1484344465,True,"I tried to play around with life expectancy calculators to find a satisfying answer but wasn't confident I was doing it right.

"
Quality Control Sample Rates,3,2,False,False,False,statistics,1484344581,True,"If a person was looking for defects an didn't know how likely they were, but wanted to accurately detect if the defect rate rose about a certain %, say 1 in 1000, how would you determine what sample size you needed for a 95% confidence that if you were detecting under 1 in 1,000 defect that it was true?  
"
what binary test to use,1,1,False,False,False,statistics,1484347809,True,"I am looking at the distance of nerves from a constant anatomic landmark. I want to test the hypothesis that the nerve will be within distance x of the landmark. Should this be set up as a binary ""yes/no"" test. Google tells me to use an adjusted wald CI...I think. How do I interpret the results? Should I use a different test?  "
Stan: A Probabilistic Programming Language | Carpenter,0,14,False,False,False,statistics,1484351343,False,
Favorite online resources to supplement 400 level probability course?,2,18,False,False,False,statistics,1484355173,True,"I'm taking a 400 level Probability course this semester. It's my first course in this field ever, so I'm in for a ride.

When I was taking Calc III with a horrible prof, I found that the website www.17calculus.com and [Professor Leonard](https://www.youtube.com/user/professorleonard57), my performance went up considerably. Both of these resources seemed to really make an effort to be effective. Leonard especially is a captivating and talented lecturer. Straight up saved my grade.

What are your favorite online resources for learning stats and probability?"
SPSS Multiple response analysis help,4,6,False,False,False,statistics,1484359770,True,"So I am trying to analyse a set of data for my dissertation using SPSS in which I have asked respondents ""Which of the following games consoles do you own?"" where I give the option to pick multiple responses.  I am able to see the total number of responses for each answer: e.g. the total number of people who own a Playstation 4.

However, is there a way to identify the number of respondents that own all consoles from a certain brand?
For example, am I able to find out/filter the number of people answered that they own both a Playstation 3 and Playstation 4?

Thanks for any help !"
P-value and hypothesis testing help?,0,1,False,False,False,statistics,1484361683,True,[removed]
Kriging (Spatial-Temporal) Videos or Decent References?,7,8,False,False,False,statistics,1484370242,True,"Hello Reddit!

I've run into a brick wall and need a little bit of help.  I'm trying to compare a few methods against ST Kriging. But most of the info I've found online is really heavy on how to use various software and doesn't really delve into the math. Then the few paper references I've been given are a bit beyond my ability to just read and easily comprehend. Basically I'm at a point where I have a few gaps in my understanding, but unable to meet with anyone to ask questions. 

So I'm looking for any suggestions for videos, a decent (hopefully cheapish) book, or an epic poem on the subject.  

Thanks!! "
"Offered a job cleaning and analyzing data, I am a Math/Statistics major but my knowledge is limited?",39,34,False,False,False,statistics,1484415495,True,"So I have only taken two stat courses (Intro to Stats, Intermediate Statistics and I was offered this job for the summer. Not sure if I should take this opportunity because it is close to what I want. However, I have been meeting all my 'pure' math requirements and I haven't taken any stat courses in over a year. What should I should I do? I have only used R in my 1 stat course  which was well over a year ago (but use was limited) and SPSS  the same. I haven't brushed up on anything stats related in over a year. Take this opportunity or nah?Afraid of getting fired the minute they view me as incompetent. "
Am I wrong to think power analyses garner WAY too much concern/contention in the grand scheme of a study (conceptually and statistically)? Many researchers are too literal about a priori power analyses results (and prescriptions) and issues with small sample sizes can be alleviated anyway.,6,12,False,False,False,statistics,1484418543,True,[deleted]
Statistics Problem,0,1,False,False,False,statistics,1484427042,True,[removed]
"In the world of statistics, how's the reputation of Texas A&M statistics?",5,3,False,False,False,statistics,1484428657,True,"Just being curious... I am currently attending Texas A&M majoring in computer science and statistics as an undergrad.
I know that the department has a pretty good ranking, but in terms of prestige and job placement, what's your opinion?
"
Statistical Investigation: How to model the probability of drugs in each room?,2,3,False,False,False,statistics,1484434705,True,"Pretend there is a military fort with multiple floors. Each floor is composed of different rooms (One for each soldier), a living space, exercise room, and kitchen/dining room. If there is an increase of positive urinalyses (UA) from random UA's, could statistical modeling be able to give predictions/probabilities of who is bringing in the drugs? I would have information such as when people were in specific rooms, who else was in the room with him or her, when someone entered a room another soldier was previously in, whether someone might not interact with particular soldier based on the history of them disliking one another, and more.

My first thought was to use Bayesian search theory. The information about each soldiers locations and past history with one another could potentially be used for creating priors about each one. The room of each soldier could be the numbered cells used for the probability of where the drugs could be located. However, I have no working knowledge of bayesian search theory and therefore don't know if it could be applicable. Any suggestions on whether bayesian search theory would be great for this context, and if not, any other statistics that could be used such as something in the frequentist or machine learning domain? Any help would be greatly appreciated. "
Probability of winning a multi day race. Help solve a fun problem!,0,1,False,False,False,statistics,1484446512,True,"I'm competing in a 2 week race. Here's the data that will be updated (participants uploaded data periodically). It's assumed it updated daily. 

Total miles and number of trips will be the two bits of data I have on other competitors I assume all entrants work full time and will ride a bike 10-12 mph. I assume ""sane"" riding time of 5-6 hrs per day. 

I'm only considering the top 10 riders at this point due to two days having past which ehich has weeded out the weak and shown the top riders are averaging high and consistent mileage for two days now. I understand I may have to update my 10 competitors if someone does a long rally ride and jumps up in place. 

I guess as the days go by I'll be able to know how safe a margin I have, since there's a physical and time limit element, physical being 70 mi/day (more likely 50) and time slowly decays. 

Is there a day that'll I'll know almost beyond doubt that I've won (if all riders maintain their daily average)"
Tidy Text Mining with R - A new book that teaches text mining using the tidyverse,0,13,False,False,False,statistics,1484450482,False,
Advice Needed about Online Applied Statistics Masters!,0,2,False,False,False,statistics,1484452307,True,"I am a peace corps volunteer (marketing undergrad) looking to study applied statistics. I would appreciate any advice I can get!
 
1. What is the difference in job opportunities between a Applied Statistics and Statistics degree?

2. I've seen some different online programs. Which are the best online Applied stats masters program? 

3. Most schools seem to want Calc 1 & 2, and stats as pre-reqs. Would a business undergrad with 2 semesters of business calc and 2 semesters of business stats (quantitative methods) be qualified for applied stats? 

4. Would these schools accept community college courses? Does anyone know whether a school would accept someone pending completion of a pre-req.

Thanks!"
Ноw I wаs hеlped tо find sех in the Internet site.Му rеаl stоry abоut 3 times with 3 differеnt girls fоr you guуs,0,1,False,False,False,statistics,1484463387,True,[removed]
"A project of Text Classification, Sentiment Analysis and Word Embedding",0,20,False,False,False,statistics,1484486823,False,
3 timеs seх with different 3 girls or mу man story how I was lоoking fоr 3 girls fоr 1 month,0,1,False,False,False,statistics,1484501032,True,[removed]
Can you divide the standard deviation by the same value as the mean?,3,2,False,False,False,statistics,1484511728,True,[deleted]
Online Masters in Statistics,35,24,False,False,False,statistics,1484513055,True,What your thoughts on some of the programs that are available online? I've applied to RIT and NC state so far. I'm interested in stats and machine learning.
What type of study is this?,3,1,False,False,False,statistics,1484513772,True,So we have two groups suffering from lung cancer. We gave the first group standard treatment (chemo) and the other group placebo. Does it count as a case control study? Thanks 
Simple Combination problem.,5,1,False,False,False,statistics,1484514311,True,"What is the formula for this problem?  I have 5 choices A - E.  I can chose 1, 2, 3, 4 or all 5 choices.  How many possible combinations are there? Manually figuring it out I think there are 31 (32 if none is a choice).

I know this is simple I just can't remember my Stats.  I can't find it on the internet either."
Which ML model for classification on sparse data?,2,2,False,False,False,statistics,1484530741,True,[deleted]
An intuitive explanation for the car on a highway probability question.,3,11,False,False,False,statistics,1484545488,True,"During an interview I was asked this [question](http://math.stackexchange.com/questions/52113/probability-calculations-on-highway).

While I was able to solve it, the interviewer asked me how would you explain to someone with a non-math background why the probability for 10 minutes is simply not (0.95 / 3). Can anyone give me an intuitive explanation for this?"
Get in Touch with Our Tutors Who Will Do Your Statistics Assignments,0,1,False,False,False,statistics,1484549947,False,
Question on degrees of freedom,3,2,False,False,False,statistics,1484572249,True,"Doing some analysis at work, and I ran in to this problem. I have the efficiency of 70 sample devices measured, but in practice they're installed in groups of 7. I want to mean, std dev, and confidence intervals for both. Ok, easy enough. The mean is the mean, the std dev for a group of 7 is s/sqrt (7). The confidence interval for the mean is mu +/- z*s/sqrt(70) ... or z*s/sqrt (7)/sqrt (10), which is to say either 70 samples or 10 ""samples"" of what I care about with their computed std dev. Those are conveniently identical. 

However, for the confidence interval for std dev, it's [sqrt ((n-1)s^2 /chi^2 _alpha/2 )), the same thing at 1-alpha/2]. With the n-1 it seems to matter if I compute this for the 70 sample population, then take it over sqrt (7) or compute s/sqrt (7) first, then say n is 10. It's actually more of a problem because the chi-squared values change with the number of degrees of freedom. Which ""n"" is appropriate to use here? 

I guess this is also just a broader question on how to interpret degrees of freedom, so any philosophical insight is also appreciate. 

Thanks for the help!"
Statistical distribution help pls,0,1,False,False,False,statistics,1484578411,True,[removed]
What is the state of the art in time series prediction models?,30,6,False,False,False,statistics,1484593735,True,(don't say arima)
A question about the Powerball Lottery stated odds.,0,1,False,False,False,statistics,1484602481,True,[removed]
Which courses should I take: Stochastic Processes and Experimental Design or Communication Across Statistics?,3,1,False,False,False,statistics,1484609895,True,"Stochastic (300 level):

A stochastic process is a collection of random variables. For example, the daily prices of a particular stock are a stochastic process. Topics of this course will include Markov chains, queueing theory, the Poisson process, and Brownian motion. In addition to theory, the course will investigate applications of stochastic processes, including models of call centers and models of stock prices. Simulations of stochastic processes will also be used to compare with the theory.


Experimental Design/Elementary Analysis (200 level):
A fundamental fact of science is that repeated measurements exhibit variability. The course presents ways to design experiments that will reveal systematic patterns while 'controlling' the effects of variability and methods for the statistical analysis of data from well-designed experiments. Topics include completely randomized, randomized complete block, Latin Square and factorial designs, and their analysis of variance. The course emphasizes applications, with examples drawn principally from biology, psychology, and medicine.

Statistics Communication:
Statistical communication is an important component of the capacity to ""think with data."" The course will integrate theoretical and practical aspects of statistics with a focus on communicating results and their implications. Students will gain experience clearly synthesizing and explaining complex data using diverse predictive and explanatory models. Learning objectives include: understanding the role of a statistician, developing communication skills, working collaboratively on group projects, designing studies to collect information, acquiring existing data resources, utilizing publications in statistics, creating reproducible research and developing oral arguments, relevant project reports, and dynamic graphical displays. Emphasis will be placed on the use of statistical software, data management, visual presentation, and oral and written communication skills that are necessary for communicating technical content.

Problem: Communication across Statistics overlap with the times of  those two courses above so I have to choose between COS or those two courses. I'm taking Research Methods (Psychology),Intro to DS ( CS course), Psychology Independent Semester this semester as well.

Background info: Math major and Psyche minor. Courses already taken: Calc 3 (B+) , Linear (B), Discrete (B), Probability (Got an A because the prof. was an easy grader), Intermediate Statistics (A-). I had to work my ass off in Probability and it was an okay course. Not a fan of proofs though. Heard the elementary design course was easy and I know I'll pass it.

Interest: Maybe an analyst position where I get to play and analyze data sets in the future for an entry level job after I graduate college. Don't have much experience with R and teaching self Pyton for DS through data camp. Would like to maybe have stats grad school in my reach one day. I want 2-3 years to think about it because I'm a bit tired from school. 

Would like to build on the Intermediate Stats course knowledge that I gained a year ago so that's why I'm interested in COS and the course seems to be really putting you in a place of being a Statistician. Also it's a rigorous course.I want more experience playing with data sets and focusing on stats. Note: Junior in College.  Weary if I should drop it though for those two other stat courses because that would mean I am only taking 1 stat/math course this semester and I don't know when I apply to grad school or jobs in the future, it would be frowned upon. In total by the end of my senior year (I'm a junior in college right now), I would have taken 10 math/stat courses as opposed to 11. Advice? Which opportunity would benefit me more in the long run in terms of experience, learning, yadda?"
Can someone help me?,0,0,False,False,False,statistics,1484618030,True,[removed]
Which Statistical test to use?,3,1,False,False,False,statistics,1484618068,True,"As the title says, I am curious which statistical test I would need to run to test for significance.

Study: Survey of two groups of individuals was done. First group is the higher ability. Second group is the lower ability. Questions asked were done with a 5 point likert scale with questions pertaining to their preferred learning styles.

If I am trying to see if ability level affects the preferred learning styles, which test would I run? Chi-squared? Mann-Whitney U? T-test? I've seen all three of these as possible answers."
I love statistics but have very little formal training... what's the best route?,16,36,False,False,False,statistics,1484621104,True,"I'm 28, studied Business and Accounting, and have worked in accounting/marketing roles since.  I do a lot of forecasting for my job, dig into past sales data to find trends etc using Excel and SQL.  I enjoy it and feel like I'm pretty good at it, providing great insight to my company.  But I also realize, compared to an actual statistician I'm such a beginner when it comes to statistical analysis.  This has me thinking I want to get a masters into applied statistics. I'm reading into it, it seems exciting.  Thing is I haven't taken an actual stats course since AP stats in high school. Last calc class was calc I first semester of college.  So I'd basically have to dedicate a full year of prereqs, before I even get into the gist of a masters program.   So I'm wondering, does anyone have any recommended routes on acclimating myself in the stats department before I decide to go in on something like this?"
Residuals,3,1,False,False,False,statistics,1484624478,True,"The ideal forecasting method will have uncorrelated residuals & residuals with zero mean. 

Does residuals with zero mean have the same meaning as residuals that are normally distributed? (zero mean = normally distributed?)"
Statistics - Financial Forecasting,0,0,False,False,False,statistics,1484625358,True,"Hi All,

I was recently given a problem related to financial forecasting, but am unsure how I would go about predicting sales, given the advertising dollars. All of the information is included at the dropbox link below,

https://www.dropbox.com/s/hn3nt8fe4g5wdre/Question.xlsx?dl=0"
Online Biostatistics Help Service from Statisticshelpdesk.com Is a Savior for Students,0,1,False,False,False,statistics,1484631758,False,
How can I come up with a 95% CI for a phone number?,0,1,False,False,False,statistics,1484632729,True,[removed]
Fast Unit Root Tests and OLS regression in C++ with wrappers for R and Python,0,3,False,False,False,statistics,1484655679,False,
Dolgozat,0,1,False,False,False,statistics,1484656391,True,[removed]
Resource that explains which model is used for which type of study?,1,8,False,False,False,statistics,1484668481,True,"I'm trying to figure out when I use x2 tests, when I use linear regressions, when I use logistic regressions etc. etc.

Is there a resource that explains it, as simply as possible, when each type of model is used? Preferably connected to studies, such as medical studies (I study medicine).

Thanks!"
Stats Interview Question...?,0,1,False,False,False,statistics,1484669877,True,[removed]
Reasonable Survey Questions to Ask in Person,2,1,False,False,False,statistics,1484672323,True,[deleted]
Serial correlation of residuals in panel regression,3,3,False,False,False,statistics,1484676183,True,What are some ways to correct serial correlation of the residuals in a fixed effect panel regression? I am forecasting a level of an economic variable by geographic area with lagged economic independent variables by cbsa.
Which Calculus approach?,1,1,False,False,False,statistics,1484679115,True,[removed]
Trying to apply some simple statistics to a hypothetical problem.,1,2,False,False,False,statistics,1484680153,True,[deleted]
"AP statistics students/teachers, help!",0,0,False,False,False,statistics,1484683081,True,"I am looking to acquire a copy of the DVD accompanying the textbook ""Stats: Modelling the World"" by Bock, Velleman, and De Veaux. It is the one with the poorly made skits and the tutor Rashi in it, for those who are taking/teaching the course. I want to get it for study purposes but cannot seem to find it anywhere online. Any information would be much appreciated!"
"MANOVA - levenes test significant for one dv, how should I proceed?",1,1,False,False,False,statistics,1484689722,True,"Currently conducting a 2 x 2 MANOVA on 8 DV's. Levenes test for homogeneity of variances show 7 out of 8 dv's are non significant. With one DV being significant, not exactly sure how to proceed. I know I shouldn't proceed with the MANOVA but I'm unaware of what else to do. Ive read a little on ""transform"" data but still confused"
"Confused with GLS, GEE, and Mixed Effects Model",8,11,False,False,False,statistics,1484697531,True,[deleted]
What method could you use to identify the source distribution given a sample and a set of distribution choices?,12,5,False,False,False,statistics,1484697620,True,"If I have a set of normally distributed sample data that I know has come from either normal distribution A, B, C or D (which have a known mean and standard deviation), how can I decide which is the most likely source distribution of the sample data?"
"What are the coolest, most interesting concepts in statistics? I have to give a short statistics presentation! As a curious, but non-stats person, what are some interesting or jazzy topics?",11,1,False,False,False,statistics,1484699026,True,"I am a grad student taking an advanced stats course. As part of this, I am to present briefly on a statistical concept to inform my classmates and expand my own knowledge. The presentation is short, and we are not required to explain the mathematics in full detail, rather, the presentation is conceptual. 

I'm looking for an interesting or current concept that I could share that would be valuable to my learning or the learning of my peers, or that is just plainly interesting. Does anyone have any recommended topics that I could research further? I've been searching to no avail, and would love some input. "
Bhh,0,1,False,False,False,statistics,1484700478,False,[deleted]
The Difference Between Correlation and Causation,0,0,False,False,False,statistics,1484706916,False,
Had a PyMC question (on the Rugby hierarchical model example). Wondered if anyone /r/stats might be able to help me understand working with the posterior?,2,1,False,False,False,statistics,1484709441,False,
Is a t-test the same as a multivariate test (e.g. A/B test from email marketing),2,1,False,False,False,statistics,1484710978,True,[deleted]
Stan vs. WinBugs: A search for informed opinions,16,16,False,False,False,statistics,1484713232,True,"Hello everyone,

I'm a soon-to-be graduated, graduate student taking a class in Bayes Methods. 
In the class we are taught how to use WinBugs to sample posterior distributions.
Separately, I happened to attend a talk by Dr. Andrew Gelman earlier in the year where he demoed Stan- a software that appeared to offer similar features but implemented in languages like R and python. Does anyone have strong opinions on the pros/cons of either of these software tools?

Obliged in advance for your input."
Good sources to learn time series analysis,3,1,False,False,False,statistics,1484717830,True,I'm currently enrolled in a time series analysis class at school and it's hard. The book is gibberish to me and my teacher essentially doesn't speak English. He tries but no one in the class understands what he is saying. I can't drop the course because I need it to graduate. Are there any sites that are good at explaining the theories and concepts? Thanks for the help!
Students of Statistics - Professional Help for Your Assignments,0,1,False,False,False,statistics,1484719164,False,
Standard Deviation Question,0,1,False,False,False,statistics,1484723113,True,[removed]
What is Statistics and its role in Information Technology?,0,1,False,False,False,statistics,1484725617,False,
"First time doing statistics, a few simple questions",1,0,False,False,False,statistics,1484735696,True,[deleted]
Panel Data versus Mixed Effects Model,4,8,False,False,False,statistics,1484747342,True,[deleted]
How is AI related to Statistics?,2,0,False,False,False,statistics,1484755802,True,"Hey fellow redditors,

I am very interested in Statistics and have been studying it for a month now. I am also studying machine learning and AI, and I am a bit confused about how statistics is related to AI. Some clarity would be appreciated since I will be deciding my future career based on it :). "
Got a data app idea? Apply to get it prototyped by the JHU DSL! · Simply Statistics,1,1,False,False,False,statistics,1484756739,False,
Help?,0,1,False,False,False,statistics,1484759741,False,[deleted]
Advanced MLE: Looking for Resources,2,3,False,False,False,statistics,1484769454,True,"Hello fellow statisticians!

In lieu of taking a summer course and spending lots of money, I was wondering if anyone knows of books or resources that cover survival analysis models, as well as panel/time series data using Maximum Likelihood Estimation. 

The description of the course I was hoping to circumvent is as follows:

""The first section of the course covers methods for making inferences with repeated observations data, focusing mostly on the theory and estimation of models for panel and time-series cross-section data. Topics covered include fixed effects, random effects, dynamic panel models, random coefficient models, models for spatial dependence, and models for qualitative dependent variables.

The second section of the course will cover methods and models for survival data. Survival data record the length of time until some event occurs, for example, the termination of a cabinet government or the time until an unemployment spell ends. Because time-to-event occurrence is an important feature of these kinds of data, methods suitable to duration data are sometimes called ""event history analysis."" This portion of the course will consider a wide variety of survival methods, including some non-traditional models for categorical data.""

Many thanks in advance.

"
Extremely low Rsquared but good uplift performance from my linear models--problem?,0,1,False,False,False,statistics,1484770568,True,[deleted]
Any good books on sample size and power?,7,13,False,False,False,statistics,1484770659,True,My professor just said the book he's using for sample size determination (aptly named Sample Size Determination and Power by Thomas Ryan) isn't a very good book. Was wondering if you guys had any better ones to put forward before I put $100+ down on a book that the professor doesn't even like.
Quick question for you guys.,3,2,False,False,False,statistics,1484771610,True,"Hello,

So this saying always comes up and when it does my friend and I always get into this debate.  ""You miss 100% of the shots you don't take.""  She insists that you cannot get data from things that don't happen.  What are your guy's input into this.  Anything I can add to the next unavoidable encounter with this saying."
Not sure if this is the right subreddit,0,1,False,False,False,statistics,1484772656,True,"Hello.

Not sure if this is the right subreddit for this but since www.elorankings.com was shut down, does anyone know of a good ELO website to enter game match data in?"
Testing statistical significance of differences in dollar values rather than conversions in A/B conversion tests for digital marketing campaigns?,14,8,False,False,False,statistics,1484776971,True,"I'm a digital marketing consultant. Part of my job is testing multiple variations of marketing campaigns and website landing pages. 

One example of a frequent task I do is A/B testing of something like a FB ad campaign. So I'll create a new ad set (group of individual ads), and then make an exact duplicate of that ad set with one targeting variable changed. 

My goal is to look at which variation performs better. I'm looking at several metrics... mainly Click through rate, ""Conversion Rate"", Cost per Click, and ""Cost per Conversion"".  A conversion is anytime someone who clicks on an ad performs a certain action on the site such as a purchase or email signup.  When I analyze this, I want to make sure the differences in performance are statistically significant.

In the past I've been using online calculators like this one to test significance. https://abtestguide.com/calc/  However this tests only based on only one metric...the total number of ""conversions"" driven by each ad set vs the total number of site sessions. 

But I also want to test the differences in cost per conversion vs one another. For example, Ad set variation A might get $30 per purchase with 1000 clicks.  Ad set B might get $20 per purchase but only 500 clicks. How do I test if that $10 difference is statistically significant enough to choose B over A?

Was wondering the process of going about this. I only have an elementary understanding of statistics and could use a refesher! 



"
SEM model selection,9,3,False,False,False,statistics,1484781261,True,"Hi everyone - I have a question about structural equation modeling. I have a model based on theory that I want to evaluate. Here is the model I built based on theory:  
  
y1 ~ y2 + y3 + x1 + x2  
y2 ~ x2 + x3  
y3 ~ x2 + x3  
y2 ~~ y3    

I'm using the lavaan package for R. When I calculate AICc for all possible models, the optimal model is just y1 ~ x1. None of the other variables or relationships are included. This is disappointing because the theory hinges around the relationship between y1 with y2 + y3, and that effects of x2 & x3 on y1 are indirect through y2 + y3.  
  
So: **Is it valid to report both the simple optimal model and the larger one based on theory? Is it valid to interpret the larger model even though it wasn't selected as optimal? Is it even necessary to use model selection with AICc if I just want to evaluate the theoretical relationships?**  
  
I know model selection is sort of a way to evaluate the theoretical relationships, but I want to do more than say ""none of these things were included in the best model"".  
  
I'm new to SEMs - thanks for any help or insight"
Is the use of Poisson's regression correct for this study?,6,6,False,False,False,statistics,1484787744,True,"I am looking at a cohort study that examines long-term symptoms after World Trade Center disaster exposure. Study is [HERE](https://www.dropbox.com/s/bpzl85iiumekciu/Long-term%20respiratory%20symptoms%20in%20World%20Trade%20Center%20responders.pdf?dl=0). They use a yearly survey for participants to self report presence of symptoms (asthma, cough, SOB etc.) and use the same cohort of people for each survey.

I am wondering about the type of model they used in their methods. To describe the methods, the authors indicate 'Poisson's regression was used to estimate relative risks...'

Considering the following assumptions of Poisson's regression:

* that outcomes are independent given covariates
- Independent intervals: probability of observing an event in any particular interval does not depend on whether we observed event in any other interval 


And considering that in this study, reporting a symptom in one year would (logically) increase the odds of having the symptom in another year (irrespective of order). For example, if shortness of breath is reported in year 1 and 3, it is more likely that it would have also been reported in year 2. 

Would this violate the above assumptions?"
Detect trend from multiple time series,0,1,False,False,False,statistics,1484792348,True,[removed]
Suggestions for a project involving linear regression and movie data?,2,2,False,False,False,statistics,1484795457,True,"I need to webscrape some data from sites like IMDB or BoxOfficeMojo and use linear regression to find out something interesting with this data.  But everyone is pretty much doing the same thing, so I wanted to see if anyone has any ideas on something less obvious and more interesting?"
Is an internship at SAS good for a statistics student?,0,1,False,False,False,statistics,1484804682,True,[removed]
Student T-Test problem,9,0,False,False,False,statistics,1484839380,True,Hey all I hope this is the right subreddit but I am currently stuck on my data. I work in a research lab and we have 4 groups of 3 mice (12 mice total). I am responsible for finding the student t test for this data and was stumped because of the groupings. I have 12 columns of distance values (1-12) and down each row is a time interval. Is there any particular way to do this? Thanks for your help.
What is artificial intelligence? A three part definition · Simply Statistics,2,28,False,False,False,statistics,1484843146,False,
Why is the degrees of freedom one less than n?,2,0,False,False,False,statistics,1484844405,True,This question just came up in my ap stats class and I really wanna know the answer.
Simulation Of Sample Size For A Contraceptive Study,0,1,False,False,False,statistics,1484848274,False,[deleted]
[WIP] Simulation of a Contraceptive Study,2,2,False,False,False,statistics,1484848910,False,
How should I scale down a proportion towards the null with an insufficient sample?,3,2,False,False,False,statistics,1484850334,True,"Hi guys,

Specifically, I am looking at a single hockey player's data, and comparing the amount of points they get at home vs. on the road to see if they play better in either situation. Say they have played an equal number of games at home and on the road and ignore all other effects such as home team advantage etc..

The league average proportion of home points to total points would then be 0.5. If a rookie player has 10 points at home and 5 on the road, then their home points ratio (sample proportion) would be 10/15=0.6666. However, given the extremely small sample size, it would be ignorant to say that this player produces more points at home than on the road, and the p-value would probably be very large. However, if a veteran player has a career 600 points at home and 500 on the road, then their sample proportion would be 600/1100=0.5454 and it is much more likely that they produce better at home than on the road with a small p-value.

How can I scale down (or scale up if the player has more road points) a player's proportion towards 0.5 to account for the sample size? Doing this probably lead to a better estimate for the rookies proportion of 0.6666 to somewhere in the low 0.5's, but would hardly effect the veterans proportion because of their large sample size.

Thanks, and sorry if this is in the wrong subreddit.
"
Need Advice Regarding Career Change into Statistics,0,1,False,False,False,statistics,1484858309,True,[removed]
Need Advice: Career Change into Statistics,0,1,False,False,False,statistics,1484858940,True,[removed]
A blog post regarding this question!,0,1,False,False,False,statistics,1484861073,False,[deleted]
Blog post about :> Statistics: A Branch of Mathematics or a Science?,11,4,False,False,False,statistics,1484861944,False,
Normalising categorical variables??,0,1,False,False,False,statistics,1484864466,True,[removed]
Tool for a zero inflated poisson regression?,6,1,False,False,False,statistics,1484875076,True,"Have a project I'm helping out with that needs a zero inflated poisson regression but I don't see that in my minitab options.  I can probably get my hands on most of the other common tools (JMP, SPSS, Statistica) but I would prefer not to use R.  SPSS would be best but we can't find that.  I *think* JMP has that capability.

Anyone know if any of those other tools have a way to do it easily?"
GLS regression assumptions?,7,5,False,False,False,statistics,1484875360,True,"Hi,

Quick question regarding regression estimates using GLS instead of OLS: I understand that this method allows us to make estimates with heteroskedastic errors, but does it require normality of errors? Sorry if this is a dumb question...

Thanks in advance!"
How statistics lost their power - and why we should fear what comes next | William Davies | Politics,0,0,False,False,False,statistics,1484893715,False,
"QA & Statistics By Mr. Sachin For CA CPT, CMA Foundation, B.Com & SSC -...",1,1,False,False,False,statistics,1484901881,False,
How statistics lost their power – and why we should fear what comes next,35,62,False,False,False,statistics,1484904424,False,
An artificial intelligence example that isn't that artificial or intelligent · Simply Statistics,2,1,False,False,False,statistics,1484938021,False,
Should I commit to a college major change to statistics?,0,1,False,False,False,statistics,1484944363,True,[removed]
Clustering Algorithm?,9,16,False,False,False,statistics,1484946102,True,"Hey all, given a set of points on a 2d graph what's the best algorithm for determining the statistically significant clusters? Can you point me in the right direction?"
SPSS chi-square testing?,7,6,False,False,False,statistics,1484962428,True,"Hi, I got some great help with SPSS functions last time on this subreddit so hopefully someone is able to help me out again.

So I am running a chi-square test for independence, however a good number of cells (over 20%) have expected frequencies of less than 5... so what would be the appropriate statistical test to use instead?  

I've read the survival guide of SPSS by Pallent who suggests to look at fisher's exact test.  But as I found out, fisher's test only applies for 2x2 comparisons.... whereas my table is larger.  I'm not able to see a suggestion in the book or find another answer online.

So on SPSS, what would be the appropriate approach for non-parametric testing?

Thank you for any help!"
What is central tendency/average and its different forms?,0,1,False,False,False,statistics,1484976735,False,
Here Is the Best Biostatistics Tutoring Service,0,1,False,False,False,statistics,1484983830,False,
Online Biostatistics Help Service from Statisticshelpdesk.com Is a Savior for Students,0,1,False,False,False,statistics,1484988681,False,
Why we require median and mode if we can find the accurate average through mean?,0,1,False,False,False,statistics,1485005452,False,
Spearman/Pearson,9,17,False,False,False,statistics,1485017932,True,"Is it mathematically possible that, for the same dataset, Pearson coefficient is positive (above 0) and Spearman is negative (below 0)?"
Is it possible to take a mean of means?,24,4,False,False,False,statistics,1485026932,True,[deleted]
Question regarding time series data,0,1,False,False,False,statistics,1485040807,True,[removed]
The limitations of randomised controlled trials,10,18,False,False,False,statistics,1485081207,False,
Any stats PhDs out there willing to help an ecology MS student understand the inner-workings behind 3 sentences in a publication?,4,0,False,False,False,statistics,1485089717,True,[deleted]
Unevenly distributed independent value?,7,6,False,False,False,statistics,1485107850,True,"When evaluating the independent variable, is it valid if it isn't evenly distributed? For example if the independent variable is the area, the areas I used were 1 m^2, 2,m^2, and 4m^2."
"Can't find object in R, even though I spelling is correct and object appears in summary command",9,0,False,False,False,statistics,1485110446,True,[deleted]
Is an LMM the appropriate choice for my data?,1,2,False,False,False,statistics,1485113405,True,"Hey there! Trying to keep this short! I've got some evaluation data about students at different schools. The schools have been seperated into treatment and control group and questionaires have been conducted before the treatment group recieved the treatment and after (for treatment and control obviously). That being said while all schools in the treatment group did recieve the same treatment the treatment quality varied widely (this is confirmed by prior research, qualitative analysis etc.). After a first glimpse via plots it seems like indeed there are only effects in the treatment group for some schools while other behave pretty much like the control group.

Now my question: I was wondering if a LMM with random slopes is the most approriate model here (I'm not really sure if I'd need random intercepts as the goal is to measure the difference between t1 and t2). Of course this is prior to any tests like Hausmann or Lagrange multiplayer. Just wanted to hear if my general idea for such a situations seems to be the correct one.

Thanks for you help! :)"
Please help me debunk this numerology video...,11,0,False,False,False,statistics,1485117313,False,
Predicting a binary variable with other binary variables?,6,6,False,False,False,statistics,1485178232,True,"What are the best models for this? I have a binary target (action/no-action) and a bunch of binary features, some of which are dummy variables I made for leveled categorical features (income tax bracket). Any links or ideas would be appreciated."
[animation] Estimating the Weibull distribution with censored data,12,49,False,False,False,statistics,1485182443,False,
Question: Random Drug Testing,0,1,False,False,False,statistics,1485186675,True,[removed]
How to run an A/B test for non-converting actions?,2,2,False,False,False,statistics,1485198870,True,"Hello all,

I'm trying to put together a calculator for me to test ad copy variants based on associated costs per click.  I've built out a similar calculator for tracking clickthrough rate, conversion rate, etc... but cannot wrap my mind around CPCs.  Because there is no conversion being tested, I'm not sure how to accurately run this test.

Can anybody help me figure this out?"
Sub4Sub TheTruth,0,0,False,False,False,statistics,1485201840,False,
Simply Statistics Podcast #1,1,12,False,False,False,statistics,1485204175,False,
Which industry does compensate the most for the master's degree in biostat?,2,1,False,False,False,statistics,1485206273,True,"Hello, I am currently in biostat master's program with thesis completion as a requirement. I spend a lot of time to manipulate the time to event data because my thesis will be probably multi state modelling with this type of data. But also I try to learn about broad topics for example, procedures in clinical trials or many other types of data to broaden my coverage on this field. 

I read that the private industry such as pharmaceutical firms would compensate the best from salary statistics from american statistician association website but I rather have an advice from the real world experiences. I do not have any plans to continue to phd right now but did not completely rule out that option. 
Also I can do R,SAS and stata. and I am from canada not states...

Any opinions from biostat grad?



thank you guys   "
Is there a statistical test that can be used for when there are two IV's and one IV effects the DV linearly while the other IV effects the DV nonlinearly?,0,1,False,False,False,statistics,1485215200,True,[removed]
Sample size calculations for logit models.,3,1,False,False,False,statistics,1485253045,True,[deleted]
What is the difference between sample mean and population mean in statistics?,0,1,False,False,False,statistics,1485253730,False,
"If I have a standard error, can I calculate what percentage of people fall within 1, 2, 3, etc. SEs as I do with SD (Assuming Normal Distribution)?",2,4,False,False,False,statistics,1485269340,True,"In other words, is there an equivalent 68-95-99.7 rule for SEs?

Not homework.  Just curious.

"
Statistical Likelihood of Winning a Grocery Store Giveaway?,1,1,False,False,False,statistics,1485274671,True,[deleted]
"Undergrad Econometrics question, is f(x)=P(x) and F(c)=P(x<c)? Topic: truncated dependent variables regressions",18,1,False,False,False,statistics,1485275490,True,"Hello, i have a problem in finding the log likelihood parameter function of a dependent variable truncated function

1) The mathematical Problem goes like this:
y=xb+e     where  y={""-"" if y*<c , ""y*"" if y*≥c}

 Where ""y""'s distribution is unknown, ""x"" behaviour is unknown, and ""e""'s distribution is g(e)

While talking with my assistant professor, he said that x is a linear variable, b is constant and E(e.x)=0), this is assumed even though he didn't state explicitely this information when the problem was presented.

2) The mathematical problem's solution according to my main professor.
        
This is what my main proffesor considers a correct answer :

First: 
 g(y|y*≥c)=[g(y ∩ y*≥c)]/G[c] .  
Where G(c)=P(x<=c) 

Second, to find log likelihood parameter's function, 
a) calculate the joint probability distribution 
b) apply Ln to the j. p. d.

Third, express the derivate of this expression. 

End.


3) Now my personal problem:
I am having a discussion with my econometrics assistant professor.

Take into account that: even thought it wasn't explicitely stated, my main prof is saying that, first, by knowing that ""e""'s distribution function is g(e), one can know ""y""'s distribution function is g(e).  This means that, even though he didn't say that E(x.e)=0, he is assuming it.

Second, he says that, g(x), where g(x) is the density function of x, can't be replaced by P(x) as i did, Where P(x) is the probability of x. In my evaluation i did exactly what i just described to you guys in section 2 mames as
""The mathematical problem's solution according to my assistant proffesor"", but instead of f(x) and F(x) i used  P(x) and P(x<c).

I also said that P(xb+e)=P(xb) + P(e), as it is assumed that E(xb.e)=0, and that x is linear and b constant ( this is assumed by my main profesor).

So my final question: is P(x) = g(x) and P(x<c)=G(c)? 

Thanks a lot!

"
Help with correlation analysis,0,1,False,False,False,statistics,1485280333,True,[removed]
Data Classification: best way to construct proxy variables for class types,4,2,False,False,False,statistics,1485283457,True,"I am trying to adapt a general nonlinear regression algorithm I have to use in a data classification context. It uses Bayesian inference and a RJMCMC code to construct a generalized nonlinear model. It accepts multiple predictor variables but only one response variable. It uses training data (with known responses) to construct the model, which is then applied to test data and/or data with unknown responses.

Applying this to a classification problem with only 2 classes if fairly easy: I set the response to be either 0 or 1 (depending on the data class), and I get results that are (almost) entirely ranging between 0 and 1. I then construct a simple Bernoulli distribution to determine the probability of each class. I have tested this and it works well.

My question is: is there a good way to make this single response variable a good proxy for more than one class?

I currently trying to get around this problem by re-running the model for each data class, and (in the training data) setting the response to be 1 for the current class and 0 for all other classes.  I can then compare the probability for each class at a given point. I believe this will work (I am running the code right now). However, this requires re-running the code multiple times, which I would like to avoid if possible.

Also, the response has to be a real value, so using complex numbers and setting one of the response classes to i wont work.

Thanks."
Any benefit to converting a multi-level variable to binary for regression purposes?,14,1,False,False,False,statistics,1485289449,True,"When running out a logistic regression, is there any benefit to collapsing a scale variable (Excellent to Poor) into a binary variable?

For example:

1 - Excellent
2 - Very Good
3 - Good
4 - Fair
5 - Poor

and recoding it to

1 - Good (5, 4)
2 - Bad (3, 2, 1)

Please be gentle if the above question is dumb and or elementary; I am a market researcher trying to beef up my statistics knowledge."
"Trying to understand the best way to assess ""effective dosage"" in a school program",5,1,False,False,False,statistics,1485297528,True,[deleted]
Introduction to Natural Language Processing,0,3,False,False,False,statistics,1485297826,False,
No stats degree - how to convince employers I can do data analysis?,40,28,False,False,False,statistics,1485299639,True,"So, I have no statistics or math degree, but have been doing applied statistics and data analysis during my (linguistics) PhD. I've tried applying to data analysis jobs  where all they want is to run some t-tests (I'm exaggerating, of course), but have been unsuccessful so far. 

In my CV I mostly list my academic work, but I don't know whether this helps or harms my chances. Are there any tips you could give me? Is there anything I could do to improve my chances besides a new BA?"
Applying Functional Analysis to Probability and Statistics,5,22,False,False,False,statistics,1485299778,True,I've done a few google searches about using functional analysis in a prob/stat setting but was curious if any trained statisticians here have used concepts of FA in their research or work. Thanks for your time!
Struggling with what appears to be a simple problem...,2,2,False,False,False,statistics,1485300463,True,"I think the previous person was going under a different logic, but I'm trying to figure out his calculations as well as questioning my own.
The challenge is a marble game where you draw marbles until you pull a black one. 
The previous person drew out the probability of each as:
9 marbles, 5 white, 4 black.
Pulling a black: 44.44%

Pulling 1 white: 27.8%

Pulling 2 white: 15.9%

Pulling 3 white: 7.9%

Pulling 4 white: 3.2%

Pulling 5 white: .8%
I get:
44% black (4/9)

55% 1 White (5/9)

27.7% 2 White (4/8)*(5/9)

11.9% 3 White (3/7)*.277

3.96% 4 White (2/6)*.119

.66% 5 White (1/6)*.0396


Am I missing something? Should I be following some logic that my predecessor did for something similar?
Thanks in advance!"
"Uncorrelation vs. Independence, Binary Random Variables",3,2,False,False,False,statistics,1485302175,True,[removed]
New to statistics,3,5,False,False,False,statistics,1485303707,True,"Hi everyone,

I'm interested in learning about statistics. I don't have any real background in statistics aside from an introductory course I took in university a while back. I really only remember the very basics (mean, median, std deviations, and some cursory details on probability). I would really like to learn more about statistics and was wondering if anyone knew of any good books and online resources.

Cheers!"
"Calculating confidence intervals for ICC(3,k)",1,1,False,False,False,statistics,1485304532,True,"Hi clever people. 
I'm currently trying to calculate the CIs for ICCs but the papers on how to do this are not written in an intuitive manner. In the [Shrout  & Fleiss 1979](http://www.aliquote.org/cours/2012_biomed/biblio/Shrout1979.pdf) (Intraclass Correlations : Uses in Assessing
Rater Reliability Psychological Bulletin 86 2 240-248) they explain the method on page 424 

In equations 8 and 9 they define F(lower) as F*0*/F*1-1/2alpha*[(n-1), (n-1)(k-1)]


does this mean divide F0 by the Fvalue you would find given the alpha and degrees of freedom ?

The upper then multiplies these f values it doesn't seem right.

Cheers for any help.
"
Job Prospects for a Bachelor's degree in Statistics,25,13,False,False,False,statistics,1485305066,True,"I am in my final of year a Statistics Bachelor's degree in Australia. I was wondering what my job prospects are after finishing/what your experiences are in positions relating to statistics 

Thanks"
"Interpreteting significance of ""redundant""/final term in a multilevel categorical variable.",2,1,False,False,False,statistics,1485305626,True,[removed]
Which regression and type of test to use?,0,1,False,False,False,statistics,1485311349,True,[removed]
Job prospects for bachelor's degrees in Statistics,4,2,False,False,False,statistics,1485330863,True,[deleted]
How to do econometrics properly,0,4,False,False,False,statistics,1485337949,False,
R Users - I need help please.,10,4,False,False,False,statistics,1485346702,True,"I've got an online quiz for my stats module and the lecturer tries to encourage us to use R so has uploaded some of the data as a data frame in R. The data concerns age, FEV, height and sex of 20 children. The sex column is coded by 0 for female and 1 for male, and I'd like to set up a 'male' and 'female' vector, but as I'm trying to learn R I'd like to do it in R rather than manually. I'm trying to form the vector using an if statement where if the 'Sex' column is 0 it'll return the height value to my vector, and if not 0 ignore it (this is the female height vector, I'd do it analogous for male). 

I had a go at the if statement but I got so many errors in the console it would seem pointless showing what I have, if anyone could help me out with what code to use I'd be grateful! 

Thanks. 

Edit: it seems the first bit I can't code correctly is the if part, I am typing {if(fevSub$Sex=0) but it says the '=' is unexpected. "
Are you aware of gender bias in your data?,9,0,False,False,False,statistics,1485362592,False,
Any other market researchers visiting this subreddit to beef up their Stats knowledge?,6,0,False,False,False,statistics,1485371305,True,"What are your levels of experience and which industries do you work in?

I've been learning stats through osmosis for about 4ish years working across CPG, financial and call center."
Thinking of a Statistics Masters Degree from a CS/Math Undergrad,11,8,False,False,False,statistics,1485375828,True,"Hey all,

I'm currently a junior doing a joint major in CS & Math at a non-target university for either major (but ranked around ~60 for overall universities). I'm really interested in Machine Learning and would love to understand it on a much deeper level, but my lack of statistics knowledge is holding me back a bit. As a result, I'm looking into some statistics masters programs. However, I messed up throughout my first few years of undergrad and ended up with a few Ws on my transcript, a 3.5 GPA, and a 3.7 in-major GPA with some Bs/B+s in Discrete Math, Multivariable Calculus, and Algorithms. With my background, would I be able to get into a good (maybe top 30?) statistics program if I finish strong? Or should I set my sights lower than that? If it helps, I am generally a very good standardized test taker (my scores are usually in the top 1-2%), so my GRE score should hopefully be pretty good. Thanks in advance for your help! I'll list some of my background below.

 

**Courses Completed Already:**

* CS I & II

* Programming for Math and Science

* Data Structures

* Algorithms

* Database Systems

* Theory of Computation

* Data Mining

* Machine Learning (Graduate course)

* Discrete Math

* Calculus I & II

* Multivariable Calculus

* Vector Calculus

* Linear Algebra

 

**Plan to complete:**

* Probability Theory

* Statistics I & II (Econ department)

* Mathematical Statistics

* Numerical Analysis

* Computer Organization

* Operating Systems

* Real Analysis or Econometrics"
Misbehaving Semi-variogram,0,1,False,False,False,statistics,1485377817,True,[deleted]
What methods do you suggest for this type of problem?,16,2,False,False,False,statistics,1485378023,True,"I need to understand how a number of parameters contribute to decomposition in wood.

The parameters in question are (1) temperature, (2) humidity and (3) exposure to sun light. We already know that the all contribute, but we don't onow exactly how they affect the specific decomposition process.

My need to asses to what extent a 1% increase in either of these parameters will increase the decomposition process.

At my disposal, I have a dataset covering some 30 datapoints where the degree of decomposition is recorded together with the three other parameters.

What method would you recommend for this type of problem?"
Alternative to Minitab?,0,1,False,False,False,statistics,1485378359,True,[removed]
"If somebody questions the certainty in your statistics you tell them. ""If we had certainty, statistics would be irrelevant.""",18,69,False,False,False,statistics,1485383602,True,
Regression type and how to approach this model,0,1,False,False,False,statistics,1485386613,True,[removed]
Homogeneity assumption in ANOVA,11,1,False,False,False,statistics,1485389337,True,So I understand that ANOVA compares the variance between samples to the variance within samples. But if that's the case then isn't the assumption of Homogeneity of variance in some way conflict the comparisons?
Disagreements in statistics,0,1,False,False,False,statistics,1485400163,True,[removed]
What is the correct statistical method to handle this holdout test regarding a difference in proportions?,0,1,False,False,False,statistics,1485407317,True,[removed]
How can I determine the odds to reach a predicted value based on past results?,2,2,False,False,False,statistics,1485424487,True,"In what I am trying to figure out, I have a predicted value and the average value to date available. In addition, I have a table of past results where it provides a list of increasing value in each column and the percentage of the time that value has been reached or exceeded. 

Please use the table below as an example. The value of 30 or higher has been reached 98% of the time, the value of 35 or higher has been reached 93% of the time, etc.
 
    20          25           30          35          40          50
    100%	    98%          98%         93%         89%	     78%

Say I have a predicted value of 53.2, Average Value of 48.11, and Standard Deviation of 9.455. Can the chart be used to determine the probability of at least 53.2 being reached? How would I determine the probability since I have the past results available? The predicted value has been reached 78% of the time in the past, but that doesn't mean there's a 78% chance of it being reached now, does it?

I have had a couple stats classes before, but I don't remember ever having a table of past results available to help you. Thanks. "
Looking for a way to graph significant values after MANOVA analysis.,1,2,False,False,False,statistics,1485439939,True,"Hey /r/statistics,

I have a dataset with around 2500 subjects that rated music by 27 predefined parameters (ordinal scales, -2 to 2). I also have information on what type of audiosetup (nominal scale: Headphones, HIFI-system, internal speakers, external Speakers) the subjects used.

I now designed a MANOVA analysis to see if the audio setup influenced the rating of the presented sound. This proved to be overall significant (Wilks-Lambda ,000) and I found 10 of the 27 values to be significant when testing for intersubject effects. This was all done in SPSS.

I am now looking for a way to plot these results. I thought about having a cluster-map that shows all subjects and places them closer to each other if their choices on the 27 parameters are similar. I would then color them 4 ways by their audiosetup to see if the clusters that are generated by similar rating patterns are related to the audiosetup. I would then see if certain colors dominate in a cluster.

I'm pretty new to statistics, so please forgive any inaccuracies.

Thanks in advance!"
"How statistics lost their power and why we should fear what comes next: The ability of statistics to accurately represent the world is declining. In its wake, a new age of big data controlled by private companies is taking over – and putting democracy in peril",22,51,False,False,False,statistics,1485446315,False,
How to graph Hazard Ratio vs. Continuous Predictor?,1,3,False,False,False,statistics,1485454949,True,"Hi everyone, 

I'm looking to graph the values of a continuous predictor on the X axis vs. the hazard ratio associated with each value in order to determine the point in which the hazard increases more dramatically (to determine the best cut-off point for the variable). 

Can anyone help guide me? I have SPSS and R (but I am very inexperienced with this)


Something similar to this

http://www.ats.ucla.edu/stat/stata/examples/asa2/f4-3.png"
What do they mean by categorical analysis procedure of the data?,0,1,False,False,False,statistics,1485457817,True,[removed]
"If you teach statistics, here could be a long term project for the students.",4,2,False,False,False,statistics,1485462962,True,"When I was in high school, I found it a little buggy that teachers never let the students apply what they had learned through the subject(s) on their own environment.

.

This sort off answers the classic question students have with the ""Why should I learn this?"" 

. 

Also this would be best at the beginning of the year/semester.

.

   Students should look at something they are quite interested in, such as: business, social, civil, sport, etc. As you teach away through the chapters, and different formulas pop up left and right, the student should try to apply these newly learned formulas to the data they are collecting as they pass each chapter, followed by a brief summary and analyses at the end of each chapter. "
Need help using R?,7,1,False,False,False,statistics,1485471794,True,"Hi, 

This is my first using R. I was given a file with statistics on mother's smoking status. There is a variable called smoker. How can I write the code to know how many mothers smoke (YES) and how many do not smoke (NO)?

Thank you for your help"
Where can i find statistics about book genre consumption by gender?,0,1,False,False,False,statistics,1485475011,True,"I am trying to find some statistics about what percentage of books do men and women consume by genre?

For example: Men consume 30% novels, women consume 70% novels, etc.

Sorry if wrong subreddit, didn't know where to ask for this."
Can I just get a HW check. I am new to statistics but I feel pretty confident in how I did. Its all very very basic.,1,0,False,False,False,statistics,1485478145,True,[removed]
Variance in Expected Attempts,4,0,False,False,False,statistics,1485478344,True,[removed]
Summation notation formula: Need help explaining how r was calculated?,0,1,False,False,False,statistics,1485481444,True,[deleted]
In this summation notation formula. How is r solved?,10,0,False,False,False,statistics,1485482199,False,
started a graduate statistics course...,4,0,False,False,False,statistics,1485483801,True,[deleted]
1.645 * Standard Deviations,16,0,False,False,False,statistics,1485486510,True,"Hello,

I'm a biologist and I have a stats question, it's more an issue of terminology I think but I'd love an explanation so if I might look like I have the faintest idea I know what I'm talking about.

I'm a biologist working in drug research field and I work with qualitative assays that define a sample as positive or negative for having an immune reaction to a drug. We define the level above which a sample is positive by screening a series of samples we know are negative, calculate the mean and variability and calculate a cut point. We want the cut point to have an approximate 5% false positive rate generally so we multiply the mean response of the negative individuals by 1.645 times the standard deviation.

That's a pretty standard process across the industry, captured in industry white papers and accepted by the regulators (FDA, EMA etc.)

When talking about this it is commonly referred to the 95th percentile in that (from my understanding) 5% of negative samples will be above this value (based the variability seen when we analysed them) and thus positive. However I'm told that's not strictly true because we haven't square rooted something (here's where I'm out of my depth).

I'm not looking to change the maths/stats/calculations as it's the industry standard and laid down by people far smarter than me but I'm wanting to know what to call this or how to refer to it if not as ""the 95th percentile"".

Most scientists I know don't have a great grasp of the finer details of stats (most companies I've worked for have a stats group fortunately) but it would be good for me to have the correct terminology or vocabulary should I come across someone who knows what they're talking about!

Hopefully that makes sense! Can elaborate further if needs be.

Thanks!"
How much money was spent at McDonald's worldwide in the last 5 seconds?,1,0,False,False,False,statistics,1485491106,True,[deleted]
What is measures of dispersion and how do we calculate variance of a population?,0,1,False,False,False,statistics,1485491980,False,
"Stats exam tomorrow, this question is not in my notes",0,1,False,False,False,statistics,1485493130,True,[removed]
"We Will Sort you out with Your Statistics Online Quiz, Trust Us",0,1,False,False,False,statistics,1485500266,False,
Confidence Interval when dealing with percentages,5,2,False,False,False,statistics,1485501485,True,"Hello all!

I've been trying to dust off my old knowledge of confidence intervals from uni and ran into a sticky situation - i'm not certain how to work out the 95% confidence interval for some data in terms of a percentage.

I've simplified the problem below.

Lets say I want to work out the average percentage of household spend that is spent in the garden in a particular village. The data I have is for say 10 households, each with different household spends, and splits their household spends into ""Garden"" and ""Non Garden"", by percentage. See below for example.
House #, % of household spend not spent in garden each year, % of household spend spent in garden each year
House 1, 4.33,	95.67
House 2,	2.76, 97.24
House 3,	9.33, 90.67
House 4,	5.89, 94.11
House 5,	0.96, 99.04
.......
House 10	1.53	98.47


I have no information about the population, only about the sample.

How do I work out the 95% confidence interval for this? I started looking at confidence intervals for proportions but It doesn't seem to fit ideally as a simple survey of pass/fails. Should I be calculating a regular confidence interval, using the percentage garden spend simply as the number/result? 

I know the sample size is very small, so think I should be looking at student t distributions - I then started getting confused and having nightmares of 2nd year uni stats. Any pointers please?

Any help is much appreciated - I'm looking to do the calc on paper, not a program."
Students of Statistics - Professional Help for Your Assignments,0,1,False,False,False,statistics,1485517478,False,
Fun Facts about Valentine’s Day – Part 2,0,1,False,False,False,statistics,1485522055,False,
Statistics,0,1,False,False,False,statistics,1485529031,False,[deleted]
ICC 2.1 or 3.1,3,2,False,False,False,statistics,1485533063,True,"Hi r/statistics

PhD student in biomechanics here. I have a minor discussion with my supervisor about which ICC to use in our analysis.

We are conducting a study on the concurrent validity of a new motion detection system. We are validating the new system using another very precise but time-consuming older system as a gold standard (I can provide specifics on this, but I think its irrelevant for my question).

We have recorded a number of particpants (ratees) simultaneously with both systems. For all ratees we have measurements from both systems.

My assumptions are the following:
a) The participants must be viewed as being random individuals from a larger population.
b) The two systems are NOT random, but actually represent fixed raters. I assume this because I expect that any other identical systems would provide the same results as the two we have in our lab.

Given my assumptions I believe I should use the Two-Way mixed model 3.1.
My supervisor is of the opinion that we should use 2.1 but hasent provided me with a rationale for this yet. I´m guessing he is choosing this because its what we normally do at our department.

My two main questions are:
1) Are my assumptions correct?
2) Is the conclusion I make based on my assumptions correct?

And a bonus questions:
I plan on providing estimates of both absolute agreement and consistency, but most studies similar to what we are are doing only provide estimates of one of these. Is there anything wrong with providing both estimates?

Hope you can help
Cheers"
Models for count and binary outcomes with variable length follow up time,1,1,False,False,False,statistics,1485535498,True,"Hi all,

I'm conducting a longitudinal study comparing health outcomes in two cohorts of patients (cases vs controls).  The cohorts are propensity score matched to adjust for baseline covariates.  Each patient is lost to follow up at a different time due to insurance dis-enrollment (ie  they are right censored).  I am interested in comparing the difference in outcomes between the two cohorts.  I have a number of different outcomes:

1.  count outcomes (ex: number of falls/fractures)

2.  binary outcomes (ex: incident dementia/AMI)

3.  zero inflated, non-normally distributed measures of central tendency (ex: healthcare costs)

Independent variables: indicator variable for cohort (0 = control, 1 = case), continuous variable for duration of follow up before censoring (in months), and a variable for the outcome of interest (number of falls, presence of incident dementia, health care costs)

For each of these these outcomes, I'm primarily interested in the relative difference between cases and controls (ie difference in mean number of falls, difference in proportion of patients with an AMI, difference in mean costs).

My question is how to model these outcomes with an indicator variable for the cohort (case vs control) given that follow up time is variable and possibly right censored at different times.

For #2, I can use a cox proportional hazard model, correct?  Then exp(beta_case) would represent the incremental relative risk of my cases

For #3, I know I can use Lin's regression method which will also me to come up with bootstrapped confidence intervals for the coefficient for my indicator variable.

For #1, this is where I really am at a loss.  **How would I model a count variable when my outcome is censored?**

Thanks all!

"
Comparing Two Trends?,3,5,False,False,False,statistics,1485540810,True,"I have two populations and want to create a basic, easily interpretable summary statistic comparing their trends over time. My  initial thought was to just divide the linear trend of the population of interest by the linear trend of the other population, but then I realized that negative trends would throw a wrench in the whole thing (e.g. how the hell could you interpret 2/(-1) versus (-2)/1 without losing information on which is increasing/decreasing since they'd both equal -2?).

Is there another alternative to or variation of that approach that you know of? It has to be understandable to people without any statistical training, so I can't use many of the more complex, though better, measures.

Thanks in advance for your help!

Edit: Perhaps the geometric mean of population A's % change divided by the geometric mean of population B's % change?"
Doing magic and analyzing seasonal time series with GAM (Generalized Additive Model) in R,4,25,False,False,False,statistics,1485544143,False,
Using a chi-squared test -- help please!,11,2,False,False,False,statistics,1485545151,True,"I'm terrible with statistics, and my group can't figure this one out. So we have two species of clams, and we want to see if they're different based on:
The number of clams with successful predation events & the number of failed predation events between species. 

So, we have:

Successfully predated Blood Arks -- 184

Failed predation Blood Arks -- 9

Successfully predated Incongruous Arks -- 702

Failed predation Incongruous Arks -- 13


We're trying to do it by hand to fully understand it, but we can't figure out how to calculate the expected values. 

Let me know if you need anything clarified. Thanks!"
A real life example of how people interpret bad statistical evidence. Don't forget to do your stats right folks!,2,1,False,False,False,statistics,1485545376,False,
Covariance / correlation of computer generated pseudorandom variables?,4,1,False,False,False,statistics,1485546183,True,"I was trying to show that by averaging N independent random variables drawn from a distribution with variance ""v"", the variance would tend to go to ""v / N"". 

I tried this many times for a range of N using computer generated pseudo-random variables, and then averaged the variance from each try to get an overall picture of how (on average) variance changes with N. 

I found that ""v / N"" didnt actually work very well. However, mostly through trial and error, I eventually found that ""v(1 + log(N))/N"" works extremely well. 

After some looking around, I found that if the variables arent independent but correlated, then the predicted variance becomes ""v(1/N + p*(N - 1)/N)"", where p is the average correlation between variables. 

My results would then seem to indicate that the average correlation between N computer generated pseudorandom variables is (roughly) ""log(N)/(N-1)"". Can anyone confirm this and/or explain why this might be the case?

EDIT: typo"
Compare two dependent ICCs,5,1,False,False,False,statistics,1485549194,True,"Hi --

Way over my head here.

Here's the scenario:

1. Student performance on a task is rated by 4 judges (who can vary across students).   
2. Student performance on a task is then rated using a difference process by 5 judges (judges are fixed and consistent across students).
3. I now want to compare the reliability of the rating methods.
4. I calculated ICCs, but I am at a loss on how to compare equality - which presumably are dependent.


Any thoughts?

Thanks."
Where Predictive Modeling Goes Astray,1,27,False,False,False,statistics,1485556744,False,
Help! Weighted or normal Cohen's Kappa?,3,3,False,False,False,statistics,1485564679,True,"Hey guys, med student and complete statistics newbie here, was hoping someone could clear something up for me. Trying to describe what I need to do as best I can...

 I have 20 patients who answered the same questionnaire twice (after an interval of roughly 60 days). Their answers on the questionnaire results in a score of 0-23 on an ordinal scale, which is theoretically supposed to represent a fixed, personality-dependent trait of theirs. What equation would I have to use to find out the reliability of their results, given that my hypothesis is that they would be roughly the same for a given person on both applications?

Is it the weighted Cohen's kappa I've read about? If so, what program can perform it? Are there other ways of figuring this out? Sorry for my incredibly tenuous grasp on these concepts, this is my first real research :/
"
"Only a true student of Statistics know value of this tshirt ""ALL MODELS ARE WRONG BUT SOME ARE USEFUL""",0,1,False,False,False,statistics,1485603048,False,
Suggestion: Canadian looking for a Pharma job in the US.,8,0,False,False,False,statistics,1485632821,True,"Dear Redditors,


Currently, working in Canada as a Statistician in a research institute that conducts P3 multi-centre CV trials with Pharma sponsors. I am responsible for stats during the conduct and analysis of the trials (sample size calc., CRF design, Randomization, DMC reporting, final study analysis, site monitoring, etc.). Due to limited career growth, lack of experience in early phase studies and oncology trials, limited pharma opportunity in Canada, I would like to pursue a career in the US, possibly with a Pharma. 


I am not sure where should I start and I would be thankful if someone in the industry could throw light on: **1** how to equip myself, **2** possibility of finding an opportunity, **3** how daunting the immigration process might be to obtain TN visa. Additionally, I am more interested to work in the technical/conceptual aspects of clinical trials than the statistical programming part of it. By industry standards, does that mean I must have a PhD?
 
 
*Background:* Masters in Stats as well as another Masters in Biostats, 3+yrs working experience in Clinical Trials, decent collaborative publication but no technical publications so far, BASE SAS certified with moderate-good programming skills using both R and SAS, in process of becoming a Canadian citizen.

Many thanks !!!"
Mean and Standard Deviation,1,1,False,False,False,statistics,1485633622,True,[removed]
"Data normalization suggestion, comparing change in area b/w two data sets",0,1,False,False,False,statistics,1485633632,True,[removed]
Mental Health Use v. Shootings--Regression analysis for your pondering,4,6,False,False,False,statistics,1485636324,False,
If anyone could take this survey to help me and my peers out in our Maths degree group project it would be most appreciated.,0,1,False,False,False,statistics,1485637723,False,
Help creating a statistically significant VIP program for startup!,13,0,False,False,False,statistics,1485704224,True,"Hi Everyone, 

I am developing a startup concept with the use of VIP program, that will help clients differentiate their customer base. This is a very simple VIP program where the customers will be divided into three categories Platinum, Gold, Silver based on three calculations, Average order value (AOV),Purchase Frequency (PF),Total Customer Value (TCV). The calculations will be done everyday on time scales of 30,60,90 days, Ex. AOV(30),PF(30),OF(30). 

The difficulty we are having is creating statistical logic that will qualify individuals for different VIP program groups. In other words how do I statistically signify a particular value of my three calculations that will place customers in the silver,gold,platinum group? 

Ex. AOV(30)= $50, PF(60)=90, TCV(90)=$100 ----- Silver group ---- 20% off next purchase. 
     AOV(30)= $70, PF(90)=100, TCV(90)=$200 ----- Gold group ---- 30% off next purchase. 

How are these VIP groups significant in the real world? I can just come up with random calculations like above but that is not a scalable or correct solution. 

I hope my question makes sense. "
likert versus sliding scales (and others0,1,3,False,False,False,statistics,1485709594,True,"There is a lot of debate over the use of 5-point vs 7-point; even versus odd numbered scales; and sliding/analogue scales in social science research. Does anyone know of any key papers that have looked into this using quantitative, rather than opinion, based methods. i.e., has anyone distributed two versions of the same questionnaire and conducted a t-test or looked performed Cronbach's  and factor analysis on both versions?"
"Study: People Claiming to Work More Than 70 Hours a Week Are Totally Lying, Probably",12,55,False,False,False,statistics,1485715994,False,
How can I compare three ratios for statistical difference in Excel?,0,1,False,False,False,statistics,1485716360,True,[removed]
non-technical skills / qualities to put on a resumé ?,8,1,False,False,False,statistics,1485719610,True,"as someone without work or research experience, but with exceptional grades, i feel the need to put a lot of filler material on my resume in addition to education and technical skills. you know stuff like 'good communication skills' and 'good team player' type of bullshit that a 15 year old would put on their resume when applying to work at McDonald's -- those kind of statements but more tailored to tech and finance firms that do statistics and machine learning. my background is in mathematics, but i've got a lot of statistics and machine learning under my belt. i would really appreciate any help. "
"TIL There are less Muslim terrorist attacks in the US than Jewish, Left-Wing, Latino etc.",6,0,False,False,False,statistics,1485721593,False,
Trouble understanding false positive stats for cancer,0,1,False,False,False,statistics,1485723354,True,[removed]
How to know if I should reject my null hypothesis?,0,1,False,False,False,statistics,1485726856,True,[deleted]
Two level t-test?,5,2,False,False,False,statistics,1485729459,True,"I am working with a dataset of cycle times over the period of several years for a given office. I want to see whether the implementation of a certain process reduced the cycle time in a statistically significant way (note that the new process went in at different times for different offices). I have several thoughts about how to approach this: 

For a given office, I conduct a t-test for average cycle time before and after the implementation over the same period. For example, for Office A, the process was implemented in June 2015. I would collect the average cycle time from June 2015 to December 2015 and compare that to the average cycle time from June 2014 to December 2014. This would tell me whether there is a statistically significant difference for that office. 

The second step is to look at all offices in aggregate. I would do a t-test on the difference (negative to positive) to see if it's not zero (this could be expanded into linear regression if I manage to collect descriptive data on offices). 

My question for you all: is this the appropriate method to assess cycle time both at the office level and on aggregate? "
Do I have to be a mathematical genius to graduate?,5,0,False,False,False,statistics,1485736371,True,"Well, I know it seems weird, but I'm not THAT good at math and I'm interested in majoring in statistics this year. I'm actually just OK, not bad, but not great either.

I would to like to work on the economy field, if that's a thing for stat majors. So, should I go with statistics or should I choose something like accounting or business since I'm not that good in math? Any advice before I make a terrible decision?"
student,1,0,False,False,False,statistics,1485737746,False,
Critique my permutation test R implementation!,0,1,False,False,False,statistics,1485739204,False,
Statistics while Driving To Work,6,2,False,False,False,statistics,1485739834,False,
Any correlation with Wii revision and serial number?,0,1,False,False,False,statistics,1485747093,True,[deleted]
Help with code using R?,2,0,False,False,False,statistics,1485749836,True,[deleted]
"""Statistics Department at US Research University Starterpack"", x-post r/starterpacks",2,63,False,False,False,statistics,1485750977,False,
Chance models for thinking about inferential statistics,0,1,False,False,False,statistics,1485771135,True,[removed]
Strange readings.,0,1,False,False,False,statistics,1485773750,True,[removed]
Stats newbie looking to expand knowledge,7,6,False,False,False,statistics,1485792005,True,"Hi guys, I'm interested in taking my understanding and application of statistics to the next level. I have taken a few statistics courses at both the undergraduate and graduate level and for some weird reason really enjoyed them. 

Any suggestions on how to progress from basic probability, test interpretation, etc.? 

Should I take a closer look at analytical software (i.e. R, SPSS, SAS)?

Should I be introduced to concepts that are more theoretical?

Any advice is greatly appreciated - thanks all!"
Only 5% of Americans voted for Trump.,0,1,False,False,False,statistics,1485793412,True,[deleted]
More questions about biostatistics,2,3,False,False,False,statistics,1485793595,True,"I posted some questions on here a month or so ago regarding obtaining a MS in biostatistics. I honestly don't know if this is the right place to ask my questions, so if any of you know of another forum that might be able to help, please send a link. I will still ask my questions on here, however, just in case someone may be able to answer some of them. :)

1. I am seeing a lot of discrepancies regarding payscale for Masters of Biostats degree holders. Anywhere from $50k to $130k. Can anybody explain what the discrepancy is due to? What would be an accurate estimate of what one could expect to make upon graduation with perhaps not a whole lot of experience? Alternatively, what could one expect to make 5 or so years down the road with a master's degree?

2. Is the job market extremely competitive? How hard is it to land an actual job upon graduation?

3. Is the work fulfilling? I am looking for a career that will challenge me for years to come, and also one that is fulfilling. I currently have a finance degree, but have little interest in working in the finance field.

4. Are there ever any telecommuting or part-time opportunities? I have 3 children and would be interested in working from home or working part-time if this is an option.

5. The other option I have been tossing around is becoming a Physician Assistant. The schooling and healthcare experience required for this route would take me much longer, but I do find the hours, the pay, and the job-satisfaction to be appealing. Do any of you biostatisticians regret going into the field, or is it as challenging and fulfilling as you had hoped it would be? Just hoping to make the right decision as I need to decide between two different pre-requisite paths for the fall."
Inaccurate Results from Maximum Likelihood Parameter Estimation,6,4,False,False,False,statistics,1485796349,True,"So I'm working with some wind speed data, which supposedly follows the Weibull Distribution. I began by importing my data into R and fitting it to the Weibull Distribution using the fitdistrplus package.  By default the package uses the Maximum-Likelihood Estimation method (MLE) of parameter estimation.


[Result](http://imgur.com/WNr0tXR)


The resulting parameters *did not* fit the data well. Unsatisfied, I fit the data using two other parameter estimation methods the fitdistrplus package offers, the Maximum Goodness of Fit Estimation (MGE) and the Qunatile Method of Estimation (QME).


[Result](http://imgur.com/b5tm5Sw)


Hmm, these two look much better.


To see if this was a Weibull-exclusive issue, I next fit the Log-Normal Distribution to the same data using 4 methods of parameter estimation (the same three as Weibull + the Method of Moments, which was excluded from Weibull because I ran into some difficulty implementing it.)


[Result](http://imgur.com/xWVh0eR)


As you can see, the MLE is even *further* off on this one! What's going on!?


Finally, I fit the Logistic Distribution to the data to see if the trend of MLE being inaccurate would continue.


[Result](http://imgur.com/G3GPmnw)


Clearly no issues with MLE here. In this case , all four parameter estimates were nearly identical. 


So I have to wonder, why was MLE so far off for the Weibull and Log-Normal Distributions? Has anyone encountered this before?


If I'm missing any information that I should have provided please let me know. Did you want to see my dataset/syntax so you could reproduce the results? Should I have posted Goodness-of-Fit Statistics as well?"
Correlation and simple linear model.,0,0,False,False,False,statistics,1485796505,True,"I got the simple linear model via https://cran.r-project.org/web/packages/L1pack/L1pack.pdf lad. (Residuals - residuals of Median line )  / (mean of deviations from median, same as residuals of median line really)  correlates highly, .89, with correlation ^ 2 of same data.   Slope * (range of Xplanatory variable) is  5. Correlation^2 is .0312 and similar-to-correlation is .025. What is the average of the possible differences of Y values from the data point with the lowest Xplanatory value and the highest?

Also, how is likelihood computed?"
"A/B testing, t-tests, and p-values.",8,11,False,False,False,statistics,1485801129,True,"I have a physics PhD with a background in many-particle quantum statistical physics. I've been reading books like *Patten Recognition and Machine Learning* by Bishop and *Elements of Statistical Learning* by Hastie et al. I find these texts familiar in rigor and tone compared with physics texts on statistical mechanics, quantum mechanics, quantum field theory, etc. Recently I've been trying to get into the machine learning and statistical modelling fields for employment. When I apply for certain jobs, particularly with ""statistician"" in the title, instead of being asked questions about particular models or techniques, I'm asked about a/b testing, t-tests, and p-values; none of these concepts appear anywhere in those aforementioned texts. Am I missing something?"
Choosing between clustering algorithms non-interval data,1,3,False,False,False,statistics,1485803067,True,"I have a survey that's primarily made up of ordinal (Likert scale) and nominal (Boolean values from ""select all that apply"" Qs) data. I want to see if there are distinct ""types"" of respondents. I've used k-means for this in the past, but that requires Euclidian distance (and therefore interval data).

I've read about several options for mixed- or non-interval data, but I don't know how to evaluate or choose between them. I'm thinking of using Gower's distance + PAM, but I'm not confident in this choice. (Also, I don't think that ""because i googled and found a good [tutorial](http://dpmartin42.github.io/blogposts/r/cluster-mixed-types)"" is a great line for my methods section. . .) 

Do you have any resources or experience with clustering non-interval data? I'd love to hear advice that applies to more situations than this one, but  FWIW I'm using R, I'll have about 400 observations and will likely use ~30-40 behavioral variables for the cluster analysis. I do intend to at least try using PCA to reduce dimensionality. 

Thanks a ton. "
Does the distance you drive to school correlate to your performance in school?,0,0,False,False,False,statistics,1485803409,True,[removed]
Does ice cream increase happiness,0,1,False,False,False,statistics,1485803688,True,[removed]
Book Request: Statistical Procedure Reference In R For Those Who Know Statistics,1,1,False,False,False,statistics,1485812340,True,"I'm looking for a book (preferably digital/ePub) that shows the steps to perform a wide range of Statistical Procedures. Every book I find is teaching me Statistics. I have books for that, I just need a statistics in R reference that is in-depth."
"Statisticians fear Trump White House will manipulate figures to fit narrative: ""If you control the denominator, you control everything.""",43,117,False,False,False,statistics,1485816849,False,
Help with question for statistics?,4,3,False,False,False,statistics,1485817008,True,"is an increased number of registered vehicles is a direct cause of increased life expectancy?
"
Question regarding sampling plans for quality control,0,2,False,False,False,statistics,1485819793,True,"Good evening.  I'm not sure if this is the right place to ask this, but I've been trying to understand double and multiple sampling plans for acceptance testing in the context of quality control.  I've found some methods for deriving single sampling plans (one large sample is drawn, and the decision to reject or accept the lot is decided with that information).  With double and multiple sampling plans, it seems I find a large number of precomputed tables for specific values, but no explanation of how they are generated.  Might anyone know of a resource where I could learn how to to this?  If this isn't the right place, where could I ask?  The stats part of stackoverflow hasn't turned up any resources."
Statistical Significance and Sample Size,6,2,False,False,False,statistics,1485846576,True,"If I want to ask my customers if they prefer A or B, how big must my sample size be in order to get a statistically significant response?

Is there a table that says at 5% difference between A and B = X sample size and etc.? "
Top 50 Statistics Blogs,2,13,False,False,False,statistics,1485859188,False,
Can I become statistician without math degree?,6,0,False,False,False,statistics,1485880212,True,"I need some advice, I would like to pursue a career as a statistician. But in my opinion I feel like I'm not competitive enough to enter the field. I received my bachelors degree in biochemistry with a minor in math. In undergrad I took 1 year of statistics, but I did not take linear algebra. I am now a first year masters student in neuroscience. I have some experience working with SAS, and I am now learning R programming in my biostats class. Are there any statistics entry level positions that will hire someone like me? To be honest I am looking for a career that will pay well, any suggestions? I feel like science is not the field I want to have as a career, little to no money unless you're a doctor. "
Statistics help!!!!,0,1,False,False,False,statistics,1485881788,True,[removed]
Turning data into numbers · Simply Statistics,0,0,False,False,False,statistics,1485882801,False,
Research paper on Importance of programming in Statistics?,0,1,False,False,False,statistics,1485884239,True,[removed]
Help... multiple regression analysis OR multiple linear regression? Not sure of difference (or which one I should use),8,3,False,False,False,statistics,1485893942,True,"So I need to analyse data as part of my dissertation. I have three independent variables which come from three sections of a questionnaire (attitudes, motivation and psychological needs). My two dependent/outcome variables are objectively measured accelerometer data (time spent in mvpa and step count).

When presenting my proposal, I stated that I was going to perform multiple regression analyses for both outcome variables and my dissertation advisor seemed happy with my statement. 

Now I have to prepare my data analysis plan and one of the exemplar responses is 'a multiple linear regression will be used to test the relationship between physical education (PE) and academic performance. Several covariates will be entered into the model first, to see how much variance in grades are explained by these factors. Then PE grade will be entered to determine whether this adds to the explanation of academic performance above and beyond the contribution of the covariates. Strength of the relationships will be determined by examining percent explained variance and also beta weights'. I was thinking this might be what I was looking for (but obviously substituting in my own variables), but this is multiple linear regression not regression analysis. 

Stats isn't my strong point so I apologise. Analysis will be done using SPSS. My research question relates to examining determinants if it helps. 

Assistance much appreciated"
Is it possible to obtain standard errors for a reference category in multiple regression?,5,5,False,False,False,statistics,1485896443,True,"To elaborate, say you're doing logistic regression on some variable Y, with age and gender as your 2 explanatory variables. Gender is categorical (M/F in this setup), and age is also categorical (<10, 10-20, >20). 

If you run the model in R you will get coefficient and standard error estimates for F and the categories 10-20, and >20. Where the reference categories M and <10 are essentially sucked up in to the intercept term.

Say I wanted to know the standard error for the <10 category, is that possible? (think I'm being dumb, but brain is dead today)

Thanks for any help!"
Interesting correlation.,3,0,False,False,False,statistics,1485900627,True,"We are doing a project in my statistics class and we have to come up with two things to compare and see if there is any correlation. I dont wanna do anything boring, so just wondering if you guys had some idea about interesting variables to compare. "
How many road signs are there in the USA? How many do you guesstimate at least?,8,2,False,False,False,statistics,1485900673,True,"I mean everything. Small, big, speed limit, mile marker, yeild, reflective turn sing, street name, info sign, etc.

 In all of the 50 U.S states, how many?"
Why is the US News best graduate school in statistics listing so broken?,5,2,False,False,False,statistics,1485901280,True,"http://grad-schools.usnews.rankingsandreviews.com/best-graduate-schools/top-science-schools/statistics-rankings

TL;DR
UC Berkely, UM Ann Arbor, U of Washington, UNC all appear twice in the the top 25.

What's the real top 10?"
Introduction to Correlation,0,2,False,False,False,statistics,1485908315,False,
Question About (what I think is called) Reverse Causality,1,2,False,False,False,statistics,1485910323,True,"Sorry if this isn't the right place for this question. If it's not I'd love to know where I should submit it.

I was messing around with R when I came on a result I don't understand. I basically created an independent variable that was caused by the dependent variable instead of the other way around. What surprised me was that when I included it in my regression it affected all the other coefficients. Omitted variable or collider bias only affect the variables related to the bias. Here all the variables were affected. Is this correct? What is this sort of bias or problem called?

Here is the R code I used:

N<-10000

x <- rnorm(N)

w <- rnorm(N)

k <- rnorm(N)

y <- .8 * x + .4 * w + .6 * k + rnorm(N)

z <- .8 * y + rnorm(N)

summary(lm(y ~ x + w + k))

summary(lm(y ~ z + x + w + k))"
When the bootstrap doesn’t work,3,37,False,False,False,statistics,1485916200,False,
Need help with discrete and continuous,2,1,False,False,False,statistics,1485916220,True,[deleted]
Question about Confidence Levels.,10,10,False,False,False,statistics,1485920483,True,"Hey guys, I'm in AP statistics right now and I'm really having trouble with two things.

What are the implications of a confidence level? We learned how to find a confidence level and how to find an interval from a confidence level, but what does that tell me besides the proportion of standard deviations that I used to calculate my interval? What does it actually mean in practical terms?

Also how can I differentiate clearly when interpreting a confidence interval and a confidence level?

Thanks."
The necessary steps before running a GEE in Stata,0,1,False,False,False,statistics,1485925611,True,"I'm working with a data set in Stata that spans over 3 years. I paneled the data using `xtset` and I was wondering if I needed to do anything else before running a `xtgee`? I really wasn't understanding the [IDRE explanation](http://www.ats.ucla.edu/stat/stata/library/gee.htm), so if someone can help me out, that would be great.

I just want to make sure that the results I got for the `xtgee`I ran are valid in order to compare the results of my robust random and fixed effects regressions controlling for each year using `i.year`."
Will Trump Kill Statistician's Jobs,1,0,False,False,False,statistics,1485926518,False,
Help - Hockey Stats - How to calculate odds of winning - More info in comments,1,0,False,False,False,statistics,1485931192,False,
I need help using personal fitness data to predict a reasonable goal date.,2,6,False,False,False,statistics,1485933934,True,"I've been collecting very detailed data on my weight, calorie, macronutrient intake, and more for over a year using My Fitness Pal. I've been trying to gain muscle lately, and have found that I seem to be gaining weight at a rate that's not quite linear; as I expected, growth seems to be slowing down over time. I'm not sure what tools I can use to predict when I will reach my goal weight, but I'm sure that it can be done fairly easily.

Additionally, I'm very interested in finding what the most and least significant factors have been in my progress (e.g. daily grams of protein, calories per day, workout frequency, hours of sleep). I understand this could be more difficult.

I have very little experience with statistics. I'm in Calc 1 at the moment, but math has never been my strongest subject. What I can offer is a willingness to learn. I'm more than willing to look into resources which could help me figure this out for myself. 

Thanks in advance for whatever help you can offer!"
Online Biostatistics Help Service from Statisticshelpdesk.com Is a Savior for Students,0,1,False,False,False,statistics,1485936322,False,
What are the odds I'll get self-consistent experimental data by chance?,6,1,False,False,False,statistics,1485951163,True,"I'm testing batteries. When I test a battery I get a voltage reading of between 2.00 and 2.50 volts.

Pretend I test, say, 5 batteries. What are the odds that I'll get 3 voltage values that are in +- 0.1 volts of each other just by chance?

The real life scenario is that we build and test the same battery 5 times and measure the voltage. If the values of 3-4 are close enough to each other we say that the battery can be built reproducibly and therefore our measurement is accurate.

However I'm worried that we're hitting that threshold by chance every now and then. How often\likely is this?"
Dice averages.,5,3,False,False,False,statistics,1485956776,True,"I am a big gamer and I play everything: D&D, board games, Warhammer etc... I have noticed a trend in most of them. When a game says you can take the average on a dice roll it is almost always higher than the median. Like  4 on a D6 a 5 on a D8 and a 11 on a D20. A lot of games have a tendency to round against the players witch makes this seem odd to me, so I was curious if this is just an industry standard to round up or if the actually average result of dice rolls tended to be higher than they would be theoretically. "
[Question] Two level comparison test?,7,2,False,False,False,statistics,1485963243,True,"Hello guys,

I'm doing a study that requires a statistical treatment that I'm not used to and therefore am not sure how to proceed.

Imagine you want to compare differences between a group, with those of another group. For example you want to compare the difference of height between males and females from 2 different countries.

Your sample consists of a column with their height, gender (0 or 1) and country (0 or 1). Is there any specific test that I should be aware of?

Thanks in advance."
The high-tech war on science fraud (The Guardian),9,29,False,False,False,statistics,1485965123,False,
How to calculate species diversity based on line-intercept data,0,1,False,False,False,statistics,1485972780,True,"I am designing protocols for an approach to measure plant communities using the Line-Intercept method as described here: https://www.fs.fed.us/rm/pubs/rmrs_gtr164/rmrs_gtr164_11_line_inter.pdf


Essentially, we measure the length of overlap of each species above or below the transect line, which allows us to quantify percent cover of each species based on the length of its overlap in relation to the total transect length.


My question is: how would you suggest calculating changes in species diversity using this data?  I am accustomed to having actual population numbers rather than percent cover, so I'm not sure I can really use it the same way.  I did find this website which uses the Shannon-Wiener Diversity Index, but I'm not convinced this is an acceptable approach: http://www.forestry.gov.uk/pdf/HowtocalculatetheShannonIndex.pdf/$FILE/HowtocalculatetheShannonIndex.pdf.  length of the intercept vs. the total transect length (which is essentially what is described in the link)?


Would it not be better to base it off of length of intercept of target species vs. the cumulative **intercept** length of all species along the whole transect, rather than target species intercept vs. total **transect** length?"
[Question] Books advice as follow-up,8,5,False,False,False,statistics,1485973127,True,"Hello everyone!

I'm currently a Medical Student and I am collaborating with the Biostatistics and Epidemiology department at my faculty, mainly as a consultant for other physicians' research but I really want to start doing my own research in Biostatistics connected to Medical Practice as it is an area I find absolutely fascinating (thinking about doing a MS in Biostatistics when I finish residency) and I feel I should dive deeper into more knowledge about Statistics to do a better job and avoid many of the mistakes most MDs do regarding statistics.


Thus, I would like to ask this reddit about some books that would complement and deepen those I have already read:  

* Discovering Statistics with R - Andy Field  
* Introductory Statistics - Daalgard  
* Clinical Prediction Models - Steyerberg  
* Data Analysis Using Regression by Gelman and Hill  
* Statistics Done Wrong - Alex Reinheart  

Regarding my background, I program fluently with R and Python but I only had math up to Single Variable Calculus and never had Linear Algebra so I would prefer books that use a more ""computational"" approach or don't have steep mathematical requisites such as Casella & Berger and the such :) (I promise to learn MV Calculus and LA during summer break but until then I have no time :P)

Many thanks everyone!
"
Any good resource for planning multistage sampling?,1,1,False,False,False,statistics,1485975756,True,"Briefly, I work for my country's Ministry of Health and my boss just asked me to select a nationally representative sample of health establishments to conduct supervision on.

I understand basic probability well and know how to do simple sample size calculations. I'm guessing each major political subdivision would be a strata and then I'd need to select clusters within each stratum. Not sure how to allocate sampling probabilities or account for design effect at this stage though."
How does an actuary determine the probability of an event?,3,2,False,False,False,statistics,1485986552,True,"As an example AppleCare+ insurance will replace a ""totaled"" phone up to 2 times in a two year period.  They will also repair a broken screen two times during that period.  Naturally there are deductibles. 

For each type of ""event"", one can either file 0, 1, or 2 claims.  If one knows the likelihood of all the events, you could calculate the expected value (or cost) of the insurance vs self insuring. 

So how does an actuary determine the probability of all 6 outcomes?"
IV and DV?,0,1,False,False,False,statistics,1485998877,True,[removed]
IV DV question,0,1,False,False,False,statistics,1485999877,True,[removed]
Here are my mouse and keyboard stats for a 22 day period from January 10th - February 1st. I think I'm spending a bit too much time using my computer.,0,1,False,False,False,statistics,1486006002,False,[deleted]
Statistics Homework Help,0,0,False,False,False,statistics,1486017239,False,
When is estimating the transition probability matrix of a system with unknown states impossible?,4,2,False,False,False,statistics,1486030012,True,"Really out of my element here, so excuse the ignorance.

Assume I have a time series with an unknown number of states. 

Wouldn't estimating the TPM become more accurate over time as more states occurred? So couldn't I just collect data over a very long period of time?

Are hidden markov models used at all to estimate the TPM in this sort of situation?"
How Online Statistical Simulation Assignment Help May Help Students,0,1,False,False,False,statistics,1486033193,False,
Economicshelpdesk.com Offers Best Online Help with Statistics Homework Can Help a Student,0,1,False,False,False,statistics,1486034315,False,
Online Biostatistics Help Service from Statisticshelpdesk.com Is a Savior for Students,0,1,False,False,False,statistics,1486036424,False,
Statisticshelpdesk.com Offers Online Statistics Exam Help Service,0,1,False,False,False,statistics,1486038827,False,
Statistics teacher with Standard Deviation question,27,4,False,False,False,statistics,1486043906,True,"Good morning,

I taught a lesson today in my senior high school class about standard deviation. The context was the variation in test scores in a class. Suppose that the data were 80,90,70,60,100 so then the mean score is 80. 

A student asked me a question that I could not answer intelligently. When calculating deviations from the mean, we get 0,10,-10,-20, and 20. Calculating the population standard deviation, we got 14.1. The student asked why we couldn't simply make the deviations all positive (so 0,10,10,20,20) and then divide that by the population size of 5 (so we would get 60/5 = 12). Can anyone shed light on why this is not the way that it's done? Thanks!"
"Does X~B(∞, 0.5) have the exact same shape as a normal distribution?",6,1,False,False,False,statistics,1486055629,True,"Assuming we squish the binomial distribution so closely that it looked like it was continuous, would it tend towards the exact same shape as a normal distribution?"
"AMA with Chris Stevens from Quandl over at /r/datasets on dataset curation, gathering and analysis",1,2,False,False,False,statistics,1486059935,False,
is there a new version of the 'nortest' package for R?,3,2,False,False,False,statistics,1486061032,True,"I'm trying to do some normality tests, like the Anderson-Darling test, but my (latest) version of R doesn't seem to support the 'nortest' package that all the resources I can find keep pointing towards. Any advice?

This is what I'm referring to: https://cran.r-project.org/web/packages/nortest/nortest.pdf"
Unlearning descriptive statistics,5,12,False,False,False,statistics,1486061225,False,
Python vs. R vs. Matlab,52,53,False,False,False,statistics,1486062919,True,"I'm a graduate student and will have to work on genetic sequencing and bioinformatics analysis of Melanoma tumors for a few years. I need to choose what program I want to do all my statistical analysis on from the beginning and stick with it. I'm very new to statistics software and am wondering what the main differences (advantages/disadvantages) between Python, R, and Matlab are so I can choose the most appropriate platform to use. Does anybody have advice on these 3 platforms?

Edit: Wow, thank you everybody for your feedback!"
"What scenarios would median, mean, and mode be best to used to find an average?",8,7,False,False,False,statistics,1486063717,True,
"[Question] When comparing 2 regression models, which results should I give preference to: those from the F-test or the uncertainty on the fitted parameters?",0,1,False,False,False,statistics,1486065909,True,[removed]
How do you calculate expect values using two predicted probabilities,0,2,False,False,False,statistics,1486066576,True,[deleted]
Logistic Regression from scratch in Python,3,1,False,False,False,statistics,1486067614,True,"I'm trying to create logistic regression from scratch using Python. Specifically I am trying to implement this algorithm: http://komarix.org/ac/papers/thesis/thesis_html/node19.html

My code is below and it returns ""LinAlgError: Singular matrix"" after several iterations. Anyone spot my error?

    from sklearn.datasets import load_breast_cancer
    data = load_breast_cancer()
    X = data.data
    Intercept = np.array([1]*X.shape[0]).reshape((X.shape[0], 1))
    X = np.append(Intercept, X, 1)
    Y = data.target

    def InverseLogit(x):
        return np.exp(x)/(1.0+np.exp(x))

    def LogisticRegression(X, Y, tolerance):
        B = np.zeros(X.shape[1])
        X_transpose = X.transpose()
        while True:
            eta = np.sum(X*B, axis = 1)
            mu  = InverseLogit(eta)
            W = np.diag([mu_i*(1.0-mu_i) for mu_i in mu])
            Z = np.dot(X, B) + np.dot(np.linalg.inv(W), Y-mu)
        
            Part1 = np.linalg.inv(np.dot(np.dot(X_transpose, W), X))
            Part2 = np.dot(np.dot(X_transpose, W), Z)
            B_new = np.dot(Part1, Part2)
        
            if abs(np.sum(B-B_new)) < tolerance:
                return B_new
            else:
                B = B_new
    B = LogisticRegression(X, Y, tolerance = 10**-6)
"
"Best software for intro level Stats course (Mathematica, etc)?",1,1,False,False,False,statistics,1486083025,True,[removed]
Bayes' Rule Question,0,1,False,False,False,statistics,1486084667,True,[removed]
Hypothesis testing - How do I know I am looking at sample data?,11,0,False,False,False,statistics,1486087777,True,"I understand that hypothesis tests are used to infer population parameters from sample statistics, but I am having difficulty understanding whether the data I am looking at is indeed sample data. My textbook uses the example of flipping a coin 100 times and asks whether 52 heads is sufficient to show that the coin is bias. 

However, if I have flipped a coin 100 times, aren't the 100 results the population as there were only 100 coin flips? Or is the population the potential infinite times I could have flipped the coin?

Alternatively, if two basketball teams played each other 10 times the past year, and one team wins 6 times would this still be sample data? If so, why? I 

Please help me understand this :(. Thanks!"
Some Good Reading Recommendations,2,1,False,False,False,statistics,1486099189,True,"I am a stats grad student and am in the mood for a recently published book. Lots of the books I have used in class lately are > 15 years old. Does anyone have suggestions on quality advanced stats books that are <5 years old? I like survival analysis, longitudinal data, one of my favorite books is ""In all Likelihood"". "
Economicshelpdesk.com Offers Best Statistics Assignment Help,0,1,False,False,False,statistics,1486103816,False,
"Is there a way to reverse a net promoter score, in order to find out the likely ratio of promoters, passive, and detractors?",2,2,False,False,False,statistics,1486107775,True,"Link to explanation of what NPS means: http://www.medallia.com/net-promoter-score/

I presume since it's a type of statistic you guys might know already?

So like say I had a 60% NPS, and say that score came from 30-35 surveys, would there be a good way to gauge how many of the surveys were likely positive or negative?"
We link statisticians to clients. Launching soon. Check out our blog.,1,0,False,False,False,statistics,1486121710,False,
Creating a volcano plot in R,0,1,False,False,False,statistics,1486122448,True,[removed]
Modification of Kaplan Meier curve question,4,3,False,False,False,statistics,1486137967,True,"Kaplan Meier curves are typically used to assess time of death (or poor outcome) following a treatment. 

I have a measure I would like to substitute for (poor outcome) that can both appear and reappear. Essentially, I am asking if there is a suitable way to use a Kaplan Meier curve (or similar curve) to show when participants ""resurrect."" Any guidance?

Preferably using Prism."
95% CI for an Actual vs Predicted chart.,3,1,False,False,False,statistics,1486139988,True,"Hello all,

I've developed a model and plotted my predicted values against the actual experimental data points.

The relationship is fairly accurate with an adjusted R2 of 0.964.

The problem is, I'm not sure how to calculate the 95% confidence intervals for the ""regression""

Any help or links to look at ???"
Question: Datasets for political beliefs and voting record,4,4,False,False,False,statistics,1486142263,True,"My statistics-fu is pretty weak, but I thought of an exercise that would help me flex it, and teach me something along the way.

My hypothesis is that the political spectrum is much more diverse than simply 'left' vs 'right', but I'm curious if it all boils down to left-vs-right once you do something like principal-component-analysis.

To test this hypothesis I'm wondering if there's any datasets out there that have the features I need - i.e., a list of rows containing peoples answers to a series of columns which answer questions ranging from abortion, guns, government spending, war, etc, as well as how they voted last election.

My questions for this subreddit are:

- Is this the right way to statistically prove out what I'm looking for?
- If not, are there better ways of doing what I'm trying to do?
- Does anyone know where I can get such a dataset on the Internet?

Thanks!"
Z Score of Observed Values in Linear Regression,6,1,False,False,False,statistics,1486145525,True,"I have a collection of observations that I'm regressing over. X and Y are both one dimensional and highly correlated (R^2 > .95). Over time, outliers revert towards the line (the model is constructed at discrete intervals of a time series, and if at time t there is a significant outlier, at time t+1 it is highly likely to move closer to the regression line).

I would like to get a z score for each of the input observations. Is there an accepted method of doing so? I imagine I would treat the yhat as the population mean and the regressions standard deviation as sigma (and obviously each y as the raw observations)."
What is the state of h-likelihood?,5,9,False,False,False,statistics,1486146646,True,"One of my favorite books is Pawitan's ""In All Likelihood"". I just found out about another book he wrote with Nelder (The same Nelder that wrote the classic Generalized Linear Models). It is called ""Generalized Linear Models with Random Effects: Unified Analysis via H-likelihood"". From some of the papers it have read it seems like H-Likelihood may be [misunderstood](http://link.springer.com/article/10.1007/s11222-006-9006-7) at best. What are your thoughts on the topic?"
Correlation isn't preserved under the cdf?,2,1,False,False,False,statistics,1486167558,True,"The normally distributed variables have the correct correlation but the uniformly distributed variables from the cdf are close but not quite there. I don't understand this.

    cov = numpy.array([[1, .33, .66],
                       [.33, 1, .1],
                       [.66, .1, 1]])
    Normal = numpy.random.multivariate_normal(mean = [0]*3, cov = cov, size = 1000000)
    Uniform = scipy.stats.norm.cdf(Normal)
    scipy.stats.pearsonr(Normal[:, 0], Normal[:, 1]) ## == .330
    scipy.stats.pearsonr(Normal[:, 0], Normal[:, 2]) ## == .660
    scipy.stats.pearsonr(Normal[:, 1], Normal[:, 2]) ## == .101

    scipy.stats.pearsonr(Uniform[:, 0], Uniform[:, 1]) ## == .317
    scipy.stats.pearsonr(Uniform[:, 0], Uniform[:, 2]) ## == .642
    scipy.stats.pearsonr(Uniform[:, 1], Uniform[:, 2]) ## == .096"
"Can someone debunk this ""statistic""? How does it make sense that if 50 women out of 100,000 are raped, that eventually 1 in 4 will be?",38,22,False,False,False,statistics,1486169331,False,
What Percentage of people fall between 70-115 IQ Stats question,0,1,False,False,False,statistics,1486171393,True,[removed]
Best place to start a career.,19,1,False,False,False,statistics,1486175537,True,"Being a few months from finishing a masters in (bio)statistics I have been wondering what are good places to start a career. Is there such a thing as a Big 4 for statistics? Where are the best places to go for the money? I don't mean to sound like a ""gold digger"", but lets be honest - statistics programs equip students with analytic and programming skill. These are valuable. Where do we go to find the $$$."
Is there something wrong with this survey? I feel like the sample size is no where close enough to represent the actual population of the US.,11,7,False,False,False,statistics,1486178968,True,"Now I am terrible at statistics but I did pick up some stuff taking the class freshman year and I have a question about this survey. I am looking for unbiased answers, not trying to spark up any arguments but I saw this on r/politics and it seems odd that they have such a small sample size to represent the entire population of the country. Especially since this survey is optional and gives people the option to say no. Aren't the silent majority of people the ones that made it seem like Hilary was going to win the election? I would love to hear what you guys, who are probably a lot better at stats then me, have to say.

Tl;dr I suck at stats but this survey seems off. Too little people to answer a optional survey to represent the country. 

Link to poll :http://thehill.com/blogs/blog-briefing-room/news/317845-trump-has-lowest-approval-ratings-of-any-new-president-ever"
SAS Help,18,6,False,False,False,statistics,1486179438,True,"If anyone is currently around I would really appreciate some help. I am having trouble doing a run using previous SAS data. Basically, I'm using an example of how a data set was coded to be entered into SAS but I am not getting analysis results from the program. 

here is the code

http://pastebin.com/FXyPepMx

 From the data set in the code, I'd like to use SAS to get information such as total number of males, females, histories of cancer or chronic renal conditions, etc. Basically, tally up that raw data in the different categories. Also would like to compare between groups with infections and no infections and see within those groups how many had problems with cancer, renal disease, needed cpr...etc. "
Homework1.com Provides Best Statistics Homework Help,0,1,False,False,False,statistics,1486194267,False,
Question: Statistical method to check the effectiveness of drug dosage modifications on post-organ transplantation patient.,0,1,False,False,False,statistics,1486200543,True,[removed]
Any recommanded Statistics course?,3,9,False,False,False,statistics,1486224501,True,"I wish to study a broad statistics master. It is silly that I have a math bachelor, I could probably self-study everything, but I still need a master to progress my career (bureaucracy). While I am still in the medicine sector, I wish to include a Biostatistics course. Is there any suggested or nice, well cost-performance ratio online course suggested?

I am living in Hong Kong. While there are loads of university around, most of them does not provide a statistics course. Either it is too heavy into biostatistics that narrow my future career options (CUHK) and simply too medicine-related that it is too hard for me, or it is just a very popular course I have problem getting into it for years (HKU mstat).

I do have a low score in bachelor, perhaps that is the problem, (Second class low honour)

Any suggestion? thank you :)"
Statistics made easy..,0,1,False,False,False,statistics,1486233902,True,[removed]
What is a good introductory statistics book,31,31,False,False,False,statistics,1486246197,True,"Here is some information about my self so you can get a better idea about what exactly I'm aiming for in terms of an introduction into stats.

You can assume I am mathematically inclined. I have taken courses on multivariable calculus, introductory course to PDE's (heat equation & wave equation stuff), intro to complex variables, couple QM courses, intro to thermal physics, intro E&M, I also have some programming experience in C# and Python.

Now I am about to graduate and take a gap year working a job which will give me ample time to sit alone in an office and learn things on my own. I want to learn SQL, Python, and R. Im not too worried about the first two languages. Im actually not that worried about R in terms of learning the syntax. However I am worried about the lack of my knowledge in statistics.

So I am seeking a book that can give me a nice overview of everything stats, or as much as a newbie can understand, so that I can get the lay of the land and start to conceptualize some of the branches of statistics, some of the most useful methods etc. I thought that **daniel v schroeder thermal physics** was a good introductory text book for thermal physics, so if you know this book this is the type of pace that I would like to learn at. Any how Id just like something to give me a nice overview of the field so that I can specialize in certain branches at my discretion later on. I don't expect to find the holy grail of stats intro books or anything. Any and all input is welcome.

Thanks for taking the time to read this and help me. Please feel free to recommend any other books you may think I would find useful. 

Edit: Thanks everyone for the great suggestions, should keep me busy for a long...long time. "
Distribution of Arrival Times for Extreme Events,0,1,False,False,False,statistics,1486250201,True,[removed]
Making a two column pdf in Rmarkdown?,4,14,False,False,False,statistics,1486257810,True,"I know this isn't strictly about Stats but I want to make a two column pdf in Rmarkdown and I can't figure it out, any suggestions?
"
How do you remove some sections of a column in R?,14,5,False,False,False,statistics,1486315161,True,"I want to remove answers with a 0 in them, without just copping out and doing it in excel. Would I use an if statement for that? How would you go about this?

Let me know if this isn't enough information to judge."
Statistics made easy,0,32,False,False,False,statistics,1486322553,False,
Z value from probability(Standard Normal Dist),7,5,False,False,False,statistics,1486331213,True,"Hello,

A while ago I made a little game in python and I used the normal distribution to generate random numbers.

Now I've moved onto using unity (a game engine) which uses C# and I wanted to perform a similar trick, but there doesn't seem to be any built in functions for anything other than uniform distributions.

So I was thinking maybe I could generate a random number from 0 to 1 (uniformly) (let's call that number phi)

Then I could treat phi as if it was the area from -inf to Z in the normal distribution (as phi could range from 0 to 1 picking any real number for Z)

Then if I could just find Z I would have generated a random number which is normally distributed (I think) or at the very least it feels like it should be pretty close.

Anyway, that's lead me to my issue.
Given a phi (as I described) how would one find Z. I believe what I'm really asking is, what is the inverse CDF of the standard normal distribution. If there a formula I could use or at least an approximate formula?

- sincerely a confused programmer who dropped his stats modules by the second year of university "
Do these student class demographics make any sense?,6,2,False,False,False,statistics,1486331635,True,"Not wanting to use identifying information. Let’s say a class of 650 students are made up of :

* 24%	African-American
* 1%	American Indian
* 58%	 Asian
* 4%	Hawaiian/Pacific Islander
* 27%	Hispanic
* 43%	White
* 11%	Other/Non Specified

Note that the numbers add up to more than 100% because some people must have picked more than one race.

Then, Administration claims that only 30% of the class is from underrepresented groups (composed of “African American, Hispanic, American Indian, Pacific Islanders”) and 70% not.

How is this possible, assuming that someone is deemed ""underrepresented"" if at least 1 of the race groups they identify as is underrepresented? 

*My thought: Wouldn't it require every single underrep student to have picked more than 1 underrep group (and even then it seems almost impossible)?*"
Question about sequential Bayesian updates (does the order of updates matter?),5,5,False,False,False,statistics,1486333143,True,"Here is the problem setup:

Say Y_i ~ Normal(theta, S_i^2 ), and the prior distribution is for theta ~ Normal(u_0, sigma_0^2). Where S_i^2 uses the sample variance from the data from trial i.

Initially, we have u_0 = 0, sigma_0^2 =0, and we obtain a posterior theta|Y_1 where Y_1 has its own sample mean and sample variance (which are plugged into the update).

The mean and variance obtained for the posterior distribution are then used as a prior distribution for the next data set Y_2, and a new posterior is calculated. I want to repeat this process with 3 more datasets, each with their own sample variance which is used for S_i^2.

Does the order in which I do the updates matter? According to the wiki page there seems to be something about exchangeability - https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling#Exchangeability, but would that not be applicable here since the variance being used (the sample variance) is changing each time?


New to this, so any help is appreciated! Thanks"
Estimation of Intervention Effect on Categorical Outcome: Two-Group Comparison,9,8,False,False,False,statistics,1486334018,True,"I have been tasked to evaluate a prison program on recidivism (re-arrest) for prison inmates. It is a quasi-experimental design, and I plan to address self-selection bias with propensity score matching. The dataset is quite large (~18,000) but may be significantly reduced after cleaning. The program has additionally been implemented in multiple prison facilities, but I am unaware if the data I will receive indicates the facility an offender took the program. 

I also plan to conduct a secondary analysis to model the program effect on institutional behavior (continuous dependent variable). I'm told they have institutional behavior data for both 1-year and 2-years before and after the program. This data should also have a comparison group of offenders who have not been exposed to the program.

I'm struggling to decide which is the best approach to evaluating the program. I'm leaning towards a mixed effects model to estimate an effect of the program on recidivism between intervention/experimental group. However, I have minimal experience with mulitlevel and mixed effects models so am looking for advice and guidance. 

I am also considering using a mixed effects model to measure program effect on institutional behavior between each group, and I  think mixed effect models can utilize the pre-test (before program) and post-test (after program) data too. 

I appreciate any suggestions!"
Benford's law and coefficient reporting,4,3,False,False,False,statistics,1486336729,True,"A novel approach to reviewing reporting biases in the journals could be to use Bedford's law. That is, lower numbers have a higher chance of naturally occurring. Does anyone see a usefulness in using something like Benford's law to check across large quantities of journal data reporting and see if Benford's law could apply? For example, do coefficients leading with a 1 really show up 30.1% of the time?"
Get in Touch with Our Tutors Who Will Do Your Statistics Assignments,0,1,False,False,False,statistics,1486363315,False,
What information is more useful for predicting the future?,1,0,False,False,False,statistics,1486364288,True,"If I randomly generate a list of 350 integers that range from 1-20, which information is more beneficial for guessing the next number? The mode of the whole list, or the mode of the last 50 integers?"
Statistical fractions question,4,3,False,False,False,statistics,1486391736,True,"I often see statistics that use a fraction that isn't reduced to its lowest common denominator. 8 out of 10 instead of 4 out of 5. Is there a statistical reason for this or does it just sound better? I'm sorry if this sounds like a silly question, but I've wondered about it for years."
How do you decide which classification ML method to use on Test set?,2,5,False,False,False,statistics,1486397362,True,[deleted]
Factor analysis vs Bayesian Networks,10,15,False,False,False,statistics,1486399026,True,"My data is a very wide table with attributes, locations, behaviours, opinions of a set of users (distilled as summaries from many other tables).

My goals include: 
(1) understanding the structure of my data set (what correlates with what? what are the latent factors?) 
(2) making meaningful segments of users (likely based on these factors) 
(3) prediction of certain columns

For handling (1), there seems to be a strong parallel between FA and BN: they both provide for models in which items are influenced by unobserved or latent variables.

How would we characterise these two approaches, their pros and cons, and how would we choose between them? Any pointers welcome.

My current understanding is: 

* BNs inherently handle missing data well, not clear how to handle that with FA 
* EFA allows determination of the number of latent factors 
* BN structure learning allows automatic creation of structure using existing variables, but if you want to include latent variables in BN you need to do that manually (like in LDA)
* BN makes prediction of items easy; I can imagine using factors to estimate missing items but I have not yet seen that method documented. (It seems more correct to me to use PCA for prediction than rotated/simplified factors) 
* I expect the structures found by both methods, may be somewhat related

Would it makes sense to use the two in concert? Ie. use EFA to determine latent factors, then add those to the BN structure and proceed to estimate distributions in the BN.
"
Good sources for time series math/proofs?,3,13,False,False,False,statistics,1486404387,True,I'm looking for good books or online sources that can break down the math and proofs behind time series analysis. 
Logistic regression vs Neural Networks,21,26,False,False,False,statistics,1486414205,True,"Neural networks/MLPs (especially with sigmoid activation) look a lot like stacked logistic regressions.  (Of course with different training methods).

Now LR is very interpretable, but can only model monotonic functions., where NNs/MLP are generally opaque, but can model arbitrary functions.

Is there anything in the middle between these extremes?  Like a 2-layer logistic regression?  

Ideally I would like some degree of interpretability, as well as ability to model some (even limited) amount of non-linear/non-monotonicity.
"
"I'm planning to get a Masters degree in Statistics, but...",27,12,False,False,False,statistics,1486415175,True,"I’m planning on going to grad school for a master’s in applied statistics; however, I’m getting second thoughts about it. My main issue I’m concerned with is failing the program and throwing a bunch of money down the drain. I have a degree in sociology and a minor in Mathematics. For the mathematics minor, I have done the basic requirements: Calculus I, II, III, Differential Equations, Linear Algebra, Probability and Statistics, Numerical Analysis. It was hard for me, but I dedicated a lot of time to learning. I got all As in my math classes. I remember studying 6-7 hours a day for Calculus III. This was pretty much routine for most of my math classes. However, I feel like I spend more time learning how to do the problems rather than learning the material. This is why I don’t feel confident pursuing a master’s degree in applied statistics. 


In sociology, I took some research classes, but I felt that they were a joke. 
I need to get a master’s in statistics so that I can qualify for more interesting and higher paying jobs, such as data analyst. (that is like my dream job). I don’t want to live my whole life making 20,000 dollars a year. 


Anyways, for the master’s in applied statistics program, I have to take 30 credits (10 courses). Here are the requirements:
Required Courses: (15 credits)

563 Regression Analysis 

582 Introduction to Methods and Theory of Probability 

583 Methods of Statistical Inference 

586 Interpretation of Data I 

590 Design of Experiments 

Electives: (pick 5 classes)

540-541 Quality Control I and II 

542 Life Data Analysis 

545 Statistical Practice 

553 Categorical Data Analysis 

554 Applied Stochastic Processes 

555 Nonparametric Statistics 

565 Applied Time Series Analysis 

567 Applied Multivariate Analysis 

575 Acceptance Sampling Theory 

576 Survey Sampling 

584-585 Biostatistics I and II 

587 Interpretation of Data II 

588 Data Mining 

591 Advanced Design of Experiments 

595 Intermediate Probability 

I don’t know if my minimal math background is enough preparation for these courses. Mainly, the high expectations of grad school (less than a B grade is a fail, thesis, comprehensive statistics test for graduation) are worrying me. I’m worried that I will fail. I don’t remember much from what I’ve learned in my math classes. Anyways, I just need some advice about my situation. How hard will graduate school be for someone like me?
"
Help with Statistics and probability?,5,0,False,False,False,statistics,1486415401,True,[deleted]
Can't quite wrap my brain around how I would go about solving this problem,5,7,False,False,False,statistics,1486420140,True,"Hey! 

So I have some basic knowledge on statistics/ probability theory, have followed an entry level course before, but it hasn't provided me with enough knowledge to solve a problem I have.

My question is as follows: 
Imagine I have 100 plastic coins. If I flip one and it lands on heads, I am given 55 dollars. If it lands on tails I throw it away. I keep going untill all my plastic coins are converted to cash. The average return would obviously be 5500 dollars, but what is the variance and how large is 1 standard deviation? And how do I calculate this? 

Thanks in advance for any help or insight!"
"Expected value: predicted probability of event X predicted ""size"" of event?",2,1,False,False,False,statistics,1486431201,True,"Suppose you have some client data and you're trying to figure out towards which clients you should devote your business development/marketing dollars.

The way I'm thinking about approaching this problem:

Build two models:

1) Predict the *likelihood* your client buys one of your products again.
2) Predict, based on how much the client has spent previously, the *size* of that purchase.

Multiplying the predicted probability of buying again X the predicted size of the purchase should give me effectively the ""value"" of a client, right?

When I've dealt with expected value problems in the past, it's usually been with an actual probability X an actual ""size"" of the event. Like your customers *actually* buy again 50% of the time, and the one product they buy costs $5, so your expected value is $2.50. 

IS the interpretation of this predicted-predicted EV problem different?"
Is my approach to this question right?,5,2,False,False,False,statistics,1486452977,True,"Hi guys!

I'm trying to perform some exploratory data analysis on a dataset I've collected in the last few days (and which, hopefully, will increase from 40 records to 200+ in the next week). Nothing too fancy - some lab results from some subjects, their age and whether or not they have significant diseases (dichotomous variable, with either 0 or 1).

As I'd like to examine whether there are significant differences in the population, I'd start with order() and dplyr::ntile in order to divide the population in quartiles; then, after assessing whether assumptions aren't violated, I'd perform ANOVA and some post-hoc test in order to examine if there are any significant differences between groups.

Is that right? I feel like I'm missing something. It's my first ""go on, and play by your own"" match with statistics and so I'm not quite sure I'm doing everything right.

Thanks a lot!

**EDIT**: What I'm looking for is the presence/absence of any relationship between two of the numeric variables and if there is any difference in regards to this association in different age groups (or by age). "
"Trust Us with Your Statistics Assignment, We Never Disappoint",0,1,False,False,False,statistics,1486460857,False,
New Statistics about the Safety of British Roads,0,0,False,False,False,statistics,1486475751,False,
Bayesian Linear Regression w/ pymc3,0,1,False,False,False,statistics,1486476448,False,
Considering Stat or Applied Math PhD,2,3,False,False,False,statistics,1486477718,True,"Hey everybody, I am currently a junior and am trying to figure out what to do after graduation.  Any advice from people who were in similar situations would be great!

So I am a stats major and math minor at a large public school with a strong stats program.  I've done pretty well, about a 3.6 overall gpa and 3.8 in stat classes.  The math classes I've taken are calc 2, 3, linear algebra, intro-proof writing, analysis, and game theory.  I know both R and SAS, as well as Latex (idk if this matters at all hah).

I worked on an undergraduate research project this fall, building a pretty extensive shiny application that scraped and analyzed data on tennis players over their careers.  I also will have had two ""data science"" internships at a software company under my resume.

I plan to take the GRE and subject tests, if needed, this summer.

My general questions are:
* Does my resume indicate I could succeed in a PhD program?
* What other things should I consider when deciding whether or not to apply?
* And just any other information/advice would be awesome!"
covariance of x and subset of x,16,2,False,False,False,statistics,1486481286,True,"say x1 is a subset of x then is the cov(x,x1) = variance(x1)? haha I'm having a hard time here"
A modern approach to learning statistics,0,0,False,False,False,statistics,1486481473,False,
Simulating a ranking/draft system in R,0,1,False,False,False,statistics,1486483586,False,
Can the upper limit of a confidence interval be bigger than 100%?,4,1,False,False,False,statistics,1486484195,True,"I'm not really sure if this is the right place to ask this, but maybe somebody can help me here.
I'm trying to calculate a 95%-confidence interval for a very high prevalence in a rather small study population.
This is the way I calculated it:

n=86 

r(prevalence) : 0.989

I used this formula for a 95%-CI in a Bernoulli distribution : p1/2 = r+-1.96sqrt((r(1-r)/(1-n)). Hacking all this into Wolfram alpha and I get a 95%-CI from 0.9668-1.0112 ....
What do I do with this information? A value >1 doesn't make sense logically (there can't be a prevalence higher than one). Even a value of 1 would be streching it in my opinion, because if we assume 1 would be correct (which I do assume within a 95% possibility), than I could never have fould the one case that was not affected.
Do I regard this as mathmatical quirk and just state the CI as [96.68%-100%] ?"
RIP Hans Rosling - a tribute to the legend,16,277,False,False,False,statistics,1486484988,False,
Number and interval of panel waves for causal inference,0,1,False,False,False,statistics,1486486585,True,"Hi there, 

a friend of mine got some feedback from her doctoral advisor, which led me to  reassess my views on how many panel waves are needed (and how we deal with time intervals) for somewhat valid causal analysis in social sciences.
Disclaimer: We're sociologists with an emphasis on quantitative  methods. That means that we work with different regression models for panel data (myself favoring the SEM approach) and are able to deal with various regression equations and their assumptions...but more serious statistics/maths stuff (e. g. proofs) tend to go over our heads. Please bear with us.

That said, her advisor told us about Vaisey/Miles paper “What You Can—and Can’t—Do With Three-Wave Panel Data”  [http://journals.sagepub.com/doi/full/10.1177/0049124114547769] which raises various points about panel analysis, one of which is the handling of causal order. If I’m understanding correctly, they state that falsely assumed temporal ordering of causal effects (i. e. using lags of independent variables) can heavily bias the effects of a model. This would be the case if the true causal effect of the observed relationship is contemporaneous instead of temporally delayed. 

The thing is: The panel surveys we use usually collect data in one or two year intervals. This virtually leaves us little choice between contemporaneous effects, which are modeled via correlation of the variables (aside from some special SEM) or temporally anteceding causes, which would be modeled with lags of variables (e. g. y_it =μ + ρy_t−1 + x′_it β + z′_i γ + ε_it     , t=2.  --or other forms.)

So, my questions are:

1) Apart from Vaisey/Miles I have mainly read about this topic from a philosophical or theoretical viewpoint (nearly everyone who wrote about causality touches the topic of temporal precedence/order). What are your thoughts on the relevance of time intervals in panel studies? Do you know of discussions which deal with the practical implications in empirical analysis?

My impression in social sciences here is, that not even all people are aware that contemporaneous variables aren’t the best setting to determine causal effects… Other than that I never really considered looking further into the nature of the time intervals of my data, because a) I can’t change the study design in most cases and b) I felt temporal precedence is kind of enough (in conjunction with other methods to deal with unobserved heterogeneity, endogeneity etc.). 

2) The data set we’re currently using consists of only two waves. The advisor raised the question, if an additional wave would really increase our options in data analysis. I thought he was joking – three waves allow for model designs using differences of variables, more robust effect estimates and improve our options in modeling mediation/moderation models (which we were doing). Am I missing something here?

Thanks for your input!
"
What is being a statistician like?,6,1,False,False,False,statistics,1486487018,True,"So I have a BA in theoretical physics, master's degree in pure mathematics (modern analysis mostly) and I've also done pedagogical studies. I graduated two years ago and have been teaching in middle schools mostly. I hate it and need to find some new line of work outside education and universities. I've understood that my studies so far have really no practical value as I know no programming and I've taken practically no courses in applied mathematics.

I'm thinking about going back to study a bit but preferably the minimum amount needed to get a secure job. So my question is what is being a statistician like? Is it super demanding or can I find a low stress job with a decent pay? (I've had a burnout so can't handle too much stress). Is it mega theoretical or more practical? How much programming should I learn? 

I've also been thinking about programming but all other ideas and advice is appreciated too."
What is an oracle inequality?,1,2,False,False,False,statistics,1486491752,True,
Why should *hard* be secure enough? Information and non-invertibility,0,1,False,False,False,statistics,1486495400,False,
What is a simple way to analyze how I'm studying for exams and what is paying off versus not paying off?,3,0,False,False,False,statistics,1486495441,True,"I'm in medical school and the volume of information to study is too much to go through everything for a particular exam block more than once (certain important things, I'll go over multiple times, but it's not feasible to review *everything* multiple times). 

I've started keeping track of how I'm studying in a really rudimentary sense. I made a spreadsheet and listed each lecture in a particular exam block and kept track of whether or not I pre-read for that lecture, attended the lecture, reviewed it immediately after class, and finally, I have a few columns where I key in the date for subsequent reviews of that lecture.

My exams are also returned with a breakdown with how many questions were asked by each lecturer and also how many questions per topic. They also provide me with stats on how I performed on that topic/lecturer's questions in comparison to the class.

What I found was that I tend to do well above average for all lecturers but two (we have about 8 for this exam block), but I do so poorly for those two that my score is being brought down to the average score. 

What I'm wondering is: what are the best methods to keep track of how/what I'm studying and relate it back to the exam stats I'm given. It needs to be something I can easily do in a few seconds while I'm studying. I can't run a statistical analysis every time I move to a new lecture.

TLDR: Looking for the best way to analyze how/what I'm studying and relate it to exam statistics so I can optimize how I improve for the most bang for my buck."
"I want to analyze a bunch of categorical variables, how should I proceed? This is my first ""practice"" consulting project and I'm at a loss. I would love your inputs!",2,1,False,False,False,statistics,1486497082,True,"Hey guys,

I want to analyze if variable A, perceived quality of care by hospitals, is affected by prenatal visits to the hospital. I also want to see if perceived quality of care by hospitals affects a mother's decision to give birth at the hospital, rather than at home. Basically, if mothers think the hospital is competent at checking up on their yet-to-be-born baby, will they be more inclined to visit / keep visiting hospitals and receive prenatal care / give birth there?

These moms are given a survey and asked to rate the quality of care, categorical with five choices. They're also asked to rate the quality of giving birth at home, which is more traditional in the country this study was done in.

I also want to see if distance from these hospitals plays a factor. Distance is defined as the time it takes to walk to the hospital. It's split up into a bunch of categories with 5 minute windows, so I'm assuming this makes it categorical as well.

Prenatal care is defined by visits, ranging from 0 to 10. In this study, we think of 4 or more visits to be good and the minimum number of visits we want mothers to make. Less than 4 is not ideal. 

We're trying to see if after visiting a hospital for care 4+ times, mothers have higher perceived quality of care from the hospitals. We also want to see if the visits get the moms to give birth at hospitals rather than traditionally, at home.

This is a cross-sectional analysis.


I'm thinking I can just perform some sort of categorical data analysis, but I'm not really too sure here. Is this too simple and straightforward? Do you guys have any suggestions on how to start? There's a lot of data involved, such as specific walk times depending on what village mothers are from and stuff. A lot of it seems kind of irrelevant, but I'm just at a loss on how to start my analysis.

Oh, and any suggestions on how to handle missing data? There's not a lot - some subjects are missing survey answers in one place or another, such as forgetting to fill in how long it takes to walk to the hospital, how many visits they made, etc.


Thanks!"
Accuracy of Standard Deviation,3,2,False,False,False,statistics,1486501171,True,"I wanted to get confirmation of something that I believe to be true based on a statistics class I last took...well too many years ago to count.

I am engaging in a discussion with someone who is comparing 2 data points and calculating the standard deviation of those points.  It is my understanding that because of the low number of data points, that the standard deviation doesn't mean much.

The data points are 119.3 and 124.3.  The mean was calculated as 121.8 with a standard deviation of 3.5.

Is my understanding correct?"
MS in Statistics - best electives for a career in data science?,11,4,False,False,False,statistics,1486504438,True,"Hi guys,

I'm currently a graduate student in a MS in Statistics program and I want to eventually work as a data scientist after I graduate (currently fluent in R, Python and SQL.) Looking at my college's course website, I wasn't sure which elective classes to take that would best supplement a data science career. The courses are:

* Multivariate Methods
* Categorical Data Analysis
* Design of Experiments
* Operations Research
* Mathematical Modeling
* Statistics Quality Control
* Time Series and Forecasting
* Survival Data Analysis
* Clinical Trials
* Nonparametric Statistics
* Sampling Methods
* Stochastic Processes
* Bayesian Statistics
* Data Mining and Analytics

I'm strongly leaning towards Data Mining and Analytics as well as Bayesian Statistics because I hear that is a fundamental part of machine learning. Any other recommendations?

"
Analysing multivariate model when i have switched the independent and dependent variables round.,0,1,False,False,False,statistics,1486504536,True,[removed]
How to Compare Drop Rate between Two Sets of Line Graphs,2,1,False,False,False,statistics,1486518094,False,[deleted]
Can you recommend a good book on probability distributions and examples?,4,3,False,False,False,statistics,1486519360,True,"I'm doing a self-study in statistics and would like a good book on how to deduce the appropriate distribution given a modelling task. I see excellent questions on this sub all the time about how to model particular chains of events, and I realize I need more experience in enumerating the event space and modelling stochastic processes. Any good reccommendations?"
Does anyone knows minitab?,1,0,False,False,False,statistics,1486520386,True,I need help with a task for my statistics 2 class.  Could someone help me out? 
interpreting multivariate regression when i have switched the dependent and independent variables,0,1,False,False,False,statistics,1486550590,True,[removed]
"We Will Sort you out with Your Statistics Online Quiz, Trust Us",0,1,False,False,False,statistics,1486552677,False,
Can someone give me a solid reference for the minimum sample size required for an online survey in order to use it quantitatively?,7,2,False,False,False,statistics,1486556007,True,"Alternatively, a solid reference for acceptable confidence intervals and margins of error would also help. There are many online calculators to calculate required sample size at a predetermined CI and MoE, but I cannot see where the recommendations of 90-99% CI and MoE of 5% come from."
interpreting multivariate model when I have 'switched' the dependent and independent variables around.,0,1,False,False,False,statistics,1486567910,True,[removed]
References for Going Back to Statistics,0,1,False,False,False,statistics,1486572888,True,[removed]
Way of tackling this questions with statistical tests?,6,1,False,False,False,statistics,1486575298,True,Say I had two growing businesses and I wanted to see if one company was cannibalizing parts of the other business's growth.  My thought is using times series data that is evenly spaced and calculating acceleration at each period and checking correlation between the two.  Does this make sense or is there a better way of trying to do this?
"If I'm trying to analyze data about time between certain events, do the years themselves matter or just the difference between them?",6,1,False,False,False,statistics,1486578176,True,"For instance, I've got this data:

Sample 1|Sample 2
:-:|:-:
1878|1901
1894|1909
1922|1933
1925|1934
1933|1942
1937|1947
 |1956
 |1966
 |1967
 |2012


If I'm analyzing this data and trying to find the average distance between each year in each sample, I should be looking at standard deviation, right? And if this is the case, should I be more concerned about the actual number of years between these? So instead of listing 1878, 1894, etc. I should do 16, 28, etc. and *then* do descriptive statistics? "
Estimating a multivariate distribution from data,9,5,False,False,False,statistics,1486580906,True,"I have a dataset where 3 separate (but correlated) measurements are taken at every location. I'm trying to find a good way to use this information to estimate a multivariate distribution to describe the probability that a given combination of these 3 measurements will occur. 

I know that I can create a 3-parameter multivariate normal random distribution without too much trouble, but the measurements dont quite follow normal distributions so I would like something more general.

Any thoughts on how I could accomplish this?"
help: I have a list of SKUs and their corresponding qty & transaction ID; how can I start to see which SKUs tend to be bought together?,0,1,False,False,False,statistics,1486582413,True,[deleted]
Switching dependent and independent variables in multivariate regression - HELP,0,1,False,False,False,statistics,1486588477,True,[removed]
"When considering a grade 8 science fair project that measured knee extension, what graph or figure would be the best at representing a single average value based on 11 subjects?",8,1,False,False,False,statistics,1486601671,True,
Difference between Machine Learning & Statistical Modeling,7,0,False,False,False,statistics,1486601732,False,
STATS HELP,0,1,False,False,False,statistics,1486606271,True,[removed]
Suggestions for Survey Tool,0,1,False,False,False,statistics,1486615786,True,[deleted]
Looking for a survey tool,0,1,False,False,False,statistics,1486617260,True,[removed]
Trying to correlate price to quantity. Not getting usable results.,6,6,False,False,False,statistics,1486632397,True,"Hello,
As it says on the title, I'm currently working as a part time junior analyst to make ends meet for college, and I've run into a wall regarding this set of data I'm trying to analyze. I work for an entertainment company that has various attractions. I'm currently analyzing the impact of their price changes on their bowling alley but I'm not getting anything statistically usable so to speak.

To be more precise, they have gone through 3 price changes over a period of about 5 years. I was trying to find a relationship between their daily number of transactions with the price of their games at a given point of time.

I'm not really an advanced statistician, just a graduating undergrad majoring in finance with minimal statistics background. So any advice on how I can go about this would be great!
For reference I've tried using a linear regression model and ended up with a 0.07 R Square value lol."
Testing for the lack of relationship between variables,4,5,False,False,False,statistics,1486634462,True,"In the process of writing my thesis I have encountered a rather funny problem. One of my key hypotheses is that a number of independent variables are NOT related to the dependent variable after applying an interaction factor, while before that there was a relationship.
Is it sufficient that after applying the interaction the relationship between the IV's and the DV is no longer significant in order to conclude that they are not related? Or, is there a specific approach to prove a lack of relationship? 
Thanks for the responses (even though the question might appear to be a bit dumb)!
"
Program evaluation or government analytics,0,1,False,False,False,statistics,1486645061,True,[removed]
Testing for differences in population means - why?,5,6,False,False,False,statistics,1486645267,True,"So I'm trying to wrap my head around this example the lecturer used the other day in regards to ANOVA testing. 

In the first semester, all statistics students was divided into three lecture groups. Their grade means were as following:

Group 1: 3.67
Group 2: 3.13
Group 3: 3.12
(std. dev and variance was also known ofc). 

Testing whether the means differ using the ANOVA table, he couldn't reject the H0 hypothesis which was that the means did not differ. 

So here's my question: Because we tested the population means and not sample means, why can we even conclude that the means do not differ? I mean, group 1 had a mean of 3.67. How can that be anything other than 3.67, when its the entire population? I just don't understand this. 

Let me know if you need clarification. Thanks in advance!"
Analysis of a strategy,6,0,False,False,False,statistics,1486650511,True,"Suppose Moe and Larry bet on on a coin flip. Moe bets a dollar on heads and Larry bets a dollar on tails.  Unbiased coin.  So neither has a advantage.

But if Moe bets another dollar, this time on tails, it seems that Moe has an advantage.  Assume that the winnings are split when tails is the outcome.

50% of the time Moe gains $1 (he wagers 2 and get back 3)
50% of the time Moe loses $0.5 (he wagers 2 and gets back 1.5)
Moe's mean return is $0.25.

Does Moe truly have an advantage, or am I making a mistake?"
The Undoing Project by Michael Lewis - My five Lessons and Takeaways,0,1,False,False,False,statistics,1486651133,False,
What does missing data actually mean for a study?,10,7,False,False,False,statistics,1486651522,True,"I have a lot of missing data in a study that i'm running. I've accounted for this in data analysis, but what does taking all that data out actually this actually mean for the significance of my study? Eg/ does it make it less generalisable etc etc?"
Basic question: how do I find the power function for a given hypothesis test?,3,7,False,False,False,statistics,1486657238,True,"Let's say I want to test the mean of a normally distributed random variable. What is the process for finding the power function?

Also, if I were given a PDF instead of the normal distribution, how would it change this process? 

I have a test today in 4 hours so i would greatly appreciate it if someone could walk me through this.

Thanks!"
how to find a value from percentile on a histogram?,2,5,False,False,False,statistics,1486665128,True,I am given a histogram and it asks to find the approximate value at the 10th and 90th percentile.  Does this have anything to do with the 68-95-99.7 rule?  Any tips to better understand this?
Basic question on interactions between blocking (RCB) and fixed factors.,0,1,False,False,False,statistics,1486667284,True,"Don't want to get removed for asking a design question, just curious if it's appropriate when using randomized complete block to consider interactions between the block and the fixed factors."
A very simple stats question,16,7,False,False,False,statistics,1486670939,True,"The mean fuel consumption for a population is 7.3L/100km SD of 1.77L/100km. A new additive can reduce the mean to 6.205L/100km, this has no effect on SD.
What is prob that a randomly selected car using the additive will consume over 7.3L?

I said (6.205-7.2)/1.77 gives Z value of -0.62 but how could it be negative I must have done something wrong?"
"Can we all agree to use the same Exponential, Gamma and Weibull distributions?",0,1,False,False,False,statistics,1486672131,False,
ACE algorithm,0,1,False,False,False,statistics,1486676058,True,"I'm reading an article that uses the ACE algorithm, and I just want to get a better understanding of it, as the article didn't explain it in depth. 
  When can it be used? When should it not be used? 
  What are the assumptions being made?
I've tried reading the original article in which it was published (Breiman and Friedman 1985), and I can't make heads or tails of it. My impression of the ACE algorithm is that it just looks to transform all independent variables to have the best fit for regression, and, please correct me if I'm wrong, it honestly feels like fishing for relationships. If someone could please shed some light here, I'd be really appreciative. Thanks in advance!"
Where could one find general reliable statistics on the last years of the Soviet Union?,2,3,False,False,False,statistics,1486687881,True,"Edit: ""Reliable, general statistics"" makes more sense actually"
"Hans Rosling, Swedish Doctor and Pop-Star Statistician, Dies at 68",0,42,False,False,False,statistics,1486690435,False,
Expected value/SD of number of distinct outcomes,0,1,False,False,False,statistics,1486690998,True,[deleted]
Ominous Intro,3,68,False,False,False,statistics,1486693878,False,
Querying Data.gov Dataset with Python and SQL,0,3,False,False,False,statistics,1486711480,False,
SPSS p-values in hundreds of thousands,5,2,False,False,False,statistics,1486722312,True,"Hi everyone, 

I'm teaching a stats intro course. I have to give tutorials in SPSS (not my choice :(). Some of my students are getting a p-value of 143000 instead of 0.143. Does any of you know why and how to fix it? I'm not that familiar with SPSS to be honest. 

"
Trying to work out if staff survey results make sense (explaining variance),6,2,False,False,False,statistics,1486723894,True,"First, I hope this is an appropriate place to ask, feel free to redirect me if need be.:)

Our company engaged a well-known company to run an engagement survey, and some of the key takeaway points are...questionable. The main one is that the line manager variable explains the most variance in engagement scores (when compared to gender, age, nationality etc.). What my colleagues have argued (and I agree) is that when you have a survey of 3000 people with many subsamples of line managers (some people have only one subordinate) this result is pretty much inevitable. On top of that, this finding seems to hold constant across all of the consulting company's surveys.  

Would be grateful for any thoughts on this!"
Basic questions on a problem analogous to identifying disease clusters,1,4,False,False,False,statistics,1486739234,True,"I have an interesting problem at work that I haven't experience in and I'm looking for advice on a decent starting point. The data is x,y point data (all unique and non-uniformly distributed) with a few other variables associated with each point (e.g. age). Each member of the population has been subjected to a test of which around 10% are positive. 

My aim is trying to work out if there is a pattern in the positive cases or if the distribution is down to chance. My initial thoughts were to use something like randomised logistic regression to identify the ""good"" features but I don't think this will work with spatial distributions. In researching I've found a few test statistics but I'm unsure if any of them are applicable to this exact problem. For example, I don't want to throw Moran's I at it as I think the problem may be more nuanced than that."
Combining M's & SD's for meta analysis,0,3,False,False,False,statistics,1486740336,True,"Right, so in my analysis I have some designs which feature a pre test, a treatment trial (hypnosis) and a post test, only the treatment trial contains hypnosis, some of these studies also have a control group, (with no treatment across three trials) as well as a pre/post

We have decided to ignore the post tests as they may be subject to habituation effects, however, is it:

Suitable to subtract either the first control M from pre test M (or vice versa) to act as your non-experimental M in calculating effect size, (so the difference in means) and if so what SD would be used? 

Is it acceptable to average the 3 control M's to use that?

Will clarify/add anything else that's necessary "
"What is the definition of ""research hypothesis""?",4,4,False,False,False,statistics,1486745387,True,"I'd like to find the source of the terminology ""research hypothesis.""  Who said it first, in what publication?  Nowadays, what's the accepted definition and why?  This is in relation to statistical hypothesis testing.

I'm assuming a research hypothesis is a statement in ordinary language about an outcome that is to be tested.  It is not a null or an alternative hypothesis, because those are mathematical statements about population parameters.  The debate I'm involved in is whether it is possible for a research hypothesis to be mappable only to the alternative hypothesis, or whether it is possible for the research hypothesis to be mappable to the null hypothesis.  For instance, wikipedia says this: https://en.wikipedia.org/wiki/Alternative_hypothesis

In industrial experiments, we might be interested in seeing if the coolant fluid used in a machining operation affects the resulting surface finish of machined parts.  The null hypothesis is that the population averge of surface finish is the same for any coolant, and the alternative is that the population means for different coolants are not the same.  What's the research hypothesis?

If I have a theory of physics that says that there is no way that coolant can have an effect on surface finish, then why isn't that my research hypothesis?  It would get mapped to the null hypothesis, not the alternative."
Question on Logit versus Probit,0,1,False,False,False,statistics,1486752452,True,[deleted]
Need help with home work!!,0,1,False,False,False,statistics,1486758821,True,[removed]
Elementary Statistics Books for Business/Finance?,0,1,False,False,False,statistics,1486768622,True,[removed]
Looking to learn statistics programming languages,0,1,False,False,False,statistics,1486769425,True,[removed]
State-of-the-Art Machine Learning Automation with HDT,0,0,False,False,False,statistics,1486772779,False,
What's a good place to start out for Erlang C model for telecom?,2,2,False,False,False,statistics,1486783624,True,"Hello everyone,

So just starting working at a call Center and I'll be part of the WIT, Workfoce intraday team, part of the job is predicting (forecasting) how many calls are going to occur each day to determine how many people we need working everyday on the hour.

So to start off this is a very statistical question, I don't know if all call centers use the same programs but the idea or topic behind all this is called, ""Erlang C model for telecom"" since I just started I wanted to know where do I start learning about this, I've been task to take a look at it and see what I understand since I have a degree in statistics. If anyone knows what the heck I'm talking about please respond

P.S. this is like my first big thing I'm tasked to do after recently graduating with my BA in Statistics!"
What a Small World,3,2,False,False,False,statistics,1486783782,True,"So I've just ran into the strangest thing and was wondering if I could get help with the math? This might be a bit of a read because I'm not quite sure how to word this. One of my out of State friends, on PSN, met someone else that went to/lives in the same State, County, City, and the same High School, at the same time, as I did. I had no interference in this whatsoever. I didn't introduce anyone to anybody or anything. I also might have played a game of Overwatch with the person who my out of State friend met, while I was solo queuing. We didn't talk to each other though. This is so unreal and I would like to know the probability of this even happening. "
"Is there any useful information to be gained about a used car by computing its ""Lifetime Speed"" in MPH? Number of miles on the odometer divided by the number of hours that it has simply existed? Example: 10 year old car (87600 hours) with 250K miles on it. Avg speed: 2.853 MPH.",8,10,False,False,False,statistics,1486790753,True,"What would be regarded as a high value and a low value? What, if anything, could be inferred about its condition from extreme values at both ends?"
Can R power a commercial SaaS application?,0,1,False,False,False,statistics,1486799957,True,[removed]
Question on age specific incidence rates,0,1,False,False,False,statistics,1486804616,True,[removed]
Sexy Girl,0,0,False,False,False,statistics,1486813968,False,
Statistical consultant in USA,0,1,False,False,False,statistics,1486825163,False,[deleted]
Is there a way to resolve 2 types of statistical data and how?,0,1,False,False,False,statistics,1486825481,True,[deleted]
Having some confusion on understanding Ratio and Interval,6,11,False,False,False,statistics,1486825559,True,"I am taking Elementary Stats at University of North Texas, and very confused how to recognize ratio or interval data in word problems. 

I keep hearing ratio data has a clear 0. But I do not fully understand what that means.

Also hear that Interval data contains ratio data, but when there is not a clear 0 and the gap between the numbers mean something.

Anyone have a real clean cut definition, and easy tricks to help pick out the type of data in word problems?"
What statistical test is best?,0,1,False,False,False,statistics,1486837114,True,[removed]
"Suicide rates of these groups?: very smart people (prodigies, Mensa etc), smart people (doctors, phds, etc), average people",3,0,False,False,False,statistics,1486840490,True,
Beautiful Teen Girl Sex,1,0,False,False,False,statistics,1486842196,False,
[probability] roofing,3,0,False,False,False,statistics,1486849199,True,"
there are 16 tiles in a row on a roof, 11 are blown off at random. A 'run' is
defined as a sequence of gaps or tiles.

What's the probability of having

* (i) 10 runs,
* (ii) 9 runs.
"
Minitab Alternative,14,5,False,False,False,statistics,1486849365,True,"I had Minitab with my old job, and loved all of the functionality it has to offer.  I've since moved on to another job, and unfortunately I don't have it at my disposal.  I'm doing some little projects at home, and I'm trying to do some analysis in Excel (DOE analysis, correlation, etc), and I keep thinking about how using Minitab would make this so much easier.

Does anyone have any experience with something better than excel, but cheaper than Minitab?  I really don't feel like spending $1500, as that's well above my confidence interval of a good return on investment."
"Good, free R tutorials?",15,26,False,False,False,statistics,1486852238,True,"So I'm very familiar with SPSS, and relatively average on STATA, I'm aware R uses actual programming language, and I would love to learn it, so if anyone can point me in the right direction that'd be great, thanks "
ANOVA - full and reduced models,0,1,False,False,False,statistics,1486855841,True,"This concept is really frustrating me. I am using SAS to generate an ANOVA table on what I think is the full model. However, I can not figure out how to find a reduced model. Can anyone help? "
Resources on musical data mining (bonus if includes analysis with R)?,1,1,False,False,False,statistics,1486858468,True,[deleted]
"If I have two predictors and one criterion, how do I figure out the percentage of variance that each explains that is unique?",9,1,False,False,False,statistics,1486859198,True,"I have two predictors A and B, and one criterion X.

I want to know how much of the variance that A explains is unique to A, and how much it shares with B?"
Has anyone attended University of Washington's Summer Institute in Biostatistics?,2,9,False,False,False,statistics,1486861330,True,Was looking to apply for the scholarship to attend some of the modules and was curious others' thoughts on the sessions. Thanks
Inductive or Deductive (Stats),0,1,False,False,False,statistics,1486866426,True,[removed]
One Man's Quest for Statistical Truth,0,24,False,False,False,statistics,1486871121,False,
Student taking intro to Stats course. Struggling with Combinatory and binomial probability,7,5,False,False,False,statistics,1486875610,True,"I don't get it y'all. If the problem is stated plainly like: N=6 P= .6 X=2 Then I can plug the values into the formula and work them no problem and get the right answer. But if it's a word problem I'm lost. For instance, ""A gas station recommends 1/3 cars get oil added to them. So out of the next 4 cars that come through how what is the probability that 1 of them will need oil added?""

I set this up as N=4 P=.333 X=1 Q=.667

Then I work it and get .396. Enter as my answer into the online homework module and told I'm wrong. I also entered the unrounded answer and it was also wrong. Thinking it likely my arithmetic might be wrong I decided to use the Binomial Probability Chart with N=4, X=1, P=.3 and the answer is .412. This answer is also marked as incorrect. 

I'm at a loss as for what I'm doing wrong because I went over these problems with my professor and I'm following her instructions step by step and arriving at the same (wrong) answer EVERY time. I'm at my wit's end.

Any advice, help? I'm not looking for someone just to give me the answer but to help me figure out how to get it right."
Calculating overlap between two groups based on samples of different proportions,0,2,False,False,False,statistics,1486884642,True,"Let's say there are two groups Group A and Group B. Both groups contain 100 people. I want to calculate the probability that a member of Group A also belongs in Group B and the probability that a member of Group B also belongs in Group A.

Though both groups are made up of 100 people, I only have a sample of 50 of group A and 20 of Group B. When I compare these two samples I see that 10 people are in both Group A and Group B i.e. 20% of Group A sample is in Group B sample and 50% of Group B sample is in Group A.

Ignoring statistical significance (only because it's irrelevant to the main thrust of my question), is it valid to extrapolate the above and say that 20% of Group A is in Group B and 50% of Group B is in Group A? Or is the math more complicated because I'm comparing two samples which represent different proportions of their respective populations? A relevant citation would be helpful if you have one!"
Show r/statistics - I made an interactive statistical distributions,23,60,False,False,False,statistics,1486885622,False,
Excuse me for butting in but,0,0,False,False,False,statistics,1486915237,False,
Creating Homework Material for Statistical Writing Classes and Workshops,0,3,False,False,False,statistics,1486927433,True,"This summer, I'll be offering a workshop on statistical writing to the graduate students in the SFU Stats department. In the year that follows, I hope to teach SFU's Stat 300 course, an undergraduate writing course that is mandatory for all statistics majors. I want this course to be offered at more universities, but since the typical undergrad degree in stats is already overloaded, this would need some additional motivation, so here's my anecdotal hook:

When graduate schools ask for a letter about your motivations for applying, or about your general background, there's a few things that are being examined. First, different graduate supervisors have different specialties, and they may be looking at your letter for signs of a good match. Second, the letter is a means of seeing firsthand your personal ability to communicate in English. They ask for something personal rather than a set topic to discourage plagiarism. They are not interested in your level of motivation; everyone says they are highly motivated.

I've recently been writing reference letters and filling out graduate school reference forms for students that have been in my previous 300-level classes. All of these schools are asking about the ability to write. Many statistics undergrads are applying to fields outside of pure statistics, for example economics, medical science, and business. All of them are asking their references about skills in English, often spoken English and always written. Likewise, the Statistical Society of Canada asks referees about spoken English as a part of their A.Stat accreditation.

In order to develop these skills in a program that usually focuses on mathematical theory and programming ability, I've sketched out some projects and exercises that could fit into a workshop or course.

The instructor (me) would ask colleagues from other fields that use quantitative data, such as sociology, anthropology, biology, business, ecological restoration, and history for reports from previous or current projects.

The students would read the reports and identify what the sampling method was, what analysis was done, and to check the assumptions on those methods to the data. They would interpret the results in their own words (not in the words of the discussion or conclusion section), and compose a couple of questions about the research that weren't answered. These questions don't have to pertain to the domain; they should pertain to the methodology. These would be questions like: were there any outliers that affected your results? Did you consider multiple testing? Did you consider regression/anova?

For example, for this research report, Water Quality of Stoney Creek and its Effects on Salmon Spawning, by SFU undergrads by Oak, Tony; Thai, Michelle; Orgil, Indra; Ngo, Kevin; Lu, Jerry , available at http://summit.sfu.ca/item/12770

A writing assignment could be:

* What were the parameters to be estimated?
* What biologically important thresholds or critical values are mentioned for the parameters being estimated?
* What sampling method was used? How many sampling units are there?
* Were there any null hypotheses being tested? If so, were they rejected? Should they have been?
* Describe the qualitative differences between the four sites. Use the map and Figs 4,5,6, and 7.
* In your own words, summarize the quantitative data described in Figures 2 and 3.
* Compose two suggestions or questions about the analysis that may not have been considered.
* Describe one way you could build upon (e.g. expand, follow up) this study, the type of information you would collect from this new work, and what additional conclusions you could make if the data matched your expectations.


(Note: Please don't take these questions as a criticism of this research report. It was selected to show how material is available within one's on campus. Even a quick skim of the report will show it's truly excellent work for undergrads.)

Another exercise I'm thinking of doing is to have students copy edit some scientific writing. This could be a publication or a blog post. I would do this twice; once with a well written work, and once in something of questionable quality like the unedited papers that come out of discount publishers. If I used something from ArXiv, with the authors' permission, I could use later, more complete versions of the same work as a 'key'. However using ArXiv is untenable for graded work because the 'key' is publicly available.

Blog mirror:
http://factotumjack.blogspot.ca/2017/02/creating-homework-material-for.html

"
Stats project ideas?,5,1,False,False,False,statistics,1486936157,True,"Hey everyone, I am in a Stats course that requires us to run various statistical analysis of our choice on data that we find interest. I have spent hours looking at data to use but nothing that I thought could work. I thought about Wisconsin firearms crimes data before and after concealed carry was enacted but the data wasn't there. Does anyone have any suggestions on where to look / what to look at? I check out /r/databeautiful but it is hard for to conceptualize what data to compare. Any help would be great, thanks!"
Stats to collect for 1v1 competitions?,2,3,False,False,False,statistics,1486939327,True,"I run Smash Bros CPU tournaments where people sign up with a character and I put them all into a tournament bracket. Basically, I make the computer play itself.

The only data I record is the winning character, the loser, and the lives the winner had left. I track wins, losses, win percent, and head-to-head records. Are there any other neat statistics I can track with this data? I would love to use this as a learning experience because I plan on studying statistics in college."
Research emphasis on Spatial Statistics?,4,2,False,False,False,statistics,1486942668,True,"Current situation: I have an undergraduate degree in geography and have decided to pursue graduate-level coursework in statistics, at least at the MS level, for personal reasons, as well as to hone my quantitative skills.

Long term goal: If I enroll in a thesis program, or decide long-term to pursue a PhD, I would like my research to be in applied spatial statistics. I feel this would be a great way to use my graduate education in a way that I can appreciate due to my undergraduate education.
My UG GPA was 2.48 (my geography major GPA was fine, but I flunked out of engineering) and I'm not proud of it, but am currently making amends by retaking some classes. I've already aced single through multivariable calculus at CC and currently taking C++. I already know Python, and have started looking at R.

Out of curiosity, what schools do you know of currently that have faculty or graduate research in spatial statistics? I've done some searching (""spatial statistics"", ""spatial analysis"", ""kriging"" - that term I know bc of this sub!) but I don't know what sub-topics there are, and there may be other keywords that I'm glossing over in Elsevier that I shouldn't be. Apparently Ohio State used to be *the* place but the primary faculty member left a few years ago. I've looked at U Chicago but it is too much of a reach school for me. My actual ""reach"" school, UW-Madison, seems to have one or two faculty members who might fit the bill.

edit: also open to biostatistics programs, but to some extent don't want to shoehorn myself, as the job market is so competitive and I'd rather not limit my options career-wise."
Good stats course for those who already have a maths background?,9,10,False,False,False,statistics,1486943537,True,"Hi all,

I've got a Bachelors in Applied maths, but I somehow managed to make it through the entire degree without taking a stats course. I've picked up a decent amount of stats by proxy and through my job, but I thought it might be good to go through a quick online course to fill in any gaps.

The thing is, most of the intro stats courses I've come across are really targeted at those without a huge amount of maths background. Is there a good course or book out there (perhaps targeted towards math grads who need to use stats in their work), that doesn't spend an inordinate amount of time going through basics, but instead will allow me to fill in gaps really quickly?

I've come across coding courses that teach you a programming language assuming you already know another - I'd love the same kind of thing.

Thanks in advance!"
Statistics of money lost due to counterfeit goods?,1,1,False,False,False,statistics,1486946048,True,Found [this] (http://www.securityinfowatch.com/article/10490427/the-economic-impact-of-counterfeiting)  article but it was in 2009. Does anyone know any statistics of how much money big brands lose due to counterfeit goods?
what would you say if Trump asks you what p-value is,0,1,False,False,False,statistics,1486947806,True,[removed]
Does anyone know how to write those fancy equations for a generalized Oaxaca decomposition?,0,2,False,False,False,statistics,1486949910,True,"Trying to recreate something that looks like this: http://www.scielo.cl/fbpe/img/ede/v39n1/form03-04.jpg

This is what I wrote out in word:

    Change = 

    Population effect 

    (part rate y) – ((total L y - L y [age group])+ ((total pop m * proportion pop y-1 [age group])* part rate y [age group])) / total population y

    +

    Participation effect 

    (part rate y) – ((total L y – L y [age group]) + (part rate y-1 [age group] * population of [age group])) / total population y

    Note: Y = year; Y-1 = previous year; L= labour market level


The change from one period to another is calculated two ways. First, by holding the population proportion to a previous year’s proportion and applying a given year’s participation rates. Second, by holding the participation rates to those of a previous year’s and applying them to a given year’s population proportions. The calculations are conducted separately by age and sex, and the results are expressed as a percentage point contribution to the total change in the overall participation rate."
23 Irea,0,1,False,False,False,statistics,1486961571,False,
Common Probability Distributions: The Data Scientist's Crib Sheet,10,73,False,False,False,statistics,1486965188,False,
8 Benefits of Statistics Tutor Help Service for Securing High Score,0,1,False,False,False,statistics,1486966505,False,
"We Will Sort you out with Your Statistics Online Quiz, Trust Us",0,1,False,False,False,statistics,1486969380,False,
Is it possible to use SPSS outside of a university setting?,2,1,False,False,False,statistics,1486982293,True,[deleted]
"Help with figuring out how to analyze data, first time quant thesis.",2,1,False,False,False,statistics,1486986844,True,[deleted]
Frank Harrell : A Litany of Problems With p-values,10,19,False,False,False,statistics,1486992351,False,
cannot pass this online test,0,0,False,False,False,statistics,1486999030,True,[removed]
What exactly is the leave-one-out cross-validation information criterion?,12,11,False,False,False,statistics,1487008849,True,"I've been going through Richard McElreath's [Statistical Rethinking](http://xcelab.net/rm/statistical-rethinking/) to build my intuition on Bayesian statistics.  I just went through the chapter on information criteria.  They all have different assumptions that are less necessary as computational complexity increases, as AIC, the most strict, assumes flat priors and and multivariate normal response, while WAIC uses the entire posterior and has fewer assumptions.  However based on previous reading, it seems like given enough data, all information criteria converge to something in leave-one-out cross-validation (LOOCV).

If it's straightforward to calculate (which seems to be the case given [a recent paper on Pareto smoothing of importance weights](http://www.stat.columbia.edu/~gelman/research/unpublished/loo_stan.pdf)\), does it make sense to always use LOOCV instead of a more standard information criteria?  I've also heard Andrew Gelman refer to an LOOCV-IC.  How is LOOCV an information criterion?  I don't see how it falls into the realm of deviance as information gain across models."
Analysis question,0,1,False,False,False,statistics,1487009507,True,[removed]
Granular Forecasting Troubles [/r/cross-posted to MachineLearning],3,3,False,False,False,statistics,1487010556,True,"Quick background; I have a Data Science background, but I purposefully avoided forecasting type stuff for a while since I had an Economics degree and I thought I would be double dipping and wanted to have a more diverse skill set.

I've been working on producing an occupancy forecasting model, where I need to predict when our customers enter, when they exit, and from those 2 points calculate our occupancy. The kicker though, is I have to do this all at the hourly level (my boss has indicated that a future iteration will be at the minute level). 

We have the typical seasonality around week/month of the year (particularly around holidays), and then within day seasonality as well (we have a higher entry rate in the morning, for example). I don't know what the best model is.

For our entries, I ended up settling for an autoregressive model with the coefficients calculated through a genetic algorithm primarily based on the previous year's worth of entries (technically I use year over year, week over week, and day over day variables, as well as an extrapolation of our reservations). For exits, I sample from the previous year's stays and apply the duration from the sample (adjusted for the seasonality). 

Problem is, accuracy isn't fantastic. Average error is around 5%, and average absolute error around 11-12%. I'm not sure what other methods I can work with. I briefly explored a lot of the automated regressive models within Python's scikit-learn package, but because the data is very volatile at the hourly level (e.g. if regulars show up at 7:58am vs 8:02am they'll count as different hours), so the error rates are all over the place. My manual script calculates error as aggregated over an entire day to minimize that, but not sure if that's terrible practice either. 

There's some research material out there that I haven't dug into yet regarding hotel occupancy, but not sure how applicable that will be for us (we have a low, and constantly changing, reservation rate).

Sorry if this all comes out a bit incoherent. "
"When residuals are not normally distributed in OLS? In part-b, Can you tell me why we don't have to worry? Also how do we calculate the slope and the intercept in part-3?",1,0,False,False,False,statistics,1487011151,False,
Need help with stats problem!,2,2,False,False,False,statistics,1487012858,True,[deleted]
Help me understand how to build my R syntax for a glmer() model,3,7,False,False,False,statistics,1487013887,True,"I'm using the `rstanarm` package to run a Bayesian hierarchical model. The data consists of repeated observations across a variety of geographical locations in a long format. [Here's a screenshot of it to illustrate](http://i.imgur.com/IVkGKOZ.jpg). The data set consists of the following: 

* A Location ID and a Region ID (Region essentially groups several locations into a larger geographical level). They are shown in blue shading in the picture above. I'm calling each `LocID` and `RegID` in my syntax below.
* A count variable that is the outcome of the model, shown in red shading with the red outline in the picture above (3rd column from the left). Given its distribution, a negative binomial model makes more sense than a Poisson model, so I want to use the `stan_glmer.nb()` wrapper function. This count variable lists the observed count on a particular timeframe for that location (the timeframe variable is to its immediate left, but I'm not counting on using this). I'll call it `count` in the syntax below.
* A set of variables providing information about each location. They're shown in green shading in the picture above. They're repeated across the observations within each location, but differ from location to location. I'll call them `LocVars_X` to illustrate how I'd use them in the syntax below.
* A set of variables for other observations at the same timeframe as the count variable. Those therefore differ from observation to observation, and would help explain for the variation in the count variable. I'll call them `ObsVars_X` to illustrate how I'd use them in the syntax below.

I want the model to pool and ""borrow"" information at the location level to explain variations in the `count` across locations. I also assume that the locations within the region can behave similarly, so I want to pool further up a level as well. My intent is to then use that model to predict count for a variety of other locations.

Given my intent above would the following syntax be correct? `LocVars_X` and `ObsVars_x` would be repeated for each variable that I respectively consider at the location level and the observation level:

    stan_glmer.nb(count ~ (1 | LocID:RegID) + ObsVars_X + (LocVars_X | LocID), data = mydf, ...)

Also, would it be a good idea to standardize the independent variables before running the model? As noted above, I'm primarily interested in using the model to predict the count variable for other locations rather than determine the impact of each predictor."
"Data management stats: efficiency, errors, predictions for 2017",1,9,False,False,False,statistics,1487014524,False,
Why use MLE over bootstrapping?,14,16,False,False,False,statistics,1487018325,True,"Forgive me if this is a dumb question. I'm largely self-taught/self-teaching in my statistics knowledge. 

I know that MLE is a method of estimating parameters given a set of data which finds the value that maximizes the likelihood of making the observations. 

And bootstrapping of course is resampling with replacement a large number of times to generate a sampling distributions of parameter estimates. 

Both methods seem to be geared towards estimating an unknown parameter from the data. 

I followed an example from this [RBlogger Post](https://www.r-bloggers.com/fitting-a-model-by-maximum-likelihood/) on calculating the MLE. I am only familiar with the concept of MLE and not *how* to do it, so I followed this post fairly closely. Hence all code here is R code.

    #create data set 
    set.seed(2005)
    N <- 1000
    x <- rnorm(N, mean = 3, sd = 2)

    # Log Likelihood formula 
    LL <- function(mu, sigma) {
    R = dnorm(x, mu, sigma)
    -sum(log(R))
    }

    # MLE 
    require(stats4)
    mle(minuslogl = LL, start = list(mu = 1, sigma = 1), method =
    ""L-BFGS-B"", lower = c(-Inf, 0), upper = c(Inf, Inf))     

this results in

    Coefficients:
    mu    sigma 
    3.011493 1.898368 

So okay, cool. I can estimate the parameter that maximizes the likelihood of the observations I made. I take this to be a proxy for estimating the population mean and standard deviation, I suppose. In this case, why not just calculate the mean and standard deviation normally? Or rather, why not use bootstrapping so I can get an idea on how certain I can be in this parameter estimate?

    library(bayesboot)
    xmu <- bayesboot(x, mean, R=4000)
    summary(xmu)
    xsd <- bayesboot(x, sd, R=4000)
    summary(xsd)

Using the Bayesian bootstrap package, I estimate the mean to be 3.011493 with a 95% Credible Interval of [2.89, 3.15]. The standard deviation is estimated to be 1.897123 with a 95% Credible Interval of [1.80, 1.99]. This is the same as the MLE's output. 

I have also seen that MLE can be used to estimate regression coefficients, but so can bootstrapping. What is the advantage to using MLE over bootstrapping when bootstrapping can give the same information along with a confidence or credible interval (depending on if using frequentist or bayesian methods)? 

Furthermore, when might one choose to use a maximum a posteriori (MAP) estimate over MLE or bootstrapping? 


    
"
Examples of good statistics papers,2,1,False,False,False,statistics,1487018565,True,"I'm working on an Honors College project that involves a formal paper writeup. And while I've written plenty of papers on college, I realize I've never written an academic paper within my field. So, any links to well-written papers or just general suggestions? "
2SLS VS a good appropriation,0,2,False,False,False,statistics,1487020635,True,"I am currently studying something called peer effects, essentially how classmates may impact your personal performance. 

the hypothesis is essentially that average class performance will correlate positively against personal performance in for example a standardised test. 

obviously this model is by nature endogenous, and i am currently trying to find a way to proxy peer ability or performance. i could run an IV regression, however i am also consider potentially using a test score from say one year ago to proxy personal skill. 

my question really is: what's the difference between running an 2SLS to proxy a variable, vs using just another variable that while may look similar, is not the same as the original variable of interest. 

"
19 Kaylee,0,0,False,False,False,statistics,1487028282,False,
Trick Question? The answer has to be obvious right?,4,0,False,False,False,statistics,1487043325,False,
What do these numbers after the critical value symbol represent?,5,1,False,False,False,statistics,1487053709,True,[deleted]
Economicshelpdesk.com Provides Online Statistics Assignment Help Service,0,1,False,False,False,statistics,1487058725,False,
"If log(x) follows abnormal distribution, does x follow a lognormal distribution?",0,1,False,False,False,statistics,1487068989,True,[removed]
an expected value of an expected value?,8,0,False,False,False,statistics,1487080067,True,"So you should find: E[g(X)], where g(x) = E(Y|X=x). In the solution it just says: ""Using a formula for ""double expected value"": E(Y) = E(E(Y | X=x) => E(Y) = 1.625, I wonder how it becomes like that, how does E(E(Y | X=x)) = E(Y)..?

This is the table of the problem btw, incase you need context: http://imgur.com/a/qBN3O

EDIT: Since it was answered I guess, I have followup question, how would I calculate it directly from the definition. E[(g(X)] that is, without using the quick formula (that it is E(Y)"
Null Hypothesis for 2 Sample T tests in Minitab,0,1,False,False,False,statistics,1487086070,True,[deleted]
Teen Suicide,0,1,False,False,False,statistics,1487086117,True,[removed]
Teen Suicide,0,1,False,False,False,statistics,1487087580,True,[removed]
*HELP! Business Stats.,1,1,False,False,False,statistics,1487090893,True,[removed]
Mann-Whitney U-Test for Transactional Revenue,8,8,False,False,False,statistics,1487106627,True,"I'm running a relatively straightforward AB test for a nonprofit organization I'm working with. The primary ask is a $50 donation, so I'll typically just run a standard t-test to compare the conversion rate for the control page to the treatment page. 

For our text test, however, we're offering options for $20, $30, $50, and then also including a blank donation box allowing donors to enter in an amount of their choice. 

The nonparametric nature of the data from this test suggests that Mann-Whitney would be the best simple fit to test whether the donation data from the control comes from the same population as the donation data from the treatment, or if there is a statistically significant difference in the data.

So here's my question.

The site has tens of thousands of visitors during the testing period, but for the sake of simplicity, let's say I've got 10 visitors to my control page, and 10 to my treatment.

For the control, let's say that five people give $0, three people give $20, and two people give $50.

For the treatment, let's say that four people give $0, two people give $30, and one person gives $50, and one person gives $100.

When I'm running the Mann-Whitney test, do I include the $0s in my datasets, or just the actual transactions?

I'm pretty sure I'd include the zeros, but I don't have a lot of real-world practice with this test, so just wanted to ask you guys what you thought. "
Is this a well-known statistical distribution?,19,14,False,False,False,statistics,1487106961,True,"I have a dump of scored online comments. The scores come from users upvoting and downvoting the comments. This is a histogram of those scores (note the log transform on the y-axis):

[http://i.imgur.com/k8BUDCH.png](http://i.imgur.com/k8BUDCH.png)

There were a few distributions I expected, but this isn't one of them! I don't actually recognize it as a common, well-known distribution. Does anyone else? It would be great if this *were* a well-known distribution -- I would like to standardize the engagement scores!"
Part A: how do I go about finding how to distribute the numbers into four intervals?,3,0,False,False,False,statistics,1487121638,False,
"Has anyone here done statistical consulting and have any tips on incorporation, business licensing, etc?",1,4,False,False,False,statistics,1487122245,True,Sorry if this isn't the right subreddit for this. A client is interested in having me consult for some number of hours for them and since I want to avoid personal liability if anything goes wrong I was thinking of forming an LLC. Has anyone here gone this route? Are there business licensing issues peculiar to statistics I should know about? Thanks!
HBS Case - A-Rod,2,3,False,False,False,statistics,1487125479,True,"For one of my classes, we are doing a case on the Texas Rangers trying to sign A-Rod back in 2000. The question I need help with: how many additional attendees are there per each additional game won? The information we're provided is attendance, player payroll, population, and wins from 1990-1999. Specifically, our professor asks us to use a regression model to estimate it. I'm only used to using regression models on excel for stocks and was wondering if any of you guys have any input? Thanks."
Simple conceptual question but I'm so confused.,5,1,False,False,False,statistics,1487132879,True,"Hi guys, I looked for a similar question online but couldn't find a good explanation..so here I am. This is the question:

You are summarizing the variable “age” in your survey data. Which of the following must be the largest?
a. Mean b. Median c. 75th percentile d. Cannot tell, need more information


The answer is d, but can someone explain why? I was thinking 75th percentile because I know it can be smaller than the mean but I don't understand how it isn't inherently larger than the median because the median is the 50th percentile. Thanks for your help! "
What am I doing wrong? (normal distribution),0,1,False,False,False,statistics,1487135048,True,[removed]
A Valentine's Day poem for a statistician,7,75,False,False,False,statistics,1487135782,True,"Roses are red.

Violets are blue.

If you were a null hypothesis

I'd fail to reject you.

---

Not mine. Shared by a friend on Facebook from an unknown source."
Building an index,0,1,False,False,False,statistics,1487156204,True,"Hi!

I'm in the process of writing my bachelor thesis regarding the housing market in Norway. In order to compare housing prices to fundamental values I have to build an index.

I have data sets for building costs, wage increases and rental prices (starting from 1980). I would like to compare the growth in these data sets to the growth in housing prices. The problem is that none of these data sets start at the same point, I would like for all of them to start at 100 or something like that. 

I figured that the best way to do that would be to set all starting values to 100 then just add the difference between 1981 and 1980 for the next year - is that fine or is there a better way to go about do this? 

Thanks in advance"
How does one analyse property data?,1,1,False,False,False,statistics,1487162120,True,I've been given the task to create a graph showing the appreciation and rental income on properties in a specific area. How would I go about approaching this? I'm trying to do this through excel and I am concerned by data may be off due to the time frame and different age of dwellings etc.
Accounting for Missing Data,0,1,False,False,False,statistics,1487165385,True,[removed]
Model specification sanity check...,5,9,False,False,False,statistics,1487166725,True,"I cannot for the life of me wrap my head around this, and the place that I work doesn't really have any who I can bounce this off to check my thinking, so thanks in advance for reading and any help.

We have an intervention we conduct on a sample of subjects and the effect of the intervention is measured using a pre-post design with no control. My data is arranged in a long format, so each subject has 2 rows, with the pre or post time point, their outcome measure, and identical ID numbers and demographic information in each row. 

Among the multiple methods I could use to assess the effect of the intervention, one would be a mixed linear model specified in R as follows:

Y~Time+(1|ID)

So I have an effect of the time point (pre or post) and a random intercept for each subject. 

Say I had a theory that female subjects had higher baseline rates, and maybe that the intervention has less of an effect on them than males. I might explore this (among other methods) with the following:

Y~Time+Gender+(1|ID) (model 1)

Y~Time*Gender+(1|ID) (model 2)

But my question is, are the p value and variance estimates of the gender effects too 'tight' because I have two rows with the same gender for each subject, effectively doubling my 'sample size', even though I have the random intercept? Part of my brain says it seems like it should be okay for model 2 more than model 1, part of my brain says just in general something feels wrong, of which I'm trusting the second at the moment.

I also look at using sandwich estimates, mostly just to see how much that shifts my variance estimates and p values, but as I understand it this would help correct for heterogeneity of my residuals (of which visually there is a little due to issues I know about) and not something like this. Comfortingly, they are not changing too much.

Any comments or help would be great, if nothing else to check I'm not insane for feeling like something might be going wrong with my p value estimates. I work with management types who want me to tell them if something is ""true"" or ""false"" which makes me want to shake them and they will not move away from using a p value of 0.05 to guide them (which is an whole other thread of discussion). I'm the sort to find everything and anything suspicious, especially given our lack of ability to have proper experimental design."
Michelle21,0,0,False,False,False,statistics,1487168986,False,
Sampling SRS: Difference of Variance(y-bar) and S^2,1,5,False,False,False,statistics,1487176327,True,I am a bit confused between the difference of the two
question on AR and ARR,0,1,False,False,False,statistics,1487178443,True,[removed]
Risk Management: How do you calculate daily multi-asset portfolio variance?,0,1,False,False,False,statistics,1487186839,True,[removed]
Possible to become statistician without PhD or masters in stats?,16,14,False,False,False,statistics,1487187883,True,So I recently graduated with my bs in biochemistry and now I'm in grad school getting my masters for neuroscience. I'm currently taking a biostats class and we are using R program. I really like it. I was always interested in statistics when I took it in undergrad. Now I'm just trying to figure out how can get into the field with my limited math/stats background. I also have experience using SAS. Is this enough for me to get a job at a company as a biostatistician? Or should I go into more debt and pursue a PhD in statistics? 
Simple team scoring expected value question,2,0,False,False,False,statistics,1487192437,True,"If you had team A who averages 3 goals a game vs an average team, and team B who averages 2 goals against vs an average team, and the league average goals per game was 2.5, how many goals would you expect A to score on B ignoring all other factors?

I know it would be either 3 + (2 - 2.5) = 2.5
Or 3*(2/2.5) = 2.4

If the league average was 1.5 goals per game the same two options would be
3 + (2 - 1.5) = 3.5 or 3*(2/1.5) = 4."
The probability of a type 2 error,2,0,False,False,False,statistics,1487193413,True,"Okay, so from an earlier problem you had X bar = 29.1 and tested it vs expected value 30 (so H_0 was 30) (you had s = 5.35 there), and in this problem sigma is known to be 2, and the problem is: If the true expected value is 28, what is the probability of a type 2 error: and this is the solution, but I don't really understand all the inequalities and everything that is going on, would be great if any of you guys could explain it: https://gyazo.com/04d376d4b452971e09298e8aaa984c0c so where exactly do they get -1.645 from? Obviously it is from the 5% significance level, but where do they get the equation from and how can you set it up like that, what exactly is it the first line says? Is the expression that is larger than -1.645 on the first line just Z? or how does that work?. How the heck do they randomly get ""X bar"" to be 30 in this case? Would be nice if someone could help me understand it by explaining it very detailed

If something is written unclearly in the solution or I'm explaining the problem too poorly, let me know"
Can someone help me classify what type of data this is?,2,1,False,False,False,statistics,1487193428,True,"A CFO measures how many years each of the 15 individual accountants have worked at a firm.

Would this be time series or cross sectional? Thanks. "
What's this statement mean?,4,1,False,False,False,statistics,1487194988,True,"""In some instances, such as with a general linear model with normally distributed errors, the estimator may be relatively robust to violations of distributional assumptions, at least asymptotically. In general, however, this robustness may not hold...""

Does this just mean that it will be more and more robust as the sample size approaches infinity?"
How much programming should you know going into a masters in statistics program?,0,1,False,False,False,statistics,1487195399,True,[removed]
Best Way to Impute/Determine Customer Zip Codes,0,1,False,False,False,statistics,1487195623,True,[removed]
How much programming should I know before entering a masters in statistics program?,2,0,False,False,False,statistics,1487195627,True,"Hello,
I am currently in my last year of undergrad and applying / awaiting decisions for my grad school apps for a masters in statistics.
 
I'm wondering how much programming experience is expected before entering? Obviously the more the better, and I will try and fill in the gaps in this upcoming summer. But I am worried as I have almost zero experience. I took an intro to CS course as a first year where we used some python, a regression analysis course where we used R, and that was basically it. Everything was so guided that I would not say I would even have experience in those languages. In addition, they were so long ago that I see the codes I have written for those classes and barely have an idea of what I had been doing.
 
I know most people go into a statistics masters from a math undergrad. Do they usually come in comfortable in a language or too?
 
As a follow up, what is the most efficient way for me to pick up a language. Are there any online courses that will be able to get me to a proficient level? Or give me a good start? I'm guessing Python and R is what I will be using most.
 
Thanks
"
Analysis of Likert Data (xpost from r/AskStatistics),0,1,False,False,False,statistics,1487196662,True,[removed]
In which way can I best use this type of data? [ACBC Analysis],4,2,False,False,False,statistics,1487205096,False,
"need some help figuring this out. 5% chance, 9 attempts.",3,1,False,False,False,statistics,1487209176,True,Likelihood of getting a 5% chance to occur at least once over 9 attempts. What's the answer and how do I figure this out.
Introduction to Anomaly Detection,0,24,False,False,False,statistics,1487210376,False,
Statistical significance using two values,4,2,False,False,False,statistics,1487218474,True,"I'm embarrassed to ask this given how much stats I've had in my education, but I gradually several years back and I guess it's a use-it-or-lose-it type thing. Hope one of you can help me out.

I'm looking at a study that compares oxygen saturation in infants while in a car seat. Between 0-60 minutes, the mean saturation was 95.6 ± 1.7%. Between 61-120 minutes, the mean saturation was 95.3 ± 1.7%. This would suggest that the longer an infant is in a car seat, the more the oxygen level desaturates. My question: is the difference significant? What type of calculation do I need to run to come up with the significance?

For what it's worth, sample size is 200 infants. Thanks!"
why are some distributions common?,4,9,False,False,False,statistics,1487228718,True,"I have been thinking this for a while. Everybody knows that the normal distribution happens quite often in nature. Others, such as poisson or the chi-squared, also happen to be useful very often in statistical work.

Why is this the case? Why do random variables find themselves commonly distributed according to these well-known distributions or at least bearing close resemblance? Is there any philosophical approach to understand why this is the case or are the prevalence of these data-generating processes a purely mathematical event?

Sorry if the question is a little bit garbled - english is not my first language. Thank you in advance!"
Software for collecting and analyzing data,0,1,False,False,False,statistics,1487231555,True,[removed]
Help pls: How to calculate a confidence interval for the ratio of means for 2 groups?,0,1,False,False,False,statistics,1487240818,True,[removed]
[Hiring] Need some advice and tutoring for R Studio.,6,4,False,False,False,statistics,1487247706,True,"Hi guys, I need a bit of tutoring for some R script work. I think it is pretty simple stuff involving T Tests and Chi-Squared test so shouldn't take too long.

Need this ASAP, Message me if interested.

Sorry Mods if this isn't allowed!"
[Problem] How to model grouped and ordered data with multinomial logistic regression with R? [x-post from /r/rstats],1,5,False,False,False,statistics,1487251525,False,
Correct distribution for data analysis?,0,1,False,False,False,statistics,1487256234,True,[deleted]
[Problem] Decreasing logistic value when adding variables,1,3,False,False,False,statistics,1487257041,True,"Hello,

I am using R Studio to analyze my logistic values.

What I am most concerned with is my variable med1lr.

When I run only med1lr against my dependent varaible, my data  seems fine. However when I run it along with other variables, my med1lr data seems to become useless.

Here is the program used and the data for only running med1lr

code: 
LR4 <- glm(unmet_lr ~ factor(med1lr), data = total, family = ""binomial"")

Data:

 |OR  |    2.5 %   | 97.5 %
---|---|----|----
(Intercept)   |  0.1019022 |0.08580396 |0.1200945
factor(med1lr)2 |1.1222694| 0.89154996 |1.4140908
factor(med1lr)3 |1.6071240 |1.27667447| 2.0256616

This is my code/data for running med1lr along with other variables.

code: 
LR3 <- glm(unmet_lr ~ factor(age_lr) + factor(sex_gr) + factor(act_gr) + factor(job_lr) + factor(basic_lr) + factor(med1lr) + factor(med2lr) + factor(self_gr) + factor(chronic_gr), data = total, family = ""binomial"")

Data:

 |OR  |    2.5 %   | 97.5 %
---|---|----|----
(Intercept)|0.009110606|0.005579779|0.01452954
factor(age_lr)2   |  2.963566844| 2.007543773| 4.45759166
factor(age_lr)3  |   4.833116042| 3.205731500 |7.43553503
factor(sex_gr)2   |  1.461878385| 1.189521011| 1.80029434
factor(act_gr)2    | 1.656524980| 1.148898388| 2.37026617
factor(job_lr)1    | 1.711634359| 1.354415585| 2.17423102
factor(basic_lr)1   |2.684288967 |1.727536382| 4.09104447
factor(med1lr)2    | 0.517219741| 0.300147932| 0.93865062
factor(med1lr)3    | 0.424166836| 0.211506757| 0.89205717
factor(med2lr)2    | 1.936410022| 1.063017758 |3.35207966
factor(med2lr)3    | 3.842380019 |1.820054181 |7.74232240
factor(self_gr)2   | 2.070120137| 1.656827275| 2.59321029
factor(self_gr)3  |  4.245032798 |3.036495735| 5.91963606
factor(chronic_gr)2| 0.799508526| 0.558547887 |1.12784744
factor(chronic_gr)3| 2.261935624 |1.134797238| 4.34219504

All the other variables, the values increase however except for medlr1.

Any help will be appreciated! Thank you."
Stats tests for testing relationship?,6,1,False,False,False,statistics,1487257586,True,"I am doing a course project right now and am having a bit of confusion. I am comparing two variables temperature and a percentage such as below:

                 Temp       Percent of Pop. Who Drank in last 30 days
Alabama     76 (f)       20%
Alaska        67 (f)       17%
Arizona       84 (f)       15%

I was looking to use the following tests to see if there is a significant relationship:

Pearson Regression
Chi-Square 

My question is, can I use these two test on this data when one variable is in terms of whole numbers and the other variable is in terms of percentages? If not, what tests would I use to compare the two variables? Any guidance would be great, thanks!"
covariance question,1,1,False,False,False,statistics,1487261304,True,[deleted]
What is a good publicly available dataset to practice Markov Models on? (x/post with r/Askstatistics),0,1,False,False,False,statistics,1487264609,True,[removed]
What is Principal Component Analysis used for?,5,3,False,False,False,statistics,1487265650,True,"Hi all. Most tutorials I've seen online focus on how to perform PCA and sort of take for granted what the output may be used for. My question therefore is what exactly might one use the PC's for?

In my case, I have ~50 data points and ~20 variables for each, and a colleague recommended using PCA, but I can't figure out how the results would help me understand my data better or derive any conclusions.

Any help understanding the intuition behind PCA would be appreciated. Thanks!"
Cry of Alarm,4,15,False,False,False,statistics,1487267889,False,
[Help] I need to build a model,0,1,False,False,False,statistics,1487269394,True,[removed]
"What are the main differences between SPSS, SAS and STATA?",8,3,False,False,False,statistics,1487275802,True,"Hello, I'm a recent psychology grad trying to learn some more statistical programs to improve my employability.  In school I've worked on projects that used spss and had a professor who showed me the ropes of R, but now I'm trying to learn sas and stata.  From the books ive read, spss, sas and stata seem like they have more similarities than differences, so I just wanted to ask, what are the main differences between the 3?"
Chimp thesis data: GLMM or GEE,0,1,False,False,False,statistics,1487276091,True,[removed]
Can you help me translate this weighting procedure from SPSS to Stata syntax?,0,1,False,False,False,statistics,1487276440,True,"I'm working with data collected in a 2 stage sampling design with stratification at the second stage. Others have analyzed these data using SPSS complex samples, but I have very limited experience with SPSS and I would like to conduct my own analyses using the svy command suite in Stata. I have the SPSS syntax from past analyses and I was hoping someone fluent in both languages could help me translate the SPSS syntax into Stata-ese. 
Here is the SPSS syntax:  
    CSPLAN ANALYSIS  
    /PLAN FILE='Q:\file\data\plan.csaplan'  
    /PLANVARS ANALYSISWEIGHT=weight_final        
    /PRINT PLAN  
    /DESIGN STAGELABEL='Stage 1' STRATA=region_num sub_region_num comm_size_num
    CLUSTER=residence_merge  
    /ESTIMATOR TYPE=EQUAL_WOR  
    /INCLPROB VARIABLE=inc_prob_1  
    /DESIGN STAGELABEL='Stage 2' STRATA=age_gender_group  
    /ESTIMATOR TYPE=EQUAL_WOR  
    /INCLPROB VARIABLE=inc_prob_2.  

Thanks in advance for your help!

*edited for formatting"
Dynamic Programming in Python: Bayesian Blocks for constructing histograms,1,37,False,False,False,statistics,1487278458,False,
1.6 Introduction to Binomial Distribution,0,0,False,False,False,statistics,1487280006,False,
Seeing if actual bus waiting data is in fact exponential/memoryless,14,1,False,False,False,statistics,1487283039,True,"I blog about probability/statistics/data and this week [I wrote a post](https://perplex.city/memorylessness-at-the-bus-stop-f2c97c59e420#.pkzwl1qr6) comparing the theoretical act of waiting for a bus (often used as an example of the memoryless, exponential distribution) and real data published by the MTA here in NYC.

This isn't just self-promo though. I was wondering if anyone was interested in looking at (or confirming!) the [R code](https://github.com/PerplexCity/Bus_Wait/blob/master/bus_wait.R). MTA data is a series of pings, so to get estimates of when buses actually get to stops, you have to take the final ping before each bus's ""next stop"" field changes.

I was also wondering if anyone had tried something similar and found more robust examples of continuous, exponential distributions? Or if they had any advice about how to test for something like this in general? One thing I had trouble with was setting a sensible binwidth when graphing the data as a histogram.

Thanks!"
Why does it say I got it wrong? I added up everything in the right column correctly multiple times,2,0,False,False,False,statistics,1487284447,False,[deleted]
SD vs SEM,0,1,False,False,False,statistics,1487286139,True,[removed]
Why is geometric mean used here instead of arithmetic mean?,6,5,False,False,False,statistics,1487288406,True,"For the plots on the bottom of the page:

http://www.procalcitonin.com/clinical-utilities/sepsis/reference-values.html

I read on wikipedia the rationale for when you'd want to use geometric mean but I'm not seeing that apply here. So why was it used instead of the arithmetic mean?"
Standard deviation study question?,5,0,False,False,False,statistics,1487292826,True,".For a recent period of 10 years, there were 680 atlantic typhoons.Find the Standard deviation of typhoons per year."
Z-Score and Sports,3,4,False,False,False,statistics,1487294507,True,"Hello all,

I want to start this off by saying I have only briefly taken statistics a long time ago and my memory isn't the best. 

Nonetheless, I was having a debate with someone about sports and different players in different eras, and how there point productions are not comparable. Is it possible to standardize these different eras point production in order to have a way to compare. I remember z score doing such a thing but it would be great to be able to quantify and compare players based on skill and other factors. 

Please let me know 
"
Methods for visualizing the association between a dichotomous vs. categorical variable,0,1,False,False,False,statistics,1487301583,True,[removed]
1.7 Introduction to Poisson Distribution,1,4,False,False,False,statistics,1487323800,False,
Looking for the best strategy to win the lottery!,0,1,False,False,False,statistics,1487325850,True,[removed]
Test for a stationary time series?,4,5,False,False,False,statistics,1487336437,True,"Hi,
As the title indicate, can we test if a time series is stationary? And if so what methods?

"
What is the best way to do Principal Component Analysis (PCA) with Multiple Imputation in R?,8,4,False,False,False,statistics,1487348120,True,"I am planning on using PCA but my database is missing some values. I've used the ""mice"" package in the past to do regressions on this data by:

1) doing 5 multiple imputations,

2) running the linear model on each, and then

3) pooling my results suing the ""pool"" function.

Is there a similar way of doing PCA and then pooling the results of the different analyses? I'm asking because every time I run the analysis using random seeds my results are (as would be expected) slightly different, and so I would like to be able to pool all of them to get more consistent results.
If anyone has any suggestions on the best way to do this they'd be greatly appreciated."
How to Fit a Psychometric Function in R (Visualizing MLE using lattice package),1,45,False,False,False,statistics,1487349798,False,
How do contrasts and Scheffe's correction work in this example?,2,1,False,False,False,statistics,1487352750,True,"Suppose you have 4 correlation matrices C1 to C4 (all 10 by 10, each matrix estimated based on samples of different sizes). Suppose that you want to compare each entry of each of these matrices to some reference value in another matrix X (which we consider given, deterministic). Suppose for the moment we have a test statistic and a corresponding distribution against which we compare the test statistic to.

One problem with this set up is that, for each correlation coefficient in each matrix, you have a null hypothesis 

**H0: c^k_ij = x_ij** 

i.e. that correlation coefficient in entry (i, j) in matrix C_k is equal to the reference value at position (i, j) in matrix X. 

In other words, we run into the problem of multiple comparisons. I've been told that Scheffe's method is one way of countering the multiple comparisons problem. The problem is that I don't understand how to use this method in this case. How should I properly view contrasts here? 

If Scheffe's method is not appropriate here, would a Bonferroni correction make more sense. From what I understand, we have 45 correlation coefficients per matrix and we have 4 groups. Would that mean I divide alpha by 4*45 = 180?

Thanks in advance for any help."
1.8 Control Charts at your finger tips,0,1,False,False,False,statistics,1487366651,False,
Advice for how to approach a quant case study question,1,1,False,False,False,statistics,1487369682,True,"I recently had a phone interview with a consumer tech company for a quant position. The question was basically, ""imagine a facebook style social network site. Six months ago a new feature called 'mentions' was added which allows you to tag your friends with an @ sign. How would determine whether this feature was a success?""

I was a bit taken aback by how broad the question was. I first asked if the feature was given to everyone in the network or a sample, to which the interiewer responded ""you decide"" - meaning I could approach the analysis either way. I talked in general terms about calculating week over week usage of the feature as well as month over month. I also discussed computing a baseline metric for product interaction and then comparing the usage of the new mentions feature relative the baseline statistic. Overall I left the interview feeling quite dumb, as I have a pretty solid command of stats, but came away looking like an idiot. 

Are there specific statistical procedures to test for something like this? al la A/B testing, or some kind of hypothesis test? And secondly, is there a good framework for approaching these types of open ended case study style questions in general? "
Stat homework help!,0,1,False,False,False,statistics,1487372264,True,[deleted]
How to stop a layperson from doing bad data analysis,0,1,False,False,False,statistics,1487377773,True,[removed]
Interpreting where the statistically significant difference is in a chi-square analysis (SPSS),2,1,False,False,False,statistics,1487380013,True,[deleted]
"Making a huge table fit in a two column pdf in R Markdown, help?",6,3,False,False,False,statistics,1487380573,False,
How do I test how good I am with picking stocks? Here is a method that I'm using. Please give me feedback.,1,1,False,False,False,statistics,1487383192,True,"I've created a math model which predicts the future value of a stock. I want to calculate how accurate (or inaccurate) this model is. 

I'm thinking that one way to go about doing this is to monitor the stock price on the initial date along with the index which tracks that stock, and then compare these values with the final price of the stock with the final index price.

For example, let's assume that on July 1, 2016, I made a calculation based on my math models that the future price of a stock was going to be $116.70. I purchased this utility stock on July 1, 2016, and the utility stock was trading at $112. Also, since this stock was in the utility sector, I observed that the Dow Jones utility index was at 716.

Today, the stock is trading at $118, my prediction was that it would be trading at $116.70. Meanwhile, the Dow Jones Utility Index (DJUI) was at 672. 

Therefore, there are 3 metrics we must consider here:

* The actual ROR of the stock is (118/112)-1 = 5.36%
* The estimated ROR of the stock is (116.70/112)-1 = 4.20%
* The ROR that was ""estimated"" by the Dow Jones Utility Index = (672/716)-1= -6.15%

Using my approach, the Dow Jones would've assumed that my stock would've been $105.12. My model assumed that it would've been $116.70. Finally, the actual price of the stock was $118.00. 

I believe that the method to determine which model is best is to sum up all the error terms, in this case the error terms are $1.30 and $12.78, respectively, and choose the model with the smallest error term.

What do you think of this approach?


"
Reporting chi-square in a table (APA style),7,2,False,False,False,statistics,1487398472,True,"Hi Everyone, 

I have a fairly simple question: Are you supposed to report the findings from chi-square analyses in a table in an academic paper? Or, are the written interpretations in the ""findings"" section enough? 

I have spent the last 30 minutes Googling, and I cannot seem to find any good examples of a chi-square table for an academic paper (APA style). If you have any good examples, can you please share? Or, if you think a table is required, what should I include on it? 

My IV is a dichotomous categorical variable (white v. non-white), and I have 14 categorical DV's. 

Thank you!"
"Need help to evaluate players, according to their attributes",3,4,False,False,False,statistics,1487408318,True,"Look at this screenshot: http://prntscr.com/ea8oln

 

It is 33 players, some of the best defenders in a football manager game, their respective scores for 30 attributes, like tackling and marking, and the average rating they produced, when they actually interfered with the ""black box"" match engine we know little to nothing about.

 

I was thinking that if I add each attribute, and then sort them from max to min, I would find the significance of each attribute for the particular posisition.

 

For example jumping is the highest scoring attribute, followed by determination, tackling and marking and strength and heading. Some of the least important attributes are dribbling and off the ball, which all of those make sense.

 

The question is, do you have a better way to rank the attributes and their significance?

 

I want to find weights, how much more important is for example jumping when compared to heading or dribbling?

 

Do I need all 30 attributes, or could I get away with more accurate results if I chose the 10 or 15 or 20 more important attributes?

 

Can I use tools like spss to help me with my calculations and how?

 

Ultimately I want to be able to enter attributes into a formula, and get reliable scores or ratings, to help me decide which player is better and which is worse. "
Creating a new Metric for Predicting Success,21,5,False,False,False,statistics,1487428858,True,"I'm a junior in undergrad studying statistics, and I've covered intermediate statistics/probability. However, I'm curious about solving the problem of taking multiple input random variables (ie sports stats) and using them to create some sort of combined metric to predict a single success variable (ie number of wins). Most of the stuff I've learned involves regression between a single input variable and a single result variable, although I have dealt with finding distributions of functions of multiple random variables. Maybe this is super easy stuff, but what type of course would teach how to do this? Is there a name for this method of statistical analysis? "
Mullowneys Law Personal Injury Lawyer Ottawa lawyers,0,1,False,False,False,statistics,1487433958,False,
stat,4,0,False,False,False,statistics,1487461018,False,
Confused about Statistics questions. Need help.,17,0,False,False,False,statistics,1487468783,True,"I'm 6 weeks into my statistics class and I don't understand a thing. Lot of it seems to be really random and has lots of number coming out of nowhere. 

On problem that confused me is: A card is drawn from a standard deck of 52 playing cards. Find the probability that the card is an ace or a black card.

I thought the answer was simply 4+26 = 30/52= 15/26.
The correct answer is 7/13. How?


Another question I am confused on is;

Empirical evidence suggests that 25% of Florida drivers are uninsured.  If four random Florida drivers are involved in an accident, what is the probability that more than one of them are uninsured?

The answer is 0.26. How? What steps do I even take to figure this out.

Another question I can't figure out is: 

Applicants for a job first submit a written application. Based on the written applications, 40% of the applicants are invited for a first interview. Of those that have a first interview, 58% are rejected after the interview. What is the probability that a randomly selected applicant receives a first interview and is rejected after the interview?

I am also still confused about discrete vs continuous. I've read the definitions over and over again but so many of the questions about them seem like an opinion. "
Is the mean like a one-dimensional regression?,19,23,False,False,False,statistics,1487475865,True,"A linear regression is when one uses a certain number of predictor variables to predict an outcome variable: the regression looks for the model that minimizes the squared error (hence the name ""least-squares"" regression).

Can we see the mean of a variable as a special case of regression, where the number of predictor variables is 0?

In the absence of predictors, then our regression equation becomes simply y = b. If we were to compute the value of b that minimizes the sum of squared errors, then would we always find that b is the mean of that variable? Or does it not work like that?"
I am porting LibRmath.so to nodejs (browser) anyone want to help out?,3,6,False,False,False,statistics,1487494070,False,
Help with some surgery stats using SPSS?,0,1,False,False,False,statistics,1487523147,True,[removed]
How would you test the accuracy of a stock-price predicting model?,10,1,False,False,False,statistics,1487525018,True,"Suppose that you have made predictions for 10 different stocks on March 1, 2016. In a few days, you want to see how your predictions held up. 

 The initial stock prices for the 10 stocks are P1^1, P2^1 ... P10^1. The final prices of the 10 stocks are P1^2, P2^2 ... P10^2. 

However, there are challenges here:

* How would you score the accuracy of your model?
* Would you compare your model's accuracy with the accuracy of some baseline approach? For example, the baseline approach could be to measure your model's accuracy to that of an index. 
"
Thesis idea?,8,0,False,False,False,statistics,1487525161,True,"I'm a 2nd semester statistics grad student. Apparently I have to write a thesis. I don't want to. I have no field of expertise, no field of interest, no clue what I'm doing. Also, every professor is reluctant to help me for what ever reason. Except the one everyone has warned me to stay away from of course.
Yall feel like throwing out some ideas?"
ASA Student Paper Competition:,1,5,False,False,False,statistics,1487526189,True,"Friends, 

I am exploring ideas to begin working on a paper for the 2017 ASA essay contest, and thought I would ask for some help on initial ideas. 

**The only guidelines are that it must tell an interesting, data-driven story. It can come from primary or secondary analyses, but must be widely digestible (18+).** Feel free to riff off these ideas/contribute your own/source interesting material to investigate. These are intentionally broad:

**1)**The ""success"" and/or ""harm"" of dating apps on outcomes like marriage or dating in general. 

**2)** The ""ideal"" time to contact academics. This one in particular interests me as I am in my master's looking to apply again for Biostat PhD's this fall and would love to quantify the optimal time to contact professors about joining their research when they are most likely to read/respond. 

**3)**Tease out the R/SAS debate in the field of public health. Again, in searching for Biostat programs, there seems to be no consistency across departments of preference for such languages. Industry seems to be largely ok with either. Why do these discrepancies exist? Do they really? 

**4**Sneakers. I am mildly obsessed with sneakers and sneaker culture and its bizarre price point volatility. How are ""fakes"" impacting the sale of ""Real"" YEEZY's or similarly coveted sneaks. Can hype (twitter/facebook/reddit chatter and traffic) accurately predict sales or profit? 

All thoughts welcome!"
Desperately in need of help with Stata,10,6,False,False,False,statistics,1487526763,True,"I am taking a class this semester that relies heavily on Stata. The professor had us download Small Stata, which can only hold a low number of variables. I can never make it to her office hours due to time constraints so it is hard for me to get time to work on it with her. Every time I try and upload the GSS data set it says that there are too many variables. I only need about 25 variables out of the GSS data set on my Stata program. Is there a way for me to open the GSS data set in another program, isolate the variables I need, and only upload those variables/ their data to Stata? Just let me know as soon as possible.

PS: This is not a homework assignment, just need help with the basic building blocks for using Stata.  I know how to use R/ R Studio a little bit as I had to use them in a class last semester. "
Cribbage Stats,0,1,False,False,False,statistics,1487526780,True,[removed]
What kind of laptop do you guys use?,13,3,False,False,False,statistics,1487529015,True,I'm currently running analyses on a 2011 MBP.  I hear that  Lenovo Thinkpads are good work horses as well.
A basic question when reading a poll: Does it include or exclude nonvoters?,2,10,False,False,False,statistics,1487530107,False,
Frank Harrell: My Journey From Frequentist to Bayesian Statistics,12,62,False,False,False,statistics,1487540055,False,
Probable Simultaneity Issues,3,3,False,False,False,statistics,1487544111,True,"I am trying to see how movements in the employment levels of each industry (NAICS sector) correlate to movements in the aggregate unemployment rate of certain regions. Essentially I am trying to answer the question: When employment in an industry falls/rises, on average, by how much does the unemployment rate of the region in which that industry was located rise/fall? 

Using what we know about flows in the labor market (three states: employed, unemployed or out of labor force), I ran a regression which relates the unemployment in a region to changes in each industry's employment and changes in the labor force in that region. 

While I get a lot of statistical significance, some employment levels had positive coefficients on them. This is worrying, because we understand unemployment should be inversely related with employment. My intuition tells me that this is stemming from a simultaneity issue, but I have no idea what an instrumental variable would look like. I'd appreciate some help. 

Bellow I attached the results of a FE regression in STATA(by region).

EDIT: frequency is monthly from 2006-2016 on about 12 regions.


http://imgur.com/a/7LztN"
[statistics] Sufficiently large sample size of p hat?,0,1,False,False,False,statistics,1487550381,True,[deleted]
Please help! How do I determine if two different experiments are significantly different from one another?,0,1,False,False,False,statistics,1487555046,True,[removed]
What would a time plot for a stationary set look like?,0,1,False,False,False,statistics,1487555533,True,[removed]
Career in Statistics - Highest Paying Related Careers,3,4,False,False,False,statistics,1487559068,True,"What are some of the highest paying careers in North America and Europe that are in/related to Statistics.

I feel like a Statistics degree is such a versatile degree to have, and I feel like there are a bunch of careers related to Statistics that I am just not seeing or thinking about.

Was hoping this subreddit would enlighten me!"
Statistics of a Carnival Game,29,1,False,False,False,statistics,1487572025,True,"Okay, this is a problem I have been chewing on encoded into a carnival game analysis for ease of explanation. It seems as if it should be a simple application of a binomial distribution but I am stuck.

You are playing a ski-ball type carnival game in which you try to roll a ball down a plank into a smaller hoop encircled within a larger hoop. If the ball lands inside the smaller hoop, it moves an indicator on a scale up one notch (the scale runs from -10 to 10). If you miss and it falls into the large hoop, it moves the indicator down one notch. The higher the indicator, the better the prize you get. It starts at ""zero"" and if you miss an equal or greater number of shots than you make, you get no prize.

You get 10 balls to shoot with. The tricky part is that you are allowed to opt out early at any time and just take whatever prize is currently indicated. You estimate that you make about 50% of the shots you take, and you are wondering if you make the first two shots in a row, if you should simply take the prize and walk.

If you play this game repeatedly and want to maximize the value of the prizes you earn each game, what number on the scale should you quit at? Assume the actual value of the prizes scale linearly with the number scale.

"
Learning SAS for Data Analyst roles?,13,9,False,False,False,statistics,1487576358,True,[deleted]
"multiple testing, benjamini hochberg or bonferroni for 21 independent variables",0,1,False,False,False,statistics,1487576425,True,[removed]
How to open SPSS syntax in Stata?,2,0,False,False,False,statistics,1487591360,True,"I want to use the syntax that converts ISCO-08 to ISCO-88 data in this link http://www.harryganzeboom.nl/isco08/index.htm.

Its made for SPSS, but I want to open it in stata. Does anyone know how to do this?"
Can I use Spearman's rho to test the relationship between a discrete variable and a continuous variable?,9,3,False,False,False,statistics,1487596758,True,"Hi there, I'm doing nonparametric statistical analysis on some data. In this case, each subject has a particular score (say IQ, PHQ-9 etc) and a continuous variable like height. (These aren't the actual variables)


Can I use Spearman's rho to see if there's a relationship between the IQ score and height?


If it's not possible, what other test can I use? Nonparametric tests would be the most preferable.

Thanks!"
How is statistics used in aerospace engineering?,6,0,False,False,False,statistics,1487600782,True,What are some examples.
Are there any tests for causality that don't require time series data?,13,5,False,False,False,statistics,1487605540,True,"I recently conducted a survey and I have a pretty good idea of what the direction of causality is in some of my correlations based on my knowledge of the subject matter, but I'd like to apply any statistical tests that I can that would support my analysis/hypothesis (ie. that what we see is not just correlation but probably causation). However, as far as I know most tests that check for causality require time series data to check for a lagged impact, am I right? Is there anything I can do to try and test my causality assumption?"
New Study by The Center for Disease Control,0,1,False,False,False,statistics,1487614803,True,[removed]
Will statistics be doable for me?,0,1,False,False,False,statistics,1487619150,True,[removed]
"Global sea ice is currently at 4-sigma below mean, and has been as low as 8-sigma below this past year. How can I interpret the unlikelihood of an event like this? Is there a ""100 year flood"" equivalent?",20,43,False,False,False,statistics,1487622008,False,
Best way to create spreadsheet/dataset of horse track,0,1,False,False,False,statistics,1487625495,True,"I'm trying to compile data of a local horse track (look at equibase/daily racing forum) as they only have the before program and results, no compiled lists of lifetime or monthly stats or a registry of one particular jockey/horse/stable over a period of time like you might buy from Daily Racing Forum.

Basically I want to compile all the available data over the last 3 months of horses, jockeys, and stables from pdfs and in the end have a sort of registry I can look at to see how a particular jockey  , horse, etc has performed over the last x months and under what conditions.

Is there an easier way to do this other than making multiple spreadsheets and manually doing the addition for each entry?

Edit: If this isn't the best sub to ask I'd appreciate if someone could point me in the right direction"
Statistical Practice Major - Careers/future?,0,4,False,False,False,statistics,1487627668,True,"Hi all!

I noticed a post on here recently of someone talking about his struggle finding a job with a Physics degree, and most people thought the problem was the fact that he needed to improve on his interviewing skills. Nevertheless, this got me thinking about something I've wanted to ask someone before, and I thought I'd post here and see what people think.

I'm currently a Junior pursuing a Statistical Practice B.A. at the university I attend. I've had a job for about 2 years now at a sports data company monitoring betting markets and managing scouts. To summarize: I don't do any sort of data analysis, but I do work in high stress situations, and I'm constantly communicating with other colleagues around the world regarding scouts and information I'm gathering. 

I'm really interested in a Data Analytics route, but I feel like my GPA (~2.7) isn't good enough for an internship as a Data Analyst somewhere based off of the postings I've seen. I'm a pretty much screwed or is there something someone can recommend to increase my odds of finding something after I graduate? 

Thanks!"
How viable is my plan to go from a Bio undergrad to an MS/PhD in biostatistics?,0,1,False,False,False,statistics,1487627780,True,[removed]
Quantification of compounds : Experimental design,0,1,False,False,False,statistics,1487628225,True,[removed]
HELP!---Excel Data Analysis,0,1,False,False,False,statistics,1487628233,True,[removed]
Is it reasonable to run CV on whole data (in a regression model) when the sample size is small?,1,1,False,False,False,statistics,1487629685,True,"My sample size is 550, and I build a regression model with 80 features. In particular, I am using LASSO and ridge regression with 5-fold leave one-out cross-validation. To evaluate the models, I am using correlation (between the predicted and actual value) and mean absolute error (MAE). The correlation I obtain is around .5, and the MAE is around .8. The value of the variable that the model predicts is in the range of 0-8.

My results seem to be promising (but not perfect). Since the sample size is small and the feature set is large, I wonder if it is reasonable to run CV on whole data and evaluate the performance of the model based on this? I am working on an academic publication, and I wonder if this is an acceptable method (with limitations reported). Or, should I use bootstrapping to resample the data, and apply CV on only the training set? I appreciate any insights."
HELP! about solving P- Value.,3,1,False,False,False,statistics,1487650552,True,"I have conducted an experiment and i have 27 samples and make an equation of a line using regression analysis, may question is how to get the level of confidence and the alpha and what test will i use? z or t?
I have a mathematical model, and I don't know the expected value, should I the equation to get the expected value?"
What is wrong with my R code?,1,1,False,False,False,statistics,1487657982,True,[deleted]
Looking for statistics on what countries have the cheapest and most expensive long distance trains (can only find commuter stats),0,1,False,False,False,statistics,1487670206,True,
8 Benefits of Statistics Tutor Help Service for Securing High Score,0,1,False,False,False,statistics,1487670872,False,
Moran's I,2,0,False,False,False,statistics,1487680199,True,"Can some show how to work out this problem with a kings/queens case? I understand you generally wouldn't compute by hand but this is just brief example working with formula. 

http://imgur.com/a/JjVug

Thanks"
Tower Property of Conditional Expectations [need intuition],7,9,False,False,False,statistics,1487681943,False,
What statistical test could I do ? (I know the data is categorized horribly),6,0,False,False,False,statistics,1487692415,False,
Implementing point based Wombling in R - Am I reinventing the wheel?,0,1,False,False,False,statistics,1487694901,True,"I am currently working on a project where I want to compare the ""boundedness"" of frequency data associated with several point processes (essentially I want to have some way of quantifying the degree to which boundaries have rapid or diffuse fall offs in frequency in different directions). Point based Wombling seems to be a good fit for this. I found a seemingly defunct WOMBSOFT package for R that implements a grid based (moving window) Wombling approach and a few recent examples of code for areal Wombling relying on polygonal data. I have yet to find exactly what I want which is point based Wombling for irregular point patterns using Delaunay triangulation. I'm thinking I may need to implement this myself but I wanted to check and see if any folks here knew of a previous implementation I may have missed."
What's a great applied book on cluster analysis?,0,3,False,False,False,statistics,1487700159,True,"I'm comfortable with the strengths and weaknesses of many clustering algorithms like k-means, hierarchical clustering, DBSCAN, etc., but I have very little experience using cluster analysis in practice.  I need to use cluster analysis to segment customers based on website behavior and information on a variety of information on how they signed up.

What's a great applied book on cluster analysis that covers these use cases via case studies?  I'm looking for something along the lines of Applied Predictive Modeling for unsupervised learning."
[Question] - How to properly compare power spectrums (or 2 lines with multiple x values),2,1,False,False,False,statistics,1487701286,True,"Dear reddit: Statistics:

I have a question that have been bugging me for some time.

If i have a data set that looks something like the following:
http://brc.nctu.edu.tw/BRC/results/92_shinhon/ex1.bmp

I wish to know at what x values (in the example frequency) that the blue and red line are significantly different from each other.

My best idea is to take the mean over a range and compare these e.g. the mean of 1:10, 10:20, ... 30:100 and then do an one-way ANOVA followed my a post-hoc for multiple comparisons.

My problem is if i want to use smaller bins. e.g. not having to take the mean over a wide range but use the ""raw"" results, e.g. 1,2,3, ... 98,99, 100. The problem is that this would lead to nothing significant as the post-hoc will reduce everything.

What is the correct way of doing such a comparison like this?

Thanks alot!"
NEED HELP SETTING UP A 2X2 FACTORIAL DESIGN RESEARCH STUDY 4 EXPERIMENTAL PSYCH,1,0,False,False,False,statistics,1487707758,True,[deleted]
Help with Regression Model,2,3,False,False,False,statistics,1487709418,True,"Thank you in advance for your help.


I am attempting to explore relationships between organizational characteristics and performance on a set of metrics that are measured over time. So for example, Score = Region + Employees + Margin .... + year.


I am thinking that I may have a problem because all of my organizational characteristic data will be repeated for every year iteration. Is this a problem I should be worried about? If so, how does one go about correcting for it?"
How many subjects should be in my psychological experiment?,2,0,False,False,False,statistics,1487710471,True,"Hello! I'm working on my dissertation proposal, and I'm having a hard time understanding how many subjects to plan for. I've read an interesting presentation that implies at least 50 measurements per conditions, and that seems difficult/expensive.

_

The test is to get subjects to engage in different 'phases' of problem solving, by progressing to different points in a problem solving episode and then stopping them for testing. 


* Group 0 - never has a chance to examine the offerings
* Group 1 - Contemplates a variety of offerings of 10-15 minute activities and considers which one they might want to engage in.
* Group 2 - Selects from one of the 7-9 offerings and plans out how they will attack it
* Group 3 - Begins the activity

_

So do I really need 200 persons? And what if I then try to split the groups into early and late interruption, ie 1 minute of planning versus 3? 

_

I'm also planning a team version, ie a team together engages and then is interrupted, separated, and tested. Trying to extend the *Rubicon Model* aka *Mindset Theory of Action Phases*.

Thoughts? "
1.8.1 Individual and Moving Range (I-MR) Control Chart,0,2,False,False,False,statistics,1487718321,False,
Help with Panel Analysis,0,1,False,False,False,statistics,1487718326,True,[removed]
Multivariate Statistics Project Ideas?,0,1,False,False,False,statistics,1487723550,True,[removed]
Best way to score pair differences,0,1,False,False,False,statistics,1487725849,True,[removed]
Mixture Models in R,5,6,False,False,False,statistics,1487728078,True,I am working with some data which I expect a mixture of an exponential and log-normal (or gamma) distribution will be a good fit. Does anyone know any good tools (preferably in R) for fitting mixture models? I would prefer not to have to code up my own EM algorithm to fit this.
ANOVA for Conversion Rates?,10,7,False,False,False,statistics,1487728772,True,"I'm blanking on how to test ratios for more than two groups. 

If I have Visitors, Orders, and Conversion Rates for three test cells for a landing page test, which test would I use to compare the conversion rates? 

Does an ANOVA work here? "
Bell Curves and Averages,8,1,False,False,False,statistics,1487736725,True,"Question - I've been out of school for a little while.  I took statistics, calculus, etc and have a degree in comp sci...My employer is always timing things like how many days between when a task is assigned and when it is completed.  How much time does it take an employee to complete a given task.  They usually gather the timing data, taken over weeks or months, average it, and BOOM! that becomes the new metric for completing that task.  Before I am trolled, I work in a department that measures these things for process improvements.  These measurements have no impact on what is expected of me.

I'm trying to argue the benefits of a bell curve - use the outlying tails to the curve to find out if there are unique cases or bad/excellent employees at each end of the curve, rather than just keying in a bunch of times and using the average.

The problem that I'm having is explaining the difference between an average (a single point of data) and the middle 68% of a bell curve?  Help?"
SIBS Experiences,4,7,False,False,False,statistics,1487738391,True,"Reposting from /r/math career thread since I didn't get any responses.

Anyone familiar with or attended any of the [SIBS](https://www.nhlbi.nih.gov/research/training/summer-institute-biostatistics-t15) programs for undergrads? I got an acceptance to one (U of Minnesota) and I am curious about anyone else's experiences."
Statistics REU thread,9,4,False,False,False,statistics,1487746736,True,"I don't think this violates any of the subreddit's rules and I think we have a healthy undergraduate following so...

has anyone heard from any statistics REUs yet? I know of:

Oregon State

Winona

Emory

SIBS (at a handful of schools)

Wisconsin

Harvard

Dordt

Michigan

CMU"
What percentage of scores fall 1 standard deviation above the mean?,28,3,False,False,False,statistics,1487747129,True,"Got this question incorrect on a quiz and am having trouble wrapping my head around it due to the wording.

""Consider the properties of a perfectly normal distribution. What percentage of scores fall 1 standard deviation above the mean""

Appreciate any feedback!

Image i used for reference:

http://www.statisticshowto.com/wp-content/uploads/2013/09/standard-normal-distribution.jpg

"
multivariate question: Hotelling T^2 vs Likelihood Ratio Test ?,0,4,False,False,False,statistics,1487748561,True,"What's the difference?

My professor said ~~MLE~~ Likelihood Ratio Test is a more comprehensive test but didn't go further into detail what comprehensive means.


My casual google I've seen the pro of Likelihood Ratio Test is that you don't have to find the inverse of the variance-covariance matrix. 

Also is T^2 a frequentist approach where as Likelihood Ratio Test a likehoodist approach? I've casually read over the three school (on top of Bayesian) but didn't dive into that rabbit hole.

Thank you "
How can I use my Factor Analysis output?,2,1,False,False,False,statistics,1487758839,True,"The output of my factor analysis is as follows: http://imgur.com/a/hIUrV


Would it make sense to multiply the responses of my survey (scores from 1 to 7) against the factor loadings to obtain weighted means for each factor? 


For example, if respondent 1 answered X7=**3**, X9=**5**, X10 = **4**, ...., could I multiply the responses times the factor loadings like this: **3**x0.66, **5**x0.58,  **4**x0.71 , .... to obtain a total ""factor score""?"
Can lambda be a non-integer in the Poisson distribution?,8,3,False,False,False,statistics,1487769256,True,"Essentially, lets say that we have a bridge with lambda of 5 cars per hour, to figure out the probability of at most two cars driving over in half an hour is it then valid to say lambda equals 2.5 in the CDF?"
"I need some help explain this Interaction plot, please!",14,5,False,False,False,statistics,1487769320,False,
Why use ARIMA model in forecasting?,1,7,False,False,False,statistics,1487786543,True,"Hi r/statistics. May I ask your thoughts on using ARIMA model for forecasting? A comparison with another forecasting technique would be great. 

Thanks. "
What is your opinion of Malcolm Gladwell's books?,0,0,False,False,False,statistics,1487790544,True,[deleted]
"Kenneth Arrow, Nobel-Winning Economist Whose Influence Spanned Decades, Dies at 95",2,82,False,False,False,statistics,1487791011,False,
Traffic generator,0,1,False,False,False,statistics,1487791199,True,[removed]
A Question of Scientific Methodology: What is the most accurate way to find the average response times for correct-only trials?,3,1,False,False,False,statistics,1487795489,True,"I hope I am posting this to the right subreddit. I thought there might be a fair amount of overlap with statistics and scientific experimentation.

I am a research assistant working with response time data for computerized trials on a task where a person is either correct or incorrect. As it has been told to me, it is important to use only the correct trials when calculating the average response time for a participant, because it reduces the potential noise in the data that is caused by little effort or purposeful incorrectness.

As an example, a person is given 6 trials and their time is measured in seconds. Each trial was correct. Here are those values in a list:  
[3,4,2,1,5,5] 

The average of the correct RT in this case would be 20/6 = 3.33 seconds
However, lets suppose that that this person got 2 wrong. The list would instead look like:
[3,4,2,1]

How do we calculate the average in this instance? The way I was originally doing it, was to divide this sum by the amount of correct trials (4), instead of all trials (6). The problem with this is that it leads to an artificial inflation in response time for people who have more incorrect scores. The example I was using to illustrate this effect is this: 

Assuming that all sums are equal for participants, then the sum that is divided by the smallest number of trials will always be bigger.
[9,6,5]  sum = 20 avg =  6.66
[8,5,4,3] sum = 20 avg = 5
[7,4,3,2,4] sum = 20 avg = 4
[6,3,2,1,3,5] sum = 20 avg = 3.33

Yes, it is unlikely that participants will have the same sum, but the effect of having a smaller denominator on the mean will always inflate the value slightly. This become relevant when we think of studies where normal controls are compared to people with illness. Due to the illness, patient population normally have more incorrect answers than healthy controls. This means that, when looking for a significant difference between the groups, the correct response time values for the patient population will probably be larger, in part, because of the reduced amount of trials that are used in calculating their mean. 

The problem with this, is that I can really see no other alternative.
If we just divide the sum of that participant from before by the amount of all trials, then we are effectively adding 0s as place holders for those incorrect trials:
[3,4,2,1] = sum = 10 avg = 2.5
[3,4,2,1,0,0] sum = 10 avg = 1.6

But this is not the case either! Those trials were not zeros, because there would have been some possible amount of time spent on the trial. This method, instead, deflates the averages by assuming that nothing happened during those trials. 

So which is it? If anyone out there can offer me some insight into this problem or tell me my math is incorrect, then that would be very helpful.  




 

"
1.8.2 Xbar and Range Control Chart,0,0,False,False,False,statistics,1487803038,False,
Do NBA Lottery Odds Add?,1,0,False,False,False,statistics,1487805545,True,"For anyone who is unfamiliar, [here is a link to the NBA Draft lottery odds](http://www.tankathon.com/). To explain the context of my question, Team A has both the rights to their own pick as well as the rights to swapping with Team B if Team B were to pick higher than them after the lottery. 

I understand that the odds of being in the top 3 do not add, but what about being number 1? To go back to the previous example, Team A has a 25% chance of being #1 and Team B has a 19.9% chance, does Team A ultimately then have a 44.9% chance? "
ANCOVA and a categorical covariate,2,2,False,False,False,statistics,1487806774,True,"Is it possible to perform an ANCOVA with a categorical covariate? 

Like for example a covariate of gender, how do I run it as a covariate in ANCOVA since it needs to be continuous? 

"
Calculating Z Score,0,1,False,False,False,statistics,1487809187,True,[removed]
Looking for some book / resource recommendations.,1,1,False,False,False,statistics,1487815818,True,"Hi, I'm looking for books or other resources on the following topics. I am mathematically mature, so books at the graduate level are fine.

1) A general book on statistics. I've taken courses on probability, stochastic processes, and machine learning in the past, but never any more traditional stats class. I'm looking to expand my toolbox, hopefully to the point where if I don't know something, I know enough to google for what I'm looking for.

2) A book on working with time series data. I don't know if this is a standard thing or not but I'm also interested in the case where I am missing measurements for some variables at different time steps.

Thanks for the help!"
College Time Series,1,0,False,False,False,statistics,1487815958,True,"Hello, I apologize if this is the wrong thread for me to be asking this question, but I thought I'd give it a try anyway.
I am currently a college student majoring in Statistics, and I'm about halfway through my Time Series Analysis class. Long story short, Time Series is not a good class for me to be taking, as I haven't taken some of the prerequisites and I don't understand many of the things this course takes for granted. Unfortunately, I have to take the course regardless.
I was wondering if anyone had any advice on how I could catch up. I hardly know what a time series is, let alone white noise, covariances, Zt, stationarity, and so on.
Even when my professor explains things to me, they make no sense.
Any suggestions on how I can fill in the gaps?"
Imdb top 250 and bottom 100 calculation,0,1,False,False,False,statistics,1487821512,True,[removed]
NCSU Master's Question,3,1,False,False,False,statistics,1487824290,True,"Hey guys,

I'm looking for Master's programs to apply to, and I came up on NCSU, but when I was filling out the app I noticed that the [the degree says ""MR,""] (http://imgur.com/83MFnmK) not MS. The description below says MR is a ""Master of (non-thesis)"", and MS is a ""Master of Science."" Anyone else seen this before, and does it even make a difference?"
Statistics phenomenon on the tip of my tongue...,0,1,False,False,False,statistics,1487826605,True,[removed]
"Undergrad: Stuck on a dumb homework question, can't seem to get it right. Help very appreciated.",0,1,False,False,False,statistics,1487828103,True,[deleted]
What are the assumptions in performing a pooled t-test?,0,1,False,False,False,statistics,1487829197,True,"How do I explain if they are reasonable or not

Edit: Are the following assumptions answering my question? If so, how do I explain if they are reasonable or not?

1) The sampled populations follow the normal distribution.
2) The two samples are from independent populations.
3) The standard deviations of the two populations are unknown but equal.

Thanks. "
"Comparing odds ratios from the same sample. Mixture modeling (LTA, LCGA, GMM).",0,1,False,False,False,statistics,1487830198,True,[removed]
"Calculating if significant change in F-score for 2 groups, based on sentence-length",1,0,False,False,False,statistics,1487838672,True,"I have 2 sets of F-scores for 2 systems. They each have an F-score for a certain sentence length, e.g. system A scores 0.6 for sentence length 10 and system B 0.5, for sentence length 11 they score 0.58 and 0.48, for 12 0.59 and 0.42 etc, etc.

Now I already know that system A outperforms system B significantly. However, I want to know whether this difference significantly increases when the sentences get longer. The data is paired, but the observations within each group are not independent (for both groups it's the case that when sentence length increases, the performance decreases). Thanks for helping me out!"
[Basics] What's the difference between correlation and association? do people often confuse these two terms?,19,13,False,False,False,statistics,1487863318,True,
Results from 5x2 factorial design with conjoint analysis,0,1,False,False,False,statistics,1487864507,True,[removed]
Regression Comparison - Nuclear Medicine Application,10,7,False,False,False,statistics,1487871645,True,"http://imgur.com/a/bYXNK

The basics of my problem:  I have applied a small but important transformation to my raw data, and I need to demonstrate that this transformation was correct; that is, the transformed data more accurately reflects the idealized model than the raw data did.

Performing a simple regression doesn't help me out much here.  I have so many data points overall, and the transformation really only affects the values at the extreme low end of my independent variable, that my R-squared value changes from 0.999942 (raw) to 0.999965 (transformed).

Is there some kind of sliding regression technique I could use to demonstrate how much better the model fits on the first half of my graph?"
SPSS help,7,1,False,False,False,statistics,1487896890,True,How do you put in data for a multiple response checklist? 
Stabilizing variance that decreases over time in oscillating timeseries,5,5,False,False,False,statistics,1487910634,True,I have some oscillating timeseries data that decreases in mean and variance over time. I am interested in stabilizing or normalizing the variance with some relatively straightforward transformation. Does anyone have any suggestions on what to try? Most of the examples I've seen involve transformations on timeseries whose variance is increasing as a function of the independent variable.
Sample 2 solution.pdf,0,0,False,False,False,statistics,1487915256,False,
"Which non-trivial probability density do you use when teaching? Gaussian has no explicit analytical integral; uniform requires attention to bounds.. Is there an explicitly integratable distribution, preferrably over R, without any ""breaks"" like sgn() or abs()?",0,1,False,False,False,statistics,1487928439,True,[removed]
Survival analysis: loss-to-follow-up in registry data,6,5,False,False,False,statistics,1487937778,True,"Population based registries of repeatable common events will contain an observation for each time an event occurs. We could therefore theoretically perform a time-to-event analysis using birth of each individual as the start time. We can assume we have date of death for each individual and can use this as a censor. However, population registries seldom contain other loss-to-follow-up events, such as emigration. 

Is there a reasonable or even validated way to define if and when a loss-to-follow-up event occurs, using a defined interval of lack of observation in the registry as a defining characteristic? 

e.g. if individual X does not have a new observation by time t, he is defined as being lost-to-follow-up and censored. The question is: what is a reasonable time t?"
This Man Is About to Blow Up Mathematics,0,0,False,False,False,statistics,1487945195,False,
Masters Programs,6,5,False,False,False,statistics,1487946791,True,"I'm nearing the end of my undergraduate degree in statistics and will likely be applying to grad schools a year from now. I really enjoy regression and prediction aspect of statistics and would want to probably pursue something like this. However, with the way data science and machine learning has kind of become this huge thing over the past few years, I was wondering if it would make more sense to head in that direction instead? How much of a difference is there between someone concentrating on regression vs. machine learning? Lots of overlap? Also, I have an interest in biostatistics as well and was wondering if someone could describe how their masters was in that and what they did after. Finally, I am from Canada and probably plan to stay here for grad school, if anyone could talk about their experience at any of the Canadian universities, that would be awesome! So, this topic is kind of open ended but any general advice is also appreciated. Thank you. 

Edit: ignore bad grammar, typed from phone"
Advice on statistical significance between fit parameters.,0,2,False,False,False,statistics,1487951182,True,"I'm analyzing kinetics data and have no clue how to test for statistical significance between the fit parameters between different sets of data.  I'm using nonlinear regression and bootstrap analysis.  Any advice on where to turn or useful resources would be absolutely helpful.

Thank you in advance for your time."
Stochastic Calc Question,1,0,False,False,False,statistics,1487954356,False,
"Having to create a survey and have at least 30 people reply to it, and I would like Reddit to help!",1,0,False,False,False,statistics,1487955973,True,"For a project in my statistics class, we have to record at least 30 people who respond to a survey of our choosing with a specific voting method. The method that I am doing is the approval method. If any of you can take the time to just give your name and which cookies you approve of vs which ones you don't, it would be very much appreciated! Thank you!

https://goo.gl/forms/Xy1MSOGW0Aoorkt82"
Comparing beta coefficients within the same model,3,1,False,False,False,statistics,1487960705,True,"I wanted to compare beta coefficients within the same linear regression model. But I'm having trouble finding methods to do so. I'm only given beta, SE and p-values to test. I do not have access to type I SS for the full model. 

I tried using a Z test. But I relaized that it might be more apprioriate to use while comparing two different models. "
Announcing ggraph: A grammar of graphics for relational data,1,52,False,False,False,statistics,1487962508,False,
What's it Called When You Mess with your Endpoints?,4,1,False,False,False,statistics,1487966679,True,"If I've got a dataset that supports one conclusion, but I omit certain datapoints at the beginning and/or end to show it supporting a different conclusion, what logical fallacy is that?

I want to say *endpoint bias* but googling that term doesn't seem to suggest it's the right term.  What term am I looking for?"
"Can I use a paired sample t-test? Basically I am measuring how the severity of negative publicity of a celebrity endorser effects a consumers view on the endorsers credibility using 3 different levels of severity (pre scandal, post mild scandal, then again with the most severe sandal)",6,1,False,False,False,statistics,1487966923,True,"For each of my hypothesis I am measuring the consumers view towards the endorsers credulity before the introduction of negative publicity then after the endorser has been involved in drunk driving and lastly after being involved in a murder. I'm readily struggling to pinpoint which test can be used is SPSS. Any help would be hugely appreciated! 

My dissertation survey is below which may make this post make more sense. 

Thanks in advance 😊

https://plymouthbusiness.eu.qualtrics.com/SE/?SID=SV_3BIUs3LvnfOM5ed "
Accuracy of gaussian process regression: confidence interval vs RMSE?,5,2,False,False,False,statistics,1487972925,True,"(xpost r/AskStatistics)

Hi all, I am interested in using gaussian process regression for a chemometrics problem where the composition of a sample needs to be determined based on its spectrum. In the past, I have used partial least squares for problems like this, and assessed the accuracy by calculating the RMSEP of a test set.

With GPR, the prediction is probabilistic, so according to [this](http://scikit-learn.org/stable/modules/gaussian_process.html), I can calculate the confidence interval for each prediction. So I am trying to wrap my head around how to report the accuracy of a GPR prediction. Is it better to still use the test set RMSE, or does the confidence interval make the RMSE unnecessary?"
Building and interpreting random effects models,1,6,False,False,False,statistics,1487978587,True,"Hello /r/statistics!    
    
Hope this is an appropriate forum for this kind of question.    
    
So I am attempting to use a random effects model in R, not to determine difference between groups, but to obtain accurate variance estimates for each group I am looking at. The experiment consisted of a nested design as below:    
    
Patients --> >= 3 x samples per patient --> 2 x replicates per sample    
    
I have determined that the patients are random effects, with the 3 samples nested within each patient, and the 2 replicates nested within each sample. So what I want to end up with is an estimate of the inter-patient variance (*s*²_G), intra-patient variance (*s*²_I+A), and inter-replicate (analytical) variance (*s*²_A).    
    
The dataset is structured as below:    
    
    Classes ‘grouped_df’, ‘tbl_df’, ‘tbl’ and 'data.frame':	156 obs. of  4 variables:    
    $ Patient  : Factor w/ 127 levels ""UID000105"",""UID000107"",..: 1 1 1 1 1 1 8 8 8 8 ...    
    $ Sample   : Factor w/ 9 levels ""Sample1"",""Sample2"",..: 1 1 2 2 3 3 1 1 2 2 ...    
    $ Replicate: Factor w/ 2 levels ""PEG1.recovery"",..: 1 2 1 2 1 2 1 2 1 2 ...    
    $ Value    : num  19.6 19.3 14.6 16.5 12.2 ...    
         
    
Patient | Sample | Replicate | Value    
--------|---------|-----------|------    
UID000105 | Sample1 | PEG1.recovery | 19.623421    
UID000105 | Sample1 | PEG2.recovery | 19.297359    
UID000105 | Sample2 | PEG1.recovery | 14.640957    
UID000105 | Sample2 | PEG2.recovery | 16.473404    
UID000105 | Sample3 | PEG1.recovery | 12.187291    
UID000105 | Sample3 | PEG2.recovery | 12.102564   
... | ... | ... | ...
    
    
I’ve applied a model and got the results shown below:    


    
    fit.lmer <- lmer(value ~ 1 + (1|Patient/Sample), data = df)`    
    
    Linear mixed model fit by REML ['lmerMod']    
    Formula: value ~ 1 + (1 | Patient/Sample)    
    Data: df    
    REML criterion at convergence: 882.0776    
    Random effects:    
    Groups         Name        Std.Dev.    
    Sample:Patient (Intercept)  5.619    
    Patient        (Intercept) 16.541    
    Residual                    1.287    
    Number of obs: 156, groups:  Sample:Patient, 78; UID, 24    
    Fixed Effects:    
    (Intercept)    
    38.59    
    
Am I correct in interpreting the `Sample:Patient` std.dev random effect as √*s*²_I+A, the `Patient` random effect as √*s*²_G, and the `Residual` as √*s*²_A?    
        
Thanks in advance!    "
Is there a method for comparing point estimates after determining an interaction effect?,2,1,False,False,False,statistics,1487981508,True,"Suppose I'm conducting a multiple regression with two predictor variables (X1 and X2) and have found a significant interaction effect.

Let's say X1 = aggression, X2 = gender (men/women), and Y = health. 
So I want to know how aggression predicts health differently for men and for women. Because the interaction (X1*X2) is significant in the regression model, I run simple slopes and plot the slopes for men and women at 1 standard deviation below and 1 standard deviation above the mean of aggression: http://imgur.com/a/AAd1I


Great, so I know the slopes are significantly different from one another.

But what I want to know is: how do I determine if point ""Ym1"" is signficantly different than point ""Yw1""? Also how do I determine if point ""Ym2"" is significantly different than point ""Yw2""?

Does anyone know what the standard method of doing this is? And what the method is called?

Any help would be really appreciated!"
Question on Piecewise Constant Hazard Model,0,2,False,False,False,statistics,1487986576,True,"So i tried fitting a piecewise constant hazard of interval 12 months to the data. However, some of the interval does not have any events. Naturally, the coefficient estimate would fail to converge. My question is, do i need to use these coefficients that failed to converge inthe plot of my cumulative hazard function? "
Comparing Asymmetric Ranked Order Data,0,7,False,False,False,statistics,1487991205,True,"I'm looking for a metric I can use to compare two ranked order lists of different length. Any suggestions?

For more details, I have a bunch of observations from people saying what they believe the top three issues are, and another bunch of observations from people saying what their top eight issues are (out of eight possible choices, so the lists will contain the same items, just in different orders), and I want to measure the distance between the three issues and the eight issues, without just cutting out a bunch of the data from the second set of observations. "
"Most ""divisive"" films on Metacritic (highest standard deviation)?",7,17,False,False,False,statistics,1487991872,True,"I've realized that some of my favorite films are the ones that have middling aggregate scores on Metacritic.  These are the ""interesting"" films that do not please everyone, and that people cannot generally agree on.

But they are NOT middling films where everyone agrees that they are mediocre.

In statistical terms, these interesting, divisive films are the ones with the largest standard deviation (or largest variance) in their scores.

I've never seen a list pulled from Metacritc that ranks films by rating variance.  For example, what is the most divisive film of 2016?  That seems like an interesting question.

Is anyone aware of work that has been done in this area?

It looks like Metacritic isn't keen on handing out their data...  So it seems like it would be something they would have to implement internally.

EDIT:

Okay, I up and did it myself:

http://onehouronelife.com/metascoreStdDev.html

This HTML is updated nightly by a cron job that scrapes the full list of critic scores for every film on the past-90-days list (currently 141 films).

The results capture exactly what I was hoping for.

For example, *Why Him* has roughly the same Metascore as *Bitter Harvest*, but it has a dramatically higher standard deviation.  Everyone thought *Bitter Harvest* was cliche and uninteresting, but some people (like me) thought *Why Him* was pretty good.  I wouldn't want to waste my time seeing *Bitter Harvest*, but without looking at standard deviation, there's no automated way to differentiate between these two films.

Not sure how long this script will keep working.  There's a link to the source at the bottom of the page.

This really seems like something Metacritic should add, especially for the ""mediocre"" films.  Are critics divided about this film, or not?"
Goodness of Fit of Nonlinear Model,2,1,False,False,False,statistics,1487995958,True,What is the best measure of fit for a non-linear model?
mixing weighted avg and other values; normalization,0,1,False,False,False,statistics,1488006557,True,[removed]
What is a very good statistics lecture?,13,1,False,False,False,statistics,1488016895,True,"I am looking for a good, thorough statistics lecture. Does anyone know what I should start with?"
SPSS Multiple Regression question,5,9,False,False,False,statistics,1488020616,True,"I have conducted a multiple regression analysis and it all seems fine from the output, however all of the unstandardised coefficients (B and Std. Error) for each variable  are .00 (0.001 or 0.002 not 2 dec points). Is this a problem or something I need to explain?

I conducted another regression on another test within the study and the unstandardised coefficients were a lot more varied and larger values.

Thanks!"
"How does one calculate the log of the likelihood ratio, given a semi log plot of data and two predictions?",1,4,False,False,False,statistics,1488022605,False,
Calculating the likelihoods given models and data,4,2,False,False,False,statistics,1488026576,True,"Can someone please explain to me why the log of the likelihood ratio is the difference of the logs of the two given models multiplied by the empirical frequency, then summed over all m (the frequency observed). I gotta calculate the likelihoods of two models proposed with the experimental data at hand. 

I have a semi log plot of two models amd histogram of experimental results. The book says to find the log of the ratio of the likelihoods of the models, just take the difference of the logs of the two models and multiply it by the empirical frequency and sum over the frequencies.."
Fitting barrier covariance models using INLA,0,7,False,False,False,statistics,1488026810,False,
Question in ELI5: are data science and information systems the same?,0,8,False,False,False,statistics,1488028468,False,
Basic statistics,0,1,False,False,False,statistics,1488032795,True,[removed]
UMVUEs: what's the point?,3,6,False,False,False,statistics,1488033451,True,"I'm in a Casella-and-Berger stats course right now, and we're discussing the various ways that one can find an UMVUE: the Cramér–Rao inequality, and sufficiency with completeness. I have no intention on becoming an academic and have been more driven to use statistics for more practical purposes. 

It's hard for me to see the point of studying these methods when their assumptions are so restrictive, especially if more ""nonparametric"" methods like bootstrapping are used these days, because we now have the computing power. This course seems like someone took a very convenient framework (the exponential family) and thought, huh, this exponential family has very nice properties. What can I do with it?

How are these UMVUEs used in practice, when we don't necessarily make an assumption on the distributions of random variables? Please enlighten me."
Cluster analysis question,6,5,False,False,False,statistics,1488040029,True,"This is tricky because while I have some statistics background (and all through social sciences), it's by no means my strength. So please forgive me if anything seems off! And sorry in advance if this is the wrong place for it. 

I took on a project with a coworker who has an extensive statistics background to do a cluster analysis on a data set. The idea for me was to learn to do a cluster analysis with their help, and for them, I would be the one cleaning up the data set and writing the results, leaving the ""fun"" part for them. 

It's gotten weird IMO, with my limited knowledge. 

First, data cleaning didn't happen so I feel terrible that I haven't done much on this project, they kind of jumped right into it and did the analysis manually. Meaning looking through the data and picking out clusters by what seems to be arbitrary info/guessing. It's hard to explain... but there was no real stats going on. I know that stats can sometimes be more of an art, and I feel extremely undereducated in comparison, so don't feel like I can argue (because I'm very likely wrong). But with all these ways of plugging in data and getting out clusters, why would we dig through the data and pick out clusters ourselves? 

I guess I'm just looking for clarification. Coworker says that the reason we have to do it this way is that the packages for cluster analyses in R are not made for our data set (which is very similar to customer data from a store) so we can't use it. Is that valid? Is this the way we should be doing it? I looked up a way of doing it in spss and am going to try to do it myself today, but I worry about going about it alone. 

I ask because I don't want to push back against someone with years and years of experience when I've literally taken like 4 stats classes (2 undergrad and 2 grad- I'm a social science person) and primarily worked with correlation and regression. But this really doesn't feel like stats to me. "
MLR variable creation question,3,2,False,False,False,statistics,1488043499,True,"If I'm investigating whether there is a link between the ability of the sexes to migrate for work and their income (specifically in STEM fields), when using STATA, is it better to regress, for example, sex and migration occurance against income or to combine sex and migration as one variable, and regress that against income?

I believe it is the second option because I am not looking for the effect of the two separate variables, but the effect of them combined... but from a methodological standpoint does this make sense?"
Need help converting SPSS output,0,1,False,False,False,statistics,1488054240,True,[removed]
Help with code in R?,0,1,False,False,False,statistics,1488062204,True,[deleted]
Help with linear Mixed model for hypothesis testing.,4,4,False,False,False,statistics,1488074858,True,"So I have few questions regarding this as my supervisor suggested linear mixed effect model for hypothesis testing.

I am dealing with 2  to 3 groups population of 10. The data distribution is not normal. What I was trying to find is if the data values between the two groups are significantly different.

After validating assumptions I first fell upon kruskal wallis for the hypothesis test but the supervisor said that this method is conservative and a linear mixed model will give at least 20 % more effective result. I cannot seem to find literature that shows how to use this method for hypothesis testing like kruskal which is very well described in Andy Field's Book, especially the flow chart that helps in the test decision making.

So;

1) how do you perform a hypothesis test with linear mixed model to show if there is significant difference between two groups?

2) can you use this for more than 2 groups?

3) what do you mean when someone says this method is ""conservative""?

4) R environment is recommended but can this be done in SPSS?

Answers/advices with citations would be highly appreciated :)"
Help on a question on Random Variables and Expectation?,3,4,False,False,False,statistics,1488077650,True,"The questions is in this link: http://imgur.com/a/TGM7d

Find E(Z). (Find Expectation of Z)

The solutions that the instructor provided in here: http://imgur.com/a/jAN4L

What I understand so far:

I get that the definition of Expected value is the integral of the lower bound (in this case 0 b/c of Poisson) to infinity of P(Z>=t), and I get everything in the last 4 lines. But I don't get the first line. Why is it that P(Z<=t) equal to P(X<=t)5? I thought X_1 to X_5 might not be the same value?

I also kinda get how they found P(Z<=t) because of the max, but can someone explain this better for me?"
Hello r/Statistics! I have a few questions regarding P-values.....,12,8,False,False,False,statistics,1488082295,True,"I mean... I know how to use a P-value in hypothesis testing (in HWs and on tests) and I know how to calculate it by hand but.... still a bit confused as to WHY doesn't the lower the p-value the more significant of my data is?

For example 

H.null: MuuD = 0

H.alt: MuD =/ 0

So from my understanding for my professors... if my p-value turns out to be 0.06, by using 95% confidence the result isn't significant. Though if I use 90% confidence then we can call it significant? And what's even more weird to me is that using 90% confidence doesn't mean a better result than using 95% confidence..."
Help finding the confidence interval from list of numbers where pop std deviation is unknown,16,3,False,False,False,statistics,1488090912,True,"Okay, so I have an excel file with 30 numbers on it (enough to do a T test). I have to answer a problem that asks to use a 95% confidence interval to estimate the mean.

Intuitively, I would add up all 30 and then divide by 30 to find the sample mean, and then I'm not sure how to find the sample Std deviation (other than to calculate the standard deviation in excel?)

Anyways I'm a little stuck, but I figure this probably uses the t distribution. Thanks.

Extra props if you can explain the functions I need for excel"
Two groups of time series comparison,7,10,False,False,False,statistics,1488100231,True,"Hi,

I have a problem in understanding what method to use when statistically compare two groups of timeseries. 

Problem: 
I have, as mentioned, two groups that in each group include four time series. I want to see if the groups are statististically different.

Is there some method to do this? Multivariate time series?

Thank you!"
"Help figuring out how to add a ""confindence value"" to an average.",4,3,False,False,False,statistics,1488105183,True,"I'm trying to generate a couple of metrics while analyzing, say football players. Of those players I have the number of games they've played, the number of goals they have scored, and the average of goals they score per game.

As a quick example, I may have the following data:

Player A: 10 Games, 5 Goals, .5 Goals/Game

Player B: 2 Games, 1 Goal, .5 Goals/Game

If we analyze both players in terms of his scoring production, just attending at his average of goals per game, we will have two players with the same output, so we wouldn't care about who to declare the best. The thing is, one has played much more games than the other, so we can say he's proven to be a .5 goalscorer during a larger period of time, which is to say we're ""more confident"" he'll keep his production due to just having maintained it during a larger period of time or sample size.

What I'd like to know/do, is to add this variable into the calculation of the average of Goals/Game, so players with more games have ""better"" averages than players with less games. How can this be done statistically? Is there any quick/easy method to weight the average in terms of the number of games (in this example)? (Sorry about the super-dumb terminology I may have used through the test, but I have nothing close to a background in statistics).

Thanks in advance!"
Suggestions for more aesthetically pleasing Competing Risk Analysis in R? Or in other statistical software suites (if necessary)? (Crosspost from /r/rstats),0,1,False,False,False,statistics,1488111239,True,"(Copy paste and [Crosspost](https://www.reddit.com/r/rstats/comments/5w9u0v/suggestions_for_more_aesthetically_pleasing/) from /r/rstats).

Hey all,

Recent convert from SPSS here, I'm loving R!

I'm a graduate student in biomedical research, and do a lot of survival analysis. I learned how to conduct survival analyses of interest to our group, and we had some of our work published with results done in SPSS last year.

After a buddy of mine showed me the survminer package, and the following [tutorial](https://cran.r-project.org/web/packages/survminer/vignettes/Informative_Survival_Plots.html), I've really seen what R has to offer. My second and third stories are in submission with survival analyses conducted using survminer.

That said, I've incorporated competing risks analysis using the cmprsk package after following [this tutorial](http://www.nature.com/bmt/journal/v40/n4/full/1705727a.html) + [(link to package)](https://cran.r-project.org/web/packages/cmprsk/cmprsk.pdf). The analyses are sound and add value to what we're doing, however, I can't help but notice that the competing risk analyses figures are aesthetically underwhelming compared to what survminer is outputting.

Does anyone have a recommendation as to how to conduct Gray's competing risk analyses with more aesthetically pleasing outputs?"
Biostatistics - sample size for relative risk calculations,0,1,False,False,False,statistics,1488114743,True,"Dear Redditors from the /r/statistics!
I don't know how to handle some statistics problem. Let me explain.

I have a group of people which are on some medicine. Due to khorana scale the group was divided by two, one of high, and the second of medium risk. In every group, some part have the disease. I wanted to know the relative risk (risk ratio), between these groups. I got the RR factor which indicates that high risk group is more vulnerable to the disease than medium risk group, but the result became no statistical significant, probably because of small group compared to all population (we have 57 people from the population 3600). So we were asked to calculate the sample size, where the result would be statistacally siginificant and the effect size. Because I am not very familiar with statistics, I'm not sure how to do it. Could you give me some hints or guide me? I am looking forward for any help."
"Scores from a constant sum questionnaires, can they be used in a multiple regression analysis?",11,6,False,False,False,statistics,1488124734,True,"Scores from a constant sum questionnaires, can they be used in a multiple regression analysis or is there a more fitting analysis.

For example a competing values framework where people score 100 points on 4 different categories. Will the results be usable in a regression analysis? 

""A limitation of the process is that a high score in one alternative necessitates a lower score for other alternatives. The process involves a trade-off or fixed choice and as such, the measures are “not suitable for correlation-based statistical analysis, such as factor analysis and regression…” (Quinn & Spreitzer, 1991, p. 117)""

I can't find the book so I'm not sure of the context."
Comparing MS Programs in Biostatistics,0,1,False,False,False,statistics,1488130463,True,[removed]
Compiling Results from Multiple Responses from Unique Survey Participants,3,1,False,False,False,statistics,1488134096,True,"/r/statistics, I am analyzing results from a survey of medical school students who participated in an elective. I have one participant who responded twice with different responses. The survey uses a Likert scale for agreement with statements. How should I accommodate this participant's responses? Should I average their responses prior to analyzing all the respondents? Should I simply exclude one of their responses entirely?"
"John Myles White: What is an Interaction Effect? (interaction terms, interaction effects, and non-linear models)",3,23,False,False,False,statistics,1488138805,False,
Can you explain the confounding variable joke in The Big Bang Theory?,5,1,False,False,False,statistics,1488146847,True,"In [this scene](https://youtu.be/Sy0d3_av9GQ?t=1m35s) from The Big Bang Theory, Amy admonishes Sheldon by saying, ""you wouldn't know a confounding variable if two of them hit you in the face at the same time."" Can someone explain this joke?"
Stochastic Gradient Boosting (Treenet) question,2,3,False,False,False,statistics,1488150813,True,"Hello folks,

I have some questions about when to use stochastic gradient boosting (through treenet) and when to avoid it. Here is the current situation: 

A coworker used treenet for a survey assessing customer satisfaction and Net Promoter Score along with variables they considered important from previous work (KPIs). 

Treenet uses the Stochastic Gradient Boosting method for classification (I think), as the decision tree is trying to determine which cases are promoters or detractors based on the KPIs. 

I've been reading up on the method over the past few weeks and a lot of it doesn't seem like a good methodological fit to me. There's so much multicollinearity between the the predictors and I suspect the dependent variable could cause variance in the independent variables (directionality). 

For example, if you generally like a company, you may give positive scores to everything regarding that company (even when you can't recall the interactions in question). 

So, when Treenet spits out variable importance scores for the amount of influence each variable had in the decision trees created, how much faith should I really have in it? I get the feeling machine learning is more powerful when we're dealing with extremely precise measures - not for consumer opinions. Also, people tend to think of these variable importance scores as linear when they actually take into account massive amounts of interactions. When predictors covary this much, it's tough to say.

Anyone have any insights about this method? I'd really love to know when to use it and when to avoid it as well."
Help on Question about Expected Value,2,1,False,False,False,statistics,1488154815,True,"The question and solution is linked here:

http://imgur.com/a/AEKA7

Why is the E(X) = E[E(X|mu)]?

What does the X_i|mu mean in the problem? Does it have something to do with conditional probability? How do you link this with the expected value?"
MS in Statistics vs. MS in Analytics,14,7,False,False,False,statistics,1488156754,True,"This past winter I applied to a variety of master's programs and after being accepted to a handful of them, have narrowed my choice down to two:

[Duke's MS in Statistical Science](https://stat.duke.edu/ms-program/ms-statistical-science/mss-courses-and-requirements#y1)

[Northwestern's MS in Analytics](http://www.mccormick.northwestern.edu/analytics/curriculum/)

The major differences in the curricula are that Duke is more rigorous in terms of statistics while Northwestern focuses more on the computational and supplementary skills (database retrieval, visualization, etc.) and has more ongoing interaction with industry projects.

I think they're both good options, but would like to hear what the community has to say. I know that ultimately my own personal preferences and goals will be the deciding factor, but understanding what each looks like from an outsider's perspective--especially if you've been to grad school or hired people out of it--would be helpful.

Many thanks in advance!"
Sensitivity Analysis Question,0,1,False,False,False,statistics,1488160809,True,[deleted]
Plotting QQplot for Weibull with explanatory variables,0,1,False,False,False,statistics,1488164420,True,"I have a question on doing a QQ plot for Weibull with explanatory variables, i cant use a qqplot function as i understand that the location parameter changes depending on the values of the covariates. 

Is there anyway I can plot it? Thanks ! "
Problem with Binning a continuous variable,0,1,False,False,False,statistics,1488176783,True,[removed]
When is a causal effect significant?,1,1,False,False,False,statistics,1488177640,True,"I'm trying to attempt a research on causality however, I've come across a roadblock. I plan to do a randomized causality study based on the framework proposed in this paper: http://www2.stat.duke.edu/~jerry/Papers/causal.pdf .
But there's something I don't understand. The formula lets you solve for the average causal effect, a number. Based on this number, how do I know if this causal effect is significant or not? By this I mean, if the Variable really does have a SIGNIFICANT effect on the outcome? Is there some sort of scale?"
What are practical stats application of tensors?,6,13,False,False,False,statistics,1488200382,True,"Hi,

I am just curious since I only recently learned about tensors. What application do they have in statistics?
I googled a little bit and it looks like it is not a widely covered topic
Thanks"
Distribution of a sum of all but one Uniform Random variables,4,1,False,False,False,statistics,1488205734,True,"Suppose you have 4 dice.

You roll all 4, and drop the lowest roll of the 4. You then sum the remaining 3 dice.

What is the distribution of the sum of the 3 highest dice?

I was able to scratch out the distribution of the 3 highest dice using something akin to the mgf technique, but I am stumped by the sum.

I also tried to think about the Irwin-Hall distribution, but dropping one of the rolls alters it a lot...

Can anyone write a (relatively) simple expression for the PMF?

Thanks.

Edit: Thanks very much to all. The expression for the PMF can be found here:

http://stats.stackexchange.com/questions/130025/formula-for-dropping-dice-non-brute-force/242857#242857 "
Need some regression help for a dissertation,3,3,False,False,False,statistics,1488211662,True,"I am currently writing my dissertation on the accounting crisis. Certain aspects look at the relationship between various factors of a company's balance sheet. One example is how much a certain accounting method is used in relation to a firms profitability. 

I have data for 4 years (2006-2009) for both the variables across 20 companies. I am looking to find the relationship however I am unsure how to accurately do this.

E.g.

Profit

2006 | 2007 | 2008 | 2009
---|---|----|----
500 | 550 | 480 | 520
320 | 380 | 370 | 400

Fair Value use


2006 | 2007 | 2008 | 2009
---|---|----|----
20% | 30% | 26% | 24%
15% | 25% | 35% | 25%

Where each row represents a separate company. 

How would I go about finding the relationship across all 20 companies? I figured I would need to use regression however I am unfamiliar with the concept using so much data.

I have a basic university course in statistics so are familiar with basic concepts.

Any help would be an absolute life saver!"
Help with a Seasonal forecasting Approach,2,1,False,False,False,statistics,1488218406,True,"I need to find a forecasting method that takes seasonal spikes into account. Wine sales are steady January through July, recess a little bit in August, and steadily climb up from August through December. Any suggestions would be greatly apprecaited!"
Please take my survey for my Statistics class!,4,0,False,False,False,statistics,1488221847,True,"Thank you in advance!

https://docs.google.com/forms/d/e/1FAIpQLSehzY7eJxP-zEityGl9v_8pJF7N42-CyPpBSgL_yGd5-79QtA/viewform"
Metric for occurance/percentage that takes into account sample size,3,2,False,False,False,statistics,1488227162,True,"I'm looking at occurrences of a certain trait, and need to figure out a metric that takes into account sample size. 

A) 1 / 4 -> not that significant
B) 25 / 100 -> significant

The two above both have an occurrence of 25%, but B) is less likely to be a false positive due to a greater sample size (higher power). What's a good metric to use that conveys this?"
1.8.4 Attribute Control Charts,1,2,False,False,False,statistics,1488233002,False,
Multi-label classification and regression combined?,0,2,False,False,False,statistics,1488233503,True,[deleted]
"How to decide which variables are most important in PCA, Random Forest, etc?",4,6,False,False,False,statistics,1488234235,True,[deleted]
"YouTube channel on Machine Learning, API and Database Management with Python",0,5,False,False,False,statistics,1488235814,False,
"Clustering and internal validation: Are internal validation measures ""secretly"" performing a goodness of fit test?",2,3,False,False,False,statistics,1488238402,True,"Hi, /r/statistics! Let me preface my question with the statement that I am NOT a statistician by any means, but I am a grad student in math. I started thinking about a problem that I was looking at, which had to do with clustering. I know some of the algorithms for clustering, and I know many of the internal validation measures, but that got me thinking... One thing you could do is somehow do machine learning (?) to try to fit a distribution (Gaussian mixture model in a simple case?) to your clusters, then do a goodness of fit test between your data and the clusters. 

Is this what validation measures are doing behind the scenes? Are they comparing the data to a distribution in some way?

Again, sorry for the perhaps stupid question. I look at the validation measures and I can see where there's a measure of inter-cluster variance in some of them, intra-cluster variance in others, etc... but I'm specifically trying to figure out if this relates back to some kind of underlying distribution? My google searches haven't turned up anything; as you can imagine, searching for these keywords may turn up tons of different things...

Thanks for your time! :)"
"Probability of creating a circle of ""ties""",1,4,False,False,False,statistics,1488240087,True,"Sorry if the title doesn't explain this scenario perfectly.

[So I saw this on r/CollegeBasketball last week.](https://www.reddit.com/r/CollegeBasketball/comments/5vqw4k/big_12_has_completed_a_two_way_circle_of_suck/)

Was trying to figure out the probability of this occurring - that is, the probability that 10 teams ""split"" a two-game series with two other teams so that you can form this circle. It seems like the more I try to work it out, the more complicated it becomes. Any advice on how to figure this out is appreciated!

I tried to look at a case with 4 teams first so it's simpler. Call the four teams A,B,C, and D. And for simplicity, lets assume that every game has a 1/2 chance of each outcome. The probability of two teams ""splitting"" a series would be 1/2, since you can only have LL, WL, LW, WW. There are six series to consider - AB, AC, AD, BC, BD, and CD. I think I figured out the 4 team case.

What I did was break it down by the amount of ""ties"" and the probability of being able to create this scenario with that amount of ties. So the P(4 ties) = (1/2)^6 * (6 choose 4). P(5 ties) = (1/2)^6 * (6 choose 5). P(6 ties) = (1/2)^6.

Then in the case of four ties, there are three scenarios where a cycle is possible and 15 total scenarios. With five and six ties, I believe every scenario would result in this circle. So my answer ends up being 0.15625 when you do P(4 ties) * P(circle given 4 ties) + P(5 ties) + P(6 ties).

I think this is correct, but am having trouble extending it to a case with 5 teams, let alone 10. From doing some research, this circle is called a Hamiltonian Cycle, and there are (n-1)!/2 Hamiltonian Cycles with n teams. So when there are 5 ""ties"" with 5 teams, there are 12 cycles which work out of 252 total cycles. But when you consider 6 ""ties"" and above, it becomes much more complicated."
"Besides increasing sample size, what are other ways to decrease variance?",13,5,False,False,False,statistics,1488240144,True,
Where can I find a great tutorial on Bayesian statistics?,26,44,False,False,False,statistics,1488241180,True,I want to teach myself before I take a class. 
University Statistics Help,0,1,False,False,False,statistics,1488244013,True,[removed]
Difference between population means,4,2,False,False,False,statistics,1488254712,True,"I'm doing a study on the distribution of trees in a hectare plot, where I want to find if the distribution is clustered, random or dispersed. To do this, I compare the mean distance to that of a random distribution: if it's less, it's clustered; if it's greater, it's dispersed. 

I've computed the mean distance between nearest neighbours for *all* off the trees in the plot. This gives me my observed mean distance. 

Next, I computed the expected mean distance between nearest neighbours for random distributions with the same number of trees. 

The way I understand it, because I am looking at the whole population within the plot, there is no test for significance between the two means. If the means are different, they are different. i.e. there is no such thing as standard error, and no need to ask if the difference was found by chance, because it was never sampled. 

Of course, if this is true, I wouldn't be able to infer that this might be the case for the population outside of the plot, but I don't think that I'd be able to do that with this dataset anyway. 

tl;dr, and my main question: Does anyone disagree with this statement? Because I'm comparing population means from the same site, there is no test for significance, and any difference in the mean is exactly what it is. 
"
How is the SE of the mean still a thing?,6,1,False,False,False,statistics,1488260885,True,[deleted]
Another Noob Question - Skewed Population Distributions and Normally Distributed Sampling Distributions,2,1,False,False,False,statistics,1488263378,True,The fact that skewed population distributions produce normally distributed sampling distributions is very confusing to me. How could it be that the skew in the population data is not reflected in the sampling distribution? 
Can you estimate an upper bound on a non-negative random variable if all you know is the sample mean?,15,4,False,False,False,statistics,1488266313,True,"I was asked this question at an interview recently and I can't stop thinking about it. It goes something like this - 

Suppose a manager comes to you and says ""On an average we're losing $100 daily because of credit card fraud"". Can you estimate the maximum loss due to fraud in a given day?

It's kind of open ended, and I'm looking for any kind of ideas, topics in estimation theory, etc. It reminds me of the German Tank problem, but I can't connect the two."
A visual exploration of statistics,5,167,False,False,False,statistics,1488295842,False,
Minimizing a pseudo-quadratic function: is there a good way to get a rough estimate of the global minimum using VERY short markov chains? Or perhaps some other sampling method?,8,1,False,False,False,statistics,1488304592,True,"I have a perhaps unusual application for MCMC or some other sampling method.

As one part of a code I am working on, I need to find the minimum of a function. This function is only dependent on 1 variable, but cannot be analytically described. This function ranges from being almost perfectly quadratic to looking something like [this](https://postimg.org/image/oh7lwtj0r/), with multiple minima. (NOTE: that is kind of a ""worst case scenario"". most will probably only have a few local minima)

The problem will always be in this orientation - the initial point will be at x = 0, and the minima will always be at x > 0. I will be using gradient based methods to find the minimum value, but in case the function has multiple minima I want to try and improve my starting guess (from x=0 to something better), to help ensure that  I converge to a global minimum rather than a local one.

My initial thought was to define a maximum x that is larger than the minimum value; try x values at a number of evenly spaced intervals (say, 20); a find the minimum f(x) and use the corresponding x value as my new starting guess. Unfortunately, I dont have a very reliable way to estimate the ""maximum x"", meaning that these values could be very spaced out.

BUT, I then thought that maybe I could use a Markov chain or some other sampling method to try and get close to the global minimum. Is this possible with very few samples (say, 20)? What implementation might be a good place to start? Probability could be computed using P(x) = -log(f(x)).

Thanks!

.

.

EDIT - The example is not the exact function I want to minimize, but is representative of the types of functions I want to minimize.That particular example can be described analytically (in fact, its a combination of a quadratic and a sin curve), but the real functions I want to minimize cant be.

Some more info about the code:

The minimization code is just one part of a larger code. This code is a gradient-based inversion code, designed to iteratively find a model from some data. For those familiar with inversion: this particular part is for finding the stepsize for each model update. At any rate, there are ~2000 inputs that go into calculating the function value, BUT for this section of the code everything except ""x"" is held constant, hense why its a single variable function.

Regarding time: a single function call takes on the order of 0.01 seconds. BUT, this code will be applied to a large dataset, meaning this minimization process will be repeated literally hundreds of thousands of times. As such I want to avoid any more function calls than are absolutely required.

.

.

UPDATE: I may have found a better solution. Instead of starting out with some sort of sampling i instead just start with a local optimization for x. I then test a number of values (~10) around the local optimization solution. If no values have a lower function value then I accept the local optimization result as a global minimum. If I find a lower function value though I restart the local optimization using this new point as the starting point. This seems to work pretty well in practice, and doesnt require too many additional function calls."
Question about the flexibility in being a statistician,5,9,False,False,False,statistics,1488313175,True,"So I'm interested in getting my masters in statistics since I recently graduated college, and I was curious about the flexibility of the field. You can obviously use statistics in many different industries or settings but I have heard online in more than one place that you need to try to pick the area you want to apply it in right away because once you get into the field it is hard to switch out. 

Having that flexibility is a big selling point for me but is it true that once you start working in one field that it is difficult to transition into another because they expect you to be knowledgeable in the programs and methods that they use right away? For instance a conventional statistician might use SAS but if you wanted to transition into a computer science oriented company then you'd have to learn Python or R or something. Then it'd be different for the medical industry and so on. Would you have to go back to entry level position salary?

 My dad got his masters in operations research and statistics and he says it shouldn't be a problem, but that was also a long time ago and I'm not sure if the same applies to the market these days. Should I be concerned? Thanks!"
What type of analysis should I conduct?,1,1,False,False,False,statistics,1488319161,True,[deleted]
"Can't we all just get along?, Econometrics edition",0,2,False,False,False,statistics,1488324922,False,
Survivorship Bias,0,6,False,False,False,statistics,1488326565,False,
Any ideas to increase efficiency in a hospital using statistics?,5,2,False,False,False,statistics,1488331760,True,"I'm currently in my last year of graduate school, getting my masters in applied statistics. I also work at a hospital with a relatively menial job. My supervisor has recently given me an opportunity to help out the department (respiratory) by using my unique skill set. I would love to seize this opportunity but, as I am a student with no real work experience, I'm having trouble coming up with ideas. 


I'm skilled in statistical computing and most common statistical methods. Our department works a LOT with inventory as well. Any ideas are welcome!


Thanks!"
Basic Weights Matrix Question,0,1,False,False,False,statistics,1488349853,True,[removed]
How to do Games Howell on 2 way ANOVA?,0,2,False,False,False,statistics,1488355996,True,"I have Googled for both R and SPSS. I have Andy Field's textbook, but no formal training in statistics. The text claims it can be done, but SPSS won't let me select it, and when I edit the syntax it gives me an error message. 

R is the devil, IMO, but I'm practicing and I'm willing to take any suggestions there. 

I work with Eye Tracking data, specifically focused on fixation length (in seconds). As an overlay to the eye tracking data, we assign ""areas of interest"" (AOIs). I'm looking at students' fixation lengths on AOIs on different formats of worked examples. 

Variables:

+ 3 formats of examples (IV)

+ 2 types of AOIs (IV)

+ Fixation lengths (DV)

Issues: 

+ my data is normally-ish distributed. I graphed it with Q-Q plots, and it's bout normal

+ although my number of participants was small (N=26), the number of data points in my DV is >100 because each participant would have data for each example and each type of AOI (i.e. each participant contributes 6 different fixation lengths)

+ I know that my data has heterosca... whatever of variance. Levene told me so, as did the lovely plot I made in R. Field says ANOVA is robust but do Games Howell sooo how do I do that?


I would love any help. I've managed to run Bonferroni, Hochberg GT2, and Gabriel post hoc tests. My p-values are very very very small <10e-22 for main effects and interaction. My post hoc tests are also significant so far.  I just want to make sure that these results are real"
Exploratory Factor Analysis SPSS Output HELP,0,1,False,False,False,statistics,1488377060,True,[removed]
Four ways to derive the normal equation,2,6,False,False,False,statistics,1488378547,False,
Probability question... unfair coin.,0,1,False,False,False,statistics,1488379227,True,[removed]
Question regarding winrate of a team using winrate of its players.,1,9,False,False,False,statistics,1488398941,True,"Disclaimer: I know winrate doesn't mean skill, it's essentially just evidence you are moving up the ladder and how quickly, unless you have peaked the ladder. I'm just curious of the math here.

This is coming from competitive online gaming, so:

Let's assume Player A has a 80% winrate when playing ranked ladder. Assume the matchmaking system works properly and he is in a supposedly balanced match where everyone on his team has a 50% chance to win.

So Player A has a 80% winrate earned when playing with 50% winrate players.

Now lets assume Player A makes a team with other players with high winrates.

So now you have a team of:

- Player A (80%)

- Player B (75%)

- Player C (70%)

- Player D (65%)

- Player E (60%)

What would their winrate be versus a standard team of 50% winrate players?

I don't think you can simply just add and average them. You essentially have 5 skilled players who got there by playing with unskilled players. Their winrate *should* increase by playing with 4 other skilled players instead of the usual unskilled ones.

And then a second question, how would you determine the winrate of team 1 if team 2 wasn't an exactly 50% winrate?

Sorry if this is the wrong sub. Pls no murder"
How to make a proper online questionnaire for data gathering?,0,1,False,False,False,statistics,1488399472,True,[removed]
Can anyone recommend some reading on the following question about model building?,4,5,False,False,False,statistics,1488402206,True,"I'm looking for materials that discuss the pros/cons of either: 

* simply creating a model with all predictors and their interactions, and interpreting the results
* building a model (either forward or backward) using model comparisons to evaluate the addition of each term

Thanks!"
Any good self teaching introduction to Time Series book?,25,10,False,False,False,statistics,1488403191,True,"I got inspire by the bayesian book thread and was curious.

The Likelihood of reading this is decently high since the chances of me getting this one awesome internship in a state where there are more cows than people is high. 

I do mostly R, hate SAS and wouldn't mind learning Python. I have no clue if that'll help."
Way to start understanding theory?,7,15,False,False,False,statistics,1488422626,True,"I am a graduate student in Epi looking to move onto a PhD in Biostats. I have been thoroughly warned about the theoretical rigor involved in some of the courses within the first 1-2 years, so with gaps in my week I try to go through some theory on topics or models I use in my internship or applied in class settings. 

I frequently find, however, that without a strict mathematical background (admittedly a humanities undergrad and original path) I struggle to get through the motivation for some of the mathematical arguments in these papers. 

For example, I use survival analysis frequently in my internship, but was curious about the proofs/motivation behind the distributions and their uses. Even through googling and sifting through pdfs of stat lectures, I get bogged down with keeping track of parameters and ""translating"" equations. Am I wrong to try and pick these apart for deeper understanding?

What should I do? Any texts or lectures or courses that may be valuable to laying the groundwork for this understanding? I have had formal logic and classes involving proofs, but outside a ""mathematical"" course. "
Levene's test statistic and the assumed alpha threshold value..,1,5,False,False,False,statistics,1488427708,True,"Today in my upper-division statistic class the question about how to justify the use of 0.2 alpha value when using the Levene's test  to justify the use of ANOVA, which assumes equal variance.  One student was using the argument that there is no way to justify the use of a specific value, such as the 0.2 alpha value, but only that the alpha value needs to be higher than 0.05 because one is 'hoping' to accept the null hypothesis that there is no differences in variance. 
Does anyone think they have some guidance for such a student that wants to justify the use of a particular alpha value when using Levene's test statistic?"
Ecological data: model to be used ?,0,1,False,False,False,statistics,1488451863,True,[removed]
a quick interview about stats?,0,4,False,False,False,statistics,1488455863,True,"Hi, I'm currently a statistics undergrad. I'm taking a writing course that is asking about writing/reading in different discourse communities. I decided to do mine on statistics. I need to conduct an interview to complete this assignment. It will be a very simple ""interview"". Probably 5-7 questions about your personal experiences on reading and writing in the field. Current stats students or people in careers relating to stats would be very much appreciated! If anyone would be willing to, please PM me. "
Seeing theory. A website to visualise stats,4,95,False,False,False,statistics,1488457088,False,
Our Services Make Us the Best Provider of Statistics Homework Help,0,1,False,False,False,statistics,1488460897,False,
Homework1.com Offers Best Statistics Homework Help Online,0,1,False,False,False,statistics,1488462989,False,
Why does the calculation for standard deviation use mean instead of median? Wouldn't it be a better choice?,2,1,False,False,False,statistics,1488469945,True,"Or, what about using the average of mean and medians, so that each point' value is only half-weighted?"
How could I implement this sampling algorithm?,0,1,False,False,False,statistics,1488472658,True,"I want to get an idea of how a univariate function f(x) behaves in the vicinity of some point x0. f(x) can not easily be analytically described. Assume that x > 0.

A priority is to get a rough idea of what f(x) looks like around x0 using **as few function evaluations as possible** (I would like to evaluate f(x) fewer than ~20-30 times).

My initial thought was to modify the probability after every new draw (call it xi) to reduce the probability around xi. I would think it would be implemented using something like:

* 1) Initially define some probability distribution around x0 (preferably something with wide tails, Cauchy maybe?). 

* 2) Draw x1 from P(x | x0)

* 3) define a new probability distribution around x1 to limit the probability there (something like ""1/sqrt(2*pi) - N(x1,1)"" maybe?). Call this P(x1).

* 4) draw x2 from P(x | x0)*P(x1)

* 5) repeat until xN is drawn

(Note: some function evaluations were required to calculate x0. Probability reduction at these points would be included before drawing x1)

**So, my questions:**

* Is is this feasible? If so, what might be a good and fast way to draw from P(x | x0)P(x1)P(x2) ... P(xi)?

* Is there another method that might work better here that I should consider? (See info below for more info on the purpose of this sampling).

Currently I am simply evaluating f(x) at a number of evenly spaced points near x0, but I feel like a sampling based approach would be better / more efficient.

Thanks!

.

.

Miscellaneous / background info (not directly needed for the above questions):

* The purpose of this is this is to check the result of a local optimization problem, since f(x) can have multiple minima/maxima. x0 is the solution to the local optimization. I want to see if I can find a more optimal point by trying a few points in the vicinity of x0. If all these new points are worse than x0 then I assume I have found the global minimum. If I find a better f(x) then I know I havent yet found the minimum value, and I will restart the local optimization from there.

* [this](https://postimg.org/gallery/wu4eb4zw/) is an example of what f(x) actually looks like for a handful of real scenarios. In a decent percent of these cases the minimum can be easily found using local optimization, so I think it makes sense to begin with that in order to weed out these easy cases.

* there are ~2000 parameters used to find the function value (2000 inputs, 1 output), but here there is only one variable (x). assume ""z"" contains these parameters, and that ""dz"" is the same size as z and contains some amount to change each individual parameter. I am trying to find x to minimize F(z+x*dz).

* I am concerned with efficiency because this process will be repeated MANY times (1 million+), so additional function evaluations result in substantial run time increase.

Edit: typos"
Extract information out of 2 polls (Most hated / Most loved),0,1,False,False,False,statistics,1488472955,True,"Hey there. I just found a community vote about a game that I play and was wondering how you merge the information. One poll is about most hated updates ... one about most loved updates ... both have the same options and you can pick up to 6 out of 19.

 

My first idea (simple) idea was you give the positiv once points from 1 ... x bottom up so the most liked one has the highest number and then do the same on the most hated list and substrat them but there must be a better / more precised way to get a result out of it what works best.

 

The data on both looks like ""Answer options / Count / Ratio""

 

Many thanks for you help in advance and sorry for such an easy questions (maybe)"
Are there downsides to using R Studio over normal R?,15,16,False,False,False,statistics,1488473783,True,"I much prefer R Studio to R, mostly just for the fact that the layout makes it much easier for me to see everything I have going on. 

However, one of my professors insists that we use R instead of R Studio. He says that some mistakes can be made in R Studio that wouldn't happen in R, but I don't see how this can be true. 

What are the pros and cons between the two for you?"
Body Temperature regulation,4,1,False,False,False,statistics,1488477054,True,"I am trying to determine if a certain treatment is working to regulate body temperature. I have about 130 patients in group 1 and around 30 patients in group 2. Each patient had their temperature monitored anywhere from 4-10 times during their hospital stay. 

I am trying to figure out the best test method to use to determine if there is a radical difference in variance and standard deviation, since the treatment really is trying to see how consistent the patients maintain their body temperature. Would I use ANOVA or something like Levenes Test? "
Logistic regression output not containing an intercept,4,1,False,False,False,statistics,1488483762,True,What are the implications of this? 
Weltanschauung (an introduction to applying probabilistic programming),1,2,False,False,False,statistics,1488488579,False,
Lagged dependent variable in a mixed model,0,6,False,False,False,statistics,1488490824,True,"Hey guys, I originally posted this question on askstatistics but got no response. 

I'm running a mixed model which includes a lagged dependent variable. Unfortunately after I made the model I realized that this means that the lagged term is now correlated with the random effects term. I've read this [blog post](http://statisticalhorizons.com/lagged-dependent-variables) - which suggests using the xtdpdqml (which stands for cross-section time-series dynamic panel data estimation by quasi-maximum likelihood) command in Stata, unfortunately I only have access to R.

Are there any alternative commands/packages in R that I may be able to use? (I'm currently using the plm package) 

Are there any other alternatives perhaps?

Any suggestions would be appreciated. Cheers."
Help With Statistics Project,0,1,False,False,False,statistics,1488495626,True,[removed]
Interactive Statistics infographic of Univariate Probability Distributions:,3,25,False,False,False,statistics,1488496299,False,
Is it possible to figure out the fair value of a binary bet? (finance/stats),0,1,False,False,False,statistics,1488498640,True,[removed]
Tukey Test P-Value Calculation,3,9,False,False,False,statistics,1488500508,True,"Is there a way to calculate the exact p-value of a mean comparison in the Tukey test given the Q output of the test? I understand how compare the Q value to the studentized table to get an over/under on 95% confidence, but would like the exact p-value. 

I know in Excel there's the f.stat.rt function, wondering if there's something similar for a Q table? Is there a math function that defines the Q table that I can use to program my own excel/R function?



"
Statistics can't tell this tragic tale of drug destruction,0,0,False,False,False,statistics,1488524730,False,
Conditional probabilities for categorical outcome,0,1,False,False,False,statistics,1488531830,True,[removed]
"Statisticians, what kind of gift would you like?",0,1,False,False,False,statistics,1488533839,True,[removed]
Studying the association of a brain region and cognition.,0,2,False,False,False,statistics,1488547345,True,"Hi there! I have a question on how to set up my statitics analysis.

I am researching volumetric brain changes and how this is related to changes in cognition. Normally brain volumes are expressed as a fraction of the intracranial volume (ICV), this is called brain parenchymal fraction. The reason for this is that some people start out with bigger heads/brains.

Now my problem is the following.
Brain volume decreases with age. There is a general atrophy but in certain disease focal areas atrophiate. I am interested in focal atrophy. I want to look purely at the focal atrophy; how do I correct or remove the general atrophy process on which it is superimposed? I was thinking I could divide the focal area volume by the total brain volume. Would this be allowed?


"
Regression analysis - How do I find the variables I need?,0,0,False,False,False,statistics,1488548663,False,[deleted]
ASA P-Value Webinar,8,19,False,False,False,statistics,1488549629,False,
"Is it possible to analytically find the maximum probability (i.e., the mode) of this probability distribution?",13,5,False,False,False,statistics,1488558416,True,"EDIT: so I've worked out that this problem (as it is currently setup) effectively reduces to finding the mode of a gaussian mixture model, which (as far as I can tell) isnt analytically solvable in a general sense. I need to find a ""notch"" function that is easy to characterize when multiplied with another distribution of the same type. Suggestions would be appreciated.

**The Probability Distribution**

The distribution is univariate, and is analytically described as:

P(x) = (x / x0) * exp(x/x0) * (1 - exp( ((x - x0)/(c * x0))^2 ) ) * (1 - exp( ((x - x1)/(c * x0))^2 ) ) * (1 - exp( ((x - x2)/(c * x0))^2 ) ) * ... * (1 - exp( ((x - xN)/(c * x0))^2 ) )

In this distribution: c, x0, x1, ..., xN are all constants, and x > 0. N can range from ~1-20.

**What it is for**

The distribution is designed to produce a set of x values that are optimally distributed relative to x0. These will become ""test points"" at which I evaluate some other f(x) that is computationally expensive to compute (the goal it to get an idea of how f(x) behaves around x0 using as few points as possible). In the broader context of what this is for, x0 represents a calculated local minimum of f(x). This code is attempting to find a short list of new x values to evaluate f(x) at in order to try and confirm if x0 is the global minimum of f(x).

I designed this distribution, and my thought process went something like this:

1) Initially I define a gamma distribution with shape parameter (k) = 2 and scale parameter (theta) = x0. I know x > 0, so this seems like a good initial distribution to use. Its maximum probability is at x = x0.

2) Every time I choose a new xi (starting with x0), I do not want to choose another value close to xi, so I add a notch in the distribution. Each notch is formed by multiplying the current distribution by ""1 - N(xi,cx0)"", where ""N"" is a normalized Normal distribution centered on xi (such that P(x = xi) = 1) and with standard deviation = c * x0.

3) The choice of x(i+1) is determined by the maximum probability of the current distribution. Every time a new notch is added I reevaluate where the maximum probability is at, and that becomes my next choice for x.

In practice this works well when calculating the maximum by brute force. [Here](http://imgur.com/a/ALqvX) is what this looks like for x0=4 and c=0.6. 

Unfortunately, to make this worthwhile I really need to analytically find the maximum probability. The point of this is to save time by reducing the number of times I need to evaluate some other function (f(x)), but the savings is canceled out if I spend a bunch of time choosing which x to sample. 

If the way I set this up makes finding analytical probabilities impossible, perhaps there is another method? Maybe a different way to generate ""notches"" in the distribution? Suggestions would be appreciated.

Thanks!

.

.

**EDIT**

A couple of additional notes: 

1) I realize that I could just solve this brute force for x0=1, generate a list of x values, and then always just use this list and scale it by whatever the actual x0 is. In fact, I may end up doing this if I can't find a better option. However, this really isnt ideal. In practice I need to evaluate f(x) at a number of points in order to find x0, and I would ideally like to avoid those points as well when selecting new x values. This requires a unique computation for new x values every time though, which isnt realistic without analytic and or extremely efficient numeric estimates for maximum probability.

2) I tried to find the max probability by taking the derivative of P(x) and setting it equal to zero using symbolic mathematics software, and the software couldnt find a solution (not even for N=1). This makes me thing an analytic solution doesnt exist, but I am hopeful there is some clever method to find it. If not, I hope someone can recommend an alternate that does have an analytic solution."
Survey sampling statistics,0,1,False,False,False,statistics,1488563652,False,[deleted]
The Quartz Guide to Bad Data (Github),2,16,False,False,False,statistics,1488563800,False,
Keynote Session: Dr. Edward Tufte - The Future of Data Analysis,1,33,False,False,False,statistics,1488564899,False,
Quality of Life Survey.,0,0,False,False,False,statistics,1488578008,False,[deleted]
ANCOVA and R,2,3,False,False,False,statistics,1488582983,True,"Hi,
Suppose I have a response variable y, a covariate x, and categorical variables c1 and c2. Suppose there is an interaction with x and c1, and an interaction with x and c2. How would I fit such a model in R? I've hypothesied: y~c1x+c2x. Am I correct?
ThanksHi,
Suppose I have a response variable y, a covariate x, and categorical variables c1 and c2. Suppose there is an interaction with x and c1, and an interaction with x and c2. How would I fit such a model in R? I've hypothesied: y~c1*x+c2*x. Am I correct?
Thanks"
Average day for a statistician?,45,32,False,False,False,statistics,1488590313,True,I find being a statistician very interesting. How does a normal day begin and end? Did you always want to be a statistician?
MS in Stats - Advice on Year(s) after Undergrad,0,1,False,False,False,statistics,1488597395,True,[removed]
Linear Regression: Exemplified Through Stock Price Estimation,1,1,False,False,False,statistics,1488607973,False,
"Santa Anita statistics, horse racing notes",0,8,False,False,False,statistics,1488625273,False,
Recovering Expressions of Estimators From Matrices,5,5,False,False,False,statistics,1488642598,True,"I was curious how you recover expressions for estimators from their matrix representation. For example, in linear regression, the vector of the coefficient estimates is given by (X'X)^(-1)X'Y. For weighted least squares, the form is (X'WX)^(-1)X'WY where W = diag(sigma^(-2)).

How can you find explicit expressions for each individual coefficient to, say, compute the variance of the estimator? Do you even need to do that?"
"Need to give robust standard errors (White, 1980). RealStats has 4 different ways, help!",1,1,False,False,False,statistics,1488648881,True,[deleted]
Can someone here help me understand relative risk in a statistic?,2,3,False,False,False,statistics,1488654488,True,"I apologize if this is the wrong place to ask, but I am trying to understand the significance of the following statistic. 

""Overall, chances that a child will end up with an autism spectrum disorder is around 1 in 100. But the relative risk for autism is two-thirds higher for children born to dads older than 50, compared with kids born to dads in their 20s, the study found.""

Does that mean that the risk could go from something like 1% to 1.66%?  Thanks in advance!"
p values of .055,82,28,False,False,False,statistics,1488659408,True,"Hello, I'm currently writing up my final year project and one of my predictors (for psychometric research) has a p value of .055. I know this isn't significant, but I was wandering if I should comment on this regardless, say it was ""close"" ? 

Thanks for any help "
How to report contrast coefficient? (apa),3,3,False,False,False,statistics,1488672999,True,"Hey all, 

I kinda feel like an idiot but I don't know. For example, say I'm comparing *low fat diet* to *high fat diet* ,and get the following output:

    Contrast       S.E.      Lower     Upper     t   df   Pr(>|t|) 
    -0.2871667 0.07379549 -0.4411014 -0.133232 -3.89 20    9e-04

How do I report that contrast coefficient? Is it a *z*? I don't know how to talk about it. Can I just say "".... the contrast was significant, coefficient = -0.28, 95% CI [-.44, -.13]""  ?  What's tripping me up is that the confidence interval is on the contrast coefficient and not the *t* statistic, and I feel like I have to give that coefficient a ""nam"" so to speak.

Thanks"
How to adjust your confidence interval for your population.,4,3,False,False,False,statistics,1488700218,True,"Hey guys, I was hoping somebody could point me to a source or just tell me outright how I can adjust my confidence interval the your are given a realistic population sample. What I mean is 95% is typically used and so in most questions 1.96 is the value used in computation. The problem is is this is only for infinite populations, not real world populations. 

Hope this was clear, if not let me know."
Master's degree project,3,13,False,False,False,statistics,1488723361,True,"Hello guys,
I'm starting a Masters degree this week at UTT (Troyes, France) about Optimization and Security in systems.

These are the subjects of the master's degree

1: Operational research and optimization methods

2: Decision and estimation: statistical approach

3: Stochastic processes

4: Production planning and scheduling

5: Logistics and transport

6: Reliability and maintenance modeling

7: Pattern recognition and control applications

I'm an industrial engineer and I want to focus the project on supply chain (distribution and internal movements), industrial operation, risk control on industrial areas or prediction and estimation using big data.

Do you know and interesting topic or problem-related with this topic that I can research and try to resolve with the tools given in the course?

Thank you so much!"
How to interpret this equation based on regression analysis?,5,1,False,False,False,statistics,1488726486,True,[deleted]
"If the range of ability in a test group is wider than that intended by the test maker, the reliability estimate of the test will be spuriously _____ (high?) whereas if the range of abilities is significantly restricted, the reliability estimate will be spuriously ______(low?).",2,0,False,False,False,statistics,1488754437,True," How do you fill in the blanks? Are my solutions correct? Why?

I have a hard time wrapping my head around this. Thank you for any help! "
Statistics for IT Analytics,0,1,False,False,False,statistics,1488756297,True,[removed]
Network Meta-Analysis | www.Statistics.Training www.Statistics.House,0,1,False,False,False,statistics,1488759796,False,
GGplot2 axis,11,2,False,False,False,statistics,1488760136,True,"How do I make my x axis go from 1 to 3, i only get 0 to 1.75 this is my code ->
p <- ggplot(HImen, aes(x = lar, y = apob)) +
  geom_point(aes(size = Rep.Men ), alpha=0.1)
p+scale_x_continuous( limits=c(0, 3))+ 
  theme_bw()
p + labs(list(title = ""HIChlomen"", x = ""LDL-C/apoB"", y = ""ApoB(mg/dl)"", size=""Represented(thousands)""))
"
MS in Applied Statistics,39,41,False,False,False,statistics,1488764853,True,"So grad school has been of the sort:

First semester:
""Wow, I used to think I knew a lot when I earned my bachelor's.""

Second semester:
""Wow, I used to think I knew a lot a week ago.""

Midway through second semester:
""Now I'm simply questioning everything I thought I knew."""
How to explain internal estimates in random forests?,0,1,False,False,False,statistics,1488772288,True,[removed]
I'm running a/b split tests for a digital marketing Facebook campaign - my sample sizes are often very different between variations - am I incorrectly interpreting these results?,15,8,False,False,False,statistics,1488789117,True,"I admittedly have only fundamental knowledge of statistics so could use a bit of help. I'm currently running Facebook advertising campaigns for my family's business. 

When running Facebook campaigns, its usually important to ""split test"" ads. Which essentially means running the same ad in multiple variations. Each variation changes just one variable of the ad (for example text, images, colors, button text, headlines, etc). And then analyzing the results between 2 variations to see which performs better. 

For example I might run 2 variations of an ad with an identical headline, identical text copy, and an identical button text...but change only the thumbnail image.  

Then I would look at the analytics to see if Thumbnail image 1 or Thumbnail image 2 got more clicks (or some other data point). If there is a difference, I want to make sure that difference is statistically significant before determining which variation to select.

The problem is, due to nuances in Facebook's optimization algorithm, they'll often allocate budget to different ad variations unevenly.  This can result in very different number of ""Impressions"" (individual ad views) - which would be the sample size of the test.  I've been operating under the assumption that with a properly conducted statistical test this difference shouldn't matter.

I've been using simple conversion rate split testing calculators like these to determine if the differences are significant at a 95% confidence level: 
https://abtestguide.com/calc/
or
https://vwo.com/ab-split-test-significance-calculator/

For an actual example. I just ran a split test on 2 ads.

Variation A got 14,983 impressions, and 142 clicks. (0.95% click through rate)

Variation B got 2932 impressions and 11 clicks.  (0.38% click through rate)

Those calculators determined that the difference in click through rate of variation A vs Variation B was statistically significant. I would choose Variation A as the better Ad Set. Can I rely on this conclusion even though the sample sizes (Impressions) were so dramatically different?"
"ONLINE, ACCREDITED statistics master degrees : what do employers think about them?",8,9,False,False,False,statistics,1488801735,True,"I'm considering a stats master degree, but the nearest uni is quite far and requires quite a commute. What do employers think about online masters stats degrees? And for data science? 

I see online data science master degrees much more often than i do for statistics online master degrees. Also, do you think data science MS or statistics MS would be better to get in the 21st century for employment prospects (do you think Stats MS is too focused on theory and not applicable in workforce? Or do you think Datascience MS has too shallow depth and don't teach much else than programming skills?) 

"
Help with SAS multinomial logistic regression,1,2,False,False,False,statistics,1488808871,True,[deleted]
Question: Multi Variable statistical constructs,0,1,False,False,False,statistics,1488813997,True,[removed]
TRUCOS (CHULETAS) PARA APRENDER R + RSTUDIO,0,1,False,False,False,statistics,1488818522,False,
Advice on statistics career paths,7,16,False,False,False,statistics,1488819140,True,"I know only I can decide what's right for me, but I'm just looking for opinions on a couple different areas of statistics that I'm currently considering as career options.

I actually hated statistics until I started a masters related to manufacturing engineering and I took a class in statistical process control.  I then realized I'd rather focus 100% on statistics and switched to MS applied stats.  I'm still very early in the program, taking regression analysis currently, I work full time and school part time.

Anyways, my two paths at the moment seem to be either industrial vs data analyst.  My undergrad was mechanical engineering and I can appreciate the impact quality can have on an organization.  If going this path, I would pursue ASQ quality certifications as well as six sigma, and would like to eventually become a master black belt.

My other path would be pursuing data analytics, which I know is a broad term.  It just seems like there's potential for some interesting stats jobs outside of industrial stuff especially with all the data science madness and buzz words.  If going this path, I would gear my electives more towards databases, statistical programming, etc.  I'm wondering if anyone can provide some constructive opinions about either path.

Thanks!"
How to find degrees of freedom from two sets of data?,9,5,False,False,False,statistics,1488820195,True,"On the problem, I was given two sets of data, each with 30 numbers. The value I got for degrees of freedom was 58, using (n1+n2)-2. My teacher, however, insisted that the value was 50, and his attempts to explain how he got that only made me more confused. Everything I looked up indicates that I was right, but I'd like to know how he reached that number."
Suggested reading?,3,4,False,False,False,statistics,1488832048,True,Graduating with a degree in Geo in May. The new job says they'll pay for my masters and I want to do stats. What books can you recommend to me before I start a program?
What exactly is biomedical informatics?,0,1,False,False,False,statistics,1488836545,True,[removed]
Not really sure if statistics question...Need Help,0,1,False,False,False,statistics,1488845222,True,[removed]
[pop science] Is this research reliable? Overfitting?,1,5,False,False,False,statistics,1488845314,True,"I just saw on the frontpage this link
https://www.reddit.com/r/videos/comments/5xrmth/researchers_have_decoded_a_portion_of_prairie_dog/

that claims scientists have shown that praire dogs can encode shape, kind of animal, color etc in a 1/10th of second chirp.

Now watching the video it seems a case of overfitting and other bad practices, but I might be wrong.

This is the only paper I found from them
http://www.sciencedirect.com/science/article/pii/S0003347205801174

I'd like to have someone more competent comment on it...
Thanks!"
Non-Linear Regression SigmaPlot Help,0,1,False,False,False,statistics,1488845907,True,[removed]
Does anyone use Bayes' method in daily life or generalized situations?,32,10,False,False,False,statistics,1488846681,True,"If so could someone please point out what the practical steps are, or link me to an explanation?

I've seen people like Nate Silver, Julia Galef, etc advocate its use in daily life. I read Nate Silver's book and the concepts were starting to make sense, but I couldn't find any tutorial or series of steps to follow. Either I missed it or they don't get around to explaining it. Has anyone developed an approach that's sensible and actually used in daily life, not just in specialized work situations?

If I remember correctly, they assume Bayes' method is too cumbersome to be used for everything in one's life, but think it should be used more broadly than the highly specialized situations it's currently applied in. They say this would be more objective and rational than the usual approach of good guesses distorted by bias and emotion.

How would one go about assigning probabilities and quantifying things in actual practice in non-specialized situations? If I think something could have a .33 probability, or .2 or maybe .5, because I don't really know, then does it matter which I choose? If it doesn't really matter, how's that an improvement on biased guesswork?

I was thinking that maybe Bayes' method could be used in a ""beginner mode"" where there's only 4-5 probabilities one can assign, for example .01, .25, .5, .75, and .99. Then I could get good at recognizing what each event intuitively feels like, and in daily life I could say ""this is definitely a .75"" because .5 and .99 feel too far off. In that way, Bayes' method could be used in more systematic way than ad-hoc .21s and .93s being assigned and having my quantifications drift over time due to bias.

What do you think?"
What is lurking variable?,4,3,False,False,False,statistics,1488856818,True,I want to know what is lurking variable and what's the use of lurking variable in statistics. 
Question on failing to reject the null hypothesis,14,6,False,False,False,statistics,1488866869,True,"A question given out in my class asked ""If the results of a hypothesis test are that we fail to reject the null hypothesis, which of the following results is possible (check more than one if they apply)
a. we made correct decision
b. type 1 error
c. type 2 error
I circled a. and c. but was told I was incorrect. Am I? Do I have a strong case in challenging this question with a professor?"
Pearson coefficient greater than 1,8,1,False,False,False,statistics,1488867090,True,"I was recently reading this [study](http://www.learnlab.org/uploads/mypslc/publications/cmu-hcii-08-100.pdf). Long story short, they list a Pearson coefficient of 1.802. As far as I am aware, that isn't possible. My understanding is that the Pearson coefficient must exist between -1 and 1. What would it even mean for the coefficient to be higher than 1? I suspect I am missing something obvious here but it escapes me. Is the listed value just a typo?"
Calculate the probability of something depending on something,3,1,False,False,False,statistics,1488869831,True,"Hello all,

So say I have an excel sheet containing a list of students exam scores (and it include other details like gender, major, college etc). 

I want to make basic statements  like  (if you're x gender there is % chance you will pass the exam, if you're from x college there is a % chance you will pass etc). 

What would be the way to do this in Excel? "
Ask for advice on statistics major.,0,1,False,False,False,statistics,1488870999,True,[removed]
"(x-post from r/Python) PyProcessMacro: a Python library for moderation, mediation, and conditional process analysis based on Andrew F Hayes' 'Process' Macro",1,25,False,False,False,statistics,1488878705,False,
Find a correlation between sets of coordinates.,0,1,False,False,False,statistics,1488879011,True,[removed]
How do I compare two overlapping groups?,0,1,False,False,False,statistics,1488895671,True,[removed]
Equation for maintaining inventory on a weekly basis,0,5,False,False,False,statistics,1488901464,True,"For my company I have a workbook [(picture)](http://imgur.com/WfJ5yCz) that tracks pretty much everything. On one of the sheets I have our current inventory for each unit that we produce under the 'On Hand Inventory'. The 'Estimated Inventory' then takes that number and subtracts what we are expected to sell and adds what we are planning on making from the 'Production Build' section across a span of 14 weeks. The 'Min' is the average of what we have sold from the past 12 weeks times the 'Weeks of inventory' to give us an X weeks worth of stock that we should have on hand. 

So here is the problem, I have been playing with conditional formatting and I am trying to get the 'Estimated Inventory' to let me know when we are on track for any given week to make our 'On Hand Inventory' match our 'Min' number. so I have 4 conditional formattings set up per cell for each cell, white text means we have less than we should, black text means we have more than we should, orange background means we have from 10% to 25%, and red means we have more than 25%. 

currently the equation that I am using to determine how many units we should be making is (for example)

    =($B$23-($B$23/$B$38)*D38)*1.25

so it takes the number that we should have on hand (min) and then subtracts the average of what we sold per week, times the week number to give me how much we should have on hand based on the past sales. then times that all by the percentage (in this case its the greater than 25%)

First off I am not sure if this is the best equation for this and secondly we are trying to have a 12 week inventory on hand, so when the 'Weeks of inventory' is 12, then all of the week 12 for 'Estimated inventory's equations come out to 0 meaning that none past that work. 

Hoping that someone can help me out, this has been bugging me for about a week now and I cant seem to find a solution. "
False discovery rate help,4,10,False,False,False,statistics,1488911022,True,"I'm running an exploratory study with about 40 total comparisons. I reported p-values with and without the bonferroni correction. A reviewer wants me to calculate the false discovery rate and judge each comparison as significant or not on that basis.  I've mainly heard of FDR being used for studies with hundreds or thousands of comparisons.  Is it meaningful or appropriate to calculate for a regular study? Anyone have a helpful link to some SPSS syntax? (I know, I know...I should switch, but I haven't yet.)"
Need help with this first problem. Keep getting it wrong.,3,0,False,False,False,statistics,1488926206,False,[deleted]
Logistic regression?,1,0,False,False,False,statistics,1488929539,True,"If anyone can help me in would appreciate it. 
I am not sure what statistical tests to use for a study. The goal of the study is to examine if there is an association between one variable, a scale score, and a dependent variable that is a yes or no categorical variable.  When I consider possible statistical testing,  the logistic regression is what seemed like the best option.  
Any feedback is appreciated."
Student Grades - Which test to use,8,6,False,False,False,statistics,1488931990,True,"I have a data set of 100+ students containing their grades for the same degree. (Year 1, 2 and 3).

I am then differentiating these students into 2 groups:
- Students who did an additional year in industry
- Students who did not. 

I want to assess whether the grades for those who did the year in industry differ from those who did not - how can I do this? What is the most appropriate test to do? Do I need to ensure any previous requirements?

I have worked with t-tests before but my supervisor advised that this was not appropriate for this case.

I have no clue about statistics. I have done some research and a z-score/test should do the trick...???maybe?? I am not sure.

PLEASE help!"
Lasso Multivariate Regression giving strange coefficients,0,1,False,False,False,statistics,1488933929,True,[deleted]
How do I account for bias in assigning treatment when I don't have a binary treatment variable?,1,1,False,False,False,statistics,1488934975,True,"
I've got a bunch of towns that got a social program across 9 different years. Each year, the government selected the poorest towns that had not yet received the treatment to be incorporated into the program.

I want to figure out the effect of the program.

Often for evaluation of social programs, you want to use Propensity Score Matching, but since my treatment is not binary, I can't do that. Does anyone know any econometric tools that will allow me to do a propensity score matching-like thing with a continuous treatment variable?

Or similarly, does anyone know of any RDD designs with multiple breaks?

Any help would be appreciated, thanks."
What the unemployment rate does and does not say?,1,12,False,False,False,statistics,1488940139,False,
Does polynomial regression need to satisfy the assumptions of linear regression?,1,4,False,False,False,statistics,1488946407,True,(given that polynomial regression is considered to be a special form of linear regression)
"A short essay on ""Levels of Measurement and Concepts of Validity"" https://goo.gl/SJsppC #Statistics #Alaysis #Data #Help #Learning",0,1,False,False,False,statistics,1488970780,False,
What's a more accurate statement?,3,0,False,False,False,statistics,1488983656,True,"Trial 1: failure rate of 10%
Trial 2: failure rate of 5%

Statement A: there was a 50% drop in the failure rate from Trial 1 to Trial 2

Statement B: there was a 5% decline in the failure rate from Trial 1 to Trial 2"
"The odds of getting double egg yolks is 1:30, I had 4 out of 5 eggs cracked! That is a one in one trillion chance!",17,0,False,False,False,statistics,1488984078,False,
How to numerically interpret regression coefficients?,2,1,False,False,False,statistics,1488986603,True,[deleted]
Am I crazy or is there a way to measure significance around the mean of two values?,24,1,False,False,False,statistics,1488987367,True,"I have a total of 34 data points (17 from user 1 and 17 from user two). 
If I plot individual values in a bar graph then visually it appears that there is a wide variance between a few of the values...My boss wants this to be mitigated. My answer was to run a two tailed t test and graph the mean of both users with error bars and show the min and max for each group. 
He wants to measure significance for every set of values between users then somehow compare these across users...I didn't think this was possible. How can you determine significance between two values without the samples being repeated more than once. Am I missing something? Any illumination would be helpful!! Thanks!!


I do have C.V. values, but wouldn't this only speak to the accuracy between users and not the absolute value between users?"
The crowd-wisdom challenge,0,1,False,False,False,statistics,1488987784,False,
Measuring whether a measured probability function is non-surjective,0,8,False,False,False,statistics,1488988173,True,"I have the following experiment:
Let `X ~ Unif(0.95a, 1.05a)` be a random variable with `a in N`. I have a black box that takes a value `n in N` and returns either a sample of `(0, X)` or of `(1, p(n)*X)` where `p(n) > 1` seems to be monotonically increasing with the probability that the first element of the tuple is 1.

From what I can tell the black box is a sample function of a Bernoulli process with probability `p'(n)`. However I don't know if `p'(n)` is surjective or non-surjective.

My goal is to design an experiment that allows me to find discontinuities in `p'(n)` with `354 <= n <= 1400`. A tentatively measured expression for p(n) is `p(n) = 1.45 + (n - 354)/4290` and `p'(n) = 0.05 + (n - 354)/4290`.

(For anyone interested, this is for an experiment to determine whether  critical hit rate in FFXIV is tiered like spell/skill speed or continuous)

E: samples can be obtained at roughly one sample for a given `n` per 2.5s"
"Attempting to understand markov chains & regression analysis, but get confused by terminology. What kind of basics can I touch up on in order to understand terminology better?",6,23,False,False,False,statistics,1488989048,True,"For example, one lecture I'm attempting to understand is this one (regression analysis):

https://www.youtube.com/watch?v=l1kLCrxL9Hk

I understand the basics of it, but around the halfway point start getting massively confused. The equations get crazy with symbols and I just can't seem to follow."
easy question,0,1,False,False,False,statistics,1488989766,True,[removed]
Looking for help deciding what factorial study model to use for my research proposal,0,2,False,False,False,statistics,1488996379,True,"Hello,

I am proposing a study regarding the influences of situational naturalness and type of musical stimuli on helping behaviours. The reason for my study is that no other studies I could find have specifically looked at reading story passages regarding environments where music is either commonly/uncommonly heard and whether this manipulation has effects on helping behaviour. Additionally, other studies have found that the type of music (annoying vs. pleasant vs. no music) can influence a participants mood and subsequent helping behaviour.

So basically what I've gathered so far is this.

**Procedures**: 2x3 factorial design? (2 levels of settings (natural/unnatural) vs. 3 levels of music (pleasant/annoying/no music))

Participants will be taking part in a study which is said to focus on personality and mood (this is to disguise the fact the study is measuring music and helping behaviour). The participants will read a neutral mood story that occur in different environments which are manipulated for their ""natural"" association with hearing music (**2**: **Natural** (e.g. gym) / **Unnatural** (e.g. exam room)). While reading the story the participants will hear music in the background at a low level (**3**: **Annoying** condition (e.g. avant-garde jazz) / **Pleasant** condition (e.g. Top 50 hits or classical) / **Control** Condition: No music). The participants will then complete filler tasks such as a personality measure. Afterwards they will complete both a mood and musical rating measures (both on a 5-point Likert scales). Participants in control condition will only complete the mood measure since no music was introduced. After completion they will be given an envelope with five $1 bills as payment, but the participants have not been told of a payment for completion which ensures they had no preconceived ideas for how to spend the money. The participants will also be told by the confederate that there is a donation box for a fictitious charity outside if they wish to donate any money, but donation is completely optional. After leaving the examination room, the participants will see the donation box with a sign that says something along the lines of ""$1 donation guarantees that a child will have water for a full day"". This donation is the low-cost helping condition as it requires little to no personal commitment or sacrifice. The high cost helping condition occurs right as the participants are leaving the building. They will be confronted by a confederate posing as a spokesperson for the same fictitious charity. This confederate will carry leaflets and ask the participant whether they'd like to personally distribute leaflets. The participants will be measured on the # of leaflets they decide to take for distribution. This task is high-cost as it requires personal time commitment with  no personal reward.

**So my DV's are;**

1. Musical rating (1-5 Likert) (Not in control condition)
2. Mood rating (1-5 Likert)
3. Amount of money donated (low cost task)
4. # of leaflets distributed (high cost task)

**My IV's are as follows;**

1. Musical stimuli: Pleasant vs. Annoying vs. No Music (CONTROL)
2. Story setting: Natural vs. Unnatural 

**So from my understanding (I could be wrong) these would be all the different conditions/groupings;**

1. Natural story setting + Pleasant music
2. Natural story setting + Annoying music
3. Natural story setting + No music (CONTROL)
4. Unnatural story setting + Pleasant music
5. Unnatural story setting + Annoying music
6. Unnatural story setting + No music (CONTROL)


Let me know your thoughts, I have already met with my supervisor and they liked my idea for the study. I am just curious in terms of what sort of factorial design I ought to use for the data analysis/organization. Moreover, I was wondering whether it would be useful to include a third story setting variable. Any help is appreciated. I look forward to your responses, and thanks beforehand!
"
Struggling with understanding mixed logit models,0,2,False,False,False,statistics,1488997518,True,"Right now, I'm analyzing data using binary logistic regression.

It's easy to predict an outcome in this manner, using the function P=1/(1+exp(B0+B1X1+...+BnXn)^-1).  I believe it's the same for predicting logistic models with random *effects*.

However, I'm running into confusion regarding random *parameters* models.  Looking through the literature, I see that the probability function is defined as the integral of the function I previously mentioned times a density function, but the density function is never defined.  All of these articles cite Kenneth Train as developing the technique, but reading his articles and the [chapter of his book](https://eml.berkeley.edu/books/choice2nd/Ch06_p134-150.pdf) explaining it hasn't been helpful for me.  I've been struggling through trying to understand this enough to start predicting outcomes, but I'm finding the literature to be vague and confusing.

Can anyone help me understand this better, or point me to a reference that can help break it down for me?"
Comparing measured time history with a model time history -- with the intent of saying how good the model is matching the data,4,3,False,False,False,statistics,1489000079,True,"Easy explanation -- two swiggly lines (time history), one from a model, one from measurements. Trying to figure out how well my model is matching my measurements by performing statistics on the fit. Right now we use somewhat made up calculations to find percent error, scale error, and correlation coefficient. I'm looking for some standard, published ways to get these stats instead of using our made up equations.

Actually there are lots of swiggly line pairs, we have multiple sensors that give us multiple outputs that we're coming with the same locations in our model.

Data may be positive, data may be negative, model may be positive and data may be negative, etc.

I'd utilize these stats to see if my adjustments to my model are helping or not."
In what way is calculus used in statistics?,25,21,False,False,False,statistics,1489000243,True,"Back in the day, one of my graduate psych professors complained that ""kids these days"" don't have to take calculus to get a B.S. in Psychology. Among other things, he told us that it'd give us a better understanding of some statistical concepts. He never mentioned anything in particular, but I kept that in the back of my mind. Fast forward a few years and I'm now taking calculus (the whole series). I've learned a lot, and I have a lot more to learn, but I haven't encountered anything that seems relevant to statistics. At least the ones taught is graduate psych classes.

Can anyone point me in the right direction?"
Machine Learning on Sequential Data Using a Recurrent Weighted Average,0,0,False,False,False,statistics,1489000830,False,[deleted]
Simple Statistics Question?,2,1,False,False,False,statistics,1489008339,True,"Assume that the weight of students in a SOC 1200 class is normally distributed, with a mean of 125 pounds and a standard deviation of 5 pounds. 
What percentage of students in the class weighs under 115 pounds?
The answer to this question is 2.5%

What I'm having trouble is finding out how you arrive at the answer. Like what is the formula? Help would be much appreciated. "
Chi-Square Sample Size Needed with G*Power,12,2,False,False,False,statistics,1489009174,True,"I was recommended to try G*Power so I could determine the sample size needed in order to achieve an acceptable power. I'm struggling with picking the test and filling out the appropriate values.

Basically, I'm sending a survey to approximately 3,000 people and am testing different times/days for response rate. So, for example, I'd like to send out on Monday, Wednesday, Friday, and Sunday, in the early morning, afternoon, evening, and middle of the night. So a total of 16 groups (4 x 4 conditions). This would give me 187.5 people in each group. Can anyone explain how I use G*Power or other software to figure this out?

When I select X^2 (chi-square) and goodness-of-fit and then ""A priori"" to calculate the required sample size, it asks me to determine p(H1) for each group, and I really don't know what to expect groups to achieve in terms of response rate. Furthermore, is a chi-square the right test, or should I be using an ANOVA (I am under the impression that ANOVA is for continuous variables, and I'd have a proportion)?"
Having trouble with Rsquared calculation,6,2,False,False,False,statistics,1489014183,True,"I am using the following to calculate a Rsquared from a model

1 - sum((y-predicted)^2) /sum((y-mean(y))^2)

However, my (sum((y-predicted)^2)) is a much larger number than (sum((y-mean(y))^2)). Meaning the final value will not be between 0 and 1. I am not sure what I am doing wrong. 

I know I am missing something obvious, but not sure what that is.
"
Posterior mode/ Conditional mode (of intercept),0,1,False,False,False,statistics,1489014964,True,"Hello fellow stats friends,

I am working on a linear mixed model and when reading up on it I keep bumping into the term posterior mode (Bayesian) and conditional mode. However, I have not found an in depth explanation of those terms or I just don't understand what seem to be clear as day to everyone else. I think I understand that they inform you about the intercepts of your random effects.... Does anyone know more about that or can point me to books/sources to read up on this topic. Would be of big help!

Thanks a lot! -- slowsad"
"Intro Stats class question. No given Mean, Median or Mode. I don't want you to work it for me or just tell me the answer. I want to to describe to me how I should be looking at this in order to answer it myself.",6,5,False,False,False,statistics,1489017441,False,
P-value,0,1,False,False,False,statistics,1489020964,True,[removed]
Completely stumped with distribution question!,5,1,False,False,False,statistics,1489031516,True,"Hey, this is the first time in my life I've ever asked the internet explicitly for assistance, but goddamn I really am stuck. I've spent at least two hours on this single problem and I've googled, I've read textbooks, I have genuinely tried to figure this out but I've hit a dead end. I was wondering if someone could help me with this problem. The problem is as follows :

""A newspaper indicated that the mean hours worked per week of those employed working full time in the U.S. is 43.9. The article also mentioned that 1/3 of those employed full-time work less than 40 hours per week. Given this information, 

A) What is the standard deviation of hours worked assuming that hours follow a normal distribution?

B) If 20% of those working full time work more than 49 hours per week, what is the standard deviation given this information?

At this point, I really would love to know the answer. But please, please, tell me how you arrived at it. I want to be able to do this for myself, not just rely on the benevolence of the internet. The fact that I'm actually typing this out means it is my last resort.

 Thank you so much to anyone who reads and responds!"
"Butte County releases tobacco, alcohol, nutrition statistics",0,0,False,False,False,statistics,1489037602,False,
This is why we need an International Women's Day,0,0,False,False,False,statistics,1489037955,False,
Vital Statistics: 3/9/2017,0,0,False,False,False,statistics,1489038023,False,
Interesting ‘International Women’s Day’ statistics,1,0,False,False,False,statistics,1489038111,False,
StatCrunch Assignment Help by Statisticshelpdesk Is a Complete Package,0,1,False,False,False,statistics,1489043787,False,
Any good beginner resources for montecarlo simulations?,10,19,False,False,False,statistics,1489057161,True,"Hello,
Are there any practical applicaition resources for montecarlo that any of you are familiar with?
I have this forecast table I made for quantity of games played adjusted for seasonality, and I'd like to see if I can apply montecarlo to measure the probability of my forecasts."
"Over-Complicating Averages I think, advice/wisdom needed!",13,6,False,False,False,statistics,1489059125,True,"This will hopefully be a question that is resolved with a simple easy answer, but I'm breaking my brain trying to work it out. This is my first time posting on this subreddit, apologies if this question doesn't fit into the acceptable questions (please redirect me to the correct subreddit!).


I am attempting to keep track of my personal progress in the game League of Legends (a popular MOBA game online) and was interested in tracking my basic averages. 


I will use the example of **kills** (successfully defeating an enemy on the opposing team in the match). Something I want to know is how many **kills** per **game** I am achieving on average. However, I believe that this is missing the fact that kills are actually a function of **time** (or specifically, the length of a particular match). 


My convoluted logic led me to taking the average of **kills** per minute of every game, then dividing that value by the average minutes per game (or rather, game per minutes?). So instead of the basic...

>**Sum of Kills**/**Total Number of Games**

...it has become...

>**Average kills per minute per game** / **Average game length**

>= **Sum of game times** x **Sum of kills per minute per game** / (**Total Number of Games**)^2


It appears the values are similar when I run my data through, only about a 15% difference, but I've been staring at this paper for two days and have no idea if my logic makes any sense or if this is just a poorly constructed weighted average. Any constructive advice would be helpful!"
Paired T-Test Question,4,0,False,False,False,statistics,1489062891,True,"Am I allowed to make my paired data based on a percentage instead of a difference? 

For Example:

H0: (μ1/μ2)*100=100

Ha: (μ1/μ2)*100<100

Instead of:

H0: μ1-μ2=0

Ha: μ1-μ2<0"
Quantitative rule for reporting median for mean?,0,1,False,False,False,statistics,1489063744,True,[removed]
Statistical Test for Response Rate with Both Day and Time Variables,1,1,False,False,False,statistics,1489076320,True,"As the title suggests, I'm planning to run a study with 16 different groups of students (out of 3000 total students) all receiving the same survey. Both day and time are independent variables. So, more specifically, I'm sending out Monday, Tuesday, Friday, and Saturday in the morning, afternoon, evening, and night - 4 x 4, or 16 conditions total. The dependent variable of interest is response rate by group (or response by individual student?).

I originally thought I should be doing a Chi-squared test, but if I'm manipulating two variables at once, is that a fair thing to do? Should I be using multiple regression? Furthermore, response rate can be looked at at the group level or the individual level - I'm not sure what the difference would be, though.

What type of statistical model fits what I'm trying to do?"
Graduate certificate vs MS,12,3,False,False,False,statistics,1489081636,True,"So I jut finished up my bachelor's in mathematics and I really want to get into a data analysis type of role. I've come to the realization that I might need to further my education.  So I'm looking at two option's, both of them would be online, a graduate certificate which is significantly cheaper or a MS.  I would prefer the certificate because it's cheaper and I don't want to take out any more student loans, but is it even worth it?  I saw that the program is useful if you plan on continuing onto an actual MS.  But will the certificate hold any weight?  Of course there's the obvious situation where you have two exactly the same potential employees and one has a certificate and one doesn't, they would hire the one with the certificate .  

Just curious if people have gotten any certificates and what there experience is with them?"
Best way to represent a Temperature Depth Profile,2,1,False,False,False,statistics,1489083043,True,"Hi guys, just wander if I could have some advice for visually representing a Temperature Depth profile, I have done a super quick graph in excel and it belongs in /r/dataishorrifying but I was wondering whats the best way to present this data without it looking horrifying!

[Img1](https://i.imgur.com/4bQZ0Du.png)
[Img2](http://i.imgur.com/uf0H2x3.png)

Many thanks!
"
Can standard deviation be calculated with a pdf alone?,3,2,False,False,False,statistics,1489083911,True,[deleted]
Probability and dice rolls,2,1,False,False,False,statistics,1489094357,True,"Would someone help me check my logic for the following two scenarios please?

1. Rolling six dice, determine the probability of at least two 6's showing.
2. Rolling six dice, determine the probability of at least two dice matching. (This one I am really unsure of.)

* 1a. p(no dice showing 6) = p( (5/6)^6 ) = 0.3349
* 1b. p(exactly one 6 showing = p( (1/6) ((5/6)^6) ) = 0.067
* 1c. p(two 6's showing) = p(1 - 0.3349 + 0.067) = 0.5981
 
-
 
* 2a. p(two dice matching) = p(1 - ((1/6)^6)) = .9999~

My thinking with 2a is that you are equally as likely to have all the dice match as you are to have exactly none of them match. Edit: Thinking about this more that is clearly not correct as there are 6 ways to get exactly the same roll vs. only 1 to not get any duplicates."
Environmental systems survey,0,1,False,False,False,statistics,1489107057,False,
How would you analyze the blank spaces in a dataset?,11,1,False,False,False,statistics,1489107525,True,"Say you have a 1-dimensional array full of numbers and blank spaces. How would you analyze the number of clusters, the blank spaces within a cluster, and the spaces between clusters? 

I guess it could be 2 dimensional but the matching array is just full of 1's. Would it be necessary/ appropriate to place 0's in the blanks?


A is the 1 column
B is the column of interest with the gaps 
C - Lets define a cluster as a continuous set of numbers in B without 2 blanks

A|B|C
:--:|:--:|:--:
1|1|<
1||< 
1|1|<
1||
1||
1||
1||
1|1|<
1|1|<
1|1|<
1||<
1|1|<
1||
1||
1|1|<
1||
1||
1||
1|1|<
1|1|<
1|1|<
1|1|<
1||
1||
1||
1|1|<
1||<
1|1|<
1|1|<
1|1|<
1||
1||
1||
1||
1|1|<
.|.|.
.|.|.
.|.|."
What are some cool fields which crossbreed stats with another field? Eg. Biostats = stats + bio,23,7,False,False,False,statistics,1489110396,True,"I've recently been reading up on some cool applications of stats, and there are way more than I realized! Some cool combinations include:

- stats + biology

- stats + astronomy

- stats + environment

- stats + text (natural language processing)"
Ask for advice on statistics major,0,1,False,False,False,statistics,1489115125,True,[removed]
what in standard deviation?,12,79,False,False,False,statistics,1489116079,False,
Generate a new Y-Varible (1/0 Defect/nonDefect)?,0,1,False,False,False,statistics,1489134581,True,[removed]
What to Do If You Have a Swimming Pool at Home,0,0,False,False,False,statistics,1489140180,False,
This is how it works...,3,41,False,False,False,statistics,1489141218,False,
Understanding normality tests and normality transformations,0,1,False,False,False,statistics,1489141782,False,
Solutions for managing big-ish data locally,13,9,False,False,False,statistics,1489158364,True,"Hey all, this might be more of a question for Stack Overflow but I can't quite formulate the type of specific close-ended question they like there.

I work in public health and get monthly >8M-row files. We regularly get requests that involve analyzing yearly data, so we're looking at >100M rows, which R can't handle at once. What I currently do is sequentially filter data for each month and then merge it but that means I end up with several redundant subsets and it doesn't feel quite optimal.

The first idea that came to my mind is putting all the data on a local SQL server and get all the summaries I need via queries.

Now, I'm not actually sure this is the best approach so I'd like some confirmation on that as well as some advice for the implementation (was thinking of doing this with PostgreSQL)."
Time Series Alert Trigger,5,9,False,False,False,statistics,1489167170,True,"Hi All, 

Trying to figure something out, I am looking at doing some TimesSeries predictions based on some values, I am looking at either AWS or Azure, how do I go about setting a Trigger Action for a Notification if the TimeSeries Predictions are above the Norm "
Computer Simulations in Engineering,1,5,False,False,False,statistics,1489168057,False,
Probability of difference between two observations being 4 standard deviations or more?,0,1,False,False,False,statistics,1489169092,True,[removed]
Need help analyzing PFC production reports and formulating an annual PFC Atmospheric Contamination estimate,2,8,False,False,False,statistics,1489171131,True,"I need help analyzing this PFC production report from the EPA (link below) to determine about how much PFCs are leaked into the atmosphere in relation to the amount of aluminum being created.

https://www.epa.gov/sites/production/files/2016-02/documents/pfc_generation.pdf

What I want to do is take that number and combine it with the roughly 59 million metric tons of Aluminum produced worldwide last year to determine about how much PFCs were released into the atmosphere globally.

For those interested, PFCs are some of the worst greenhouse gases known to man.  They are roughly 6,500 times more efficient than CO*_2_* at trapping heat and can take 50,000 years to leave the atmosphere.  With such a powerful effect and long lifespan, it is sensible to believe that long-term PFC production may become a more discerning factor in global warming than other more common greenhouse gases over a long period of time."
Can anyone help me with deciding a statistical methodology for my dissertation?,1,1,False,False,False,statistics,1489173077,True,[deleted]
Ebay data. Historical sales data. How to get?,1,3,False,False,False,statistics,1489244785,True,"Hello, I am doing research on a niche of products for a article and I would like to track sales data for a specific set of keywords. As far back as possible (years) Terapeak only lets me go as far as 90 days. Any other solutions I can look into? Thanks,"
Checking that a stochastic matrix is regular.,2,2,False,False,False,statistics,1489247422,True,What am I missing in the following argument: Suppose I want to check if an nxn stochastic matrix is regular. If some state is to be reachable from some other state then it must be reachable in n iterations or fewer.  Thus if all entries of T^k are to be nonzero for some k then that first k must be less than or equal to n.  Thus in checking regularity we only need to check powers up to and including n.  More specifically T is regular iff all entries of T^n are nonzero.
Need help understanding the intuition behind removing linear effects between predictors in multivariable regression,0,1,False,False,False,statistics,1489251649,True,[removed]
"Power of a test: what does statistical power have to say about public policy relating to very rare events, such as terrorist attacks by immigrants?",11,17,False,False,False,statistics,1489254836,True,"I last studied statistics in the mid-70's, so I *recalled* the concept -- and I googled and read [this](https://effectsizefaq.com/2010/05/31/what-is-statistical-power/) and [this](https://effectsizefaq.com/2010/05/31/how-do-i-calculate-statistical-power/) and even installed the power-calculating software from [here](http://www.gpower.hhu.de/) -- but the software is opaque to me, and I don't know how to answer my question. So here I am, asking you!

My question is instigated by [this table](https://www.cato.org/sites/cato.org/files/images/pubs/pa_798_table_1.jpg) from a [September 2016 *Cato Institute* paper](https://www.cato.org/publications/policy-analysis/terrorism-immigration-risk-analysis#full). Selecting a statistic from the table: between 1975 and 2015, a total of *three* terrorism deaths in the U.S. were committed by people with *refugee* visas. The paper puts the *Percent chance of [an American] being killed""* as 0.00000003. **The essential fact is that an American being killed by a refugee is a *very rare event*.**

Suppose that our test is ""Is Person X from Syria?"", and our hypothesis is that if X is from Syria, then X is a terrorist (and should therefore not be allowed into the US). A type 1 error would be ""X is a terrorist"" when X is not, and a type 2 error would be ""X is not a terrorist"" when X is. Do I have that right? And a test with good *power* would minimize *both* kinds of errors? Or just the type 2 errors?

You see that I am struggling.

My intuition is that with extremely rare events, it is extremely difficult to come up with a test with statistical power -- with the implication that President Trump's exclusion of Syrian immigrants and visa holders from six other Middle-Eastern countries is a poorly designed ""test"" unlikely to prevent many terrorist deaths.

Is that a huge reach? Is the statistical power of the ""test"" of country of national origin a valid concept? Does it help us assess how likely it is, that the President's new immigration policy actually reduces the threat of terrorists killing Americans?

Thanks for your answers! "
"Hey there don't know if this is the right place to ask this, however i need help with an argument. Statistically, who is more likely to use turn signals, men or women?",3,0,False,False,False,statistics,1489258415,True,[removed]
Need help with APC (Age Period Cohort analysis),0,1,False,False,False,statistics,1489259834,True,"Hello!

I'm learning statistics by myself and have a question about APC analysis. I have found a lot of articles about APC analysis, but very little practical information.

Suppose I have a triangle matrix with rates for age and cohort. What steps should I take to decompose it to three separate effects?"
Software / App idea for getting published scatter plot values from a graphic,3,3,False,False,False,statistics,1489289675,True,"I've seen tons of scatter plots in publications and online articles, but rarely is the base data included. I've been thinking about creating an app where you could basically take a picture of the scatter plot and it would auto-calculate the x,y values of all the points. 

Would this be useful? Or with the potential for errors (e.g. 0.467 is graphed and we can only estimate it as 0.4) is it not even worth it?

The purpose of this tool would be to verify people's work. They can say that correlation is 'x' but we can't verify that if we don't have the point values to graph it ourselves."
"Given many different groups of sentences, how to find words that are overrepresented in each group?",1,2,False,False,False,statistics,1489290337,True,"I have a large dataset of tweets organized by the day and I'm looking to find words that appear significantly more often in a certain day than overall. How would I do this?  
  
If it helps, I'm using R and I don't have much of a background in statistics (although I have a decent background in calc)"
Missing Candy Probability!,0,1,False,False,False,statistics,1489300542,True,[removed]
What is viable variable?,2,0,False,False,False,statistics,1489309323,True,
Gendered Toy Preferences - Innate or Learned?,9,2,False,False,False,statistics,1489314468,False,
Why type 1 errors are more important than type 2 errors (if you care about evidence),42,75,False,False,False,statistics,1489321917,False,
"MANOVA results valid with one cell ""missing""?",0,1,False,False,False,statistics,1489322069,True,[removed]
[X-post from /r/France] A French redditor has put together a Bayesian model predicting the results of the upcoming French presidential election (depuis1958.fr),8,27,False,False,False,statistics,1489341227,False,
Can you help my Bayesian intution when it comes to clinical trials?,1,1,False,False,False,statistics,1489341763,True,[deleted]
Is there a way to find the area of one Z-Score using a Ti-83?,7,2,False,False,False,statistics,1489341927,True,"With the Ti-83 I can easily find the area between two Z-scores by pressing 2nd, VARS (distr), 2 and simply put my two Z-scores seperated by a coma. 

I can also find the Z-score of an area by pressing 2nd, VARS, 3 and typing in the area.

But I can't figure a way to find the area of a Z-score.

Everything I Google gives me how to find a Z-score from it's area not an area from it's Z-score."
Anyone CAP® Certified here? Would you recommend it?,3,3,False,False,False,statistics,1489345302,True,"Hi. Figuring out if it's worth the time, effort, and money to study and take the exam. Thanks in advance!"
Statistically speaking what will result in more correct answers,5,1,False,False,False,statistics,1489345596,True,"In a multiple choice context if you are not sure of the answer, I've read that its statistically better to pick a choice like A every single time, because over time you will get it correct 25% of the time, if there are 4 choices. 

* Now what about in situations where you are going to still make a guess BUT you are able to eliminate a single choice? What should you do...? Do you still commit to the choice you make every single time when completely unsure? "
Significance of Voting Outcomes,1,5,False,False,False,statistics,1489346520,True,"i thought maybe someone here would find this interesting enough to provide some input:

i am a hockey coach, and, at the end of the every season, i have the team vote on awards. i have tried different voting methods over the years, but, this year, i chose to have each player vote for three other players for each award. in total there are twenty-six players each casting three votes, which puts seventy-eight total votes into the pool. the players can vote for themselves and the players cannot vote for the same player twice. my first question is: how do i determine the significance of the voting distribution for a particular award?

secondarily, i ran a simulation of 100,000 random ballots to determine rough frequencies for how often a player will earn a certain number of votes. here are the results:

    n   count       p  cum p
    0    53651  0.041  0.041
    1   181960  0.140  0.181
    2   296635  0.228  0.409
    3   310183  0.239  0.648
    4   231163  0.178  0.826
    5   133719  0.103  0.929
    6    60482  0.047  0.975
    7    22649  0.017  0.993
    8     7117  0.005  0.998
    9     1922  0.001  1.000
    10     423  0.000  1.000
    11      79  0.000  1.000
    12      15  0.000  1.000
    13       2  0.000  1.000
    14       0  0.000  1.000
    15       0  0.000  1.000
    16       0  0.000  1.000
    17       0  0.000  1.000
    18       0  0.000  1.000
    19       0  0.000  1.000
    20       0  0.000  1.000
    21       0  0.000  1.000
    22       0  0.000  1.000
    23       0  0.000  1.000
    24       0  0.000  1.000
    25       0  0.000  1.000
    26       0  0.000  1.000

for all the vote counts of all the simulation ballots lumped together, i found that for any one player to get more than eight votes at random has a probability of 0.2%.

i only want to give awards out only in cases where there it a clear winner. so, to determine whether or not an award should be given out, what matters more: the significance of the voting distribution, the significance of the vote total, or some combination of the two?

"
"Marginal pdf, what am I doing wrong?",0,0,False,False,False,statistics,1489358778,False,
I'm about to start college - Any tips or something I should know?,11,1,False,False,False,statistics,1489360869,True,
CANT FIND THIS STATISTIC,1,1,False,False,False,statistics,1489365956,True,[removed]
Reading a study and have trouble interpreting a line.,2,7,False,False,False,statistics,1489379023,True,"I'm reading a study on how parenting styles are linked to youth anxiety.

I'm having a hard time interpreting the sentence below:

""For self-reported youth anxiety, the MZ-twin difference in parental warmth-reasoning significantly negatively predicted the difference of youth anxiety, whereas difference of parental harshness-hostility significantly positively predicted difference of youth anxiety""

Specifically the lines ""significantly negatively predicted"" and ""significantly positively predicted""

Any help would be appreciated.
"
Suggestions on following current research in statistics and making r/statistics more research oriented.,8,23,False,False,False,statistics,1489379483,True,"I am a graduate student in a statistics/ML program and I've noticed that the ML community is very easy to follow w.r.t. research. There are active online communities, conferences with open review processes and a number of academics on social media. 

On the other hand, I find that following research in the statistics community to be difficult. Perhaps the community is fragmented due to the many application domains?

In any case, here are my general thoughts, but please post some suggestions.

* Arxiv has a low signal to noise ratio, and lackluster design/search (arxiv-sanity helps)
* Journal TOC's are daunting, but could be good if you find the right journal
* Following a chain of references is a very good strategy, once you find the right paper
* Establishing authors whose work you enjoy, and then checking on their publications


Lastly, I think it would help if r/statistics had informal research discussion like r/machinelearning. It seems like every month, we have the same discussion on p-values, frequentist vs bayesian, and ELI5 markov chains. Statistics is becoming a popular field, yet there is rarely any discussion on anything past statistics 101. I think we can make this sub more active by changing that. 

Here are my thoughts, but please post your suggestions.

* A weekly ""what are you reading"" thread to discuss current books and papers that we are reading
* A sticky with career and education questions like r/math
"
KR(20),0,0,False,False,False,statistics,1489383101,True,[deleted]
Georgia Southern defensive coordinator not concerned with statistics,0,4,False,False,False,statistics,1489395199,False,
"Researchers of /r/statistics, what are you working on?",65,62,False,False,False,statistics,1489396648,True,"Inspired by [this thread](https://www.reddit.com/r/statistics/comments/5z34si/suggestions_on_following_current_research_in/) on the focus of this subreddit by /u/apliens I would like to know what topics researchers of /r/statistics are working on at the moment.

To make it relevant for the non-academic crowd, I would appreciate it if you could include not only the topic, but also a brief description of the topic for non-experts, maybe suggest software/packages to play around with the technique(s), and potential applications of your research. I would also be grateful if you were open to answering questions about your research.

So, please inspire us with your cool research."
transformations of a random variable (cdf method),9,1,False,False,False,statistics,1489413341,True,"can anyone explain how the cdf method is done? say you have that X is uniformly distributed on [-1,3], and you should find the pdf of Y = X^2, with the cdf method"
Anyone know how to turn a book into a dataset?,14,5,False,False,False,statistics,1489414882,True,What would be the best way of taking some pdf (a book for example) and making it into a dataset with 1 column (the word) and observations equal to the number of words in that book (60k-70k or whatever it would be). Any help is appreciated.
Rate of events with weights,2,3,False,False,False,statistics,1489422799,True,"A discussion was raised during coffee at my research institute (we're Astrophysics PhD/Post-docs) and none of us quite know how to handle this problem.

10 events were observed in a time interval of 1267895.33 seconds

The detector is not 100% efficient (but efficiency is well known), so each event is assigned a weight.

After efficiency weighting, each event has a value of: [1.87, 2.432, 2.159, 2.056, 2.44, 2.624, 2.1517, 8.72, 1.855, 1.714]

Therefore the weighted number of events is 28.02, implying a rate of 2.21e-5

The question was raised as to the uncertainty on the number of events and rate, especially since one event is weight much more heavily than the others.

Some people suggest simply sqrt(10), I disagree, we are in low number statistics.

Another suggested normalising weights (nwts):
nwts=weights/sum(weights);
error = sqrt( sum(nwts^2) ) * #_of_events
- This is better but I'm still not 100% satisfied because equal weights return to sqrt(n) and we are still in the low number statistics regime.

I feel like there is something clever to be done with a poisson distribution, but I cannot work out what it is.

Has anyone come across a problem like this before?


"
Machine learning matching methods?,2,1,False,False,False,statistics,1489428875,True,[deleted]
Need help with a question that was on my stat quiz,2,2,False,False,False,statistics,1489431942,True,[deleted]
How do you derive meaning from arithmetic means without knowing the underlying distribution?,0,1,False,False,False,statistics,1489432983,True,[removed]
"How can I access specific data sets between certain time frames with specific occurrence frames (ie. days, weeks, months)?",4,3,False,False,False,statistics,1489435628,True,"Pretty much title.
I'm looking to pull data for certain time frames of with specific occurances in mind (don't know if I'm using the right wording here).
For example: If I want to find the data on traffic accidents in a county per day rather than per month. I seem to be able to find this sort of data per month, but have a problem finding it per day.
Thanks in advanced."
Statistics Help,0,1,False,False,False,statistics,1489437408,True,[removed]
Where could I find a ownership breakdown of different stock markets?,2,1,False,False,False,statistics,1489443123,True,I want to find out what portion of a stock market is owned by foreign institutions and domestic institutions....can anyone direct me?
Missing data classification,6,4,False,False,False,statistics,1489443718,True,"Hi /r/statistics. I am working on a biomarker project where there are missing data. The data is missing because of the following... Blood samples were collected from study participants. Then, part of the blood sample was used to measure biomarker A. In the case that there was enough of the sample remaining, biomarker B would also be measured. In this example, biomarker A would never be missing, and biomarker B would sometimes be missing. 

I am confused about the differences between MCAR, MAR, and MNAR. What missing data classification applies to the biomarker B data in this example? Thanks for the help!"
Where can I learn about noise in data?,4,0,False,False,False,statistics,1489445704,True,"Hello, I'm a physics major and all experiments have noise in data and I do data analysis. I want to learn more about how to deal with noise and how to calculate it. For example, how to calculate signal-to-noise properly and how to handle and calculate different types of noise. Does anyone know good textbooks or resources for learning about these?"
How do I count how many more times a group is overrepresented based on the percentages I have?,0,1,False,False,False,statistics,1489449541,True,[removed]
SAS university edition,3,2,False,False,False,statistics,1489463001,True,"So I'm thinking of trying to teach myself SAS or at least download it to get some experience with it since a lot of the jobs I want to apply to require it.  I graduated from college in January and I never knew that there was a free college edition.  I'm pretty sure my email is still active for 3-4 more months so I think I could get away with downloading it for a year, but will I  be able to renew the license the following year? If not, is there some other way I can get exposure to it for free, maybe an alternative software that is similar but free?"
Help deriving full posterior distribution in bayesian stats,0,1,False,False,False,statistics,1489467097,True,[removed]
What is the purpose of exponential smoothing?(X-post from r/analytics),4,7,False,False,False,statistics,1489474624,True,"I understand essentially what it is, as well as how to implement. I'm just curious as to the practical application? Why would you want to smooth numbers like this? Is it for aesthetics or to make it more presentable? My initial instinct says ""this makes your number less accurate, and introduces trend lag. There's no functional purpose behind it, it seems purely aesthetic."" Am I missing something?"
What's the best way to find out the probability that my husband and I would meet?,9,1,False,False,False,statistics,1489493328,True,It's our first anniversary and I want to surprise him! I was adopted from Korea and he was a Serbian refugee. We met in Milwaukee and we always joke around what are the chances? 
Appropriate statistical test to assess clairvoyance in an individual longitudinally?,17,13,False,False,False,statistics,1489494050,True,"Hypothetically, let's say I'm taking a deck of Tarot cards and trying to guess them. I score a point if I guess the card number, or the card suit. 


I do this for 20 cards, once a day, seven days a week.


What would be the correct test to assess whether my guesses were successful at a rate greater than chance?

I was hypothetically going to take an average of a week, and do a t-test between weeks. Or, alternatively average results over the course of several weeks and conduce repeated measures one-way ANOVA. 



https://hermetic.com/crowley/libers/lib9



Disclaimer: I don't think clairvoyance is a thing. "
Custom Tables in SPSS version 24,5,4,False,False,False,statistics,1489497231,True,At work I recently got an upgrade on SPSS to version 24. Since then I can't find the button to create custom tables anymore. How can I access this in this version of SPSS?
Pre- and Post-Treatment Count Data,1,2,False,False,False,statistics,1489502496,True,"I have a data set with pre- and post-treatment count data and don't know a proper analysis to evaluate differences between pre- and post-treatment. Further, the count data is zero-inflated. Any help would be appreciated!"
What syntax should i use in a spec file to output seasonally adjusted data with forced totals (D11 A)in x-13 ARIMA SEATS?,1,0,False,False,False,statistics,1489508577,True,Save = (d11) works for the final adjusted time series but i cant seem to get the program to recognise d11a
Questions about data analysis using PROC LOGISTIC,5,2,False,False,False,statistics,1489513422,True,"I'm working on a data set that has these following variables (n = 473) :

* Visit (visiting a hospital) - a binary variable coded as 0 and 1, basically 0 = no and 1 = yes

* Quality - an ordinal variable from 1 to 5, 1 being very bad and 5 being very good

* Distance - a continuous variable from 5 to 300 minutes of walking to get to the hospital

* Age - a continuous variable, from 18 to 50

* Marriage Status - an ordinal variable coded from 0 to 3, 0 being bad and 3 being very good

* Number of Previous Births (NumBirths) - a continuous variable from 0 to 9

I'm examining Visit as the dependent variable.

If I run PROC LOGISTIC and have my model statement as:

* Visit = Quality Distance Age MarriageStatus NumBirths Quality*Distance 

I will not have any significant variables at all after stepwise selection.

However, if I run quadratic terms in PROC LOGISTIC, like:

* Visit = Quality Distance Quality * Distance Quality * Quality Distance * Distance Age Age * Age MarriageStatus NumBirths MarriageStatus * MarriageStatus NumBirths * NumBirths 
(Basically every squared term is added in)

Then I'll have both Distance and Distance*Distance as significant effects. 

1. Under these scenarios, would it be better to report the simpler, non-quadratic model? Or is it better to talk about Distance and Distance*Distance being significant in the more complex model?

2. Also, is there a way to justify using quadratic terms to begin with? In terms of this study, I don't think that the relationship between Distance and Visit is linear - walking one hour might be okay for most people, but people might freak out if they knew they had to walk 2+ hours. However, I don't know if this reasoning is acceptable to justify adding in quadratic terms. I don't think I have enough knowledge about predicted probability plots to explain why, if this is applicable here.

3. Finally, when talking about the dependent variable, would it be better to report Odds Ratio estimates or just the coefficients for our significant effects? I, for some reason, can't get any odds ratio estimates to pop up in PROC LOGISTIC.

Thank you for your time!
"
What is the distribution of the sum of 2 Chi-Square random variables?,5,1,False,False,False,statistics,1489518591,True,[deleted]
Minimum amount of cases for Incidence Calculation?,0,1,False,False,False,statistics,1489524766,True,[removed]
Assigning value to data points,6,3,False,False,False,statistics,1489526566,True,[deleted]
SPSS issue - running (trying to) stepwise multiple regression but always getting warning 'no variables were entered into equation',6,3,False,False,False,statistics,1489530184,True,"??? Confused. Seems like everything has come out okay. Am i better just running a regular 'enter' regression. 1 outcome variable and 3 dependent if that helps. 

"
Probability conditioning question,0,1,False,False,False,statistics,1489531513,True,[removed]
Appropriate test to compare paths?,2,3,False,False,False,statistics,1489535129,True,"Small dog in tall weeds here...  
  
Let's say I am evaluating of the performance of people with different experience levels in performing a task.  Let's say that if you plot the path of the object in this task, it roughly approximates x^2.   My hypothesis is that experienced people have less variance around that line than inexperienced people.  What test could I use to evaluate for that?"
What would be the best route to prepare for data analytics masters degree while still an undergrad?,6,2,False,False,False,statistics,1489536657,True,"Currently, I am a college student, and I'm taking one programming class, and mostly business classes. Is an information systems degree with a math minor a solid foundation? I already took calculus 1. "
Statistics Grad,21,12,False,False,False,statistics,1489539049,True,"Hi,

I am finishing my undergrad in Biochemistry at a top University of California. But it turns out I did not like biochem.

I want to go pursue grad school for statistics. Any advice on what I should prepare? (Additional math classes, learn coding, etc).

Any suggestions would be appreciated.

Thanks."
Simple survey for my community college statistics class. Would appreciate your responses.,0,1,False,False,False,statistics,1489541588,False,
Popular use of Excel,10,1,False,False,False,statistics,1489544904,True,"Do major companies really use excel? I was just reading an article by a guy who worked at a top consulting firm, and he casually mentioned how he spent a lot of time formatting and manipulating data /in excel/. 

And this definitely isn't the first time I've seen something like this. Regularly when I've researched companies looking at Data Science jobs they say that excel experience is what they're looking for. What gives? Is it really that popular outside of uni? Have these people just never heard of Stata (I won't even ask about R)? "
Looking for terminology,4,2,False,False,False,statistics,1489552021,True,"Hi!  I'm writing my master's thesis and I'm not sure how to describe my findings in a statically appropriate way.  

I've measured an effect between three groups using two different counting methods.  Both methods gave me statistically different means between the groups, one with an F stat of 7.78 and the other 10.31.  

I wanted to state that this 32% larger F stat in the second method indicated that this method is 32% more sensitive to detecting differences between the means, but my advisor is uncomfortable with the word ""sensitive"" since it implies a sensitivity analysis which isn't being utilized here.

I'm looking for some language help here.  32% more robust?  32% stronger?  What would you recommend? "
Having trouble controlling my variables (and myself),0,1,False,False,False,statistics,1489556527,True,"I was going to do a partial correlation but I have one continuous variable and the other is a dichotomous dummy. So I could do that by running a Pearson, which is the same as a point-biserial correlation BUT then how do I control for my third variable, which is also continuous? 

Again, I have 1 dichotomous and 1 continuous variable but need to control for a separate continuous variable. Can I just run the partial correlation? Because running the point-biserial is the same as Pearson and gives me the same output. So can I just stick the control variable in there also? I tried running it and the output was the same as the point-biserial for ""no control,"" I need to know if the second half of the output WITH the control will be legit. "
What is Multilevel Models?,3,3,False,False,False,statistics,1489560022,True,"Hey I'm studying for Bayesian stat using the book 
Statistical Rethinking by Richard McElreath.

Just finished chapter 1 and did a little googling around but it's a bit unclear. That or I'm impatient.

But from my understanding, please correct me, is that:

You have more than one model? A parameter of the first level model can be model by another model via second level?

y_hat = b0 + b1 x1

b1_hat = c0 + c1 z1


Or something along this line?

Thanks "
Statistics homework help,0,1,False,False,False,statistics,1489564047,True,[removed]
Maximum value for kappa for Von Mises mixtures,0,1,False,False,False,statistics,1489574360,True,[removed]
Incremental fit indices with low RMSEA in null model,10,6,False,False,False,statistics,1489575338,True,"Hello everyone,

I'm doing CFA for a translated questionnaire and have encountered the following problem, maybe someone can help:

RMSEA, SRMR, and CMIN/DF are acceptable, good even, but the fit indices like CFI, TLI, etc are not (they are around .80).

I read on David Kenny's page (http://davidakenny.net/cm/fit.htm) that a RMSEA lower than .158 in the null model can lead to this. My null model has a RMSEA of .155. Is there a way to work around this or deal with this? Or do I just say that the CFI + TLI are low but still indicate acceptable fit because of this null model RMSEA thing.

Any help would be greatly appreciated =)"
How to report this P-value?,14,6,False,False,False,statistics,1489577327,True,"My P-value is 0.051. If I were to report it like P = 0.05, it might appear as if the result is significant, which it's not. Should I report it like P = 0.051 then?"
One Writer Used Statistics to Reveal the Secrets of What Makes Great Writing,5,41,False,False,False,statistics,1489579504,False,
Statistical Process control questions,1,1,False,False,False,statistics,1489580303,True,"Hi, i'm working on a control chart for one of the process in my company, for what i have seen there is a lot of variation in the process and i would like your input in a few things i'm not really sure about.

These are the [graphs](https://imgur.com/a/uI1I7) i have came up with so far, every graph is for a different location same process diferent place and people, sorry for the lack of information in the graphs i had to strip them for confindiality, and the problem with x axis labels not displaying correctly is becouse i'm still learning how to graph with matplotlib, i already fix it but hadn't updated the images, anyways if need be i would upload better images. 

What i need to know and i'm not sure about is the followin:

•Every day i get a new point of data for all of my five location, that means that my average is changing daily hence my limits too, should i set my limts constant and only change them if there is a clear change in the process? Or should i let my limits evolve with the new data?

•In most of the books and articles i have read about SPC they tells you if you have more than 3 or 4 data points above or below the center line that means there has been a change in the process, but my charts are fill with 3 or more data points in one or the other side of the center line. That means there is a error in my graph or how should i handle this situation?


Note:  The red line is the upper limit, the bottom limit is 0 as i am counting how many error per day has been generated."
Trying to do ANOVA. Is this correct?,5,2,False,False,False,statistics,1489584683,True,"Hie,   
  
I am trying to do ANOVA. I want to see if travel time affects student grades among different student groups.  
  
So, I have a matrix  
  
Travel Time | Avg Mid Term 1 grades | Avg Mid Term 2 grades | Avg final grades
---------|----------|----------|--------
1 | 11.097276 |	11.058366 | 10.782101
2 | 10.644860 |	10.336449 | 9.906542
3 | 10.739130 |	9.565217 |	9.260870
4 | 8.875000 |	8.000000 |	8.750000
  
  
When I did anova, I got the p-value as 0.64.  
 
Am i going about it correctly?  
  
Thank you. 
"
What type of regression should I use??,2,0,False,False,False,statistics,1489600177,True,"My data is for 28 countries, from 2001-2015. My hypothesis is that Equity Home Bias (EHB) has diminished overtime due to Globalisation.    
    

My data shows that the EHB has declined (you can tell by eye), but I'm not sure how to incorporate my proxy data for globalisation. I'm reading about different types of regression but it seems like many would be a fit (e.g fixed-effects analysis, general linear model) - so I'm assuming my lack of statistical knowledge isn't making it clear.    
    
  
If anyone could point me in the right direction or spare some help, that would be MUCH appreciated.     
    
Thank you in advance!!!"
Adding Weighted Averages to Complex Sampling Analysis,0,1,False,False,False,statistics,1489603653,True,[removed]
Question about what is being maximized in logistic regression.,8,6,False,False,False,statistics,1489611789,True,"I understand that logistic regression is trying to find coefficients that generate the most accurate probability of each case being a 1 or a 0.

But I am unclear on what exact outcome value is being ""perfected"".

Is it:

* The predicted probability for each case vs. the actual probability?

* The classification of each case as being a 1 or a 0 based on the predicted probability?


I hope that question makes sense.

To put it another way...

Andy Field (2005) says ""The chosen model will be the one that, when values of the predictor variables are placed in it, results in values of Y closest to the observed values."" (p.221). What exactly are the ""observed values"" and how are they determined?"
What type of methods and data analysis should I choose?,0,1,False,False,False,statistics,1489611816,True,[removed]
Child protection statisticians!,0,2,False,False,False,statistics,1489612273,True,"Without looking at the research (just yet) and as a former Child safety officer I'm wondering who are the statisticians (name drop) that are leading research in the modelling of risk and/or outcomes (whether it be foster care outcomes, adoption, harm minimisation, effective support programs etc). I am wondering also what people think about ID coding anonymous callers reporting allegations of harm/neglect. Such that callers who make multiple false allegations can be identified, and perhaps response urgency is moderated to some extent (valuable resources saved for the real thing). I'm thinking mixed effects modelling clustering on anon caller IDs or similar.  Any names? Just throwing it out there :)"
"Number 2, based on the way I plugged it in the confidence level is .90 or 10 percent, am I on track or am I way off?",3,0,False,False,False,statistics,1489623964,False,
is pari mutuel betting a dutch book?,0,0,False,False,False,statistics,1489626004,True,"Don't really know where to ask this question, so I'm asking it here.  It would seem to me that pari mutuel betting (a betting system in which the odds for a multi-outcome event are arranged so that each outcome off-set one another with a ""rake"" for the venue calculating the odds) is little more than a dutch book with sequential probabilities happening at once.

Any thoughts?"
DoE Project Topics,0,0,False,False,False,statistics,1489627338,True,[deleted]
Any online calculus based statistics classes?,9,11,False,False,False,statistics,1489628469,True,"Hey /r/statistics,
This next semester I will (hopefully) be attending San Jose State University for a statistics masters program. Unfortunately, it turns out I am 1 class short of their prerequisites (I thought that I had taken would count, but it will not). While I have been admitted, it is conditional on wether or not I can find a summer class to meet the missing requirement.

The class I would need is one similar to SJSU's Math 161A ([syllabus](http://info.sjsu.edu/web-dbgen/catalog/courses/MATH161A.html)) I was told it was also similar to [Stat 414](https://onlinecourses.science.psu.edu/stat414/) at Penn State University (which I am trying to get into now). There are no transferable classes from any local community college, and my google searches are turning up nothing helpful. 

So the question is, does anyone know of an online calculus-based intro to statistics course?

tl;dr Do you know of any online calculus-based statistics courses that cover: Descriptive and inferential statistics. Collection and analysis of data, discrete and continuous probability models, random variables, Central Limit Theorem, confidence intervals, and hypothesis testing?
"
1 Movies Website - Watch Movies Online for Free,0,1,False,False,False,statistics,1489629666,False,
Can I use this data for a p-value or correlation?,22,2,False,False,False,statistics,1489641515,True,"http://imgur.com/a/BcRqG

I'm supposed to use either a simple correlation regression, or chi-square test for this project and include a p-value. Can I do any of those with this data?
I already did the chi-square test but got a very high number.

Thank you to anyone who may help."
Statistics Homework Help,0,1,False,False,False,statistics,1489647890,False,
"Burned out postdoc, thinking of a change",0,1,False,False,False,statistics,1489649341,True,[removed]
Need help with data transformation,4,1,False,False,False,statistics,1489661172,True,"I am conducting an online survey on Qualtrics regarding users' literacy in search engines (ex. Google search).

I am measuring their both perceived and objective  literacy. Perceived literacy is measured with 7-point  scale. Regarding the objective knowledge, I made 7 multiple choice questions with one correct answer. So users may acquire from 0 to 7 points as an accumulated score for their objective literacy.

So I am planning to combine 7 multiple choice questions into one variable with accumulated score. (Ex. Respondent answered 4 out of 7 questions, as a result he/she is assigned 4points)

Then I'd merge objective and perceived literacy variables into one overall literacy ( ex. Respondent indicated 5 for perceived literacy and scored 4 points for objective literacy. So the mean value of newly created variable out of these two is 4,5)

However, I don't know how to put it on practice. Not sure whether Qualtrics has such features and for statistical analysis I am using IBM SPSS statistics. 

Probably I could merge two variables with Confirmatory Factor analysis, however, I still have no idea how to transform the multiple choice questions with one correct answer.

Thank you in advance!"
G*power and LMEM help,4,1,False,False,False,statistics,1489663754,True,"Does anyone know which option to select if I was trying to find a sample size for an LMEM test in g*power?

#statisticalnoob."
Help With Statistics Homework Online By Economicshelpdesk Is Unbeatable,0,1,False,False,False,statistics,1489669090,False,
The levels of data science class · Simply Statistics,6,29,False,False,False,statistics,1489675715,False,
Need help with a probability question.,2,1,False,False,False,statistics,1489677938,True,"Ok, smarties....every year I do a twist on the standard NCAA basketball bracket.  We have everyone pick 8 players, and then what they score over the course of the tournament is your total points.  Person with the highest final score wins.  Simple.  What has always surprised me is that in the last 10+ years I have done this, we have NEVER had the same 8 players picked. Same 7 out of 8, but that's as close as it's ever been.  I was just curious what the chances of that happening were?  I feel it's probably too hard to answer, as every player is not equal as people are picking these based on the probability of how far a team is going to advance, ie you aren't picking a player from a 16 seed....and if you are, I'll send you my e-mail so you can submit an entry! :P

Anyway, I thought it was interesting.  If anyone can give me a rough answer I'd appreciate it. 


**edit spelling**"
What are my Python/R options for running linear regression on categorical variables if the noise isn't i.i.d. normal?,7,1,False,False,False,statistics,1489693948,True,"I'm doing linear regression in seaborn on some categorical data. However, it's pretty obvious that the errors are not i. i. d. and gaussian.

What are some R/Python packages/functions to use to try to run regression properly in this instance?
Thanks for any recommendations!"
Probability question,7,4,False,False,False,statistics,1489698759,True,"Let's say I have a box divided into 25 smaller boxes (a 5x5). Each small box holds 3 widgets, so all together the entire box holds 75 widgets. If I want to know the probability that there will be at least 2/3 widgets in every box, depending on how many widgets there are total in the entire thing (1 to 75), how would I do it?"
How to calculate margin of error for an intentionally nearly unanimous data set?,1,2,False,False,False,statistics,1489699806,True,"Short version is there's a data set we've generated and we're looking to get an idea of how many false positives there are. We tested a little over 5% random sample for the sake of being able to use a FPC factor, but the false positives were few and far between. To the point that even with a 5% sample, n(1-p) only reached 4, while np was 123. It's my understanding that they should both be at least 10 to be able to use z-score for MOE.

Is there something I'm missing, or do I need to keep increasing the sample size until n(1-p) hits 10?"
Machine learning matching methods?,0,0,False,False,False,statistics,1489705678,True,"My goal is match each of the invoices created to each payment made. So for example, if I have a set of invoices:

    InvoiceDate InvoiceCategory
    02/21/2016  Amazon 
    08/17/2016  BestBuy
    08/17/2016  Amazon
    08/21/2016  BestBuy

Let's say each of those rows are matched respectively with each row here:

    PaymentDate PaymentCategory
    04/1/2016     Paypal
    9/19/2016     Credit
    9/22/2016     Paypal
    9/29/2016     Check

That is, `02/21/2016  Amazon` matches with `04/1/2016     Paypal` and so on.

I want to then train some model on this training set. I want the model to figure out from the example above that the `PaymentDate` comes roughly a month after each `InvoiceDate`. And that `BestBuy` matches more closely to `Check` than `Paypal` (because intuitively, Best Buy accepts check payment but not Paypal)

So if my test set consists of 

    InvoiceDate InvoiceCategory           PaymentDate PaymentCategory
    02/21/2015  Amazon                      9/19/2015     Credit
    08/17/2015  BestBuy                     04/1/2015     Paypal

Then the ML should predict that `02/21/2015  Amazon` matches with `04/1/2015     Paypal` and `08/17/2015  BestBuy` matches with `9/19/2015     Credit`

I've heard about `recordlinkage` in Python. However, it seems to only link records with identical values. That won't work in my case because in the example above ` 02/21/2015` matches with `04/1/2015` even though their dates are different

But what ML model matches records like this?
"
What do the different values in the set.seed function change? I.e. how would set.seed(10) compare to set.seed(50)?,3,2,False,False,False,statistics,1489712549,True,
"Essential Statistics for Data Science: A Case Study using Python, Part I",7,63,False,False,False,statistics,1489727930,False,
96 Percent Students Choose Monte Carlo Simulation Assignment Help By Statisticshelpdesk,0,1,False,False,False,statistics,1489734694,False,
Does someone know a procedure to estimate the median when the amount of points is larger than the available memory?,4,3,False,False,False,statistics,1489741955,True,"So. Assuming that one has little memory available ( .^1 ) and a lot of entry points that may use up all the available memory, how can one estimate the median?

Ideally one would feed to the process a block of data points, due to memory limitations, from those the process extrapolate a value that then will be adjusted with the next block of data points.

Before I try to come up with something myself (and I don't think I will get good results) and digging the internet, I ask here because maybe people have encountered this problem already.

Because otherwise one is left with the average that is easier to compute, but it may be ""too skewed"" if the distribution of points have very long tails.

Thanks in advance.

(1) ex: [hp 50g](https://www.educalc.net/2107089.page), 230 kb of memory available. Or other embedded computers."
Monte Carlo Simulation Assignment Help,0,1,False,False,False,statistics,1489746659,False,
Is someone an expert in finding the LOEC and Noec values of a toxicology exposure study?,1,1,False,False,False,statistics,1489756822,True,"Right so I've been told that I need to do a one way Anova with a Dunnett's test to find my NOEC and LOEC values. People say that these values shouldn't be used in toxicology studies anymore and I know that but this isn't getting published so don't worry. 

My problem with finding these values really comes from how I put my data into minitab (also been recommended to use), my concentrations are 0, 10, 32, 100, 320, 1000.  I have 20 repeats for each concentration, which were exposed for 21 days. I took measurements of the survival at 48hrs, 7d, 14d and 21d. I'm testing to see if one concentration is significantly different from my control conc (0). 
So how do I literally input the data for this? "
Sampling on training data and scoring on validation data,2,0,False,False,False,statistics,1489761550,True,"I am doing regression random forest modeling on a bunch of data with the response variable being the amount of revenue a customer purchases. 

~90% of the responses are 0. So when I train the model, I undersample the 0s to help train the model. 

However, when I then predict the validation data with my model, the accuracy is way out of wack because 90% are still 0, but the predicitions are all much bigger than 0 because the training data was undersampeled. IE the Rsquared is less than 0. 

is there a good way to better score validation/test data when the training data is undersampled?"
Biostatisticans in Pharma/CRO: Is Python useful to learn?,0,1,False,False,False,statistics,1489764693,True,[deleted]
Biostatisticans in Pharma/CRO: What skills do you think is useful to have to be marketable?,6,1,False,False,False,statistics,1489765305,True,"I work for a Pharma CRO as a Biostatistican. I am trained as an Epidemiologist (Masters) and the PhD I am working on is focused in clinical research. I am also a part time PhD student and currently trying to plan out my independent study courses to fulfill my elective requirements, pretty much I have no idea what to pursue as independent study training. Therefore, I am hoping to come up with topics for my independent study that would be more marketable in the field as a Biostatistican in pharma; particularly to transition from a CRO to a pharma company."
ELI5: What is a Markov Random Field?,2,6,False,False,False,statistics,1489771322,True,
"Tips predictive modeling based on multiple variables with varying correlation coefficients, but lots of sample data?",0,1,False,False,False,statistics,1489777917,True,[deleted]
How to perform hypothesis testing on correlation in R?,3,10,False,False,False,statistics,1489782958,True,[deleted]
Turning your input variables into ratios of each other,0,1,False,False,False,statistics,1489784722,True,[removed]
Joint density of Order Statistics,1,4,False,False,False,statistics,1489785736,True,"Why is the joint density of order statistics on a uniform distribution this:

http://imgur.com/a/220UJ

https://en.wikipedia.org/wiki/Order_statistic#The_joint_distribution_of_the_order_statistics_of_the_uniform_distribution

I understand the n! part, but I'm confused on how they derived this answer. Could anyone give me some insights about this?"
Methodology question on Tukey tests across subgroups across thousands of variables,0,1,False,False,False,statistics,1489788286,True,[removed]
Linear fit error in Logger Pro,0,1,False,False,False,statistics,1489789138,True,[removed]
How likely is it that biostatistics will be automated?,0,1,False,False,False,statistics,1489790370,True,[removed]
Undergraduate Prerequisites before starting PhD Program in Statistics,25,25,False,False,False,statistics,1489790825,True,"In your opinion, what are the best classes to take before embarking on a PhD in statistics, and if possible rate the importance of the class (Ex. Multivariable Calculus-Medium, Real Analysis-High, ect).

Also, can you explain how that mathematical concept was used during your PhD program that would be great!"
sports analytics question -- forecasting play time,0,4,False,False,False,statistics,1489793955,True,"can anyone refer me to any papers on forecasting how long a player will play in a given game ? i read in an article that ""there is a large body of writings on this topic"" but can't really find much ... i'm mostly interested in the NBA, but other sports will do too! thanks!"
Help with dataset!,0,1,False,False,False,statistics,1489796316,True,"I am comparing students grades from 2 datasets; those who did a year in industry and those who did not. However, these 2 datasets differ greatly in their sample sizes (28 vs 77).


I would like to know what kind of test I should use to test their means and variances. In my mock tests I was using **z-test** for mean and **f-test** for variance. However, now I am unsure how the actual data (which I just received) may change the legibility of these tests:


*Ideally, I would like to prove that at Year 1 and 2 the average grades and variances for both datasets were very similar - these years happen before the placement occurs. Year 3 would be the one that should break the H0 (after the year in industry) to prove that going on a placement had an impact on the students grades and variance of the results obtained.*



*For example: Data for Year 1*

**Sample A: Non Placement**

   n = 77
   mean = 70.06
   standard deviation = 11.19
   variance = 125.32

Grade -	Number of occurrences

   [0-39]	-   0

   [40-49]	 -  1

   [50-59]	  - 14

   [60-69]	  - 23

   [70-79]	  - 28

   [80-89]	  - 6

   [90-100]	-   0


**Sample B: Placement**

   n = 28
   mean = 70.36
   standard deviation = 8.81
   variance = 77.65

Grade -	Number of occurrences

   [0-39]	- 0

   [40-49]	- 0

   [50-59]	- 3

   [60-69]	- 11

   [70-79]	- 10

   [80-89]	- 4

   [90-100] -	0


I hope I have explained myself enough. I am no expert in statistics and could really appreciate it if someone could point in the right direction. I am unsure whether the data is normally distributed and am unsure of how to do test for it if it is not. **Note** that I have only been using excel's analysis toolpack plug-in to carry out my ""findings"". 

EDIT: Formatting"
Bias in Speculative Fiction,0,1,False,False,False,statistics,1489808560,False,
"Predicting Housing Prices with Linear Regression using Python, pandas, and statsmodels",0,22,False,False,False,statistics,1489813373,False,
Would anyone be interested in analysing one year of sleep data?,5,1,False,False,False,statistics,1489815367,True,"I am gathering a lot of data using my phone and smart watch (heart rate, sleep cycles, breathing rhythms etc) so if someone is interested in making a post on /r/dataisbeatiful, PM me."
"How is this possible? ""Greater reliability and greater scope also speaks in favor of sample surveys.""",0,3,False,False,False,statistics,1489817356,True,I read it in a statistics book. It claimed that time and money isn't the only perks with using samples instead of population surveys. But that reliability and scope are also better in samples. It didn't explain how. How is this possible?
What is the difference between z-score and t-score?,7,14,False,False,False,statistics,1489823869,True,
Exploratory factor analysis and internal consistency,4,5,False,False,False,statistics,1489836146,True,"This is probably a really simple question, I hope!

Having performed EFA, a 5 factor solution is suggested, even though factors 4 and 5 are theoretically linked. If I ignore the suggestion I get an alpha coefficient = 0.62 (factors 4 and 5 combined) and if I follow the solution alpha = 0.51. I know, even the highest score is lower than what many find acceptable but why is it higher if I override EFA?"
question forecasting expenses,0,1,False,False,False,statistics,1489841206,True,[removed]
Test choosing and inputting data... I need help!,0,1,False,False,False,statistics,1489845043,True,[removed]
How can I randomly increase the amount of correlation between two sets of data?,12,2,False,False,False,statistics,1489850769,True,"Is there any way I can gradually increase the amount of correlation between two random datasets? Rather than picking a random number of points in each set and assigning the same value to each of them, I feel like there should be better method out there to gradually modulate the strength of the correlation. 

For example, in the method I briefly described above, I do not want to immediately set the two datasets equal at certain points as this would immediately give a skinny standard deviation. Maybe a way to gradually change the direction of the correlation and the strength at the same time? I don't know...

Thanks for your help"
Estimating covariance matrix for very tiny values?,11,9,False,False,False,statistics,1489854411,True,"Sorry about the double post!

In a completely unrelated matter from the previous post, I'm having a lot of trouble working with a dataset that has really tiny (1e-12) values and accordingly really small relate differences between values in the dataset. 

Currently, I'm using a shrinkage estimate, but it's giving me lots of trouble. Any advice for other estimation methods?

Thanks!

UPDATE1:I do think that the issue with my calculations comes from rounding errors, but I think I fixed it!
I added a small constant (1e-25) to the diagonal of the matrix and it seemed to get rid of the errors I was having trouble with. The only problem is that I have to painstakingly choose a new constant for every new covariance matrix I'm using. Is there any way to optimize this process?"
Rearranging questionnaire response data in excel,4,3,False,False,False,statistics,1489860600,True,"I have been doing this manually, however I have been told I need to provide working proof of what I have done.

Essentially the data are a 5 point Likert scale, some questions gauging positive opinions other questions gauging negative opinions. 

How do I change certain responses so that 1 becomes 5, 2 becomes 4, 4 becomes 2 and 5 becomes 1. If that makes sense.

Can also be done via SPSS if anyone knows how to do it on there."
"What would be more useful in comparing a team's statistics - ""percentage away"" or ""deviations away?""",4,5,False,False,False,statistics,1489860842,True,"I'm comparing a bunch of sports teams stats from a season. What do you think would be a more useful statistic to show how far above or below average a team is?

Example: say a team averages 10 points per game. The league average is 7 with a standard deviation of 2. Percentage away: 10/7=1.43 (143%) or deviations away: (10-7)/2=1.5."
What is the best software suite for data visualizations?,17,28,False,False,False,statistics,1489866760,True,[deleted]
A little help on a research paper,0,1,False,False,False,statistics,1489871528,True,[removed]
a little help on a research paper I want to write,0,1,False,False,False,statistics,1489872220,True,[removed]
Why are these chi-square test results presented as odds ratio?,7,5,False,False,False,statistics,1489874261,True,"I'm currently reading a relatively old article on psychiatric diagnoses in Emergency Department attenders and I'm trying to work out why the results are presented the way they are. They have used chi-square test to look at difference between number of patients with psychiatric diagnoses in frequent attenders and routine attenders but have presented the results of the chi-square test as odds ratio and confidence interval rather than with p values. 

I can't find anything online to explain why they're presented this way. I was wondering whether anyone here could help?

Page 164 of this paper if this helps: http://s3.amazonaws.com/academia.edu.documents/46031828/s0022-3999_2800_2900228-220160528-5091-zxaxme.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1489877811&Signature=SsQq0v9F2Aa4Q2FknIiMqzuphfU%3D&response-content-disposition=inline%3B%20filename%3DPsychiatric_status_somatisation_and_heal.pdf 

Edited to try to make it clearer"
How can i compare categorical data in SPSS,4,5,False,False,False,statistics,1489888018,True,"I'm doing a 2x2 design, and asked my participants a series of questions pre and post treatment. I now want to compare the groups and see if there is any significant difference in their answers before and after the treatment. But i just can't seem to do this when its categorical variables in SPSS? 
each question had 3 different options for example ""yes"", ""no"" ""occasionally"", I would like to be able to see how each group in the 4 conditions answered the question (for example in %)  before and after the treatment and if there is a significant difference
any help would be greatly appreciated
"
Why do we square root error instead of taking absolute value?,38,17,False,False,False,statistics,1489901234,True,Just trying to figure it out. As i understand we want to fit the line with least amount of error. But how is square root provides us with least amount of error?
Control Variables in LASSO,2,4,False,False,False,statistics,1489936992,True,"I'm wondering about the following, not sure about the best way to approach my question.

I have a case where I want to perform variable selection on a wide dataset. There is one variable that is expected to have a strong relation with the response. Then there are many other variables to select from that are expected to have small contributions to the overall fit. I'm interested in inference on the variables with small contributions to the fit after the first variable is taken into account. I was planning to do this with glmnet in R.

I want to use lasso, but the variables are measured on different scales - the variables expected to have small contributions are counts/transformations of counts, and the variable expected to have a large contribution is continuous. I understand that lasso is influenced by differences of scale in the predictors, but I need to maintain sparsity in the model matrix (for memory related reasons), so I can't scale the predictors. 

One possible solution glmnet allows, is that you can choose to not apply any shrinkage to particular predictors - so I could just not apply any penalty that one variable. What other solutions exist for this problem or what other ways could I approach the question?"
"Limits of a random variable given a probability function. How would I go about this question? Part a has me clueless, could probably do the rest given part a.",0,0,False,False,False,statistics,1489938884,False,[deleted]
"Sample size of 43 for correlation. 3 outcome variables, 5 predictor variables. Which analysis to run?",0,1,False,False,False,statistics,1489939095,True,[removed]
General rule of thumb for multiple regression and small sample size?,9,1,False,False,False,statistics,1489939732,True,"If I have a sample size in the late thirties, 3 dependent variables and 6 independent... is the sample too small for a multiple regression? 

If so - what should be done "
Advice on plant growth experiment,0,2,False,False,False,statistics,1489941045,True,"Hi there,

I have a modest background in statistics. I have taken an introductory statistics course, have received instruction here and there from biologists I have worked with, and have cruised internet blogs/forums/etc quite a bit.

So here's my conundrum...

I conducted a plant growth experiment in which I grew 400 plants in 20 different soils (20 plants/soil type) and then measured their growth after 10 weeks. I did a number of chemical and biological tests on each soil. I would like to relate these soil variables to plant growth. My independent variable will be soil chemical or biological properties (e.g. pH, nitrogen content, abundance and diversity of various microbes in the soil) and my dependent variable will be plant growth (measured as biomass). 

I have a spreadsheet of growth data where each plant is a separate row. I am tempted to just paste the soil variables into that spreadsheet and do regressions, but that seems wrong to me. It seems like that would be treating each line of my data as a different observation of the soil properties, even though there would be a lot of duplicated data.

Another idea I had was to just take the mean growth in each soil and then regress that against soil variables, but that would be ignoring the standard deviation within each soil type and artificially reducing my sample size. 

Does anybody have any suggestions about what kind of model to use for these data? I've heard a bit about multi-level models and they seem like they might be what I need.

Thanks in advance!"
Difference between Pearson's and Linear Regression?,12,9,False,False,False,statistics,1489941622,True,"So I have weight and Latitude as variables.

Would there be any point in doing both or would they both tell me the same thing?

Is there any point to linear regression with only one independent variable?

Thanks for any help."
Reality Class - The Magic of the p-value,14,0,False,False,False,statistics,1489943876,False,
Looking for a methodology to (quickly?) select the most even selection of people to match target quotas,2,0,False,False,False,statistics,1489944431,True,"So I'm attempting to balance survey respondent data and am looking to automate this process or at least prove I have the best balance possible. I've had some reasonable efforts at getting close to reach the same results I can manually but feel like I'm almost missing something that should make this a lot easier. Almost always I do not have enough respondents to reach all targets so I'm looking to get the closest possible result.

Currently I am looking at the ratios of how close I am to each target, and testing adding respondent by respondent to see which improves the most.

[Here](https://drive.google.com/file/d/0B_1I7rAVqIP8dy1fOGk0Zm5yQWc/view?usp=sharing) is an example set of respondents and targets. The format should be clear.

I am familiar with weighting methodologies but do not want to weight data.

Any recommendations on what directions I should be looking at would be great.

Thanks,"
Help with T-ratio formula.,2,0,False,False,False,statistics,1489944738,True,"The question I am asking is a question I got wrong on an exam. I need help understanding how to compute this.

An independent variable has a coefficient of 2.9 and a standard deviation of 0.8. What is the T-ratio?"
Government/statistics class survey,0,1,False,False,False,statistics,1489961235,True,[removed]
Help on doing some basketball analysis in SAS,2,6,False,False,False,statistics,1489967089,True,"Basically what I want to do is this (http://www.gregreda.com/2013/12/26/three-pointers-after-offensive-rebounds/) in SAS rather than python. I know the basics of SAS, but my class only covered up to basic regressions. I am most focused on how to identify a shot as coming after a rebound or not. "
Help me understand why this Monte Carlo estimation is wrong?,8,2,False,False,False,statistics,1489974769,True,"Hey all,

I just got back a test in my statistics class, and one of the answers I thought I felt comfortable on was marked wrong.  The question was:

""X follows a standard normal distribution N(mean = 0, sd = 1), and Y follows a Chi-square distribution with degrees of freedom df = 4.  Assume that X and Y are independent.  Please estimate E(X^2 / (X^2 + Y)) accurate to two decimal places.""

So before I even started, I assumed that the answer would be approximately 0 given that the expected value of X would equal the mean of the distribution which was given as 0; and 0 over anything is 0.

But I did a Monte Carlo approximation because he wanted an approximation.  This is what I did:

    > rm(list = ls())
    > nsim <- 1000
    > expected.values <- c()
    > for (i in 1:nsim) {
    +   x.values <- rnorm(50, mean = 0, sd = 1)
    +   e.x <- mean(x.values)
    +   y.values <- rchisq(50, df = 4)
    +   e.y <- mean(y.values)
    +   expected.values[i] <- e.x^2 / (e.x^2 + e.y)
    + }
    > mean(expected.values)
    [1] 0.005132028

Because the value of 0.05132028 is so close to 0, I assumed that I was correct.  However, I was marked wrong, and the only feedback given was:

    > nsim<-1000000
    > X<-rnorm(nsim,mean=0,sd=1)
    > Y<-rchisq(nsim,df=4)
    > Z<-X^2/(X^2+Y)
    > mean(Z)+c(-1,1)*1.96*sqrt(var(Z)/nsim)
    [1] 0.1999386 0.2007778

Because the value the professor's code gives is so far off from the 0 I expected, I'm wondering if he may be incorrect.  However, it is entirely possible that I'm making a mistake somewhere, and I'm wrong.  Either way, I'd appreciate someone else's input before I bring the issue up with the professor.

Thanks!"
"Need help with a homework problem, can't seem to figure out this question.",1,0,False,False,False,statistics,1489981214,True,"Why is a sampling distribution theoretical and not empirical?

a.) The term empirical only applies to population parameters and not samples.

b.) Samples are only used to conduct unscientific polls.

c.) We would not conduct multiple samples in order to create a sampling distribution.

d.) Samples cannot tell us much about a population because they are limited.

I picked D but I need some opinions.

Thanks!"
Help determining the shape of the distribution for this graph?,7,3,False,False,False,statistics,1489983503,False,
Delinquency Forecasting Using Transition Matrices (Markov Chain Modeling),3,3,False,False,False,statistics,1489988830,True,"I am working on creating a delinquency model leveraging Markov Chains to predict delinquencies, paid off and charged off loans. The data used is for 12 month loans with data collected over past 3 years. I went to break up delinquency buckets into 30 day segments of days past due intervals (DPD):

• 30 DPD

• 60 DPD

• 90 DPD

• 120+ DPD

• CO – Charge off

• PO – Paid off

I decided to create multiple transition (delinquency) matrices for each month of the 12 month terms. The reason I went that route is that I notices that payment patterns change from month to month. For example, people tend to pay off / pre-pay loans early on loan’s life and delinquencies start to increase from month on book 3 to 6. So I thought creating a single matrix for entire term of the loan would not give me accurate forecast.

So now I have 3 years of data that I split into 12 quarterly vintages with 12 month loans (starting month on book 1 to month on book 12). Below is an example of a matrix transitioning from month on book 6 to 7 for one of the most recent completed (loans went to term e.g. 2015Q4) vintages:

______________________________________________________________________
TO:    |0           |30          |60          |90          |120         |120+        |Loss        |Paid
______________________________________________________________________
FROM: 0|0.96610976  |0.013746431 |0.002432061 |0.000475838 |0           |0           |0           |0.01723591

30     |0.274725275 |0.18956044  |0.502747253 |0.005494506 |0           |0           |0.019230769 |0.008241758

60     |0.012195122 |0.012195122 |0.020325203 |0.894308943 |0.008130081 |0           |0.044715447 |0.008130081

90     |0.015       |0.005       |0           |0.02        |0.895       |0.005       |0.06        |0

120    |0           |0           |0           |0           |0.017751479 |0.029585799 |0.952662722 |0

120+   |0           |0           |0           |0           |0           |0           |1           |0

Loss   |0           |0           |0           |0           |0           |0           |1           |0

Paid   |0           |0           |0           |0           |0           |0           |0           |1
______________________________________________________________________


So my matrix sums up to 1 across and I have 12 different matrices for all complete vintage.  Loss and Paid columns are terminal states, while all other are transition states.

My questions:

Is there a way to generate transition probabilities for future months leveraging information from prior months on book and historical vintages? For example I have a vintage with 1000 loans that are 3 months on book and I want to know what transition probabilities would be for months on book 4 – 12.

Is there anything in machine learning packages that could predict then next 9 months on book? For example, can I plug in my historic matrices say into tensorflow or other framework to generate future matrices? 

Also, the solution does not need to be via ML and I will always favor statistical approaches. What statistical approaches can I use to predict the next 9 months on book?

Can I also incorporate additional attributes like seasonality or changes in debt to income (dti)?

My current solutions that I came up with, but not too confident about them:

1.	Create generic matrices from mob 1-12 using historical actual matrices where I take weighted average of transition probabilities where the weight is number of loans. Not sure if this is the right approach or if anyone else used it.
2.	Based on prior vintages I individually predict delinquency rates to 12 mob using ARIMA for each delinquency bucket, but this requires me building / testing a bunch of models where I need to constantly track noise / overfitting / AR / MA / Lags etc. Seems like an overkill and would take a very long time to create and maintain in the future.

I would greatly appreciate your thoughts or ideas, thanks Reddit!"
"Please help, Small P-value but a good correlation value",9,4,False,False,False,statistics,1489992764,True,I got a P-value of 0.0001053 and correlation value of -0.63. Is there a potential for a linear relationship between the two variables I am testing (trying to do a Hypothesis test). 
Struggling in my Statistics & Data Science courses,0,1,False,False,False,statistics,1489995312,True,[deleted]
Bogus statistics undercut city program to help Portland renters,2,7,False,False,False,statistics,1489997202,False,
Relative Importance of Variables,1,4,False,False,False,statistics,1490014067,True,"Suppose your significance level is 0.05, and some coefficients are significant and others are not (i.e. some of the p-values are less than 0.05 and others are greater than 0.05). Does this give anything about their importance in relationship with the outcome variable? In other words, do non-significant variables indicate that they are not important?

Could I just divide the absolute value of each of the z-values by the absolute value of the sum of the z-values to get the relative importance of each of the variables?"
Multinomial Logistic Regression - How to deal with unevenly distributed group frequencys,0,1,False,False,False,statistics,1490015582,True,[removed]
What topics in Statistics are important(or widely used) in Datascience?,26,17,False,False,False,statistics,1490016047,True,"I have searched on multiple sites like Quora and here on reddit too, but can't seem to find an exhaustive list.
I would prefer to know a list of topics like one way ANOVA ,one tailed t tests etc. which are often used in datascience and are important for it. I am looking to create an exhaustive list for my and everybody's reference.

**Note:** *I am extremely sorry if this has already been asked. But I am looking to create an exhaustive list of topics since I can see plethora of topics but can't seem to prioritize on important ones.*"
Florence Nightingale is usually regarded as a nurse but she was also one of the first statisticians!,1,57,False,False,False,statistics,1490022146,False,
Detecting outliers in non-linear data,3,1,False,False,False,statistics,1490022306,True,"Is there a way to detect outliers in data that would fit a non-linear shape? For example, I have data that fits an exponential decay with an obvious outlier. I have simulated data with an example of this:

https://i.stack.imgur.com/FeFgx.png

I want to know the index locations of where these outliers occur in the real data and currently I am doing this by applying an exponential decay fit to the data then finding the residual of each point the fit. I then find if any points go over a certain threshold that I find as some multiple of the standard deviation of the residuals and find any point with a residual over that threshold. Is there a better way to find outliers in a model like this?

Here are the points used in the above array:

     [10., 8.46481725, 7.16531311, 6.0653066,
     5.13417119, 4.34598209, 3.67879441, 3.11403224,
     2.63597138, 2.2313016 , 1.88875603, 1.59879746,
     1.35335283, 1.14558844, 0.96971968, 0.82084999,
     0.69483451, 0.58816472, 25, 0.42143844]"
A question about multiple regression,0,1,False,False,False,statistics,1490034369,True,[removed]
R: difference in the estimates derive from gee versus geepack,0,0,False,False,False,statistics,1490037391,True,"Context: I fitted a marginal GEE model with AR(1) correlation structure using geese in geepack and then using gee in geepackage. the estimates were close but still differ by a few decimal points (eg, -0.367 vs -0.357) and i notice the robust SE from gee tend to be slightly larger than geese. 

Is there any difference in the way they derive their estimates, like how they are weighed, etc? 

"
Linear Programming Question,0,1,False,False,False,statistics,1490038656,True,[removed]
Help,0,1,False,False,False,statistics,1490041434,True,[removed]
Is this considered a correlation?,4,2,False,False,False,statistics,1490042251,True,[deleted]
Is this the right way to perform hypothesis testing on correlation?,3,1,False,False,False,statistics,1490050026,True,"Hey all,

I have two vectors of data.  I'd like to perform a statistical test to test H0: correlation <= 0.64 vs. HA: correlation > 0.64 at the a = 0.05 level.

The way I'm attempting to solve this problem is by looking at the confidence intervals of the correlation.  Because the confidence interval at the 1 - a level would hold all values with critical level less than a, if the lower confidence interval is greater than 0.64, I can reject my null hypothesis.  Or so I think...

Here's the R code + Output I used to solve the problem:

    > cor.test(GRO2, GRO3, alternative = 'greater')
    
    	Pearson's product-moment correlation
    
    data:  GRO2 and GRO3
    t = 7.9074, df = 36, p-value = 1.101e-09
    alternative hypothesis: true correlation is greater than 0
    95 percent confidence interval:
     0.6702984 1.0000000
    sample estimates:
          cor 
    0.7966283

So I think that because the lower confidence interval is greater than 0.64, I can accept the alternative hypothesis.  Is this correct?

The one thing I'm unsure of is setting the alternative equal to 'greater' in the cor.test().  What this would do is test the alternative hypothesis that the correlation between the two vectors is greater than 0.  I'm unsure if I should use that, or set alternative to 'two.sided', which would test the alternative that the correlation is not equal to zero.  The issue, is that using two.sided would give me 95% confidence intervals of (0.6399101, 0.8897262); because 0.64 falls within that range, I wouldn't be able to reject my null, so it is important that I choose the correct alternative.

Thanks for any help!"
Ward's linkage + correlation distance,2,1,False,False,False,statistics,1490050311,True,"So, I've read ward's linkage needs euclidean distances for working correctly. I am using correlation distances, and using ward produces much nicer clusters than using other options like complete or single.

Is there any scenario where using ward+correlation distances is acceptable?

Thanks!"
What kind of statistical test should I run?,8,1,False,False,False,statistics,1490051145,True,"I am testing 20 PTSD patients for cortisol secretion patterns. I am testing their saliva cortisol levels at 0, 15, 30, and 45 minutes after they wake up. Then, there is two groups: one group of PTSD patients that has to describe their traumatic incident, and another group that does not have to describe their traumatic incident. After (at noon) they have to describe/not describe their traumatic incident, another saliva test is going to be conducted. Then one final one at 6p.m. The concentrations should be similar at the 0,15,30,45 and 6 p.m. times but the levels should be lower at noon in the group that had to describe their experience. What test should I run to determine if the concentration significantly differs from the other group only at the 12 p.m level reading?"
Interpreting strange results,0,1,False,False,False,statistics,1490052710,True,[removed]
I'm a dunce - is my second equation redundant?,0,1,False,False,False,statistics,1490054422,True,[removed]
Statistics vs. Applied Statistics? Can anyone explain the differences?,19,2,False,False,False,statistics,1490055515,True,"Would anyone be able to give me a breakdown about what the difference in the two is? I'm pursuing a career in Sports Analytics and every job I research wants candidates to have a degree in Statistics (assuming ""Statistics"" in a general sense). 

Are there major differences? Is one degree more difficult than the other?

I will love you all if you can help."
Sock Drawer Probability,0,1,False,False,False,statistics,1490062270,True,[removed]
4-D Interpolation,2,6,False,False,False,statistics,1490064033,True,"I have a set of data describing the behavior of a system through a dependent variable (% Transmittance) over three distinct independent variables. I'm currently using MATLAB's Curve Fitting Toolbox to create a bunch of 3-D scatter plots and fit each plot to a surface (large degree polynomial equations seems to yield a sufficiently high r-squared value). I'd like to analyze the data across the third variable; is there a tool I can use to fit the entire set of data to an equation with three inputs?

Also, what's the best way to clearly visualize data in 4 dimensions?

THANKS!"
How should I use PCA to identify features for Regression tree analysis?,7,2,False,False,False,statistics,1490077115,True,"I asked [this question](http://stats.stackexchange.com/questions/268223/how-should-i-use-pca-to-identify-features-for-regression-tree-analysis) on cross validated the other day and didn't get any help, so I figured I'd try here.  Can anyone give me some advice on how I should use PCA to help select factors for a regression tree?"
How to determine if variables fit E(y) = β0 + β1(x1) + β2(x2) or E(y) = β0 + β1(x1) + β2(x2) + β3(x1)(x2) in R programme.,0,1,False,False,False,statistics,1490078641,True,[removed]
Unsure of which test to run,0,1,False,False,False,statistics,1490088909,True,[removed]
Curiosity about the test value formula,2,1,False,False,False,statistics,1490096866,True,"Why is the test vlue formula as it is?
I'm talking about the test value = (observed  - expected) / standard error. Shows up everywhere and I never bothered to question it. Just a little shower thought."
"But When You Call Me Bayesian, I Know I’m Not the Only One (40 min video)",1,38,False,False,False,statistics,1490097761,False,
The Funnel Plot is Invalid,0,13,False,False,False,statistics,1490102986,False,
"34 statistics for Medicare admissions, costs, margins and charges at hospitals",0,0,False,False,False,statistics,1490104556,False,
R.A. Dickey isn't concerned about his ugly spring statistics,0,0,False,False,False,statistics,1490104644,False,
Could someone please explain MSb/MSw for a one way ANOVA inferential stats test?,3,1,False,False,False,statistics,1490109983,True,Just learned about this test in my psychology (qualitative methods in psych) class and I am stumped and very confused. 
Autocorrelation / Stationary Panel Data,0,1,False,False,False,statistics,1490116312,True,"I have data with N=644 and T=10. The dependent variable appears on average to be decreasing over time and exhibits first-order autocorrelation, some independent variables exhibit autocorrelation as well. If I run a fixed effects or random effects model I guess this will pose a problem? If I run a first-difference model, the autocorrelation of the dependent variable becomes negative rather than positive and the sign of of one my estimators also changes from positive to negative.

Could a viable solution be to run GMM with the lagged dependend variable as an estimator? I'm not sure how to deal with the possible non-stationarity or autocorrelation of the data. Will incorporating time fixed effects be a solution for correcting the general trend so I don't get the spurious correlation problem and my estimator will be at least unbiased?

Thanks for any help!"
Using 1.65 or 1.96 *SEM,0,1,False,False,False,statistics,1490117357,True,[removed]
Animation of 15k slave ships in 2 mins,5,33,False,False,False,statistics,1490121441,False,
Is Your Data LYING To You? The Simpson's Paradox,26,102,False,False,False,statistics,1490122850,True,"This past weekend I was helping a friend at a local startup accelerator. He was really frustrated - after making a few adjustments based on the data he collected - his profits seemed to have gone **DOWN** - *instead of up.*

 

Since he ""followed the data"", he was convinced that he made the right decision. He asked me to help him figure out what went wrong. 

 

Turns out... his data was literally ""lying"" to him. This wasn't a case of ""garbage in = garbage out"", but one of those rare/tricky incidents where *your data can actually trick you into making the **OPPOSITE** decision*.

 

I decided to write a post explaining (and an [Animated Video](https://www.youtube.com/watch?v=zj2QV7cfUfQ)) how this happens so you guys don't get fooled down the line. I use the **UCBerkeley Sex Discrimination Law Suit** example, and a **business case example** to illustrate. 


 

Take the time to read through this - it may pay off sometime down the line. For those of you who want to watch an illustration (easier to understand) here's the video: **[YouTube - Simpson's Paradox Explained](https://www.youtube.com/watch?v=zj2QV7cfUfQ)**


 


**Simpson’s Paradox - How Data Can Fool You**


------------------------------------

Data - data - data. We’re increasingly becoming a society obsessed with data!i Important decision-making  meetings will often parrot the phrase “Well...What does the data say”.

Being “data-informed” is all well and good. But using it at face value to drive decision making can be rather dangerous. In this post, we will discuss one of the ways data can trick you into making the wrong decision - The Simpson’s Paradox.

In 1973 UC Berkeley was sued for sex-discrimination. It turned  out of all the female students who applied - only 35% of them were admitted.  While  out all the male students who applied 44% of them were admitted. 


 

        |  Applicants  |    Admitted | 
---|------|---|
 Men| 8442 | 44% |
Women | 4321 |  35% |



 

The data raised a lot of eyebrows. And the witch-hunt was on! UC Berkeley set out to  find the main culprits of this gender discrimination.

To do this they broke open the data to see which departments were mainly responsible for this GENDER BIAS ---  And here is what they found:

 

Department | # No. of Men| #No. Of Women| Men Accepted| Women Accepted|
---|------|------|------|------|------|
A | 825 | 108 | 62% |**82%** | 
B | 560 | 25 | 63% |**68%** | 
C | 325 | 593 | 37% |34% | 
D | 417 | 375 | 33% |**35%** | 
E | 191 | 393 | 28% |24% | 
F | 373 | 341 | 6% |**7%** | 
- | Total 8442 | Total: 4321 |  | | 


 

Now this is where the data gets funny. After breaking open the data, we see a different story.  Out of the 6 departments 4 of the departments accepted women more than men. There definitely was a gender bias - but it was in favour FOR the women. Not against!
 

But that begs the question? Why did the aggregated data tell a different story? 

 

This is a classic case of the Simpson’s Paradox -  when grouped-up data tells the opposite story of the ungrouped data.  This happens because of a confounding-factor that is hidden from sight WITHIN the data.

 

So what’s this “hidden factor” that’s causing all the mischief? Take a look at firstand the last rows of the table. You’ll notice that Department A has a pretty high acceptance rate - especially for women at 82%!  However, out the 4000+ women only a 108 of them applied to Department B. That’s only 2% of all women who applied across departments.

 


On the other hand, 825 of the men applied to Department A! That’s 10% of all the male applicants. You may have already spotted the mischief. But let’s go on.  Take a look at the last row. Again, the women have a higher acceptance rate than the men. But over here - Department F, in contrast to Department A, has a very LOW acceptance rate.

 

And this is where it goes wrong. Compared to the men,  a much larger portion of the women applied to this low-acceptance department.  Around 4% of all the men applied here. While 8% of all the women applied to Department F. So in truth, women weren’t being discriminated against. It just so happened that a large proportion of them were applying to a low-acceptance rate department while a large proportion of men were applying to high-acceptance rate department. That skewed the overall results.


 

This sort of data mischief - The Simpsons Paradox - can happen everywhere. Even in businesses who use data to make decisions. Here’s a business-case example. A CEO and his team were deliberating whether to use a One-Click advertisement campaign or Two-Click campaign. That’s when the marketing manager - who happened to support the Two-Click campaign showed him some data:


 

                 | #Users (mil)  | Rev (mil) | Rev per thousand users |
--------------|-------------|------- |------------|
Single Click|230 Users  |$2.9 |11.60 RPM  |
Double Click  |140 Users  |  $1.7  | 12.14 RPM  |


Single Click had more users allocated to it, and thus more revenue - but the RPM (revenue per thousand users) is higher for double click. When you look at the data - the decision is obvious. Double Click is generating more money per user - so they should go with double click, correct? 

 

Turns out, picking the Double-Click campaign would have been a costly mistake. Let’s break open the data again - into its subgroups of International users and Local users:

 

                             | #Users (mil)  | Rev (mil) | Rev per thousand users |     User% Per Group
------------------------|-----------------------|-------------|----------------------|----------------|
 Single Click-Local     |      50 Users   |       $1.8   |         25.71 RPM   |                        42 %  |
 Double Click-Local      |     70 Users      |   $1.2     |       24.00 RPM        |                   58% |
 Single  Click-Inter         |   180 Users     |   $1.1         |   6.11 RPM         |                   67% |
 Double Click-Inter  |      90  Users   |       $0.5        |   5.56 RPM          |                  33% |


 

Suddenly, the data tells a different story. Single click is outperforming Double-Click in both subgroups - Local AND International? How is this possible?

 

Simpson’s Paradox at play again. The grouped up data has a hidden factor that tells the opposite story of the ungrouped data. In this case, the hidden factor was that only 33% of international users were shown the double-click page, while only  58% of the local users were shown the double-click page.

 

And in general the local users had a much higher RPM than international users. So the local users who had a much higher proportion of double click users and a higher RPM skewed the overall data.

 

**Final Thoughts**

This was a tough example. Take a minute to  analyse the data. Simpson’s Paradox can be tricky - the key is to look out for any hidden variables that may be influencing your data!  Don’t rely too much on your data. If something smells fishy - look into it. Do not trust your data blindly.  Use your data to formulate hypothesis and be data-informed, not data driven. Test your hypothesis, if it goes wrong, you know you need to look for something else.

 

---------------
Holy shit, formatting those tables took forever!!!

For more on analytics & stats I recommend these books:  Naked Statistics - Charles Wheelan & Web Analytics - A. Kaushik.

**[Original Post](http://www.skipmba.com/simpsons-paradox-is-you-data-tricking-you/)**

**[YouTube Animation Explanation](https://www.youtube.com/watch?v=zj2QV7cfUfQ)**"
KS Test -- differences between matlab and scipy,0,2,False,False,False,statistics,1490124808,True,"Matlab's KS test for 2 samples (kstest2) returns a p value for a specified alpha.  In scipy, ks_2samp returns just the p value.  

Why does matlab have the alpha? "
Post-hoc for chi-squared analysis?,4,4,False,False,False,statistics,1490134017,True,"I'm running a chi-squared analysis that basically looks like this:

Gender|Var1|Var2|Var3
:--|:--|:--|:--
Male|25|3|4
Female|21|12|10

The Chi-squared gives a significant p < .05 value, so I know there is a difference in proportions somewhere. What I want to know is which variables are significantly different here. It is clear that var1 is driving the result, but I'd like to answer questions like - is var1 different than var2? Var1 vs var3? Var2 vs var3?

I thought of just running chi-squared analyses on two vars at a time, and using a bonferroni correction to assess significance. Is there a more established method?"
Bounds on multiple imputation,3,3,False,False,False,statistics,1490149345,True,"Hi folks, 
One of my professors was recently victim of a replication study (jokes obviously. Replications are important) that stated his results were mostly nullified if multiple imputation was utilized over the standard list-wise deletion. 

He has complained that the multiple imputation was done is such a way that produced impossible values contributing to weakening his results. I am replicating this replication using slightly different imputation, but I am confused about the impossible values element. I have a weak understanding of the math going on in the background, but I found that King recommends [**against**] (https://gking.harvard.edu/files/gking/files/amelia_jss.pdf) (pages 24/25) setting bounds as this results in better representation of the true distribution.

My dataset has variables such as LnPopulation, LifeExpectancy, GrossHumanitarianAid. When the imputation is run without bounds (by the replicator, and myself), there occur values such as -14 for LnPopulation, LifeExpectancy ranging from 10 to 120, and negative GrossHumanitarianAid. 

Should these values be accepted, or would setting logical bounds be beneficial to modeling? 
Does anyone know of some good sources that discuss the dangers and merits of doing this?

Thank you"
"If I don't care about being funded, could I get into a Masters in Statistics program at a public university in California with a 3.1 to 3.3. GPA?",0,1,False,False,False,statistics,1490160841,True,[removed]
Today’s Pre control and Statistical Process Control (SPC),0,1,False,False,False,statistics,1490176640,False,
Back-transforming log-transformed coefficients.,0,1,False,False,False,statistics,1490187587,True,[deleted]
London attacker odds,3,0,False,False,False,statistics,1490200235,True,[Serious] what are the odds that today's attacker in London is from one of the countries on the proposed US extreme vetting list?
Need a probability genius...,2,0,False,False,False,statistics,1490208128,True,"Not sure im on the right sub but whatever...

I have this hard math problem involving the rubiks cube.

Here it is:

X: Number of cubes on one edge (Ex:2,3,4...)

Y: Number of possible combinations (Ex:big number)
-----------------------------------------

Y = Formula(X)

What is the formula?

Thank:)"
Need help with statistic equation,0,1,False,False,False,statistics,1490208557,True,[removed]
Can you only put continuous data into a Mann Whitney test?,2,2,False,False,False,statistics,1490208558,True,"My data is discrete, but it appears that i've managed to get a p value below 0.05? but is it relevant anyways because my data is not continuous, therefore it should not work with a Mann Whitney test. Is there any sort of statistical test to compare differences between two independent groups when the dependent variable is made up of discrete data?"
Advanced Machine Learning with Basic Excel,7,0,False,False,False,statistics,1490218652,False,
Job Interview Question,3,0,False,False,False,statistics,1490220415,True,"Hello  Everyone – I am not a student and this is not my homework.  I applied for a job and I’m in the last phase before getting my interview and an offer.  I am trying to calculate my odds and my probability for each of the scenarios here:

There are 30 vacancies.  32 interviews are being conducted.  What are my odds (expressed as odds and percent) of getting this job?

There are 30 vacancies.  37 interviews are being conducted.  What are my odds (expressed as odds and percent) of getting this job?

As any FYI – I came up with 30:2 or 93% chance of getting the job and 30:7 or 78% of getting this job.

Thanks – I am just posting this to overcome my madness as I wait until the interview next Monday and (hopefully) the offer a week after that!
"
R certification questions,7,2,False,False,False,statistics,1490225237,True,"Hello! I have a few questions about getting certified in R. 

First of all, I'm a second year graduate student studying statistics. I've used R for about a year now and I feel very comfortable and confident with it. I consider myself to be a fairly proficient user.

For those of you that do have some sort of certification on your resume, have you experienced any positive feedback from employers? Has the certification landed you a job you might not have gotten otherwise?

Also, which course or test would you recommend for getting your R certification? I've found a few online, but I'm not willing to pay for a course, since I'm already paying out the wazoo for grad classes. 

Any feedback would be appreciated!"
Bayesian election forecasting,3,27,False,False,False,statistics,1490225260,False,
Longitudinal data analysis question,2,2,False,False,False,statistics,1490238193,True,"Hi! I am hoping for some help in figuring out what kind of test to use for longitudinal interval data. I did read the guide posted on the right, but I am really struggling still. We did not go past basic tests in my stats courses and never touched longitudinal data for multiple observations.


Ex. I have 50 ngos and their government funding* for the past 3 years. I want to see if the budget ratios stay consistent, increase, or decrease.
*government funding in this case is compared to their own revenues so that organizations with different revenues can be compared. (gov funding/total revenue)

My data sheet would look something like this:
Organization Year 1   Year 2    Year 3 
Org 1           .75       .80         .85

Advice?"
What's the difference between bivariate and multivariate regression analysis besides the number of independent and dependent variables?,0,1,False,False,False,statistics,1490238712,True,[removed]
Get the Best Biostatistics Assignment Anytime You Need It,0,1,False,False,False,statistics,1490253973,False,
MOOC for Markov Chains Monte Carlo,9,31,False,False,False,statistics,1490267619,True,Does anyone knows any good online courses that delve into Markov Chains Monte Carlo?
Which stats are right when it comes to how many companies there are in the US? Data seems to be contradictory and varies in ranges of 10-20 million,3,1,False,False,False,statistics,1490284348,True,"The US Census Bureau gives me a number of 5.8 million firms, and about 7.5 million company locations.

Avondale asset management website gives me : There are ~6 million companies operating in the United States employing 114 million people.

The SBA tells me: In 2010 there were 27.9 million small businesses.

Another source citing the census bureau tells me: There are 6,000,000 private companies.

Forbes tells me ""Wall Street gets the headlines, less than 1 percent of the 27 million businesses in the U.S""

DMD databases gives me a number of ""18,204,679""

There's a database company that says they have a 30 million company information document for sale.

Which information is the correct information?"
Need help finding an appropriate statistical test similar to Pearson's Chi Squared test.,5,2,False,False,False,statistics,1490287241,True,"So Pearsons Chi Squared Test tests whether differences in observations between categories (say A and B) occurs by chance. The way I interpret one of the assumptions of the test is that A and B must be drawn from the same sample or samples of the same size.

I need to do a similar test, but the samples that A and B are drawn from are different sizes (they are both drawn randomly, so there shouldn't be sampling bias). A and B can't be observed at the same time because of the technique. My initial thought is that I can use Fisher's Exact Test, but I don't know how this would be set up (not that familiar with the exact test families). Or is there a completely different test or adjustment for the Pearson's X2 test?"
How to actually learn statistics?,13,2,False,False,False,statistics,1490289521,True,"We've heard how statistics are abused and misused, in the public and academia, but they never follow up with the actual way to learn how to do things properly.

Can anyone point me in the direction of a book, or resource that can help me get a good grounding in statistical theory?

In a more practical bent, are there some good resources to learn epidemiology, as that's closer to my area of interest."
Resources in model building?,0,1,False,False,False,statistics,1490290191,True,"I want to know the best way to go about building a linear mixed effects model. In particular: how to build the fixed effects and how to choose which random effects to include.

I know that there are arguments against statistical driven model building, but ignore those for now.

I also know that there isn't a ""best"" way. But there must be resources that describe more accepted ways than others.

Can you provide some of them?"
Modelling interactions in Stata.,0,2,False,False,False,statistics,1490290985,True,[deleted]
Question about the logic of using a manipulated variable as a DV.,2,1,False,False,False,statistics,1490291265,True,"Sorry for posting two questions so quickly.

Say I have a study in which people are given 50 red stimuli and 50 blue stimuli. We are curious about many things (e.g., how long they take with each stimuli, what kinds of movements they perform for each stimuli etc.).

Does it make sense to use colour as a binomial dependent variable, and predict it using a logistic regression, with these variables of interest (e.g., RT, types of movements) as predictors?"
Structural Equation Modelling - Model-Fit,2,2,False,False,False,statistics,1490293326,True,"Hello everybody! I hope you can get me some help on the Model-Fit of my Structural Equation Modell Analysis.

I got almost everything figured out but there is one thing about the Model-Fit that I don't understand:

I want to check the RMSEA-Value, which should be ≤0.05
Connected to the RMSEA is PCLOSE.
Now I have been doing some research but I can really not find out which Value PCLOSE is supposed to take.
Some sources say it should be below 0.05 to indicate a close fit, some say it should be higher ([Example](http://zencaroline.blogspot.de/2007/04/global-model-fit.html)).

Does anyone have an idea what is the right answer?
Thanks in advance!"
Comparison of dissimilar sized companies,1,1,False,False,False,statistics,1490295757,True,[deleted]
Estimating Values by Using Correlation Coefficient,0,1,False,False,False,statistics,1490296325,True,[removed]
One way anova: p-values with near zero log scale data points,0,1,False,False,False,statistics,1490299059,True,[removed]
Help: p-values from one way anova on log-scale data with near-zero values,0,1,False,False,False,statistics,1490299809,True,[removed]
Complicated ANCOVA model,4,1,False,False,False,statistics,1490301475,True,"Hi,

I'm trying to fit a model into R, but am having trouble. Suppose I have a response Y, a categorical variable C, and a continuous variable X. I have four categories in C, and I want my model to have a different intercept for each category. I also want to fit an interaction between X and C into the model, however, for 2 of the categories in C, the mean of X is identical. How do I fit a model that could account for this, and would this model also be nested in Y~C1*X??? 

Thank you"
Surveying allowed?,2,0,False,False,False,statistics,1490302215,True,"Hey, I'm new to this subreddit and just wanted to know if this was a good place to post my survey and whether that is allowed in this subreddit or not. If this isnt a good place to post my survey, where do you suppose I go? This survey is for a school project."
"How to calculate prediction bounds for an ARMA(p,q) model",0,1,False,False,False,statistics,1490302945,True,"I'm trying to calculate prediction bounds for an ARMA(p,q) model. I have the 95% confidence interval given. I know the MSE. I just don't know how to find the t statistic of the 99% confidence interval (if I am even supposed to use the t statistic)"
What does work in theoretical statistics look like?,17,14,False,False,False,statistics,1490306134,True,"My program offers two broad tracks that one can follow in stats (this is a grad program); the informatics track and the theoretical track.

I'm leaning towards the theoretical track, because the math seems to grab my attention a little more than the informatics track. But I'm mostly basing this off of the course work. I have no idea what the research will look like.

What kind of math is used a lot in theoretical stats (it looks like a lot of linear algebra and measure theory)? And are there decent job prospects for theoretical statisticians?

Thanks in advance for any thoughts"
Statistical consultant in USA,0,1,False,False,False,statistics,1490307606,False,
Baseline hazard in Cox PH model,0,1,False,False,False,statistics,1490321644,True,[removed]
issues with working in a corporation and technology,7,13,False,False,False,statistics,1490328667,True,[deleted]
Question about statistics in physics,12,3,False,False,False,statistics,1490345842,True,"Hi guys! I have a question and I think this is the right place to ask. I am teaching physics in school and I see that my background in statistics is not good enough and I want to go deeper into it and get a good overview about the concepts needed to analyse the numbers obtained from experiments. I only had some lectures on statistics in experimental physics at the university I studied at and everything is gone. Thank you!
Edit: I am looking for a great book on the topic!"
What is a good way to characterize the similarity between rows of a matrix?,11,5,False,False,False,statistics,1490366918,True,"How do I characterize a matrix by the amount of (dis)similarity between the rows? For example [[1,0,0],[0,1,0],[0,0,1]] has more dissimilar rows than [[1,0,0],[1,0,0],[0,1,0]] -- the elements are either 0 or 1. Any measures that are useful here? I understand the different measures of similarity, but they all seem to be for comparing two objects. (I just need a starting point or the key phrase to search in google if there are no common measures or many)."
Stats problem for some students - Texas Hold em,3,0,False,False,False,statistics,1490367663,True,"The question is as follows:

In a Texas Hold Em game with six players, what's the probability that player 1 is dealt ""big slick""? 

Big slick consists of an ace and a king. 

(The probability of dealing two cards and getting big slick is .02116. )
"
Help with proportion stats,0,3,False,False,False,statistics,1490368842,True,"Hey everyone! I'm looking for some help with proportion data and generating a p-value to determine significant differences. 

As I right now I have done an ANOVA, however I read that doing an arcsin transformation before the ANOVA is the correct procedure.

Just looking to get some advice on how to handle this kind of situation.

Thanks!"
Quick help!!!!,2,0,False,False,False,statistics,1490382866,True,Can you use both a t test and a wilcoxan rank sum test in a trial? Giving an important presentation in 45 minutes and this is an emergency.
What information does a characteristic function possess that PDF or PMF does not?,5,7,False,False,False,statistics,1490389372,True,"To elaborate, in many texts it is emphasized that a characteristic function completely determines a probability distribution. However that language is not used to describe PDFs or PMFs, why?"
Good way to calculate which parameters are the most important in a Bayesian-based Data Classification problem???,8,3,False,False,False,statistics,1490390621,True,"I'm trying to setup a data classification problem, where I take multiple observations at given location and predict what class the data at that point falls in to. The classes are pre-defined and there are a number of ""training"" data points where the correct data class in known.

I have a probabilistic nearest neighbor algorithm that I am using for this process (it uses a MCMC approach and is probabilistic in the sense that the number of neighbors used can change). This code is working, but I want to improve the process.

My main thought for improving the process was to generate new perturbations that combine the inputs in various ways (e.g., if I have 3 different observations at a given location, I can make a 4th that is observation #1 multiplied by observation #2). After making these new combined observations, I would then look for which observation best differentiate the training data into classes and use these. 

**My question is: what is the best way to determine which observations best differentiate the data into classes?**

My initial thought was to normalize the data, split each observation (and new combined observations) into seperate distributions for each data class, and look at the standard deviation of these split distributions (the idea being that if a distribution has a small standard deviation it means that a given data class is strongly linked with a particular value of that observation). But, I figure there is likely a better way to do this.

Any suggestions are appreciated. Thanks!



.

.

EDIT: **To hopefully make things more clear, this is an analogous problem**

You have a painting. You subdivide the picture into a 100x100 grid, forming 10,000 grid points. These are the ""locations"". 

Your goal is to estimate what color is being shown at each grid point, with the only possible color choices being red, blue or green. These are the ""data classes"".

To accomplish this, you collect multiple different types of data at each grid point (say, intensity values of reflected light at 10 discrete frequencies). These are the ""observations"".

To give the program data to work with, you choose a few of the grid points (say 10%) and manually assign a color. You then feed the program the chosen color and the collected observations for each of these points. This is the ""training data"".

The code I am using uses the training data to learn about how the observations are distributed for each data class, and then applies that information to estimate what the class *should* be for the data points where class is unknown but where we have the same other observations that are in the training data."
Quick question: Correlating one variable with time,3,6,False,False,False,statistics,1490394523,True,"Lets say I have a variable that changes over time intervals of 2 hours.

Something along like


0 | 2 | 4 | 6 | 8 | 10 | 12 | 14 | 16 | 18 | 20 | 22
-|-|-|-|-|--|--|--|--|--|--|--
10 | 13 | 23 | 33 | 50 | 55 | 52 | 31 | 45 | 50 | 44 | 39

Where the top row is the time interval, and the bottom is the variable we want to correlate.

Would it be possible to use pearson's correlation? if so, would I need to do the mean of the time? that doesn't make much sense to me.

cheers"
Does anyone have any experience in using the clue (cluster ensemble) package in R?,3,8,False,False,False,statistics,1490396389,True,"It's a bit of a shot in the dark, but I find the [package's vignette](https://cran.r-project.org/web/packages/clue/vignettes/clue.pdf) remarkably unhelpful when it comes to wrapping my head around what the various functions of the package do. I'm having trouble finding much information elsewhere on the application of the package's functions. This is unfortunate, as I find the aim of the package quite useful to me."
Master's in econometrics transition to a biostat job?,4,2,False,False,False,statistics,1490404842,True,"I'm applying for jobs now, and I was wondering if I am also qualified for biostat jobs? Do biostat people learn radically different things? Would someone trained in metrics (including RCTs) not be taken seriously by biostat hirers?"
Rating system for free-for-all type competitions,11,8,False,False,False,statistics,1490435223,True,"I'm a member of a game community which revolves heavily around time-trial competitions. In such a competition, any number of players (typically around 10-30) will compete for a set amount of time (eg. 20 minutes), racing against the clock to complete a circuit as quickly as possible. After the 20 minutes have passed, the players' times are compared to each others', and player with the quickest time will be the winner, second fastest time gets place #2, etc.

I was wondering if there are any established rating systems, similar to the ELO rating in chess, designed for this type of free-for-all competition? Could the fact that the number (and ratings) of players in any given competition may be very varied add to the complexity of such a rating system?

I'm not sure if this is the right subreddit for such a question, but if you have a better suggestion please let me know."
Statistical Analysis Services | Statswork,0,0,False,False,False,statistics,1490436621,False,
Statistical Analysis Services,0,1,False,False,False,statistics,1490437270,True,[removed]
Choosing variables for a correlational study - any help would be appreciated.,2,1,False,False,False,statistics,1490439965,True,"I am just heading into my psychology honours and am starting to compile my thesis. Unfortunately this means a lot of statistical work, and I have zero confidence. I passed all undergrad subjects with good marks, but thanks to a motorbike accident towards the end of my degree, my memory of my course is extremely limited. 

I would like to study the effect of social attitudes on womens anxiety toward breastfeeding. 

However, anxiety is not the right 'term' for this and is too big of a variable within itself. Basically, I want to find out if women feel judged about their breatfeeding choices (and practices) based on what they hear, see, and read, and if this makes them feel unwarranted or extra stress. 

I am thinking about looking at pregnant women with healthy pregnancies, from age 18 onward. My supervisor and I have discussed a possible correlational study. 

Any help in furthering my theory here would be so great. I am meeting with a tutor on monday evening, but thats after my meeting with my supervisor. I'd really like to have some more of a clear idea by then. 

Just to clarify, I'm not asking anyone to do my work. I have been researching this topic for weeks, and fully intend to put in the hard work. Part of that is asking for help when I need it. 

Thanks :)"
Wondering what to write for literature review. Any help would be appreciated,8,3,False,False,False,statistics,1490449548,True,"Hello, so I'm currently doing my undergrad thesis on Nonparametric Statistical Analysis on Brainwaves. Basically, I'm analysing some quantitative EEG data using nonparametric methods.
I've currently hit a mental block when it comes to my literature review. My supervisor doesn't have a background in statistics, so she's sort of letting me figure this out on my own.


Right now, I've decided that I'll break my literature review down to:
- Background on QEEG
- Previous studies on using QEEG to detect brain abnormalities (although these are not using nonparametric techniques)


..and that's it. I'm not sure how to make it more robust and informative. I've been trying to find some papers on using nonpara stats in mental health diagnosis but holy crap, it's so niche.



Any help at all on how I can improve would be greatly appreciated. Thank you!

"
"Hello, r/statistics. We need help picking the right statistical test for a graduate research project. Any suggestions would be of great help. Thank you in advance.",17,4,False,False,False,statistics,1490452067,True,"Hey everyone, we were unsure where to turn for such an inquiry but we hope someone here will be able to guide us as to which test would be the most valuable. We appreciate any ideas and suggestions

The research proposal is quite straightforward. We have 5 groups of 50 raters, divided by age. 20-29, 30-39, 40-49, 50-59, and 60+. We plan to ask them to rate the esthetic appeal of a smile and 4 variations of that smile. Each variation will add a 0.5-mm gap between the upper front teeth, up to 2.0mm. We believe that the ""gap-toothed"" look will be perceived better by the younger group/s (up to a certain size). 

Neither of us are very good with statistics, but we would like to know what would be the best way to find some statistical significance in our study. Thank you again, your time and help is thoroughly appreciated.

- 2 broke dental students
"
Best approach for making a suggestion of a similar item,4,3,False,False,False,statistics,1490452135,True,"If I have a dataset of a large number of items with properties/variables like ""Name, brand, price, weight, height, strength, lifetime, color, number of sold items, etc"" and if I pick a random sample item and want to calculate which other item is the most similar, which approach to make the best suggestion should I take?

So far I've set up a matrix with the items as rows, properties with numerical values as columns. Then considered all properties to be of equal importance, scaled all values within each column (type of variable, ""price, weight, etc"") so that the range is the same, 0-100 for all columns. 
Then only done a Euclidean distance calculation between the of all items, created a square matrix, and then sorted each row to find the one with the shortest distance for a given item.

But it does seem too simple to be the most ""accurate"" or reliable way of finding the most similar items. Do you have any suggestion or pointers in the right direction?"
"Hey r/statistics, I'm currently doing some research on chewing gum and its relevance to the Canadian market. Any help you guys could give will be greatly appreciated.",1,0,False,False,False,statistics,1490473719,True,[deleted]
Statistical proofs for experts,0,1,False,False,False,statistics,1490476669,True,[removed]
Help: how do I translate 9.230705e-01 into a understandable form.,9,0,False,False,False,statistics,1490477757,True,"I know I've gone over this in classes before but now I cant figure out how to translate these numbers. What is 9.230705e-01 mean? I know its a small number.... And similiarly 8.339610e+03, 1.046247e+04, and 6.908932e+00. I'm doing some data mining and just cant remember how to translate these. Thank you! P.S. I swear i'm not asking you to do Homework for me. Its assumed in class that I have learned these before, which I have, its just bean a while. "
Comparing two linear relationships,0,1,False,False,False,statistics,1490478137,True,[removed]
Skewness and Kurtosis,10,22,False,False,False,statistics,1490482422,True,"A question came to me that was not adequately explained by my textbook today. 

Why does the calculation for skewness raise everything to the third power, and for kurtosis raise everything to the fourth power?

Thanks!"
I need help with #9 I don't know what I'm missing.,2,0,False,False,False,statistics,1490488749,False,[deleted]
Statisticians! What is your 'go to' statistics test?,7,0,False,False,False,statistics,1490499818,True,[deleted]
"Just recovering from an injury, I want to make a statistical analysis of my recovery",4,4,False,False,False,statistics,1490508528,True,"Hello! I'm not sure if this is the right subreddit for this. I've tried in 
some others, but no one has really understood what I'm doing.

Basically, within a month, two important things are happening for me. One, I am graduating. And two, I just went on my first walk since my ankle injury last year.


As for the first, I am finally going to be able to consistently have routines and hobbies, and I'd like to record that progress. I've done similar things before, and I love seeing the progress, but school usually gets in the way, and I would have to stop whatever hobby I was recording data on.


As for the second, and I think the most interesting/important, is my physical recovery. I have never been an active person, but two years ago, I started pursuing more active hobbies. I had made a little progress, but my ankle injury in November has kept me from doing anything since. Just last week, I was finally able to go for a walk. 0.7 miles! I think that recording data from where I am now and where I'm going to be in a year (or more) would be really fascinating.


Basically, I'm asking for more ideas of what to measure and what data to collect. In my fitness list, I have:
* daily calories
* weight
* general size measurements
* sleep time
* number of pushups
* distance/speed run/walked/swam/biked


In my emotion health list, I have:
* anxiety/depression levels
* hunger levels
* sleepy levels


In my hobby category, I don't have much, but it mostly includes:
* number of books read over year
* time spent streaming on twitch
* subscribers on twitch
* general how I'm spending my time


If you have any suggestions of things that would be interesting for me to record and analyze, I would appreciate hearing them! I want to do some data mining on the results, maybe just to see how I progress and what helps me, and hopefully it could help others progress too"
GIVE and GMM efficiency,4,0,False,False,False,statistics,1490519193,True,"Hello ! Here's a question for you all.

In an IV regression with one endogenous regressor and one instrument, GMM is more efficient than GIVE as long as the errors in the outcome equation are heteroskedastic.

Is this statement true or false ?"
Including outliers in box and whisker plot SPSS,5,7,False,False,False,statistics,1490525808,True,How do I include outliers in box and whisker plots in SPSS. I don't want some random circles and asterix on my graphs. Please help I am bloody frustrated!
Statistical proofs for experts,1,1,False,False,False,statistics,1490536968,True,[removed]
Reading Recommendations,5,2,False,False,False,statistics,1490536988,True,"Hey folks! I'm a new subscriber looking for some reading recommendations to both deepen my current understanding of stats and hopefully widen it as well. 

As background, I'm currently finishing a PhD in a Research, Evaluation, Statistics, and Assessment program. I just took my comprehensive exams and in brushing up on what I thought was ""early level"" stats stuff I realized I was getting a lot more out of it and understood it in a somewhat fuller way it seemed. 

What's specifically driving this is finally starting to grasp the issue with p values and significance testing. I had plenty of professors casually toss in that effect sizes were important, but never connected the dots on why. I think I'm getting there now, but could definitely fall deeper down the rabbit hole. 

I recently scratched the surface with Bayesian stuff due to my wife being curious about the possibility of a false negative test, but I don't really know how to connect that to stats. 

Anyway, I've got some decent background knowledge, but I want to know more. So whatcha got?? "
Great learning book recommendations?,0,1,False,False,False,statistics,1490541409,True,[removed]
"Novice question: does the CLT mean we can apply student's t test or ""normal"" stats to any data set?",10,1,False,False,False,statistics,1490553226,True,"Does the normality/non-normality of the *distribution of individual data points within a data set* matter? In the context of creating a confidence interval or performing a hypothesis test or t-test. 

It seems to me that a single data set's distribution is kind of irrelevant and rather it's the known normal *distribution of averages* described in the CLT that allows us to build confidence intervals and utilize tools like students t-test. 

Am I way off base?"
"Masters or Bachelors, comp sci undergraduate",8,0,False,False,False,statistics,1490553420,True,"Hi guys,

Wanted to ask you all a question regarding what would be the best route to take here.
I graduated summer 2015 with a bachelor of arts in Computer Science. I have since been working as a production support analyst at an investment bank, where I work with financial reporting software. I work mainly with SQL and Unix.

I want to go back and get a masters in statistics. I had a low GPA coming out of college, and did not get into any calculus in college. I'm wondering if a bachelors in statistics might be necessary to catch up on the basics before heading into a masters. Ideally I could go straight to masters but i'm unsure of how much the undergrad work is necessary.

What are your thoughts? Thanks in advance."
request for help; what test should I use?,0,1,False,False,False,statistics,1490557281,True,[removed]
Incest by country (2010).,8,0,False,False,False,statistics,1490557326,False,
true or false.,2,0,False,False,False,statistics,1490565674,True,"Generally, weights matrices describe the contiguities between spatial objects.

I wanna say true but the ""generally"" is like a safety for a teacher to argue either way and its hella annoying.  What do you guys think and why."
Is it possible to do multiple tests on the same set of data? Can I do a correlation and a t-test?,2,2,False,False,False,statistics,1490578989,True,"I am not sure if I can conduct a correlation and then a t-test on my set of data. Is it a statistical taboo? 
I'd like to compare the health and entertainment attitudes towards cigarette smoking and the intentions of smoking in smokers and non smokers, and see if the variables interact with each others. Would a MANOVA be more appropriate? What do you think?
I am using a visual analogue scale for both health attitudes, entertainment attitudes and intention "
"Different sample sizes for ""paired sample t-test"". How do I write up the findings?",13,4,False,False,False,statistics,1490579663,True,"I have dug myself into a hole and not sure how to get out. 
I have 1 year's worth of data, I hardly know jack shit about stats and I need to get this paper published. 

I have a 2 group pre/post test experiment. I simply need to compare the difference in means between pre and post test scores. 

I have n=266 and n=350. I am teaching myself stats as I go. I know, bad idea, come up with what analysis you want before you start the experiment. 

So when I run paired samples T-test it just comes back with a t-value using the lowest n, 266. From my understanding a t-test doesn't need a an equal number of subject, but I don't know how to report this in a publishable paper without sounding like I totally don't know what I am doing.  

The following chart is usually how I write up my t-test results. (its what I did my last paper at least... when I was compared means in single group). Can I ethically just delete 73 test subjects to have equal groups? Or say group B was reduced to 266 to match group A sample size. The problem originated because I had about 400 and 400 and excluded all subjects who did not complete both pre and post test. There was no way to know what my final test subject number would be. 

 | mean | SD| T-value n=266 | p value| effect size
---|---|----|----|----|----
Group A change| 7.85| 15.81|  |  | d=.21
Group B change|3.64 | 12.99|3.39 | p<.001 | Small 


Edit: I came across an online example similar to my experiment. Run paired sample t-test on group A to determine if significant difference in means occurred. Do the same on group B. Then run an independent sample t-test on those difference found in the two paired sample t-tests. an independent samples t-test will assume equal variance (right?) BUT is valid to run an independent samples t-test. I guess so because Group A and Group B are not effecting each other? "
"Do we ever use complex math in statistics? ie imaginary numbers, etc",14,13,False,False,False,statistics,1490586133,True,"As a math and statistics major, I was curious to see if we ever use complex analysis in statistics."
Is this true for standard deviation?,15,0,False,False,False,statistics,1490597672,True,"Statistics is not my thing. I was doing some research on what exactly the standard deviation means in terms of data. I found a site that's says if the 2 standard deviations of are larger than the difference between the two means, our hypothesis is not supported (aka the null hypothesis is accepted). Is this true?"
How to study/expand on income comparison between to populations,0,0,False,False,False,statistics,1490611978,True,[deleted]
Help studying income difference between populations,2,2,False,False,False,statistics,1490612418,True,I want to learn more about income and income inequality using statistics. Primarily I want to see if the difference in income between two factions of my family is statistically unusual. I know that I will have to gather data from a data base to pass normality test but I was wondering how to actually approach this? I'm a bit lost and I want to know if anyone has a surefire method to test this. Also other suggestions in regards to anything related to income and statistics are welcome. Thank you for your response and sorry if this isn't the place for this. 
Should I list out the statistical techniques I have used on my resume?,15,32,False,False,False,statistics,1490625716,True,[deleted]
Question about regression and dependent variables,8,1,False,False,False,statistics,1490627048,True,"Hey all, for my thesis I'm doing research about insider trading. The dependent variable is a percentage and can reach from 0-1. The independent variables are mostly dummies and I also have some control variables. 

I don't know how I have to make a regression, do I just use OLS or do I have to take the log of my dependent variable because it can only have a maximum value of 1? FYI this is not a dummy, it's a percentage!

Thanks all, I hope I made myself clear, English is not my native language"
I want to move from FP&A --> Data Analyst,1,0,False,False,False,statistics,1490631594,True,"Without starting on the absolute ground floor.


I know strong Excel skills are appreciated but what else can I be doing in my downtime to make this transition?"
Calculating the chance of loss,5,3,False,False,False,statistics,1490635480,True,"I haven't taken a stats class since high school so I'm struggling to remember how to calculate this. 

Assume you have two games you can bet on. In one game you flip a coin. If you win you gain 24%, if you lose, you lose 10%. In the second game you roll a 6 sided die. You gain that percent on your bet. 

Calculating the edge on each bet, game 1 has a 7% edge and game 2 has a 3.5% edge. Assuming you must pick one game and play it n times, what is the chance that game 1 will make more money than game 2. 

Thanks in advance. For those wondering I'm trying to compare the stock market to whole life insurance to a friend."
Suggestions for statistics book(s),10,15,False,False,False,statistics,1490635730,True,"Hi. I'm taking a graduate Data Mining course, and the professor requires that we be familiar with some statistics, as a pre-requisite. I have taken (and loved) statistics courses when I got my bachelor's degree almost 10 years ago, but obviously i'm rusty. 

What book would you recommend to quickly catch up on the material below? (ideally under $50 or an online book)

Displaying distributions with graphs & Describing distributions with numbers

Describing distributions with numbers & Normal Distributions

Scatter-plots & Correlation; Correlation & Regression

Design of experiments & Sampling design

Towards statistical inference; Randomness & Probability models

Random variables

Means and variances of  random variables

The sampling distributions of a sample mean; Estimating with confidence; Confidence Intervals

Tests of significance & level of significance 

Inference for the mean of a population Inference for a single proportion

Comparing two means
"
IBM SPSS 24 CRACK,0,1,False,False,False,statistics,1490639918,False,
Statistics M.S. if I'm getting an Applied Statistics B.S.?,5,3,False,False,False,statistics,1490644193,True,"I've heard that it's better to get a masters in something other than statistics if you get a B.S. in stats since it's more or less the same material but, almost being a 4th year applied stats major, I really feel like I didn't get that much stats training and statistics is really the only subject I'd want to do graduate work in (aside from data science but most data science programs are too young/underdeveloped to be a really viable option right now). 
Should I even get a masters at all? 
Thoughts?"
Probability of Max / Max of Probability,1,2,False,False,False,statistics,1490644955,True,"This question came up in the context of a discussion on order statistics today: Let's assume that I have the quantity 

    P[max_i T_i >= a] 

and that the T_i are independent, identically distributed test statistics. Can I relate this quantity to 

    max_i P[T_i >= a]? 

In particular, can I say that 

    max_i P[T_i >= a] 

is no larger than 

    P[max_i T_i >= a]? "
statistics help,0,1,False,False,False,statistics,1490657334,True,[removed]
"Oslo, 27th of March the last 3 years... now, tell me again global warming is just a hoax",5,0,False,False,False,statistics,1490677416,False,[deleted]
Earthworm experiment: which statistical test to use?,13,1,False,False,False,statistics,1490678145,True,"Hello everyone! So I did a choice experiment on earthworms.

The earthworms were given a choice between two treatments (wet soil and wet sand).

I placed 5 earthworms at a time in a container containing the two treatments.

I repeated this for 6 trials.

Can anybody give me any insight into which statistical test would be best to analyze the data? My professor suggested I could convert the number of worms that chose each treatment to a proportion (for instance, if 4 worms chose wet soil, and 1 chose wet sand, proportionally 0.8 chose wet soil and 0.2 chose wet sand), and then run a t-test on that to compare the 6 trials (n=6).

Alternatively, I thought I could do a Chi-square analysis on them and just explain that the independence assumption was violated due to time and resource limitations (I didn't have the time to allow 30 earthworms to make a choice for 10 minutes each).

I appreciate all of your feedback!

"
Statistical problem solved,0,1,False,False,False,statistics,1490695121,True,[removed]
How do i solve this problem? statistic + probability?,0,1,False,False,False,statistics,1490697203,True,[removed]
"""X is Y times more likely""",9,0,False,False,False,statistics,1490699219,True,"So recently the Canadian Armed Forces (CF) conducted a survey to assess the level of sexual assault/harassment within its ranks.

The main headline afterwards was ""Women 4x as likely to experience Sexual Harassment/Assault (SA/SH)"".

When you look at the raw numbers you find that during the previous year the number of men who experienced SA/SH was 578 and for women it was 365. So 63% more men experienced it than women. Now of course what the survey makers did was use the fact that there are a lot more men than women in the CF in order to come up with the 4x as likely number.

My question is , can someone ELI5 as to why this is a valid thing to say.

From a layman it just doesn't make sense because 

a) It seems to say that if there were even number of men and women that the raw numbers would be that there would be 4x as many women who experience SA/SH. Which imho says that the reason there isn't more SA/SH is a lack of victims.

b) Since a large portion of the perps for men are in fact women, if you increase the number of women than the number of male victims would also go up.

I am not an expert at stats so perhaps it is time I learned something about how ""More likely to"" works when it comes to uneven numbers in the comparison.

"
Scaling variance.,0,1,False,False,False,statistics,1490699679,True,[removed]
Covariance/expectation question,7,2,False,False,False,statistics,1490700985,True,"Hi I'm working on an regression equation that has the product of two correlated variables and i need to know the variance of the equation. I did most of the work already but I'm just not 100% sure it's correct. Can someone check this?


X and Y are correlated with with eachother. Both variables are standardised to variance of 1 and expectation of 0.

What is the covariance of X with X*Y?

I have:

cov(X,XY) = E(X*XY) = E(X^2*Y) = E(E(X^2 *Y | X)) = E(X^2 * rho*X) = rho * E(X^3) = 0

Is this correct?"
mixed ANOVA help,0,1,False,False,False,statistics,1490703250,True,[removed]
Help with report,0,0,False,False,False,statistics,1490705645,True,[deleted]
Estimating mean and variance of Folded Normal Distribution,3,6,False,False,False,statistics,1490713989,True,"I am struggling a bit to find a good way to estimate the mean and variance for calculating statistical process control metrics. 

I would like to use python, does anyone have advice? "
What method to use?,5,2,False,False,False,statistics,1490716401,True,I'm doing a personal research project for time traveled for ambulances using lights and sirens versus no lights and sirens. I have over 24000 pieces of data of the distance traveled and the time It took to travel that distance. I was curious as to what statistical method or equation I should use to see if these sets of data are related to each other. 
How to simulate a multivariable linear model with fixed $R^2$?,3,0,False,False,False,statistics,1490719541,False,
Hypothesis Tests for Machine Learning,6,6,False,False,False,statistics,1490724993,False,
"How to interpret a case where an analysis of means finds a significant effect, but trial level analyses don't?",2,2,False,False,False,statistics,1490730895,True,"
0
down vote
favorite
Suppose I have a within subjects dichotomous IV with two levels, and a continuous DV. Imagine each subject gets 100 trials, half of which come from one level of the IV, the other half come from other level of the IV.

A paired samples t-test finds a significant effect of the IV.

However, a linear regression keeping the data at the trial level, finds no such effect. (I'm not sure if this on its own is okay to do, but then I also...) A linear regression with random subject and item intercepts also finds no effect of the IV.

My question is: how do I interpret the fact that this effect is significant when looking at subject means (i.e., the paired samples t test), but not when looking at trial level data?"
How do you assess Relevance Condition of an Instrumental Variable?,0,1,False,False,False,statistics,1490732523,True,[removed]
It's been about a year since ASA set restrictions on reporting p-values. How much of an impact do you see in your field? (and what field?),3,0,False,False,False,statistics,1490736367,True,[deleted]
"The Gaussian correlation inequality — A Long-Sought Proof, Found and Almost Lost",12,90,False,False,False,statistics,1490736678,False,
Determining the number of topics hyperparameter in LDA models,1,3,False,False,False,statistics,1490749864,True,"I have recently been modeling corpora with Latent Dirichlet Allocation, and am wondering if there is some sort of error metric that can be used to assess whether or not the correct number of (unknown) topics parameter is a reasonable one. That is to say, although LDA is an unsupervised learning problem where the topic distributions are latent parameters, in the coding implementation the user must supply the algorithm with a value for the number of topics the model will use.

Essentially all I can see so far is that I pick, say, 100 topics, and let the algorithm crank out topic distributions. I look at these and say, ""yeah, that one is about 'tacos', 'tortillas', 'horchata', etc. It's about 'Mexican Food'."" It's clear that overshooting the number of topics will result in many documents having a small overlap with some topics, while undershooting it will probably result in topics intersecting.

Any help welcome."
Comparing Two Sets of stimulus-response curves,0,2,False,False,False,statistics,1490749891,True,"Hello /r/statistics, I have tried combing google search results for a lead but haven't found one (I think).  I have multiple vectors (for each of seven conditions) which are individual recordings of stimulus response curves (firing rate of a neuron for different frequency sound presentations) and I need to determine if the difference between two sets of distributions is significant.  Specifically, the peak is shifting.  Here is an example.  Any leads would be helpful, sorry to ask here.

[image](http://imgur.com/a/f1NFc)
"
help to determine what N is required?,0,1,False,False,False,statistics,1490751533,True,[removed]
"Computing distance between 2 ranked sets, where unique values differ",1,2,False,False,False,statistics,1490751533,True,"I am doing a search engine ranking analysis. I am looking for ways to quantitatively evaluate the distance between two ranked sets of values - but some values may be only present in set 1 or in set 2. 

Example: 
Set 1: 
{1234,
5678 (not found in set 2),
2037,
4023 (not found in set 2),
5843}


Set 2:
{2037, 
1234 ,
3784 (not found in set 1),
4283 (not found in set 1),
5843 }


I looked at Kendall's Tau, and Spearman's foot rule, but both of them rely on the sets having identical unique values. 

Any advice on an algorithm that takes this into account?  "
"If multiple t-tests are being carried out between groups in the same data set, but some comparisons involve groups of different sizes than the rest, should correction of the alpha value for multiple comparisons be carried out over all comparisons, or just within group sizes?",2,2,False,False,False,statistics,1490752958,True,This question arises because there is a different t-distribution for every sample size. 
Terrible Statistics: Video Games Exposure and Sexism in a Representative Sample of Adolescents,5,0,False,False,False,statistics,1490758277,False,
Has anyone had a job that uses service-oriented statistics?,1,1,False,False,False,statistics,1490772291,True,"http://stattrak.amstat.org/2012/10/01/service-oriented-stats/

http://community.amstat.org/statisticswithoutborders/home

Came across these two links and found them very interesting. I was hoping I could spend my summer working for a non-profit and wanted to get some insight from anyone with experience."
"The seven deadly sins of statistical misinterpretation, and how to avoid them",2,26,False,False,False,statistics,1490772493,False,
testing significance of 2 percentages,5,3,False,False,False,statistics,1490775548,True,"I'm not great with stats so please go easy on me....

I'm comparing the amount of trees browsed by deer in 2 locations.
9 samples sites were measured at each location and browsing (as a %) was measured.
I averaged the coverage percentages and ended with 4.6% at one and 8.1% at the other.

What test should I use to test the significance of these figures?

Thanks

"
Standard errors for a fitted smoothing model.,3,4,False,False,False,statistics,1490783330,True,"http://imgur.com/a/Booet

Suppose we have an unknown fixed curve f(x) which has a local maximum at x_max. Suppose we take a sample of points x_1,...,x_N such that x_1<x_max<x_N.

Now, suppose we observe y_1,...,y_N which are independent estimates of true values f(x_1),...,f(x_N) but have some known standard error. See picture.

My question is, is there a method for getting an estimate and standard error for x_max? Any ideas would be appreciated."
"Anyone want to try solve a problem of overall confidence percentage?!! In dire need of some opinions, not sure if my way of thinking is correct",0,1,False,False,False,statistics,1490787742,True,[removed]
Thoughts/opinions on George Mason University's dual OR and statistics masters programs?,1,2,False,False,False,statistics,1490797393,True,"http://statistics.gmu.edu/pages/dual_degree_ms_operations_research_statistical_science.html

Does anyone have experience or thoughts about this program specifically? Or any thoughts on dual masters programs in general?

Thanks "
Linguistic Research Paper,0,3,False,False,False,statistics,1490803647,True,[deleted]
Multivariate modeling with dependent but not necessarily correlated predictor variables,2,10,False,False,False,statistics,1490804361,True,"I'm by no means a statistics guru, so please bear with me and potentially any misuse of terminology.

I'm trying to do some predictive modeling that involves multiple input variables. Specifically, I'm trying to essentially predict where specific types of plant communities occur based on things like soil composition, soil chemistry, and hydrology. One set of very important variables is ratio of sand, silt, and clay that makes up the soil. The thing is, because they are in a ratio, they are not independent. However, they also aren't necessarily correlated with each other. For example, I could have 70% sand, 30% silt, and 0% clay. Or I could have 70% sand, 0% silt, and 30 % clay. Or 70% sand, 13 % silt, and 17% clay. Etc. There is no direct cause and effect relationship, so if I go somewhere else and measure 25% sand, I have no way of knowing how the remaining 75% breaks down.

If it was just two variables, like the ratio of gin to tonic, it would be easy to deal with because there would be a clear negative relationship between the two.

Including all three separately is causing issues because what should ideally be one predictor is now three, which is reducing the contribution that my other predictors are having.

Is there a good (appropriate) way to deal with this ratio thing? Most obvious thing I can think of is PCA, but I'm reasonably sure that doesn't work here.

Thanks"
Regressions on percentage/fraction,13,4,False,False,False,statistics,1490805084,True,"Hi Redditors,

I'm doing a research where I try to predict a %. What regression method is suitable to do so?

Note: i'm still very much a layman. Bear with me. 

Specifically, I want to see how much % of someone's wealth is allocated to asset1, asset2 etc. That means that the dependent variable will be a %. The values are therefore bounded between [0,1] (or [0, 100] in percentages). However, most responses will lie between 0-30%. Perhaps the left side is ""truncated/unequal?"" (e.g. not all 0% responses are equal, because simply cannot afford it vs do not decide to invest). 

Multivariate (predicts out of the 0,1 bound) and general logistic (assumes a discrete 0/1) seems poor fitting. I'm reading a bit about fractional/truncated regressions but i'm not entirely understanding it. What is the way to go? Previous papers seem to use tobit or fractional regressions(?). 

(I've already done logistic regressions on dichotomous dummies where the dummy term signals a (1) when there is any investment! Now I want to measure more of a relative preference).

Thank you. "
How could I quantify how two distributions differ?,4,6,False,False,False,statistics,1490808710,True,"I have plotted some experimental data of mine, and these data points fall into the following distributions:

http://imgur.com/a/KJFAt

http://imgur.com/a/2mJcS

So, these are fairly non-trivial looking distributions. I would like to figure out methods to quantify how these distributions differ. Perhaps a Kullback-Leibler divergence? 

What other methods could I use to do this? There's also a question of how to deal with differing levels of sparsity. "
Created a list of online resources I found helpful in my data science journey. Thought of sharing that. Thanks.,9,71,False,False,False,statistics,1490809155,False,
Is there a term for this?,3,1,False,False,False,statistics,1490819811,True,"I take statistics of A and apply to B. There are good reasons to think that the numbers for A and B are the same, but still there might be some difference. For example, 90% of cats are black, so I infer that 90% of cats that ran accross your path were black. It seems a good estimate, but in reality the latter would not always be 90%. Is there a term for this, or can you suggest a paper dealing with problems as this, or can direct me to some examples where inferences as this have turned out to be totally false, or some paradoxes, or something...?"
Root mean square error vs. standard deviation & sq. error,2,1,False,False,False,statistics,1490839970,True,[deleted]
"Anyone have photos of statistics offices prior to computers (ie: with rows of people manually calculating stuff for the census, etc)?",3,3,False,False,False,statistics,1490841503,True,.
Review Mostly Harmless Econometrics?,0,0,False,False,False,statistics,1490860499,False,
Prediction techniques/algorithms for bivariate data,1,4,False,False,False,statistics,1490871955,True,"I am a newbie in statistics so please forgive me if this is a stupid question. I am looking for different prediction techniques/algorithms for bivariate data. I have data has one dependant variable and one independant variable, both are quantitative variables on the continuous scale. I know that linear regression can be used, but not sure about any more. "
"Multiple linear regression, I seek kind help for a result interpretation :)",0,1,False,False,False,statistics,1490873169,True,[removed]
Question About Inter-rater Reliability,6,2,False,False,False,statistics,1490897382,True,"First time here, so I apologize in advance if there's something inappropriate about this question.  For the life of me I haven't been able to find the answer to this question.  I'm planning a retrospective chart review with 2 abstractors who are dividing the charts to review.  I'm planning a pilot study to help improve the inter-rater reliability.  My question is how do you calculate the number of charts that need to overlap between the abstractors in order to calculate inter-rater reliability?  How would you calculate inter-rater reliability in that case?  "
When a sample size of 1 perhaps is perfect,20,7,False,False,False,statistics,1490897983,True,"I will be presenting a business case for a tender next week, and a key component is an estimate of the obtainable price. I have assessed this price, using a single contract from a comparable case.

I know that there are skeptics in the steering committee, and that some of them will try to nail my case by claming that the sample size is too small. I, on the other hand, believe that a single, very comparable price book from a recent tender, is sufficient. My reasoning behind his is first of all that I know from past experience that prices vary over time  due to major market trends, but that prices usually are stable and have a low tendency to vary within shorter time frames. 

Second, the number of data points available is very low (maxium 5 contracts awarded since early 2016), so I'm under all circumstances not able to get hold of a decent sample in usual, statistical terms. If I wanted n=30, I would need data dating as far back as 2010 or to combine my samble with international data, meaning that I include less representative data.

How should I approach this dispute?"
Confidence intervals and factorization,1,1,False,False,False,statistics,1490899926,True,[deleted]
What's the chances of flipping a coin(assume 50% probability) 221 times and it landing on heads 94 times or less?,5,0,False,False,False,statistics,1490902730,True,
"Need Help Understanding and computing Gibbons, Ross, Shanken (GRS) Test statistic!",0,1,False,False,False,statistics,1490904176,True,[removed]
Career guidance,0,3,False,False,False,statistics,1490908497,True,"Hello everyone. Next year I'm starting university and I have yet to decide what career to pursue.

I've done quite a bit of research and so far I'm really interested in statistics, but I want you, people who I assume are already knowledgeable about the subject, to tell me what are the reasons why you would encourage anyone to major in it.

And also, from your experience, are job prospects good? What people tends to think about staticians and statistics as a whole? Is it considered a ""rigorous"" subject? And last but not least, although I know it's extremely subjective, is it considered a ""hard"" (cognitively wise) subject? In the case it was it wouldn't be a problem, quite the opposite.

It's worth mentioning I really enjoy mathematics, and I'm also contemplating the possibility of majoring in it. Thanks for taking the time to read it! Your guidance would be very much appreciated. Sorry in advanced in the case I committed any linguistic mistakes, as you might have already noticed I'm not a native english speaker.
"
What's a technique to measure the weight of certain variables?,0,1,False,False,False,statistics,1490913000,True,[removed]
Markov Chain Game,1,1,False,False,False,statistics,1490913416,True,[deleted]
Help with a 2 Sample T Test?,12,1,False,False,False,statistics,1490917501,True,"Currently putting together the results for my Honours project and I need a little guidance in how to analyse means. 

I have two sets of data, entropies of passwords (Sample sizes: Na=9 and Nb=11). As they are not paired this eliminates the possibility of a paired-T test. Decided to use a 2 sample T test following [this example](https://www.youtube.com/watch?v=rMfNIyQDJY8) that I found after finding it was a way to compare means, working with that I get a value of p < 0.05. 

Just wondering if the method that I followed is accurate and if this is the actual test that I should be using? I've seen different results on T-test calculators online but I decided to do it by hand - I'll post my working in a comment. "
is this appropriate to use weighting,0,1,False,False,False,statistics,1490928490,True,[removed]
Land Ownership Versus Population,0,1,False,False,False,statistics,1490928957,True,[removed]
What are some statistics related books good for someone just going to grad school / are interesting?,0,1,False,False,False,statistics,1490933768,True,[removed]
elite entertainment - dj entertainment services - YouTube,0,1,False,False,False,statistics,1490941230,False,
"My Y variable is normal, but when I take the log returns (difference of logs) I get a not normal distribution.",0,1,False,False,False,statistics,1490943757,True,[removed]
"My professor keeps contradicting himself, so I have a small question for anyone that can help here at /r/statistics",24,5,False,False,False,statistics,1490944964,True,"When I want to compute an empirical cdf, I look at a set of given points, such as d = {2,3,4,5}, and compute:

Fn(x) = 1/n * [# of elements of data set 'd' less than or equal to x], where n is the number of elements in the set 'd'

So if I wanted to give an ecdf for F_4(x) of 'd'.

I would get F_4(2) = 1/4

F_4(3) = 2/4 = 1/2

F_4(4) = 3/4

F_5(5) = 1.

In an attempt to plot this in R, I try the following code:

x = seq(2,5,by=1)
F <- ecdf(x)
plot(F)

You get a step function that has a step of 1/4 each time x reaches a value that is in the set.

Is this the correct graph of the empirical cdf in your opinion?

My professor marked me wrong on my exam and shifted all of the steps on my graph by 1.

edit: he corrected it in class today"
"How do I refer to obtaining a random number, such that an infinite amount of them would have a particular mean and variance (or coefficient of variation)?",0,1,False,False,False,statistics,1490963778,True,[removed]
Need help understanding formula to replace zero values in compositional data.,2,2,False,False,False,statistics,1490971662,True,"I have ipsative data that I need to replace zero values before calculating geometric means.

An article that did the same thing writes:

> Computation and interpretation become problematic for data containing zeros because ratios
> are either 0 or infinity, and the geometric mean is zero by definition. In the context of the
> OCAI questionnaire, if a part has a score of zero, then the respondent did not see any traces
> of the culture in his or her organization. To circumvent the problem we have replaced profiles
> that contained 𝑧 > 0 zeros as follows: Part 𝑋𝑖𝑖𝑖 was replaced by 𝛿 if Xijd= 0, and by
> (1 − 𝑧 𝛿)Xijd , if Xijd > 0 (see Martín-Fernandez et al. 2003). We set 𝛿 = 0.5, halfway between
> score 0 ̶ the total absence of a culture ̶ and score 1 ̶ the smallest score that
> acknowledges the existence of a culture.

I don't really understand the formula... If i have zero values i replace the with 0.5, if not adjust them by Xijd'=(1-z𝛿)*Xijd
What is z?
They do refer to Martin-Fernandez, and I cant find the same formula but only this: [Image link] https://ibb.co/enNUgF

 
My data looks like this: 
(j1) . d1.2.3.4   
i1= 45 - 0 - 25 - 30 = 100
i2= 40 - 0 - 60- 0 = 100

If i replace 0 with 0.5. How to i compute the other variables back to sum=100 using above formula. ? Thank you :)"
Online Statistics Tutor from Statisticshelpdesk.com Help Has Created a Special Niche,0,1,False,False,False,statistics,1490980292,False,
Data cleaning spss syntax question,0,6,False,False,False,statistics,1490982104,True,"Im working on cleaning some data with possible response patterns that would indicate careless responders. Ive been using syntax that identifies a standard deviation of 0 on sets of items to flag persons providing the same response for many answers that are unlikely (i.e., all respond 1 out of 5 on a series of personality variables even with reverse coded items). 


My question is,  does anyone know of syntax to identify a zig zagging reponse pattern? This pattern would include responses  like this: 1. A 2. B 3. C 4. D 5. C 6. B 7. A 8. B 9. C 10. D 11. C 12. B 13. A 14. B....etc. 

I feel that there is likely a method using a LOOP command or DO REPEAT command but am not sure the best method when creating the syntax to adapt across multiple data sets with different variables. 

Has anyone developed this syntax for their own cleaning and be willing to share or point me toward somewhere to find it?

Thanks!


"
"Two people meet in a metro suburb. One currently lives in the exact same apartment #, in which the other lived several years ago. What are the chances of this?! Q&A/details in comments.",0,1,False,False,False,statistics,1490985876,True,[deleted]
Two people meet online and discover that one lives in the exact same apartment# that the other lived in years ago. What are the chances of this?! (Details and Q&A in comments),10,0,False,False,False,statistics,1490987102,True,"We met online via an app, and we are relatively the same age and income, of the opposite sex. We just happened to both be currently living in the same metro-area suburban neighborhood. It's located in a census-designated place that had a population of 15,000 seven years ago but has many more now that there's the new neighborhood in which we live, which contains three apartment complexes, each about 5-7 stories high, and about 140 townhouses. The city we live in has a population of 24,000. This city is just outside a larger city about 12 miles away, and has a population of 670,000 residents. This city swells to 1 million commuters during the daytime. Any other information needed? Is this a dumb question with an obvious answer? Anyone have any idea of how one could even begin to calculate it if it were possible to calculate? We don't have any knowledge in statistics."
Can someone help confirm statistical relevance? I'd love to credit you.,2,1,False,False,False,statistics,1490992023,True,"Hello statistics fans, I have a problem.  I'm trying to determine if something is statistically significant. The subject is found in [this article] (https://www.pcgamesn.com/overwatch/overwatch-loot-legendary-skin-chance) where they take 250 chances, each with 4 payouts in each chance, along with 4 types of payout, each with a different chance rate. 

We are trying to determine the rarity of each drop type between two different occasions of drawing 250 chances.

Are 250 chances enough to determine an accurate drop rate for each of the four rarity types? And could two different occasions of this be used to see a shift in rarity?

Thanks for any help! This is for an article so if you'd like to be referenced by website or name, I'd love to oblige!"
What books for better conceptual understanding of math stats?,5,28,False,False,False,statistics,1490997468,True,"The math stats course I took was fairly mechanical and disjointed. We didn't really discuss how concepts were connected and the textbook, Freund 7e, didn't help either. 

Any suggestions?

On a similar note, does anyone have book suggestions for survival analysis and missing data problems? 

Thanks"
I Thought of an Interesting Math Problem While Playing a Pokemon Game,0,2,False,False,False,statistics,1491005871,True,"I play a mobile game called Pokemon Shuffle. It is a match puzzle game where a 6x6 grid is populated by 4 different types of icons ([Screenshot](http://i.imgur.com/hkOvYai.jpg)). Levels are completed by making matches of 3 or more icons. There are no restrictions to the movement of the icons (i.e. they can be moved from any location to any other location on the grid), and their placement is random. 

One of the gameplay mechanics is that certain abilities can be triggered when matches are created. These abilities have different chances of triggering based on a 3-, 4-, or 5-match occurring (the larger the match, the higher the chance). This led me to wonder:

 

Given that the user will always make the largest available match, on average, what percentage of matches with be a 3-match, 4-match, or 5-match?

 

EDIT: BONUS QUESTION: Some levels can introduce something called disruptions, where they randomly change 6 icons into blocks, which cannot be moved and prohibit an icon from being moved into their location. 

How would the presence of these 6 blocks (placed at random) affect the percentages?"
String variables not allow?,2,0,False,False,False,statistics,1491009004,True,[deleted]
Answer a simple problem for me?,0,1,False,False,False,statistics,1491013199,True,[removed]
The blogs vs. Case-Deaton,1,6,False,False,False,statistics,1491013676,False,
calculating power in statistics...please help!,2,1,False,False,False,statistics,1491014411,True,[deleted]
Can someone help me understand/read this chart: independent t tests for percentages and means (what do the grey and red text mean beyond the obvious) Thanks!,0,1,False,False,False,statistics,1491029949,False,
Statistics of battleship(Schoolwork),0,1,False,False,False,statistics,1491053591,True,[removed]
Statistics of Battlefield(School Project; please do like and share for our grades.),2,0,False,False,False,statistics,1491054289,False,[deleted]
Best resource(s) for Statistical Theory?,10,5,False,False,False,statistics,1491062178,True,"I've got about a month left before the final exam in my Statistical Theory class. I'm not in danger of failing, but I've not done as well on the midterms as I'd like. Part of that is personal (marriage planning, a Capstone project, graduation), the other part is my instructor. I love her, and she clearly knows the material, but her lecture style is a little chaotic and difficult for me to follow at times.

So, do you guys have any resources (videos, online texts, etc.) that you have found particularly useful re: Statistical Theory? We're using Hogg/McKean/Craig's Intro to Mathematical Statistics, which is a fine enough book, but I'm hoping to find something a little more accessible for review purposes.

Thanks in advance!"
Difference between percentage points and percent,0,1,False,False,False,statistics,1491067556,True,[removed]
Are there any simulators to determine who would win in a fight of my friends?,0,1,False,False,False,statistics,1491069927,True,[removed]
Statistics Graduate School,15,14,False,False,False,statistics,1491079130,True,"Hi,

I have a biochem BS from a top University of California.


It turns out I don't really like biochem and wish to pursue a masters in statistics.

I am getting ready by taking linear algebra, several R programming classes and brushing up on my calculus.

Any suggestions for a solid foundation statistics textbook?

Thanks a bunch everyone.
"
"I'm writing a Undergraduate term paper for a statistical learning class. I'm using a Random Forests method for my analysis, but I'm worried I won't have enough content to meet the required page length. What are some things I should touch on/ look for so my professor thinks I'm a good boy?",5,2,False,False,False,statistics,1491082703,True,The data is NBA Wins data compared against las Vegas prediction odds. 
probability a value is part of dit. A and not B,0,1,False,False,False,statistics,1491083592,True,[removed]
What is going on in r/dataisbeautiful?,5,0,False,False,False,statistics,1491090510,True,I already hated the sub but felt a moral obligation to stay subscribed and monitor popular visualisations.  Now there's just a bunch of terrible posts with the word data repeated over and over.  I couldn't take it anymore and unsubscribed.  I guess I'm getting old not wanting to get in on these jokes anymore.  
Any help ASAP on this stats question?,1,1,False,False,False,statistics,1491091638,True,[deleted]
Any help ASAP on this stats question?,0,1,False,False,False,statistics,1491093721,True,[deleted]
Good tutorial videos for SPSS? (political science),0,0,False,False,False,statistics,1491094387,True,[deleted]
Statistician vs Actuary: Pros and Cons,8,5,False,False,False,statistics,1491097422,True,[deleted]
"When is it ""legal"" to bin data before PCA?",6,0,False,False,False,statistics,1491102034,True,"I'm working with a dataset that does not cluster very well after PCA when all data points are used, but does when I use 20% of the data. 

Are there cases where it is okay to bin data before PCA? I'm assuming that I am biasing my data by arbitrarily binning it then running PCA. I would like to argue that my data is noisy before binning, and that PCA after binning is a more accurate representation of the data.

Obligatory apologies for stats ignorance"
Are Applied Statistics (or Data Analysis) Masters Degrees worth it?,24,29,False,False,False,statistics,1491103448,True,[deleted]
I have a question regarding graduate school too...,4,0,False,False,False,statistics,1491104408,True,"I will be getting my BS in Mathematics in May. And I want to continue to grad school to study statistics. As of now I have different options and I don't know what's the best choice. 

1. I can stay at my current school, our school offers a BS to PhD program where I don't need to get a Master's. However my current school does not have a statistics specific program (only a Math program with Statistics options). On top of that my current school isn't very well known. This will take me about 5 years to complete. On the bright side, the school is willing to fund me full way. 

2. I can go to other schools. I have applied to some and was accepted. But they don't have a BS to PhD program so I must complete my Master's first. And funding isn't guaranteed yet. The  up side is that they do have statistic specific programs and are ranked higher than my current school. 

I've read online that getting your graduate degree and undergrad degree at the same school doesn't look as good as if there is diversity. But the full ride offer is very tempting.... 

Please any advice will help. Thanks!"
How to Quantify the Quality of Uncertainty Predictions?,4,1,False,False,False,statistics,1491112175,True,"here's my problem:

I'm training a couple types of binary classifiers with some data I have, so that on my test data gets classified (a 1 or 0) with some certainty (0.50-1.00).  I want to quantify how good my classifier is at predicting it's own future certainty.  I have no idea how this should be done.


what I'm doing right now:

(1) I'm binning predictions in my test set (say everything from 55-65% confident) and comparing the actual number of correct/incorrect classifications with the ideal (60%) assuming they're uniformly distributed.  (So in this case, if we make 100 predictions that we believe have 55-65% certainty, we would hope that 60 of them are actually correct and 40 are actually wrong).

(2) I've also come up with this metric-- 

SUM over predictions( (1/certainty)* [1 if correct, 0 if incorrect] ) / total#predictions.  

I came to this after playing around a bit-- obviously if we have a bunch of values with .8 certainty, 1/.8* (8/10) will give 1, as will 1/.5 * (5/10) if we have a bunch of values with .5 certainty.   For large datasets, generalizing this will give 1.00 if we're perfect at predicting our future certainty, under 1.00 if we're overconfident in our predictions, and over 1.00 if we're underestimating our certainty.  If you're good at estimating your own uncertainty, you hit 1.00.

I don't have any justification/prior work to cite here, I'm just doing things that seem intuitive to me.  Any guidance/citable references would be greatly appreciated! 

"
Programmed Statistics by B L Agarwal - Download free,13,8,False,False,False,statistics,1491138277,False,
Statistics in Arguments,0,1,False,False,False,statistics,1491167021,False,
Biostatistics problem involving statistical power and poisson distribution,0,1,False,False,False,statistics,1491172471,True,[removed]
Interpreting kurtosis,13,11,False,False,False,statistics,1491178952,True,[deleted]
What to do between now and start of masters to maximize employability in the future?,9,1,False,False,False,statistics,1491180485,True,"I am in my fourth year of undergrad, graduating this spring and will start my masters in statistics in late august. My main goal upon attaining my masters would be to find a career in industry. The problem is that the program I am going to be attending is just one year, which doesn't leave me with a spare summer to try and gain experience. I currently have no research / TA / internship experience whatsoever. Nor do I really have any programming experience. I want to do whatever I can now to maximize my employability upon graduation. What can I do now and in the summer that will benefit me most in the long run?

 

1. Learn programming. Seems like learning R or Python would benefit me in the long run. I don't exactly know where I should start though. Is it better to learn through a book, or should I take some online courses on coursera? Any certifications that would look good? Should I attempt to learn both R and Python or focus on one? SQL? SAS? Excel? Any specific recommendations on the path to picking up a certain language? 

2. Look for an internship. Honestly it is a bit late, but I am sure there are still some opportunities If i look hard enough. Problem is I am not sure what type of internship I should be looking for. And I guess a bigger problem is that I don't really have the necessary skills to land any useful internship. Seems like any relevant internship position requires proficient knowledge of a programming language or excel. I want a chance to learn, but I don't see myself landing any position w/o having any skills. Are there any options I'm not aware of here, or should I write off trying to grab an internship at this point?

3. TA position. Not really sure how TA positions are granted but would it be worth it to try and ask about a TA position for one of the summer courses at my current undergraduate institution? How about a different school? 

4. Research assistant. Same as above I guess, but I don't even really understand what a research assistant would do. Would there be any benefit into emailing professors from my university? How about other universities?

 

Thanks!

"
"Home sweet home? I'm from Florida and was not aware the of this statistic. As I write this, HenryHugglemonster posts: ""Yeah but if you look at murder rates you'll find very similar charts. More violent criminals require more forceful police."" Very good point, Henry. I sure could use a huggle rn...",0,0,False,False,False,statistics,1491185415,False,[deleted]
LDA document similarity,6,1,False,False,False,statistics,1491187889,True,"Suppose I've trained an LDA model and have topic distributions for each document. Is there an accepted metric for saying that two documents are similar under LDA? Off the cuff I would imagine constructing an inner product between two documents. That's is, I can think of a vector space with the dimensionally of the number of topics. Each document will be a vector in this space with coefficients in each dimension given by the topic distribution probabilities. Then I could compute an inner product as usual. Am I on the right​ track?"
Quick question about the t-distribution,4,2,False,False,False,statistics,1491192054,True,"Hey guys, in a two sample t test, I was wondering if you can end up with the results t=0? Thanks!"
Chi squared significant (p<0.05) ANOVA normal,11,2,False,False,False,statistics,1491212729,True,Basically my t-test is showing a normal value but my ANOVA (2 sample) is normal between my control and experimental groups. Any idea what is the reasoning for this is? I am basically testing whether or not a intervention vs. control is having effect on reading speed.
8 Benefits of Statistics Tutor Help Service for Securing High Score,0,1,False,False,False,statistics,1491235425,False,
Z score Question.,7,0,False,False,False,statistics,1491239750,True,[removed]
"How would you go about pulling a population within a large data set that is comparable to a smaller, separate, dataset in SPSS or SAS?",0,1,False,False,False,statistics,1491248190,True,[removed]
Statistical or heuristic approach to determining a change in a demand signal?,0,6,False,False,False,statistics,1491254249,True,"So I work for a company that systemically aggregates volumetric information from external sources for our clients. Unsurprisingly, our data sources are unreliable and have data quality issues along the lines of: active items not getting any information signals, item volumes getting attributed to incorrect segments of the supply chain, etc.

My goal: I'd like to weekly run a script that looks at past usage and estimates when an item may not be reported correctly.

My thoughts: Perhaps I could use some sort of SQC charting (but I have a feeling any sort of statistical assumption requirements would fail). My second thought would be to estimate the time between re-orders and set a threshold for 2-3x to send a notification.

Thanks for sharing your thoughts or experience with this."
Statistics Major program,8,3,False,False,False,statistics,1491256553,True,"Hello! I'd really appreciate if someone told me wether the following program is considered predominantly ""applied"" or ""theoretical/mathematical"" and wether it's lacking or well rounded on paper.


*FIRST COURSE*:

-Descriptive Statistics  
-Fundamentals of Business Administration  
-Introduction to Calculus
-Introduction to Informatics
-Introduction to Operations Research 
-Introduction to Probability  
-Introduction to Statistical Inferencesory  
-Linear Algebra  
-Principles of Economics 
-Programming 

*SECOND COURSE*:

-Integer and Linear Programming 
-Multivariable Calculus  
-Numerical Methods  
-Official Statistics 
-Probability and Stochastic Processes  
-Sampling Methods  
-Statistical Inference  
-Statistical Software  
-Statistics for Quality Management
-Survey Design

*THIRD COURSE*:

-Bayesian Methods 
-Econometrics
-Experimental Design
-Files and Databases
-Linear Models
-Multivariate Analysis
-Non-Linear Programming and Network Flows
-Non-Parametric and Resampling Methods
-Queueing Theory and Simulation
-Statistics for Biosciences  

*FOURTH COURSE*:

-Demography  (Optional)
-Engineering Optimisation  (Optional)
-Financial Optimisation  (Optional)
-Generalised Linear Models  
-Industrial Statistics  (Optional)
-Medical Statistics  (Optional)
-Practicum I  (Optional)
-Practicum II  (Optional)
-Statistical Methods for Data Mining  (Optional)
-Statistical Methods for Finance and Insurance  (Optional)
-Statistical Methods for Marketing  (Optional)
-Statistics for Quality Improvement  (Optional)
-Survival Analysis  (Optional)
-Time Series Analysis 
-Bachelor's Thesis  (Project)
"
Favorite applied statistics resources?,4,8,False,False,False,statistics,1491259559,True,"Hi r/stats! I'm in a bit of an odd situation...I just started in a statistics PhD program, but my undergrad was non-stats. I don't have a Master's, though I wish I did--I skipped it mostly so I could get funding and took the first PhD offer I got after I applied to schools. I'm happy and I love my program, but I don't have as solid of a foundation as a lot of the people who took a lot of stat courses in undergrad (I took 1) or were statistics/math majors.

So, what are your favorite resources for learning applied stats? I have probability and statistical theory sprouting out of my ears, but that's pretty different from applying it. And a lot of the stuff that I skipped over completely, just because I essentially started learning stats in the PhD program, like contingency tables and Pearson's chi-square test, is completely unknown to me. I'm looking for something that isn't too slow in terms of math and has a lot of practical tips. Any replies appreciated! :)"
Calculating confidence intervals on categorised sub-samples,10,5,False,False,False,statistics,1491262896,True,"Supposed I have an unknown sized population of people (e.g. all the people in a country) and I have data on 10000 of them.

If I initially said that 20% of them pass some test, I know I can calculate the 95% confidence interval on that 20% as:

    1.96*sqrt(0.2*(1-0.2)/10000)

How would this change if I took sub-samples of my data? Suppose the 10000 people are categorised into A, B and C with the following distribution:



Category | People | Percentage that pass the test
---|---|---
A | 8000 | 25%
B | 1950 | 18%
C | 50 | 7%

Does the calculation of CI change now that I have a sub-sample of the original sample? Or would I just calculate the CI in the same way for each of A, B and C with the sample size being 8000, 1950 and 50 respectively?
"
Need help ASAP - Health stats,0,1,False,False,False,statistics,1491270095,True,[deleted]
Question on Panel Data,1,3,False,False,False,statistics,1491271203,True,"So Suppose I a wealth statistic on 10 individuals over the 28 periods (years.) I am trying to explain, eventually through regressions, how people get their wealth.

Let's say I have two variables that help me explain this. Years of education and stock market valuation. The years of education obviously vary for everybody. Maybe somebody has 19 years, another 15. There is probably going to be a positive correlation between wealth and education. However stock valuation is different. Here I am looking at the average valuation of the NASDAQ for a year. There are 28 points. The valuation is the same for everybody (of course investments in it vary, but that's not what I am looking at.) The correlation is probably positive. Wealth goes up when stocks go up. 

So here is my question:
When I am organizing my data in excel or some other program, for should I just give each person the same value for the stock market for a given year? Example:

(Person):(Year):(Wealth):(Education):(Stock)

1:1999:1000:15:3500

...

2:1999:1700:19:3500

...

Is that going to be a valid way if I am reading this into STATA, R, or GRETL?

Thanks."
Finding E(x) from Moment generating function,1,0,False,False,False,statistics,1491294657,False,[deleted]
Applying to graduate school as pure math undergrad,0,1,False,False,False,statistics,1491317215,True,[removed]
I filled out my NCAA bracket using Fivethirtyeights probabilities and ended up in the middle of the pack.,8,0,False,False,False,statistics,1491329160,True,"I filled out my bracket using Fivethirtyeights tournament probabilities and ended up tied for second place. Points wise I was pretty much in the middle of the pack. Our pool was small, 6 people. Is this to be expected? Or is it a sign that Fivethirtyeights probabilities were off? "
Online Help,2,0,False,False,False,statistics,1491329550,True,"Hey guys, do you have any websites etc that you can suggest to review some of the content that I might have learned in my first year Stats class. I was considering using StudyPug, but if someone has experience with something else, please share :) "
What tool to analyze grade distribution across teacher/subject/other variables?,7,5,False,False,False,statistics,1491331695,True,"Hi all,

I have an excel data set that has student grades by semester, with information on teacher, course number, subject (math, science, etc.). I would like to analyze this data to create box plots for grade distribution, looking by teacher (which teachers are ""harder"") or by course (ie, all science 9 teachers) and also by subject area. 

I initially tried to construct a pivot table to be filterable to pull out a 5 number summary, but I can't get it to calculate quartiles. I also know that I can calculate each thing individually with many formulas. I know there must be an easier solution. 

**One**, is there an easier way to do this in excel? Is it easily communicable to me via this forum?

**Two**, would this be even easier in R or something similar?

**Three**, will I have to learn a bunch of stuff in order to do it, or could it be pretty straight forward?

I'm not sure if these are the types of things you deal with in this subreddit, but it seemed like a reasonable place to post. Thank you. "
Psychology MA to Stat PhD program?,0,1,False,False,False,statistics,1491344440,True,[removed]
"Just because I want to pass a bunch of actuarial exams early, why am I the asshole?",0,0,False,False,False,statistics,1491349158,True,[deleted]
"Just because I want to get through actuarial exams as quickly as possible, why am I the asshole?",15,0,False,False,False,statistics,1491350178,True,[deleted]
undergraduate statistics major jobs?,22,16,False,False,False,statistics,1491353883,True,"Okay so basically I am just looking for career advice or thoughts on what type of job anyone here thinks I am qualified for, or could become qualified for in the near future, as I am graduating pretty soon without a job lined up.

Currently I am a senior with a dual degree, bs in business management and a ba in statistics from a pretty good public school. I am okay at SAS but that is the only statistical software I have really played with, and I do not know any coding at all. To be honest I only chose statistics as a major because I got a nice tuition discount and because I have always enjoyed math. So far I have gotten 1 job offer for an underwriting analyst position but I do not really want to take it, mostly due to the location. I have looked into the actuarial field and although I think I could pass at least one test maybe two if I studied all summer I still have doubts about that field as well, mostly because it seems kinda dull and I do not like the idea of working in insurance, idk maybe I am being too picky. I am also thinking of maybe grad school although it is too late to apply for this next semester and I dont have the highest GPA 3.3. Any thoughts would be greatly appreciated"
Please help me I don't know what to do,3,0,False,False,False,statistics,1491363956,False,[deleted]
How does one's interpretation of the concept of probability affect his or her work in practice?,2,5,False,False,False,statistics,1491379994,True,[deleted]
What affects statistical significance in ACOVA models?,7,5,False,False,False,statistics,1491386971,True,"This might be the wrong place to ask this, but a professor wants us to answer what can be done (besides increasing sample size) to improve the statistical significance of an analysis of covariance model. I'm finding absolutely nothing online or in any of my textbooks. Can someone please  help or point me in the right direction?"
What undergrad stats courses are absolutely necessary?,11,7,False,False,False,statistics,1491396047,True,"I'm a HS senior currently deciding between two math/stats programs (VT and Clemson). I've been looking at their course listings and I'm wondering what courses I need to take in order to be proficient enough by the time I graduate. Basically, I'm looking for some help with figuring out how to compare two programs. Any help appreciated!"
"""P-value: the test that makes or breaks scientific research"" from the statnews Signal podcast - A good discussion on the misuse of p-values and statistics in medical/drug research.",21,52,False,False,False,statistics,1491398541,False,
The Bayesian Trap,32,55,False,False,False,statistics,1491404579,False,
Liver transplant team numbers needed,0,1,False,False,False,statistics,1491409224,True,[removed]
Having Trouble with Your Business Statistics Help Talk to Us,0,1,False,False,False,statistics,1491411762,False,
What statistical test do you use for a data set with 2 continuous variables and 1 categorical variable if you want to look at the relationship between them?,0,1,False,False,False,statistics,1491411819,True,[removed]
Good explanation for MCMC algorithm.,0,7,False,False,False,statistics,1491412787,False,
Logistic Regression Isn't Interpretable,6,0,False,False,False,statistics,1491413747,False,
Non-stats person. How to predict based on a sequence.,13,3,False,False,False,statistics,1491418138,True,"I have two questions.

Say I have a sequence or series of events:
0011000011011110100101100101000011000001010

These are not coin flips. This is a series of events that should be calculated similarly to a moving average.

A. What technique do I use to determine the likelihood of the next event being a 1 or a 0?

B. How do I do the same predictive analysis but with an additional weighting on the most recent period (say the last five events)? That is to say, the last five events should be given more consideration in predicting the next event.

Thanks very much.
"
Auto-correlation with a windowed lag - Financial Time Series,1,3,False,False,False,statistics,1491428418,True,"Hi everyone, 

I am looking at bank transactions (about 100k samples) and I see that some accounts have fairly regular cycles of income and expense (e.g. paycheck -> rent -> paycheck -> debt payment etc.). People with very stable and recurring time are awesome, and I want to have an objective way to measure how stable they are.

I want to identify those accounts which have the most structured cycles. For example, see these images: http://imgur.com/a/0hR0Q. 

So far, the approach I've been taking is to use the autocorrelation and partial autocorrelation function in python to identify periodic behavior (please comment if this is a reasonable approach, and/or if partial is preferable). 

The issue I'm running into is that the intervals between payments aren't always a fixed number. For example, if I get paid on the last day of the month, sometimes that is 28 or 30 or 31 days from the previous payment. What I would like to do is to combine the correlations for each three day period, and use that as a objective measure for the periodicity in the data. What's the best way to do this? Can I just sum the correlations for 28, 30, and 31 day lags? 

Please let me know if you think this approach to my problem is reasonable and if you have any advice on how to proceed. 

EDIT: Also, for background, the data have the following fields -- Customer ID, Account ID, Posted Date, Description, Transaction Amount, Category"
Do confusion matrices apply for multi-label (not multi-class) problems?,0,1,False,False,False,statistics,1491428457,True,[removed]
Causal Inference with Difference-in-Differences Regression (an exceedingly applied/introductory explanation using R and Python) [x-post from /r/pystats],3,11,False,False,False,statistics,1491429445,False,[deleted]
Is there a difference between GLM and ANOVA?,9,2,False,False,False,statistics,1491429685,True,"What the title states. What, if there is one, is the difference between doing a GLM procedure and an ANOVA?"
Help me with Aitkens GLS,0,1,False,False,False,statistics,1491432433,True,[deleted]
Fitting a Lognormal Distribution in Python using CURVE_FIT,0,3,False,False,False,statistics,1491432623,False,
Statistics question on probabilties?,0,1,False,False,False,statistics,1491433497,True,[removed]
"If I want to compare an old treatment vs. new treatment and test the hypothesis of no treatment-effect difference on survival, how would I do this with a parametric and non-parametric method?",5,4,False,False,False,statistics,1491433500,True,"If I have data and my subjects are split into new treatment and old treatment groups, over two years, how would I test the hypothesis of no treatment-effect difference on survival with both parametric and non-parametric methods? The assumption is that this data is right-censored.


For parametric, I'm assuming I should use either a Weibull or Exponential model. For non-parametric, maybe a Rank or Wilcoxon test?

I'm having trouble being able to understand and explain this in more detail. Can someone lend me a hand here?

Thanks!"
"I have no idea how to figure out this, please help me",1,1,False,False,False,statistics,1491440326,True,"First of all I don't expect anyone to do any work for me, I just don't know where to start, so any links to wikipedia or just a pointer would be immensely helpful! Thanks in advance!

So I'm renting out on airbnb and one conundrum struck me today that you guys probably have no problems answering:

The idea is that I rent out the whole house plus 1 bed for one person for a cost of $50.

Then, for every additional guest it costs $10, because the expenses are the same no matter how many visitors we have (not entirely true).

Now the problem is that it's way cheaper for groups to book all the rooms than it is for one person or couples. And the largest groups will raise the expenses a little bit.

So I have no problem to rent out to groups, but I don't want the statistics to get lopsided towards groups as there are more expenses over time.

So the question is: How do I price it so that any number of guests isn't significantly a better choice than any other? Is there a formula for what I'm proposing? airbnb allows me only to set one base price + one price for additionals, or else I'd just find a curve.

----

Here's some details for anyone interested or confused by my post.

I have a big house, and I put three of the bedrooms + access to all of the house on airbnb. The rationale is that it should be cheap enough for one person to go here, and attractive for families as well. The reason I want families or groups as well is that the net profit is bigger while the expenses only grows a bit (there's also the wear and tear etc).

Basically I wish for any group from one to six people to evenly book a room. So assuming every guest do the old ""divide total by how many we are"" would arrive at the same or near same price/incentive.

I see that a single guest will pay more, but that's just how it is seeing as I pay almost the same for expenses (laundry is the same price if it's just the laundry of one or six, cleaning is the same). It pays off to be two or more, that's fine because it pays off for me too, but right now the cheapest alternative is to be at maximum beds which suddenly negates my profit.

*sigh*

I know this is a wall of text, but it goes beyond the high school math I had...

And reddit gold for the answer that helps me out, if that makes any difference to you. Again, thanks in advance!"
Statistics Resources,1,3,False,False,False,statistics,1491440671,True,"Hey all,
I took Statistics 101 in college, and am now realizing I'll need to know more in order to do better within academia in terms of analyzing data and researcher's statistical analysis.

I currently have the next few months off free to pursue this interest part time. Where's a good place (online) to continue my statistics education in such a way as to receive a rounded skill set? I'm not sure what courses to start with next.

Thanks!"
T test help,0,1,False,False,False,statistics,1491444290,True,[removed]
Combining multiple confidence intervals for determination of a mean,17,10,False,False,False,statistics,1491474729,True,"Hello friends,

I am a researcher looking at determining EC50 values from cell assays. Basically, grow some cells, add varying concentrations of a compound and observe a response. Each experiment makes use of replicates and then we fit a curve to extrapolate the concentration which gives a 50% of the maximum response.

Because its biology and cells are fickle, we repeat the experiment many many times. Each experiment is an 'n' and the value is the fitted parameter from the curve fit. Now, typical practice is to take the mean of these values over several n and quote the SEM.

My issue is that this does not take into account the error/precision of the curve fit. If the EC50 value was obtained as a EC50 +/- SEM then I would think it would be reasonable to 'propogate the error' by using sqrt(sum of squares of individual errors), however, as it is a curve fit, the errors are not symmetrical and our fitting program gives us a confidence interval instead.

How can I go about determining the mean from several confidence intervals obtained for each n, and report it as a value within an overall confidence interval?

Im not a statistician though I appreciate that there are probably numerous ways of answering this question. A bit of google-fu suggested that I should be using a technique like MOVER or PropImp  [paper](http://www.tandfonline.com/doi/pdf/10.1080/03610921003764225) though this is beyond my understanding of stats...

thanks for any help!"
Can type 1 errors inflate a t-score rather than simply push it beyond the significance level?,0,1,False,False,False,statistics,1491488279,True,"Similarly, can type 2 errors deflate a t-score? 

I'm asking because I know of FDR correction - where we pick a threshold and set those corresponding t-scores to zero, but is there a type of correction that will just lower the t-score (or increase it for type 2 errors)?"
Can anyone review my MBA work and see if I am on the right track? It's a brief practice problem with JMP 13.,0,0,False,False,False,statistics,1491489821,False,[deleted]
Significance found in 1x4 ANOVA but not subsequent t-tests. Why?,2,5,False,False,False,statistics,1491492918,True,"Is anyone able to explain the rationale behind this? Why is that a 1-way ANOVA is able to detect significance across four groups but the subsequent t-tests do not? 

My understanding is that an ANOVA is testing all groups simultaneously so it's able to observe an overall significance that t-tests do not because they're only testing a single difference at a time. Am I on the right track?

Thanks in advance."
Online Statistics Tutor from Statisticshelpdesk.com Help Has Created a Special Niche,0,1,False,False,False,statistics,1491492954,False,
My statistics group is collecting data for a survey on people's political opinions. We though that the denizens of r/statistics would be willing to contribute. Thank you,7,0,False,False,False,statistics,1491497230,False,
Statisticshelpdesk.com Offers Best Systat Assignment Help,0,1,False,False,False,statistics,1491497337,False,
Online MS in Statistics,19,5,False,False,False,statistics,1491497384,True,First time post on this sub. Has anyone gotten an online MS in Statistics? I was looking at Texas A&M since they are one of the best stats program in the US. Was wondering if anyone went to that one specifically or has experience with any others. I'm trying to get into a data science career as some background info. 
piecewiseSEM: Exploring Nature’s Complexity through Statistics,5,6,False,False,False,statistics,1491500173,False,
Trying to get some assistance interpreting data. Three hypothesis are there.,0,0,False,False,False,statistics,1491500423,False,
ArXiv: Rethinking probabilistic prediction in the wake of the 2016 U.S. presidential election,6,3,False,False,False,statistics,1491501052,False,
Confused about ROC and the pROC package in R,0,1,False,False,False,statistics,1491502434,True,[removed]
Homework notation help (asking what a symbol means),2,1,False,False,False,statistics,1491502608,True,"Doing homework and I may have missed a class or two so I just want to clarify what [X_1:n and X_n:n](https://i.gyazo.com/307fcf3e0afcb28ccb650c730ebbfb5e.png) mean in this context. I'm assuming it's the lowest sampled value and the highest sampled value but if someone could tell me it would be really reassuring.

Thanks!"
"What does the term ""sample size"" actually refer to?",3,1,False,False,False,statistics,1491514011,True,"For example If i have 20 widgets whose length I am measuring one time each,  is my sample size n=20? If i instead measure the length of each widget 3 times is my sample size now n=60 or is it n=3?

Is each widget a ""sample"" or is the collection of measurements the sample?

Thanks for any help"
"Cramer' V, Phi, and Chi^2",2,4,False,False,False,statistics,1491514201,True,"I had not the greatest quant teacher, and I just want to clarify:  I know R or R^2 indicates correlation on the -1 to 1 scale.  Does Phi and Cramer's V basically do the same thing?  Correlation?  Same scale?  Please clarify! "
Type 1 or Type 2 errors and different group sample sizes,1,1,False,False,False,statistics,1491518244,True,"Hello everyone, Im in need of some that I can't seem to get a clear answer to.
Firstly heres some context, Im running a study on children due to time constraints and extraneous variables in participant recruitment I have uneven amount of participants in 2 of my groups.

My groupings are 
Year 7 and 8 (group 1) = 25
Year 11 and 12 (group 2) = 40

My question is does this lead to increased Type 1 or 2 errors and is there a fix? please help im desperate.
Also if there are any papers or articles that you can refer to that would be most helpful 
"
Tutorial: Conditional Probabilities and Bayes Theorem,1,1,False,False,False,statistics,1491519503,False,
"""Beyond subjective and objective in statistics"" (upcoming RSS read paper) [PDF]",1,1,False,False,False,statistics,1491521035,False,
What happened in from '96 to '97 that caused this spike in the amount of people diagnosed with diabetes ?,17,26,False,False,False,statistics,1491521341,False,
Factoring Massive Numbers: Machine Learning Approach - Why and How,1,2,False,False,False,statistics,1491530583,False,
ELI5: why are there different kinds of correlation coefficients?,4,6,False,False,False,statistics,1491536720,True,"Hi all. 

There are different kinds of correlation coefficients ... https://en.wikipedia.org/wiki/Correlation_coefficient

Some of them are specialized only for dichotomous (e.g. 0 or 1) variables. I *think* in that case its called a tetrachoric correlation coefficient but I could be wrong. Others might be only for positive integers. There might be another for all real numbers, I am not sure.

My question is, why is there a need for different correlation coefficients? Shouldn't there be a general formula that would give the correct CC regardless of the values that a (random) variable can take (I understand that this statement may beg the question, but the point is I don't know)?

Sorry if this is a dumb question. 

In case this is relevant - I am asking because I am interested in whether a generalization of the concept of linkage disequilibrium (https://en.wikipedia.org/wiki/Linkage_disequilibrium) would be possible to something beyond two genetic variants that can only take 2 values."
Anyone want to help me out with my senior thesis?,0,0,False,False,False,statistics,1491540887,True,[deleted]
I need a crash course in genetics. Can someone recommend a resource?,9,3,False,False,False,statistics,1491581189,True,"I'm working an a statistical problem involving genetics in the very near future. I'd like a (very) broad overview of genetics. I don't need to understand the applied problem in detail, but I'd like to have some idea what's going on.

Does anyone have any resources for statisticians trying to learn biology, specifically genetics?"
Do My Statistics Homework,0,1,False,False,False,statistics,1491581240,False,
"Controlling for the influence of a third variable in a three-arm, prospective RCT",0,1,False,False,False,statistics,1491581499,True,[removed]
Thinking of a statistics minor - which of these courses looks most useful?,6,2,False,False,False,statistics,1491583209,True,"Hi all. I'm thinking about doing a statistics minor, but being very new to statistics (have only taken the introductory course so far), skimming through the course catalog hasn't been particularly helpful, as I'm not entirely sure what the class titles mean or which classes would be most useful to someone like me. My main interest right now is psychology, and I'm starting to explore economics/business, so classes that are applicable to the social sciences are most appealing. 

My school offers a statistics minor, detailed [here](https://www.stat.ucdavis.edu/undergrad/minor.html). You have to pick one course from a list of eleven, and I have no idea which class would be best. 

My school also offers an [applied statistics major](https://www.stat.ucdavis.edu/undergrad/major/applied-statistics.html) (BA and BS, but I would go for the BA). It's about 72-86 units of statistics coursework plus three courses in a non-statistics related field with statistical application (I would pick psychology). Doing an entire major would be kind of a long shot - I'd only consider it if I was doing really well in the minor. But just for the heck of it, if I ever decided to go down that route, which courses from the list stand out as most useful to someone like me?

If anyone could skim through the list, that would be amazing. Thanks in advance! "
"What does it mean to ""Understand and able to evaluate statistical, temporal and geospatial data"" ?",10,0,False,False,False,statistics,1491586075,True,It's part of the description of a programming job I'm interested in applying to.
Any statisticians who know cricket over here? Would love a bit of help with a little stat I created.,3,1,False,False,False,statistics,1491587520,True,"So, I am to create a few numbers to give information about the distribution of 100s over a batsman's career. 

The first thing is obviously the frequency: the average number of innings per hundred. 

The second thing I want to measure is the consistency with which the batsman scores tons. 

For this, I collected a list of the gaps between consecutive hundreds of a batsman. 

The average of this set of gaps is obviously the above defined frequency. 

I plan to use the standard deviation of these gaps to measure the consistency in scoring tons. 

My question is this:

Is the SD by itself a good measure, or will it depend on the size of the batsman's career? If it will, do I need to normalise it by the length of the career to make it more meaningful?

That is, is a batsman with 200 innings and an SD of 4 in the gaps of his centuries more consistent than a batsman with an SD of 4 but only 100 career innings? Or is he less, or equal?

The SD in the gaps between the innings should be okay, I guess. But I have my doubts. 

I'd love it if someone thought about this and had a debate about whether this SD measure needs normalising. 

"
Method to determine if events are coincidental or based on common cause?,0,1,False,False,False,statistics,1491589825,True,[removed]
Do not underEstimate the dark side,14,143,False,False,False,statistics,1491592086,False,
Sample size required to detect specified uplift in a/b test,0,1,False,False,False,statistics,1491593216,True,"I've been looking at calculators and formulas online and they seem to calculate different results based on the site or calculator I use.

My problem is pretty simple. I have a conversion rate of 4.5℅ . I want to know how many visitors I need to the site to be able to detect a 10℅ change in conversion rate with a 90℅ confidence interval.

What formula do I use?"
Any know of any good free datasets to for a multilevel analysis project?,0,1,False,False,False,statistics,1491596830,True,[deleted]
Anyone know of a good free multilevel dataset that I can use for a project?,2,1,False,False,False,statistics,1491598857,True,"Ideally a minimum of two levels but three-level data would be cool too!
"
ARIMA model order from ACF and PACF plots,5,2,False,False,False,statistics,1491611024,True,"So I'm totally new to time series analysis and I'm struggling with identifying the order of a model from the PACF and ACF plots. 

I've already differenced the data I had as it had a yearly seasonal component, so the following are plots of the differenced data, including the ACF and the PACF. http://i.imgur.com/8zWCCyB.jpg

Now I know there is a way of determining the order of the ARIMA model by looking at the ACF and PACF plots, but even after looking at examples I can't figure this out. If it helps, I'm doing all of this in R. I know my code needs to be of the form:

arima(ldiff, order=c(p,d,q), seasonal=list(order=c(P,D,Q), period=12))

but again, I'm having trouble with determining the missing values.

Thanks in advance!"
How to find a P-value using Chi-squared and the degrees of freedom?,3,2,False,False,False,statistics,1491613716,True,"I'm finding online calculators, but no actual formula. I also am not really understanding what they are asking me here, what I am computing? 

Any guidance would be appreciated. Thanks."
VAR,0,1,False,False,False,statistics,1491627796,True,[deleted]
Spatial analysis and hierarchical models,4,4,False,False,False,statistics,1491637270,True,"Hey I need help picking a book. 

What do folks think of:

An Introduction to R for Spatial Analysis and Mapping


Applied Spatial Data Analysis with R (Use R!)

Hierarchical Modeling and Analysis for Spatial Data, Second Edition 


Statistics for Spatio-Temporal Data

Any insights on the usefulness of these? Any other suggestions?

"
"Calculating the probability of a romantic relationship ""lasting"" given its current length?",0,1,False,False,False,statistics,1491642239,True,[removed]
PHP Assignment Help | PHP Project Help,0,1,False,False,False,statistics,1491652492,False,
How to read these SPSS results?,0,1,False,False,False,statistics,1491654780,True,[removed]
Tips on understanding biostatistics/stats literature,7,15,False,False,False,statistics,1491658846,True,"I'm considering getting into biostatistics but when I try to read specific research articles (roughly half the time) I find myself struggling to keep up. 

I'm from a biology background but I've taken math (linear algebra, calc 3, proofs) and stats classes (math stats I, regression analysis) so I worry if this is sign I shouldn't pursue stats. I do however like reading stats blogs, textbooks, and lecture notes. 

Why tips do y'all have for reading statistics articles?

Thanks

*Also I didn't know whether to post in AskStats since it's not really a hw question but I apologize if it belongs there."
Please help!,0,0,False,False,False,statistics,1491659985,True,[deleted]
Advice on customer frequency analysis,0,0,False,False,False,statistics,1491662494,True,"Hey all,

Not really sure the best subreddit for this question but maybe someone here can help. 

I am wondering if anyone has tried to estimate customer frequency and purchase patterns by using the first 6 and last 4 of the credit card a customer used to purchase an item. I am not sure if they are unique enough for it to reliably work. 

Any input or advice would be appreciated! "
"Lies, Damn Lies, and Financial Statistics",16,53,False,False,False,statistics,1491667368,False,
How to approach the probability of a member of a group winning a lottery?,1,1,False,False,False,statistics,1491670924,True,[deleted]
What the hell do these numbers mean?,3,9,False,False,False,statistics,1491678200,False,
Easy one for you,1,1,False,False,False,statistics,1491679389,True,[removed]
Survey for college students 18+.,0,1,False,False,False,statistics,1491694176,True,"This survey is to collect data for an experiment to see if college adults mostly live with their parents, or on their own.

https://goo.gl/forms/hpShJvZiYUjPUFrG2

Thank you for your time!"
Grad school question - Differential Equations,0,1,False,False,False,statistics,1491696095,True,[removed]
First Stat Book for Independent Study?,5,7,False,False,False,statistics,1491698002,True,Looking to study stats and math on my own. What are your recommendations for my first stat book to start?
Advanced Statistics MOOC,0,1,False,False,False,statistics,1491699369,True,[removed]
What kind of statistical test should I run?,4,0,False,False,False,statistics,1491705476,True,"Hi!
 
**My data:** I have collected the salaries of CEO's for 20 companies, I have also collected those company's revenue for the same years.
 
I'm wanting to test if there is any relation between the adjustments in salaries compared with changes in revenue. What kind of test should I be running?"
Is a ttest appropriate?,0,1,False,False,False,statistics,1491711073,True,[deleted]
Statistics Homework Tutors - Differential Equations Assignment Help,0,1,False,False,False,statistics,1491739651,False,
Data Models And Decisions Assignment Help,0,1,False,False,False,statistics,1491740261,False,
Computer Simulation Of Complex System Assignment Help,0,1,False,False,False,statistics,1491740947,False,
Computational Mathematics Assignment Help,0,1,False,False,False,statistics,1491741709,False,
Does anyone have the Theil.sas file mentioned here? The link does not work.,0,0,False,False,False,statistics,1491745619,False,
D-Prime or A-Prime??,20,5,False,False,False,statistics,1491745970,True,"Context of experiment

So i'm running an experiment where I have participants go through an initial encoding phase where they place items presented to them in the middle of the screen with either a red border or a blue border in to either the red shopping basket or the blue shopping basket. They are told at the beginning that everything they sort into a one of the baskets (either red or blue it changes) belongs to them and everything they sort into the other basket belongs to another individual (30 self trials, 30 other trials). There is then a test phase where the participants have to respond to a series of items and say whether they have seen them before and if it belonged to them self, or if it belonged to the other person or if its an entirely new item. so there are 30 self items, 30 other items and 30 new items.

Its basically signal detection, however i've run into a problem where the two groups that im looking at has an uneven sample size group 1 has 25 participants and group 2 has 40 participants.

I've also been told to look into to a-prime instead of d prime and see whether this does anything but theres not much information about it and im not sure whether its suitable. Could you help?

As someone who is a novice, but wishes to analyse my data with the most robust stats, what should i be using to improve the reliability and validity of my results and also how should I get around the fact i have different group sample sizes.

Thank you for your responses in advance"
Any books on concepts of statistics?,5,2,False,False,False,statistics,1491748817,True,"Does anyone know of a book dealing more with concepts often seen in statistics like

* Multiple comparisons problem
* Survivorship bias
* Hypothesis testing

The best book I have found to talk about concepts learned from statistics, without delving into the math of it is probably ""How to lie with statistics"", which I highly recommend.

"
Polynomial and interaction terms in regression in STAN?,0,3,False,False,False,statistics,1491764136,True,"I'm feeling silly for having to ask this, but here goes. I wrote a ZIP model in STAN (STAN code here: https://pastebin.com/3DSm564R), which I run through R. The basic predictors I pass it are continuous, and scaled. When I try to also pass it quadratic predictors, or interaction terms (created in R), I get lots of divergent transitions that aren't resolved through increasing adapt_delta or any other control arguments.

I'm wondering if the issue is caused by my way of centering and scaling (R code here: https://pastebin.com/XNUNT19V). The example run in the code works, but as soon as I add one of the quadratics I get divergent transitions. My data set consists of 30,000+ observations.

What am I doing wrong (fairly new to Bayesian statistics)?"
"What's the consensus on ""Master of Data Science"" vs a regular MSc in Statistics?",33,31,False,False,False,statistics,1491765170,True,"I did a quick search and couldn't find anything, so apologies if it's a repost.

So I was planning to go abroad for my masters, preferably Australia. A quick search showed that not many universities offered a traditional MSc in Statistics, however most did offer ""Master of Data Science"" degrees. How do employers view these programmes? One of my professors here in Canada thought it wasn't worth it, and was a waste of money. Some of the programs:

[Western Sydney](https://www.westernsydney.edu.au/future-students/postgraduate/postgraduate/postgraduate_courses/information_technology_and_computing_courses/master_of_data_science/admission_and_unit_information_-_master_of_data_science)

[South Australia](http://programs.unisa.edu.au/public/pcms/program.aspx?pageid=3144&sid=5278)

[Deakin](http://www.deakin.edu.au/course/master-data-analytics)"
"Book on Bayesian statistics for a ""statistican""",8,3,False,False,False,statistics,1491768112,True,"I have a degree in something closely related to statistics and took a bunch of statistics courses in university, but I never felt I got a real education in Bayesian statistics. I never took a course specifically on Bayesian statistics (nor did my university have one), and while i encountered a lot of Bayesian-inspired methods and models they were always treated in a way that assumed little direct knowledge of Bayesian statistics.

I now want to get a proper introduction to Bayesian statistics and have looked at a couple of books, namely Doing Bayesian Data Analysis (by Kruschke) and Bayesian Data Analysis (by Gelman). Since I'm not in academia anymore I'd like something more applied but also not something that assumes I'm a computer scientist or biologist (or similar) with no real knowledge of statistics. Are any of the mentioned books suitable, or does anybody have better suggestions?"
"Self teaching mathematical statistics for data science, machine learning",1,2,False,False,False,statistics,1491771991,True,[deleted]
Expected value and variance,5,0,False,False,False,statistics,1491773502,False,
TFW someone tells you that all models are wrong,1,0,False,False,False,statistics,1491774978,False,
Poll to determine what social media is most destructive to self image,5,2,False,False,False,statistics,1491775835,True,"I'm taking a poll for my stats class and not enough people are responding. I understand the bias I get by posting on specific websites, but if push comes to shove, I do need numbers for credit. Please help? The results could also be legitimately interesting, though everyone who answers now will be using specific websites in the first place. "
Hey! If anyone is bored try to find a pattern between these answers. If been looking for a while and if there is it could be of great help!,4,0,False,False,False,statistics,1491776366,False,
Merits of a multi-model classification approach?,3,3,False,False,False,statistics,1491779351,True,"Hi


Suppose I have a classification problem with outcome categories: low (L), medium (M), high (H) and very high (VH). How common is it to follow a modelling approach where I first pool together some categories (say H and VH) and train a classification model, and thereafter use a second classification approach (the same or a different method) to further classify the outcomes in the pooled category into, say, H and VH? In other words, are there any practical issues stopping me from employing a multi-model approach to classification problems rather than selecting one model with all outcome categories?


What are some advantages/disadvantages of an approach like this?"
Reddit demographics,0,1,False,False,False,statistics,1491784601,True,[removed]
What is the difference between a contextual effect and a context effect?,1,1,False,False,False,statistics,1491789198,True,
Trying to prove a result in statistics about the Sum of Squares Between Groups from a one-way-ANOVA,2,3,False,False,False,statistics,1491790264,True,"I'm very confused on this statistics problem I have to do...I need to prove a theorem:

The theorem states the following:

SS(B)/{sigma^2 } follows a chi-square distribution with n-1 degrees of freedom.

SS(B) is the sum of squares between groups in a one-way-ANOVA sample, sigma^2 is the variance of all of the data collected.

I think I need to write out SS(B) as a summation, which is given to me in my notes and stuff...then I need to manipulate it

My question is: how do I show something follows a chi-square distribution in general?

I would need something that follows N(0,1) first right?

Then I square this, and get chi-square with 1 degree of freedom...but then the summation for SS(B) given to me is adding 'I' things together, so I am thinking it should be chi-squared with I degrees of freedom but I know the book isn't wrong...

I hope this isn't completely jibberish. Please help me out.

The question is from Chapter 12 of ""Mathematical Statistics and Data Analysis"" by Ross

Here is a picture of the exercise and a couple paragraphs to better explain: http://imgur.com/a/uBXOi"
Effect modification vs. confounding,1,1,False,False,False,statistics,1491796222,True,"Every time I read about them, I seem to understand the difference .. for about an hour. Can somebody explain it  in a way that can be remembered."
"Help choosing which masters program to attend (UCSB, Davis, Carnegie Mellon)",19,3,False,False,False,statistics,1491798752,True,"I am going to be attending graduate school next year for a masters in statistics, and these are the programs I have been accepted to so far. I was rejected from Cornell, and am still awaiting a response from UCLA for a Masters in Applied Statistics and MS in Statistics in Columbia.

 

Here are the pros + cons of each school for me.

Carnegie Mellon Masters in Statistical Practice. It isn't a traditional MS degree, so I don't know if that closes any options for me. It seems highly applied which I like, and is only one year long. And most importantly the name of the school seems to go far. But it will be 45K in tuition and I will have to move to Pennsylvania. (I am from California.) Industry placement seems great though but I am sure it will be very competitive.

 

UCSB MA in Statistics. This is my undergraduate institution. I am familiar with the campus the area and everything. I love the school. My friends, frat, social circle, professors, family are all within reach to me. I will also be offered a TA position, most likely. I hear the program is highly applied, but the rank of the school isnt the highest. Also people suggest not to go to the same school for graduate school as they did for undergrad so i don't know how that goes.

 

UC Davis. Pretty high ranked and it will take 1 to 1.5 years to finish. seems like a strong program and I will still finish before two years and may still have a summer to look for internship. Also in California so tuition is same as for UCSB (About 15K a year)

 


All these schools seem to have great advantages. Anybody have an idea of what they think is best?


EDIT: Received acceptance to UCLA MAS too. I can finish it in two years while working part time. And stay in California. WHAT DO I DO THIS IS SO HARD."
10 stats to encourage your belief in Social Media,0,1,False,False,False,statistics,1491820344,False,
Exact Sampling Distribution Assignment Help Statistics Homework Tutors,0,1,False,False,False,statistics,1491821248,False,
Economics Assignment Help Statistics Homework Tutors,0,1,False,False,False,statistics,1491821849,False,
Financial Risk Analysis Assignment Help Statistics Homework Tutors,0,1,False,False,False,statistics,1491822842,False,
Economic Statistics and Official Statistics Assignment Help,0,0,False,False,False,statistics,1491824197,False,
Ergodic Theory Assignment Help,1,0,False,False,False,statistics,1491825008,False,
Exploratory and Robust Data Analysis Assignment Help,0,0,False,False,False,statistics,1491825774,False,
Help with stats homework?,2,0,False,False,False,statistics,1491837941,True,[removed]
Where can I find a good introduction to Bayesian Statistics?,11,15,False,False,False,statistics,1491841705,True,"So I just got an introduction to Bayesian Statistics in my stat class. It's not something we cover very in depth, but I found it really interesting. What are some good books that can give me a good introduction to this? "
Shah and Bühlmann: Goodness of fit tests for high-dimensional linear models,0,2,False,False,False,statistics,1491844527,False,
F-value won't show up for ANOVA on SPSS V. 23?,0,1,False,False,False,statistics,1491859737,True,[deleted]
Calculating the chance of getting beat up on an United flight?,27,39,False,False,False,statistics,1491863308,True,[removed]
How would you run normality test on signal detection data,1,1,False,False,False,statistics,1491865982,True,"Hi I've ran a signal detection memory experiment, where participants first sit through trials sorting items that belong to them self or to someone else. They are then in a test phase where they see the items agains (and some new) to which they have indicate whether the item they see belonged to them self, other or it's a completely new item. 

My problem is I have 2 groups and group 1 has 25 participants and group 2 has 40 how would I run a normality test to see if my data is normally distributed. I know researchers normally just assume this anyway and parametric tests can sort of account for this but for my project I have to justify everything and I feel like I'm doing it wrong so needed some clarification. All help is much a appreciated xx"
Best free resources to brush up on stats? • r/college,0,0,False,False,False,statistics,1491867328,False,[deleted]
Will changing the name I put on my publications seriously harm my career?,1,0,False,False,False,statistics,1491883954,True,[deleted]
Buy Essay Online Statistics homework help | Assignment Writing Service-Tutorspoint.com,0,0,False,False,False,statistics,1491886788,False,
Data Science Undergrad Senior needing advice on PhD!,6,1,False,False,False,statistics,1491897722,True,"Hi! I'm about to graduate from undergrad in data science (concentration in computational analysis) and I really want to pursue a PhD in statistics-- particularly a joint program in statistics and machine learning. I'm looking to work for a year or 2 in a data science job and then apply. Reddit, how do you think I can get into a top statistics phD program? Do I have what it takes? And if not, what are some suggestions/advice on how to be competitive? 

 Here is some coursework/strengths/weaknesses I've identified:

**Relevant Major Coursework**

* Applied Probability
* Applied Stats
* Linear Algebra
* Calc I & II
* Discrete Math
* Intro to Machine Learning
* Econometrics
* Computational Biology
* Data Structures & Algorithms
* Software Development 
* Intro to Programming I & II
* Data Visualization

**Strengths**

* GPA 3.7/4
* research publication
* 3 research assistantships: (2) machine learning applications, (1) open source contribution
* Will have data-related work experience before applying
* Passionate about the field!!

**Weaknesses**

* Haven't taken real analysis
* Haven't taken calc 3
* Haven't taken differential equations
* 75% Percentile on quant GRE (Not good at taking exams. I'd like to retake this soon)


"
Frequence Distribution Graphical Representation Assignment Help,0,1,False,False,False,statistics,1491905611,False,
Industrial Applications of Stochastic processes assignment help,0,1,False,False,False,statistics,1491906333,False,
Intermediate Statistics Assignment Help,0,1,False,False,False,statistics,1491907463,False,
Fourier to Wavelets Assignment Help,0,0,False,False,False,statistics,1491909031,False,
Index Numbers Assignment Help,0,0,False,False,False,statistics,1491910070,False,
Infinite Random Matrix Theory Assignment Help,0,0,False,False,False,statistics,1491915547,False,
Need some help with sampling error,0,1,False,False,False,statistics,1491916050,True,[removed]
Model assistance,13,4,False,False,False,statistics,1491925811,True,"I am working on a project at work to try and identify the combination of variables that is causing a product to be scrapped. I have a data set that is 74 different production variables and 1 item that I am trying to understand. This is really embarrassing because it seems like Operations Management 101, but I cannot figure out how to get it done. I have run regression modeling in Spotfire, but I cannot think of where to go from here. Any help would be awesome."
regression analysis in key factors driving market share,5,3,False,False,False,statistics,1491931151,True,"hello! person with extremely pedestrian understanding of statistics here with a statistics question. the field i work in sells products where market share is determined by several measurable key factors (product performance, pricing, size of sales force, etc.) i'm interested in regression-testing these factors as a determiner of future market share - i.e. if we look at results internally (or externally using industry data), can we infer the relative importance in each factor as a way to shape future business strategy?

to my question: given above, is there a specific area or method of regression analysis where i should start? something relatively simple or a way to execute within excel would be ideal. thanks!"
Intrro to stats midterm - looking for an hour assignment to make sure i pass! $100/hr,3,0,False,False,False,statistics,1491933899,True,[removed]
"Where can I find specific data sets? An example would be 50 observations, 5 variables, 2 categorical, 2 quantitative.",7,1,False,False,False,statistics,1491934739,True,"What is listed in the title is just an example but I need a data set thats 50 items and 5 observations. 

Edit: More so looking for where to find data sets like these. Not looking for someone to just give me a set."
I need help with easy stats college homework,0,1,False,False,False,statistics,1491935131,True,[removed]
Devising a scoring statistic to test a sample’s species designation based on Bowtie2 local gene sequence alignments?,1,1,False,False,False,statistics,1491935654,True,"I am conducting Bowtie2 pairwise local alignments between a species-specific reference gene sequence, and 3 sample sequences suspected of belonging to that species.

I will also be mapping each of the 3 samples against 3 other species-specific reference sequences for the same gene using Bowtie2.

The default Bowtie2 alignment scores of a particular sample’s sequence reads to its own species' reference gene sequence should theoretically be qualitatively better than the alignments against other species references.

Based on this, how would I device a scoring statistic that measures the difference between the alignment scores (sample mapped against own species reference vs. other reference sequences) to confirm that the sample sequence belongs under the species classification it is currently assigned, and not to one of the other species?

Hope this made sense!
"
I want to be a statistician. Can someone please guide me?,0,1,False,False,False,statistics,1491936252,True,[removed]
[STATS T-value question],4,0,False,False,False,statistics,1491936297,False,[deleted]
How do I describe the results of my two sample t-test?,1,0,False,False,False,statistics,1491938314,True,"Currently undertaking my dissertation. I have performed all my statistical analysis that I require. I've managed to write up the results of my ANOVA's. However, I'm not quite sure what to write for my 2 sample t-test. I just need to guide the reader through my results."
Resampling data with errors,0,1,False,False,False,statistics,1491945397,True,[removed]
Figuring out Sectioning Points,0,2,False,False,False,statistics,1491953239,True,"I'm working on a project where I have to work with functions and their Standardized Canonical Discriminant Function Coefficients, Canonical Discriminant Function Coefficients, and Centroids. I managed to get this information from entering my data into SPSS, but I'm confused to where to find the Sectioning Points from my data.

Any help would be appreciated."
Statistics Question,0,1,False,False,False,statistics,1491953304,True,[removed]
Why is the assumption of normality satisfied if a sample size is large?,21,5,False,False,False,statistics,1491966074,True,"I learned in ap stats last year that in a two sample t-test, the assumption for normality is passed if the combined sample size is greater than 30. If i take 500 samples from a population that is skewed right or left, i would still pass normality, even though the population is not normally distributed. So why does a large sample size satisfy the assumption of normality. 


from a pdf uploaded by minitab: ""Sample data should be normally distributed (although this
assumption is less critical when the sample size is 30 or
more).""
[its on page 1-18 on the right side](http://www.minitab.com/uploadedFiles/Documents/sample-materials/TrainingTTest16EN.pdf)
 "
Need help choosing the right statistics test,16,7,False,False,False,statistics,1491968264,True,[deleted]
Statistics Assignment Help with Best Quantitative Solutions,0,1,False,False,False,statistics,1491980512,False,
Life Insurance Homework Assignment help,0,1,False,False,False,statistics,1491995934,True,[removed]
Monthly residential statistics for March 2017,0,0,False,False,False,statistics,1491997157,False,
Linear Algebra online help,0,1,False,False,False,statistics,1491997206,True,[removed]
Office for National Statistics Has Not Been Including Night Arrivals in Migrant Stats,2,0,False,False,False,statistics,1491997221,False,
Campus alcohol statistics shift under new policies,0,3,False,False,False,statistics,1491997582,False,
"If Flipkart and Snapdeal merge, these are the vital statistics that will matter â Quartz",0,1,False,False,False,statistics,1491997654,False,
"Severely flawed slot machine experiment, unsurprisingly, reaches incorrect conclusion",4,27,False,False,False,statistics,1491997806,False,
lisrel assignment help,0,1,False,False,False,statistics,1491997977,False,
Management Applications of Optimization online help,0,1,False,False,False,statistics,1491998660,True,[removed]
D' Prime correction to use with small or unequal sample sizes between the groups?,2,2,False,False,False,statistics,1492004998,True,"I have unequal sample sizes between my two groups (group1=25, group2=40) I need to transform my data to d' prime however I have many false alarm scores of 0. I want to correct this however a friend told me that you should use a specific correction based on your sample sizes however I can't find a clear answer anywhere, so I was wondering whether anyone would know if this is true. If this is true, what is the true correction i should use for my small sample size and which paper mentions this so that I can cite it for my paper.

Thank you in advanced"
Markov Chains & Kalman,0,1,False,False,False,statistics,1492006448,True,[removed]
Interesting TED Talk on How Pollsters and Statisticians Got the 2016 Election Wrong,10,0,False,False,False,statistics,1492009951,False,
Bartlett's test,4,0,False,False,False,statistics,1492011372,True,"Hey guys, quick question concerning bartlett's test:

Is it used when normality holds or when it doesnt hold? Or is it a test for normality?"
Linear Regression - Weighting my Sample/Observations,4,1,False,False,False,statistics,1492033370,True,"I am having troubles figuring out how to weight the observations in my sample for linear regression.

To give full details I have a simple demand model for beer, and I have the products broken up into domestic, craft and import beers.  I  have been told I should run a regression including all 3 segments but ""where the observations are weighted by sales volume or market share"".  It's panel data with 4,300+ cross sections for craft and only ~100 for domestics so the regression is being skewed by the large # of craft observations.

I am just having troubles figuring out the best way to do this correctly?  Should I be manipulating the data itself?  Is there a specific procedure I should be implementing?  Any advice is greatly appreciated.

Thanks!"
Is it wrong to create an interaction term with an ordinal and continuous variable?,8,7,False,False,False,statistics,1492040053,True,"The norm (at least as far as I'm aware) is usually interactions of continuous X continuous, continuous x dummy, and dummy x dummy. However, is it also possible to generate an ordinal x continuous? I ask because I'm running a multivariate regression where the addition of a 0-10 scale enhances the effects of an ordinal (1-5) measure. If possible, I'd like to interact them...or is this ill-advised? "
Garch with GED,1,3,False,False,False,statistics,1492045022,True,"Hi,
can anybody tell me how do i know whether to use Gaussian or GE distribution when estmating a GARCH model?
Also, what would be the theoretical and practical differences in interpretation?
"
normalization of angles,5,7,False,False,False,statistics,1492049583,True,"I'm currently finishing up my dissertation for grad school but have run into an issue with my data set.  I have cling angles from frogs (the angle at which they slipped from a platform) that range from 0-180.  There were several species that were able to cling at 180 and of course this throws off the normality of the data because there is a heavy tail.  

Does anyone have any advice on how to correct for this so I can normalize the data and run parametric statistics?  It doesn't seem like any of the transformations that I'm familiar with help."
OpenIntro Statistics - Even Questions,1,1,False,False,False,statistics,1492055730,True,[removed]
Help finding the mean and the standard deviation,4,0,False,False,False,statistics,1492063929,False,[deleted]
Confidence Interval,15,7,False,False,False,statistics,1492072699,True,"Hello,

I am in need of some help.

I want to be able to test the following with a 95% confidence interval:

So let's say a test group of 10 000 individuals averaged 4,5 visits to a store during a time span and a control group of 500 individuals averaged 4,3 visits during the same time span. Using a 95% confidence level can it be proven that the test group on average visits the store more often that the control group?

If this post belongs to another subreddit please let me know."
Sample size for use of a 'liberal' p value of 0.1,13,1,False,False,False,statistics,1492076291,True,"Hi there. I have run a multiple regression with a sample size of 48. The regression model isn't strong (which my professor has said is fine). 

In his comments on my assignment he has asked me why p > .05 indicates non-significance. In some of his slides he said smaller (but not tiny) sample sizes may be better using a liberal p value. 

What do you think? "
Methods of regression Analysis Assignment Help,0,1,False,False,False,statistics,1492078336,False,
How to quantify how much better population A than population B?,8,1,False,False,False,statistics,1492078447,True,"This may be a rather trivial question, but I'm not a big stats guy.  Say I have conducted an experiment and have two populations and I have calculated their mean, std deviation, and confidence intervals about the mean. Those intervals do not overlap, so I can be reasonably confident that the two populations are indeed different.

But I want to know much ""better"" pop A is than pop B (to some level of confidence). How do I do that math? Ultimately I want to be able to say: with 90/95% confidence, A was 30% better than B; with 50% confidence, A was 60% better than B.

Is it just comparing two normal distributions around each mean with each std dev?"
Statistics Homework Tutors- Multilevel Longitudinal modelling Assignment Help,0,1,False,False,False,statistics,1492079290,False,
Statistical Computing Online help.,0,1,False,False,False,statistics,1492081457,True,[removed]
multivariate Analysis online help.,0,1,False,False,False,statistics,1492082317,True,[removed]
Nonlinear Dynamics Analysis online help,0,1,False,False,False,statistics,1492082918,True,[removed]
What is B?,0,1,False,False,False,statistics,1492089679,True,[removed]
Portable hand held pressure washer,0,0,False,False,False,statistics,1492096552,False,
Weighting average outcomes,0,1,False,False,False,statistics,1492097128,True,[deleted]
Scalability of generic statistical analysis in big data,0,1,False,False,False,statistics,1492105465,True,[removed]
How to calculate correlation matrix of state polling errors for general election predictions,0,0,False,False,False,statistics,1492107640,True,"Hi all,


For a project of mine, I am attempting to build a simulation and polling-aggregation model for predicting the previous election, that I can then hopefully use for future elections. I am currently struggling with how to calculate state polling error correlations.



My only thought is to go back and scrape historical state polling data for previous general elections, however I am not sure where I can find this. (I assume I can find the general election results for each state easily). Other than this approach, does anybody else have any other ideas that might make sense for coming up with state correlations?


Thanks!"
How large of a sample size do I need to create an accurate standard deviation?,20,17,False,False,False,statistics,1492107899,True,"sorry if this is a basic or obvious question here, but I've been doing some work with ecommerce stores in AB testing. I need to know how how many samples (purchases in this case) are necessary to determine the standard deviation of a product line. This company sells tens of thousands of items. 

thoughts?"
TI-84 Confidence Interval question,1,0,False,False,False,statistics,1492110893,True,[removed]
Help with some ANOVA?? (Maybe),0,1,False,False,False,statistics,1492113933,True,[removed]
"I came up with a conjecture, any ideas to prove or disprove it.",0,1,False,False,False,statistics,1492123128,True,[removed]
Question about ANOVA Hypothesis Testing,1,5,False,False,False,statistics,1492139006,True,"I have a general question. Is it possible to perform a hypothesis test on proportions of a group of samples with an ANOVA test?
For example, can an ANOVA test be used to approach the following hypothesis test:
H0: P1 = P2 = P3 = P4... = Pn 
H1: One of the proportions is not equal to the others.

If so, how?

Thanks in advance!"
Calling all STATISTICIANS!!!!!!!,3,0,False,False,False,statistics,1492148822,True,[deleted]
Non Parametric Test Assignment Help,0,1,False,False,False,statistics,1492165491,False,
Operations Research Assignment Help,0,1,False,False,False,statistics,1492166266,False,
Numerical Methods for Chemical Engineering online help,0,1,False,False,False,statistics,1492166467,True,[removed]
POM Assignment Help,0,1,False,False,False,statistics,1492167035,False,
Probability & Statistics for Engineers online help,0,1,False,False,False,statistics,1492168744,True,[removed]
Statistical Significance Is Overrated,26,27,False,False,False,statistics,1492168839,False,
What's with SOME of the animosity that epidemiologists have towards biostatisticians,4,0,False,False,False,statistics,1492179275,True,"Maybe it's just at my grad program, but the epi classes that I took always revolved around the epi faculty going on long diatribes about how narrow minded stats people are and how ""wrong"" we are. I started as an epi concentrator, but switched to stats because I was starting to get frustrated with how vague some of their concepts were and how one professor said this, but another said thing.

I'm in my last semester but once again, the typical argument of how wrong stats people are came up and I just want to scream.

It could probably just be my school.."
Odds with 3 phase power,0,0,False,False,False,statistics,1492181833,True,[deleted]
Might be a question about p-value? Regarding certainty of x being within y range.,1,1,False,False,False,statistics,1492182483,True,[removed]
Ammo Reloading and Statistics,2,2,False,False,False,statistics,1492183639,True,"So people have been using chronographs (measures muzzle speed) and group size for determining the best Powder/powder weight/bullet/brass/primer for their rifle. The goal for long range shooting is to get a very low SD in Muzzle Speed because small variances in Muzzle Speed can make very large differences down range because the bullet slows due to drag. People who shoot short distances will shoot for the best group size.  

   

In the shooting community you will see people post on forums with something along the lines of:  
I found my best load!  
<shows image of groups with text>  
5 shots .5 MOA (Group size in angular measurement), 8 ft/s SD  
5 shots .6 MOA (Group size in angular measurement), 10 ft/s SD  
5 shots .6 MOA (Group size in angular measurement), 12 ft/s SD  
5 shots .7 MOA (Group size in angular measurement), 15 ft/s SD  
The person whos makes this post is saying that they have discovered the best combination of reloading components for their rifle, which they believe is ""5 shots .5 MOA (Group size in angular measurement), 8 ft/s SD""  

   

I was doing some googling about SD and found the following article: http://www.arcanamavens.com/LBSFiles/Shooting/Downloads/EnglemanChronographStatistics.pdf  
It had a section on something called confidence intervals where he explains that those 5 shots are not a population, they are a sample of a future population that has not occurred yet and that you can't use mean, Standard Deviation and Variance but instead you must use Sample Mean, Sample Variance and Sample Standard Deviation and once those are calculated you must use Confidence Intervals to determine the range of which you are 90% or 95% confident that the actual population SD falls within.  

   

Using https://graphpad.com/quickcalcs/CISD1.cfm (I am a bit confused with the N and SD on this calculator as I thought it was n (Sample Number of Values) and S (Sample Standard Deviation), assuming this is the correct calculator to use, you get the following:  
SD = 8, N = 5 -> 90% 5.19 to 18.98  
SD = 10, N = 5 -> 90% 6.49 to 23.72  
SD = 12, N = 5 -> 90% 7.79 to 28.47  
SD = 15, N = 5 -> 90% 9.74 to 35.59  
When looking at this, one can't reasonably say that the group with Sample SD of 8 is the best because the population SD could actually be 18.98. Am I wrong in thinking one would need to shoot more rounds to the point where you get a ""max of your lowest SD confidence spread"" is less than the ""min of the next best SD"" to be sure that one is better than the other?  

   

Couldn't the group size be represented as a radial distance to the center of the group (converting something that is 2 dimensional to 1 dimension)? That way the same functions could be applied to group size as it does with Muzzle Speed, along with the same incorrect predictions of future groups when using such a small sample size as 5?  

   

Am I retarded? Is there something that I am grossly overlooking or is the reloading community committing a crime against statistics when making statements like the above ""I found my best load!"" with groups of 5?  "
"We Will Sort you out with Your Statistics Online Quiz, Trust Us",0,1,False,False,False,statistics,1492183729,False,
4 Qualities You Can Rely On Research Methods Assignment Help By Statisticshelpdesk,0,1,False,False,False,statistics,1492186488,False,
How to see whether another variable has an effect?,9,1,False,False,False,statistics,1492187653,True,"Hello sorry for the basic question but I can't work out whether I was doing it correctly. I currently have a 2x2 mixed ANOVA, I didn't find significance but I wanted to see whether another variable (self-esteem) would explain any of the noise. Its not meant to be part of my main analysis I just want to write up that I tested to see if it influenced anything or had an effect.

So my question is how would I go about doing this? Thank you in advanced you will be helping me alot. 
"
Graduate school in Stat... or Biostat,15,16,False,False,False,statistics,1492202901,True,"Hi I'm torn between choosing a MS in Bio-Stat at a Health Institution or a MS in Stat in a academic institution. 

My eventual goal is to go for a PhD in statistics, hopefully out of state (or even country). The ranking of either school is relatively the same. 

Currently I'm leaning more towards the biostat option but I'm afraid that I will be limited to just biostat in the future. Whereas if I just do regular stat I will be more open to options in the future. 

Please any advice is welcomed."
"Someone recently told me that without calculus, I couldn't truly understand statistics. How accurate is this statement?",33,50,False,False,False,statistics,1492212535,True,
Just had a really rough semester and I'm becoming worried.,6,0,False,False,False,statistics,1492213556,True,"I'm an undergrad stats student and just had a very poor semester. I didn't fail anything but I got a D in multivariable calc and a C in linear algebra. However I got an A in my sql class. My overall GPA is 3.3

I'm just worried about the implications of this semester. I know these classes are very important. But was this semester a death sentence for me? I want to get a masters in biostats. 

Sorry for this post I'm just really seeking direction and very stressed. "
Compare this map to the 2016 Electoral college map.,0,2,False,False,False,statistics,1492214164,False,
"I'm realizing I probably won't ever be a great, sought after statistician, but I'm very outgoing and good with people, any specific jobs I could go for?",0,1,False,False,False,statistics,1492216831,True,[removed]
Undergrad: Does a Stats Minor (or the lack of) matter if I will Have Work Experience Anyways?,2,2,False,False,False,statistics,1492221318,True,"So, I originally planned on getting a minor in statistics because I enjoy working with data, however I'm taking my first real course for the minor and it's not what I had imagined. I really enjoy the concepts, but the course focuses heavily on doing complicated hand calculations and more abstract stuff, like proving theorems. I have a really hard time getting motivated with this because it feels like such a waste of time knowing it would be done by computer. Also, the professor isn't the best (last midterm had a 50 average and median) or the most likable. This professor will be my professor next semester if I continue on to the next course for the minor.

Aside from that, I currently have an internship that I believe I can maintain for 2 more years. It's essentially a data science internship, I do my own projects working with large data sets handling the data-cleaning, programming, and analytic steps with minimal guidance. Through this internship, I could easily continue to, for example, read the textbooks associated with the statistics minor (the lectures are directly from the book), but I wouldn't have the statistics minor.

TLDR: I'm not liking the statistics minor courses due to them being largely ""not applied"" (hand calculations and proofs rather than concepts and programming). I have a data science internship that I believe I can keep for additional years. Would I likely miss out on any opportunities because of not having the statistics minor if I wanted to pursue an analytical career path?

Any quality input will be much appreciated.

EDIT: typos."
Any ideas? I'm stuck :( I think its d.,0,1,False,False,False,statistics,1492233764,False,[deleted]
Stationary series but ACF correlogram is weird?,0,1,False,False,False,statistics,1492245281,True,[deleted]
Normal distribution K Value is 1.5 !!!,1,0,False,False,False,statistics,1492247163,False,
Help needed ASAP with Fama & French + extra variables Regression,0,1,False,False,False,statistics,1492250802,True,[removed]
Quantitative Methods Finance Risk Anaysis Assignment Help,0,1,False,False,False,statistics,1492251751,False,
SAS online help,0,1,False,False,False,statistics,1492252834,True,[removed]
Random Variable And Distribution Function Assignment Help,0,1,False,False,False,statistics,1492252865,False,
R online help,0,1,False,False,False,statistics,1492253452,True,[removed]
Quantitative Analysis online help.,0,1,False,False,False,statistics,1492254302,True,[removed]
Is there anyway to prevent weighted average convergence?,6,1,False,False,False,statistics,1492271319,True,"**This question may not make sense, I apologize, am not a statistician just have a curious question**

To begin with my partner and I doing an IT contract for the purpose of designing a system that sorts customers for an ecommerce platform, into three 'buckets', for our purposes we call them 1, 2 ,3. With 1 being the best bucket to be in. 
Each bucket gets a different sort of marketing campaign so if you are in bucket 1 you will get amazing discounts, bucket 2 will get ok discounts, and bucket 3 will get minimal discounts. For a customer to gain entry into each of these buckets, they need to meet certain criteria (can not mention what these are) and out of those criteria the top few (can not get into particulars about this) are chosen to get the marketing campaign. Each criteria is weighted differently to get a score between 0-1[3], 1-2[2], 2-3[3]( the number in the brackets next to the score signifies which bucket you will get into, depending on your score, i.e if score 1.2 you will be in bucket 2)

Our question is for merchants using the system that have large customer base with many small transactions occurring every day, over time, would the scores of customers not coverage at particular point and if so is there any to prevent this? 

I hope this questions makes sense, sorry to be vague but we have signed some non disclosure agreements. 

Thank you in advance. 

**not a stats guy**

"
Calculating the optimal number of bins for severly skewed data,7,12,False,False,False,statistics,1492274848,True,"I have a data set with a sample size over three million numeric values. Close to 20% are either 0 or 1, with the maximum being nearly 18500. So the data is clearly quite heavily positively skewed.

I am trying to categorize some of this data by putting it into bins of equal width, so I decided to try and find the optimal number of bins. Using the Freedman-Diaconis rule it gave me a value of 126044.0262335108, this is clearly a ridiculously large number of bins for the data.

Breaking the set into the Inter-decile range also proved fruitless giving me [0, 1, 1, 2, 3, 5, 8, 17, 47]

Reading elsewhere the square root of the sample size was suggested, this gave 1732.05081 which is more reasonable. However the method is quite crude.

I also looking into Doane's formula given [here](https://www.jstor.org/stable/2683757?seq=1#page_scan_tab_contents). But reading up on this method it seems to have been based on an incorrect [hypothesis](http://www.robjhyndman.com/papers/sturges.pdf).

How should I deal with this level of skew in the data?

What is the best way to categorize this data?
"
Am I missing something?,5,6,False,False,False,statistics,1492285766,True,"Hey, I don't know if this is frowned upon: I'm not asking help with homework. I was reading this as a part of my thesis and can't make sense of something and I can't understand if the paper straight up has a mistake or I'm missing the point:

[The paper in question is](https://we.tl/F7sjn1Q3ll)

Specifically:
When testing models with more parameters, why would the N become smaller? I understand why the first model has the values that it has, but I can't understand why the other two don't. (pp.3)

Why does the number of all cause mortality change (for most of the paper it's 296, but in the supplemental table 2 it suddenly becomes 257). Did the authors just dun goofed or am I missing something?

I've been looking at this for the better part of an afternoon and now it's more personal than anything, and if anyone could lend me a hand I'd be very thankful.

Thank you
"
Compromise Power Analysis Help G Power,0,1,False,False,False,statistics,1492297789,True,"so i tried to learn as much as I could but its a lot of dense information, so im looking for a kind soul to help me.

Im trying to compute a compromise power analysis although im struggling with a few things.

Things I want to know, what can I set my alpha level to and how much more power will that give me? Or whats an appropriate alpha level based on my data.

Im running a 2 x 2 mixed factor anova, I have 2 groups and 4 measures. The 2 groups consists of a total of 65 participants, group 1 25 participants and group 2 40 participants.

Based on my current data this is the current effect size. main effect 1 (referent)— (f(1,63)=2.3)partial etasquared:.035 Main effect 2 Group-(F(1,63)=.575)partial etasquared:.008 Interaction effect- (f(1,63)=.697)partial etasquared: .011 (partial etasquared all computed via SPSS not GPOWER)

The nonsphericity correction e: 1 My main struggle is understanding the beta/alpha ratio input and then also understanding the output parameters in particular how I interpret alpha error probability and beta error probability.

If you would be able to tell me how many participants I would need to find a significant effect with alpha at .05 then guide me through the compromise power analysis and help me find the most appropriate level I can use to set my confidence interval level. It would also be really helpful if you copied your protocol window for me so I can understand for the future.

Whoever takes the time to help me id like to thank in advanced you don't understand how appreciative I would be!"
Multiple logistic regression- poor goodness of fit but significant Wald statistic?,2,8,False,False,False,statistics,1492299797,True,"Generally, how should one interpret a model with poor fit?

Newbie to regression, and I have been struggling with my model for the last 3 days, so as a last hope I've turned to reddit. The model that I have built has poor fit (Hosmer Lemeshow statistic is 16.1 with a p-value of 0.04). But all of my 9 categorical predictor variables are significant according to the Wald statistic. My omnibus test is also significant (chi-sqr=220) and my -2 log likelihood is around 1335. My sample is fairly large, n=1196. How do I interpret this/proceed? 

Do I need to go back and review which predictors need to be in the model (if so, what is the methodology)?Or, it is okay to proceed with this model acknowledging poor fit? What would be the interpretation?\

BTW, I was only able to assess collinearity through a collinearity matrix--wasn't quite sure how to check for multicollinarity since all of my predictors are categorical (making checking for VIF not possible). Could this be why my model has poor fit? How does one check for multi-collinearity when the predictors are categorical?

As you can probably tell, I need a layman's explanation to this lol. 

Thanks for your help!"
What is this probability?,10,2,False,False,False,statistics,1492308162,True,"There are 8 kids, they can choose to spin clockwise or counter clockwise, what is the chance that exactly 3 spin one way and the remaining 5 spin the other?

Now, I say 20% because if I just use a split showing the possible cases, then I get

* (0,8)
* (1,7)
* (2,6)
* (3,5)
* (4,4)

People keep telling me I'm wrong, but I don't see how.

Thanks!

EDIT: order doesn't mater, nor does the direction the kids spin."
When there is a Reddit giveaway it's you're usually supposed to comment a number from 1 to X and op will randomly pick a winner should I always pick the same number to increase my chances of winning,0,1,False,False,False,statistics,1492311275,True,[deleted]
What is the best way to share code on a group assignment in r?,13,10,False,False,False,statistics,1492333405,True,"Hi everyone, I was wondering if there was a way to cooperate in group assignment in R without having to send each other the update code all the time. Is there something like a google docs version of r that I don't know about? 

Many thanks in advance."
What kind of analysis for gdp changes?,2,2,False,False,False,statistics,1492336481,True,"Hello, I want to find out what are most important determinants that influenced gdp changes over the years(for example 1965-2015) for a given country. So first what comes to mind is to use the gdp formula (C+I+G+(ex-im)) and  see how every of those components correlate/influence with the gdp. 

First thing that comes to mind is to use regression for this but i have been also thinking about matrix of correlations and sensitivity analysis. however since i'm doing some sort time series analysis i wonder how would those tools apply in this case. 

So how would you recommend me to analyze what were most important factors for gdp changes over the years for a given country?"
Sample Surveys Assignment Help - Statistics Homework Tutors,0,1,False,False,False,statistics,1492337345,False,
Statistical Inference online help,0,1,False,False,False,statistics,1492338023,True,[removed]
Scatter Diagram Assignment Help,0,1,False,False,False,statistics,1492338224,False,
Statistical Mathematics online help,0,1,False,False,False,statistics,1492338627,True,[removed]
Statistics in Kinesiology online help,0,1,False,False,False,statistics,1492339313,True,[removed]
Is this the correct way to present a multiple regression analysis in a table?,21,15,False,False,False,statistics,1492341283,False,
When there is a Reddit giveaway you're usually supposed to comment a number from 1 to X and op will randomly pick a winner. In order to increase my chances of winning should I always pick the same number or keep changing it?,8,0,False,False,False,statistics,1492347023,True,
Appropriate statistical tests?,0,1,False,False,False,statistics,1492362693,True,[removed]
Please help me pass Stats for Business.,8,0,False,False,False,statistics,1492367954,True,[removed]
"How can I use PROC MIXED, in SAS, to analyze 4 different measures of a certain time-dependent covariate?",0,0,False,False,False,statistics,1492372713,True,[deleted]
[Statistics] How to know if something is statistically significant by looking at the regression?,5,5,False,False,False,statistics,1492379093,True,"http://imgur.com/a/B52QT

In this example, what told you that the coefficient on black is statistically significant at the 5% level?

Thanks in advance!"
Looking for an introduction to Path Analysis,6,15,False,False,False,statistics,1492380124,True,"I am an education PhD candidate, and am looking for a good resource that will explain path analysis. Most of my experience is multilevel regressions, but I have a data set that seems to require path. Anything to get me started. 

Thanks. "
"Assessing confounding in logistic regression, adjusted odds ratio and unadjusted odds ratio?",0,1,False,False,False,statistics,1492387440,True,[removed]
Need help interpreting the results of a Chi-square test. Anybody skilled at this?,1,1,False,False,False,statistics,1492387544,True,"[Results Part 1](http://i.imgur.com/nTokwmI.jpg)

[Results Part 2](http://i.imgur.com/zghrozl.jpg)

From the professor: 


> Objective: To determine whether female virgins and non-virgins yielded similar results when the Drosophila melanogaster traits *sepia* and *dumpy* were crossed. To determine why (in 2 sets of experiments of 12 trials each) each trial passed chi squared indicating the data support Mendel’s laws of independent assortment but the total fly population in each experiment failed chi squared indicating that the data did not support Mendel’s laws.
 
> You have the problem of explaining why the chi square values for the entirety of each experiment show that the data do not support Mendel’s laws but oddly the chi squares values of all (or most) of the trials do support his laws.  SO EXPLAIN WHAT IS OCCURRING MATHEMATICALLY TO PRODUCE THESE RESULTS AND THEN EXPLAIN THE POSSIBLE SCIENTIFIC EXPLANATION(S) FOR THESE RESULTS. 

The problem is that this is a scientific writing class and we never actually did this experiment.  These are results that he made up.  Moreover, we didn't learn how to analyze chi square tests; he just threw a bunch of numbers at the class and asked us to interpret.  

I watched a few videos on Chi square tests, but I still can't figure out what leads to the discrepancy between the chi square data individual trials being OK and the overall Chi square data not supporting Mendel's laws.

"
Best test for multiple categorical variables?,1,1,False,False,False,statistics,1492398172,True,[removed]
I calculated the probability and time it would take for a monkey to randomly type out the Complete Works of Shakespeare,27,50,False,False,False,statistics,1492406244,True,"""Given infinite time, a monkey could randomly type out the Complete Works of Shakespeare.""

I'm going to assume our monkey types 100 characters a minute.

There are 3,695,990 characters in Shakespeare's Complete Works, and I'm going to account for 34 possible characters being our alphabet, periods, commas, colons, semicolons, question marks, exclamation points, apostrophes and spaces. Hopefully I didn't miss anything. 

To type this many characters it would take our monkey 26.666 days.

(1/34)^3695990 = 1.727 x 10^-5660331. 

1/(1.727 x 10^-5660331) is the chance of our monkey typing all 3,695,990 characters in the right order. 

This would result in 5.789 x 10^5660330 attempts to achieve success.

26.666 x (5.789 x 10^5660330) = 1.505 x 10^5660332 days.

1.505 x 10^5660332 days/365.24 = 4.1206 x 10^5660329 years. 

The universe is 13.73 billion years old. 

To put that into perspective, if I divide this number by 13.73 billion we are still left with 3.00 x 5660319 years. 

To put THAT into perspective, this number would have 5660320 digits. It would take 1887 pages (at 3000 characters a page) just to write this number down. 

**For a monkey to randomly type the Complete Works of Shakespeare, it would take 4.1206 x 10^5660329 years, or the age of our universe multiplied by a number so big it would take nearly 2000 pages to write down.**"
"In SAS's PROC GLM, how do I test the interaction between two variables with contrasts?",1,1,False,False,False,statistics,1492411258,True,"Really quick question here, I'm having trouble testing an interaction between two terms with contrasts in PROC GLM. Could someone lend a hand here? 

One variable has two levels while the other has six levels. I'm pretty sure the 2 level variable just -1, 1 in the contrast statement, but I'm lost on the 6 level variable.

Finally, if 70% of the population belonged to the first level in the 2-level variable and 30% of the population belonged to the second level, how would I test the interaction with contrasts?

Thanks!"
"If you counted every time a news channels anchor is happy or negative about a story, could this be used to gauge statistically the trend of policies they accept?",0,0,False,False,False,statistics,1492412711,True,"Im curious if this could be graphed because I think it could be used to prove undeniable bias in journalism which should not take a side on issues and cause problems in the news and political process 

"
Statistical Computing Assignment Help,0,1,False,False,False,statistics,1492422560,False,
Statistical methods in public health Assignment Help,0,1,False,False,False,statistics,1492423778,False,
Structured Process Improvement online help,0,1,False,False,False,statistics,1492424221,True,[removed]
Statistics in kineology Homework Tutors,0,1,False,False,False,statistics,1492424693,False,
Survey Methodology online help,0,1,False,False,False,statistics,1492424963,True,[removed]
Theory Of Probability online help,0,1,False,False,False,statistics,1492425929,True,[removed]
NB regression - can the intercept indicate the best approach to reducing multicollinearity?,0,4,False,False,False,statistics,1492428848,True,"I tried 2 different approaches of reducing multicollinearity within my negative binomial regression (modelling accidents at locations).

1) Removing categorical variable X, from which categorical Y and categorical Z could be accurately predicted.

2) Removing categorical variables Y and Z.

Both models return similar goodness-of-fit. The other (continuous) variables return similar b-values with either model. Within this context, I believe I can form logical recommendations using either model.

However...

I noticed that within the parameter estimates, Model 1 has a highly insignificant intercept with a negligible b-value; whilst Model 2 has a highly significant intercept with a positive b-value. Is this a clue that I should base my recommendations on Model 2?"
"Hey everyone, I have a small question about the proof for the fundamental identity in a gamma distribution.",2,3,False,False,False,statistics,1492434130,True,"This has more to do with calculus than statistics really, but seeing as I found the question through stats hopefully someone can help me. This is my first time visiting this subreddit so if there is a more appropriate place for these kind of questions feel free to direct me there.

Moving onto my question: 

[Γ(k+1)=kΓ(k) for k>0](http://www.math.uah.edu/stat/special/Gamma.html#prp1)

During the proof after integration by parts, one side is equal to (−x^k e^−x )∞0

So: (-∞^k / e^∞ ) -  (-0^k / e^0 ) 

Which the proof then defines as = 0.

There's probably something basic I'm missing but I don't understand the reason or steps taken to calculate this section as 0. If anyone could provide a link to a page that deals with these kinds of problems so that I can learn to solve them myself in the future that would be appreciated.

I also apologize for any possible format errors as I'm on my phone, tried my best so hopefully it came out right."
Looking for help to start learning stats without going to university,26,18,False,False,False,statistics,1492442149,True,"Hi guys,

I really enjoyed stats in university and did fairly well, but can't really afford to go back to university.  I was discouraged by my counsellor to get a degree in stats as she didn't know how useful it would be so I got a degree in geography (grrr).  Now at 35 years old I have a dream of getting a degree in stats.  I had recently been diagnosed with ADHD and feel like I have a chance at it, but I feel like I need to prove to myself that I can conquer some understanding on my own first.

Do you guys have any recommendations on where to develop my knowledge and skills?  Do I need programming, if so, what would be the best programming language to learn?

I have two areas of interest:  nutrition and business.  Also just a general interest at better understanding the world around me.  I get so bloody frustrated at people's lack of understanding of 'statistics' that are thrown at people in the news and fb etc.

I really appreciate any feedback you could give me."
The Supportive Features of Biometry Assignment Help by Statistics Help Desk,0,1,False,False,False,statistics,1492447034,False,
You have to choose between two courses: one on survey sampling and another one on statistical computation with C. Which one would you choose?,2,0,False,False,False,statistics,1492454007,True,[deleted]
Are there any benefits (employment and otherwise) to getting a Masters in Statistics?,13,6,False,False,False,statistics,1492472781,True,"I have an undergraduate degree in quantitative psychology and am looking into taking a graduate certificate OR a Masters degree in statistics. I'm wondering if anyone here can speak to the benefits of the Master's degree over the graduate certificate in general or specific terms. Thank you! Ideally, I'd like to work in a research/data analysis capacity for a large governmental organization. "
Should I learn SPSS or R?,58,35,False,False,False,statistics,1492473379,True,"Hi there, I'm a recent university grad that's going into the research field. I learned SPSS in my undergrad and am wondering if it's more useful to know SPSS, R or both. Which is more useful or is it context-dependant? "
Are IBM Certifications valued in the workforce?,3,5,False,False,False,statistics,1492474302,True,"I'm looking into getting one for SPSS, since I've been using SPSS for 5 years now and feel pretty comfortable with it. 

Here's the link to the certification website: http://www-03.ibm.com/certify/index.shtml

The certifications I am looking into are: 
IBM Certified Associate - SPSS Modeler Data Analysis v2
IBM Certified Associate - SPSS Modeler Data Mining v2
IBM Certified Specialist - SPSS Modeler Professional v3
***IBM Certified Specialist - SPSS Statistics Level 1 v2 - I'm especially interested in this one. Does anyone have any experiences with these courses/tests? Thank you in advance. "
Test to compare x-intercepts of two functions,0,1,False,False,False,statistics,1492475440,True,[deleted]
Gamblers Fallacy and Video Games,17,2,False,False,False,statistics,1492475870,True,"So I have recently taken up playing hearthstone, an online trading card game.  This game has card packs you can purchase with real money or in-game currency.   The packs yield 5 cards with 1 guaranteed to be ""rare""   
The 'rarity of the cards goes:  common, rare, epic, legendary.  

The community(reddit, forums) seems has the notion that there is a ""pity timer"" in place and that if you open 40 packs in a set you are guaranteed to get a legendary.   
The ""proof"" most commonly used that this is real is ""tons of videos on youtube"" or ""posts about it""   

As this seems this is no more that wishful gamblers thinking and bad math, is there anyway to prove or dis-prove such a theory? "
"2 data sets: same slope values, but one is significant and the other isn't? Why?",0,1,False,False,False,statistics,1492479632,True,[removed]
What kind of coursework would admissions to a master's degree in statistics be looking for?,14,6,False,False,False,statistics,1492486833,True,"I'm a Mathematical Econ major thinking about doing a master's degree in statistics or applied statistics at some point after graduating undergrad. I still have one year left before I graduate but am almost done my major requirements so have some free spots, and I'd like to take some courses that would perhaps strengthen a future application. My GPA is rather low so I'm worried about getting in anywhere.

As some background, all of my stat/probability courses have either been in the Econ or Comp Sci department here. So I have Economic Statistics, Econometrics, and Microeconometrics, all of which required some statistics/distributions, as well as some exposure to discrete probability through a Math for Computer Science course. At this point, would it look superfluous to take an introductory probability course in the Statistics department, or would it look good if I can do well in it? I'd rather not take it since I wouldn't really learn anything new, but if it would look good in the eyes of admissions it might be worth it. My only other option would be to take an advanced probability course in the Math department which would require measure theory and which I'd be mostly unprepared for.

In general, is it okay to not have many stat courses taken if I have a strong general math background? I'll have taken a lot of calculus and linear algebra by the time I graduate.

I was also thinking of taking an actuarial exam, Exam P, over the summer, to see if I could pass it. If I were able to pass it, could I mention it in an application? Would it be at all meaningful?

Thanks, just looking for some direction."
Subjective Probability Assignment Help,0,1,False,False,False,statistics,1492508384,True,[removed]
Theory of Games and Statistical Decisions Assignment help,0,1,False,False,False,statistics,1492509952,True,[removed]
Types Correlation Assignment Help,0,1,False,False,False,statistics,1492510799,True,[removed]
Vital Statistics online help,0,1,False,False,False,statistics,1492511318,True,[removed]
Advanced Experimental Concepts and Applications online help,0,1,False,False,False,statistics,1492512031,True,[removed]
Applied Bayesian Statistics online help,0,1,False,False,False,statistics,1492513001,True,[removed]
Help with statistical analysis on quality program,1,0,False,False,False,statistics,1492522563,True,"I have  cleaning service where we survey our clients. We have recently went from a 4 point to 5 point review system and our quality has dropped. I am looking for someone with a solid background in statistics to help me re-do our program.  

What is our average and what should our spread be.

How to setup minimum goals and a setup a program to incrementally improve quality.

I am needing someone that can apply statistics to help us so I am not just guessing and coming up with random numbers that will end up firing people or giving them bonuses.  Please pm me if this is something you can help with."
time series question,0,1,False,False,False,statistics,1492522850,True,"Hi everybody,

I have two time series where each point is the result of a different manipulation, and I'm looking for a way to compare the two series. Can I treat each pair of values as an independent event? Or should I be looking for a way to classify the entire time series?

Any help would be appreciated! "
Steve Ballmer Serves Up a Fascinating Data Trove,6,28,False,False,False,statistics,1492524521,False,
[Question] Parameter Adjustment (xpost r/DataScience),1,1,False,False,False,statistics,1492526125,True,"Hey all,

At work I've been asked to help a client adjust a pricing model based on a new categorical parameter to their data. ANOVA shows me the values are statistically significant, and stepwise regression backs that up, but I'm having a problem applying an adjustment.

The problem is their current prediction is 2% above the actual value, so when I apply *any* positive adjustment the error gets worse. But obviously I can't negatively adjust for every value of the new parameter.

What am I missing here? I'm usually on the optimization / integer programming side of data science, so I'm sure I'm overlooking something obvious. Thanks in advance for your help and time."
Lot of data but where to start?,0,0,False,False,False,statistics,1492527435,True,[deleted]
The pitfalls of A/B testing in social networks,5,53,False,False,False,statistics,1492527470,False,
Astrostatistics. Statistical Techniques,0,0,False,False,False,statistics,1492529739,False,
Online Statistics Tutor from Statisticshelpdesk.com Help Has Created a Special Niche,0,1,False,False,False,statistics,1492529789,False,
Determining feature importance in linear regression?,1,2,False,False,False,statistics,1492530239,True,[deleted]
Can I do statistical analysis with two separate survey questions that have different types of (response) scales?,0,1,False,False,False,statistics,1492530915,True,[removed]
Tips for learning how to use R?,12,18,False,False,False,statistics,1492536100,True,"Hi there! I posted a question recently asking about SPSS vs. R and many of you recommended learning how to use R. How do I get started? I have a statistics/quantitative background. 

Can someone recommend a ""for dummies"" type resource (book, video, course, etc.)? I'd rather start from the very beginning and build confidence/work my way to the complicated skills, even if it's initially a little easy. 

I'm in the academic research field (social sciences/sciences) but may be looking into moving into healthcare or market research in the future. Hopefully I can learn a variety of skills with R that will transcend research disciplines. Thank you in advance! "
Would year and month be considered a categorical variable?,2,2,False,False,False,statistics,1492539728,True,"[Data Set](http://www.stat.ufl.edu/~winner/data/ontime.dat) 

The data set above is the one I am referring to. Doing a project and i need 2 quantitative variables and 2 numerical variables and another of either and a total of 50 observations. With the data set above I know the months are categorical but would be year # be considered numerical or categorical?"
"Biostatisticians, what should I learn before my Masters? Is this project possible?",10,10,False,False,False,statistics,1492541314,True,"So, in my country our bachelors courses have more or less fixed curriculum, and you end up only studying seriously the major field of your degree. I finished my biomedical sciences degree last year, and I only had an basic statistics course without even having any kind of algebra or calculus aside from high school math. In the short experience that I had in a research lab and writing my undergrad thesis, I noticed 3 things:

* A good amount of PhDs in biology don't have enough background of math and statistics to use it properly and simply put faith in their PIs suggestions;
* There is a lot of papers out there using bad statistics (even though I know the bare minimum of stats, when I look at the whole picture of the conclusions there is something fishy);
* There is some type of academic circlejerk where one group constantly cites the other, building up an idea without even making replications about the important findings to validate them.

After that I learned about Metrics, from Stanford, I decided to postpone my masters to learn at least the basics of stats and experiment design so I can be more critical with my research. I don't wanna to just trust what my PI says or have to study everything on my own while having graduated classes and etc. At the moment I am unemployed, doing a high school math review to start learning Calculus.

First: What math do I need to learn to have a good understanding of statistics? Now I just want to learn enough to understand the most commonly used tests (like ANOVA), the rest I will try to learn during my masters.

Second: The lab I made my internship works with pharmacology and physiology, and I had this crazy idea that the expression of some sub-types of ion channels changes during a particular pathology based on the results of an other student. So I was thinking about using statistics to cross the effect of antagonists (that have different selectivity for each sub-type of channel) in healthy and diseased cells with efficacy, selectivity and affinity coefficients to infer if and how much the expression changes. Is it possible and  doable or I am better off going for bioinformatics or molecular biology?

Sorry for my English, Thanks!"
Wondering the odds of this dice roll in mtg.,5,2,False,False,False,statistics,1492565282,True,"First player rolled a 9, 2nd player rolled a 10, and I rolled an 11.  We we wondering how rare a consecutive roll between 3 people like this would be.  Hope this is the right place.  Thank you!
"
Vital Statistics: 4/19/2017,0,0,False,False,False,statistics,1492589675,False,
China Beige Book Says IMF Takes Statistics at Face Value,0,1,False,False,False,statistics,1492589785,False,
Question about knn / kmeans classification,3,5,False,False,False,statistics,1492592051,True,"I am currently doing a project for school. It's about weather forecasting. The people that gave us the database suggested that we do classifications. The DB looks like this :

real temperature (4)   week -1 (3.5) week -2 (2.5)  week -3 (0) week-4 (0)

(4 weeks ago, the model predicted 0 degrees, 2 weeks ago 2.5 degrees, in reality 4 degrees)

 If my understanding is correct, the point of knn is to predict the class of a datapoint from it's nearest (usually euclidian distance) neighbours. But in our case, we don't have any new data, only a training set. How can we check if the model works ? Is that the point of cross validation ? And currently, we don't have any classes to predict, so I assume that we need to build some.

Is the size of the class relevant ? I thought that it would be interesting to try to ""predict"" heat wawes, but that's not a lot of datapoints, does it mean that the model won't work as good ? Should we try to predict positive / negative real temperature, even if it's less interesting ?

Also, am i correct in my understanding that the class we try to predict is the real temperature (or a transformation of it), with the datapoints being the forecast up to 4 weeks ago (seems logical to me, but i want to be sure) ?

Our teacher also suggested K means classification. She suggested that we use an MCA first, it seems the point of an MCA is to reduce the number of variables, but IMO 4 dimensions is not a lot (?), is it really necessary ? Should I use hierarchical clustering to determine the number of clusters to use ?

That's a lot of questions, but while our teachers suggested classifications, we are supposed to do it next year, so i never got any classes on this.
Sorry for any spelling mistakes, I am not a native speaker."
New England College,0,1,False,False,False,statistics,1492594016,False,
PSPP has no 'undo' function?,0,1,False,False,False,statistics,1492595273,True,[removed]
Advanced Functional Analysis Assignment Help,0,1,False,False,False,statistics,1492596128,True,[removed]
Assets Markets homework help,0,1,False,False,False,statistics,1492596743,True,[removed]
Black Scholes Theory homework help,0,1,False,False,False,statistics,1492597387,True,[removed]
Applied Complex Analysis Assignment help,0,1,False,False,False,statistics,1492597697,True,[removed]
Computational Methods in Finance Insurance Homework Help,0,1,False,False,False,statistics,1492598023,True,[removed]
Statistics Homework Tutors- Basic Population Analysis Help Assignment Help,0,1,False,False,False,statistics,1492598607,True,[removed]
Statistical analysis on 1 categorical and 1 continuous variable???,11,1,False,False,False,statistics,1492601939,True,"Please help!!


I'm looking at a dataset about low birthweight:

https://www.umass.edu/statdata/statdata/data/lowbwt.txt

and am trying to find out if there is an association between BWT and if the mother smokes? 

Specifically i am trying to do a test but don't know which test to carry out!

"
"Multiple categorical variables, best approach?",0,1,False,False,False,statistics,1492607954,True,[removed]
Categorical variables- chiSQ,0,1,False,False,False,statistics,1492609986,True,[removed]
Categorical Variable stats?,0,1,False,False,False,statistics,1492610679,True,[removed]
Need help choosing statistical test while data mining,9,14,False,False,False,statistics,1492617789,True,So I've collected lots of data on missing belongings at my hospital. I have multiple categorical and continuous variables. I'm looking at what the best way to approach beginning to find if any relationships exist. I have all of the variables within the charts that had incident reports reported and the same variables from a random sample of charts. Now the question is where to start? Comparing means? Finding relationships within my first group? Any guidance is much appreciated.
Is generalized least squares a non-parametric estimation method?,0,1,False,False,False,statistics,1492619818,True,[removed]
2 Sample z-Test for Proportions Question,0,1,False,False,False,statistics,1492620714,True,[deleted]
I'm a math major and I'm struggling with intro to probability. The most basic concepts are not clicking. What resources would you recommend?,14,8,False,False,False,statistics,1492629658,True,"I've taken Multivariate calculus and I'll be taking linear algebra this summer, but I'm having trouble with grasping things such as the multiplicative rule, n choose k, independence, and generally the basics. 

When I am faced with homework problems, I don't quite get how to set up problems. My class has progressed to probability distributions now and I had bombed my midterm exam with a score below 40% while the class has a median of 75~. I'm really scared and questioning my future prospects because I thought if I was good at solving math problems, stats & probability should be easy. I was so wrong.

Something really isn't clicking for me and I can't quite envision how to interpret questions or set up problems to solve. I'm trying Stat 110 on YouTube and while the lectures are much more clearer than my current ones, I'm still struggling to do Joe Blitzsteins homework without looking at the answers first.

This shaky foundation is messing with my understanding of the material. For the first time I had to withdraw from a course because I had done so poorly the entire way. I visited my professor after class and he threw his hands up in frustration trying to explain minimum value, pdfs and CDs to me. Is there anything you guys would recommend to help?"
"In order to find association between smoking and low birthweight, which t-test would I use???",6,0,False,False,False,statistics,1492631865,True,"Here is the dataset: 

https://www.umass.edu/statdata/statdata/data/lowbwt.txt

I also am trying to analyze in R and have no idea how to perform a t-test with one categorical and one continuous variable (Smoke, BWT)"
Undergraduate Project Question,7,1,False,False,False,statistics,1492636202,True,"For a project as part of my undergrad. I am investigating the factors in whether someone defaults on their credit card loan. I have regressed, in a probit model, the dummy default against a number of variables. 

However, the results show that the log of spending is negatively correlated. Ie that more spending results in less probability to default. This doesn't make intuitive sense. Income is one of the variables regressed so it is controlled for right so spending can't be endogenous through income?

What avenues should i spend trying to fix this? Or can any of you think of explanations regarding this?

Thank you in advance."
Help placing this econometrics course?,0,1,False,False,False,statistics,1492636633,True,[removed]
Decision Theory question - help much appreciated!,3,3,False,False,False,statistics,1492637084,True,[deleted]
Darth Hypothesis Test the Wise,2,9,False,False,False,statistics,1492641182,False,
Are French pollsters cheating? The jury is still out,0,18,False,False,False,statistics,1492646210,False,
Need an analysis on a gambling proposition,3,0,False,False,False,statistics,1492652164,True,"Hi guys. There is a local bar that offers a ""queen of hears jackpot"" the process works as follows.. 

1) players buy an unlimited amount of $5 raffle tickets

2) the tickets are drawn and one person is chosen to select a card from a 52 card deck 

3) if the player chooses the queen of hearts, they instantly win $5000

4) if the player doesn't pick the queen of hearts, the card remains face up and the process repeats itself next week and someone else will have a chance to draw

So at my bar there are 3 facedown cards and 49 face up cards. The queen of hearts is under one of the facedown cards. 

Assume 50 people who show up and each buy 5 raffle tickets. 

How many raffle tickets should I buy to maximize my EV and ROI? 

"
Probability that I won't roll a 1 over 10 rolls of 2 dice.,8,0,False,False,False,statistics,1492668982,True,"I can't figure this out. I played backgammon and was stuck on the bar and needed a 1 to get off. I literally rolled ***10 times*** and didn't roll a 1 even once. What are the chances? No, really, what are they? ;)

I used an online calculator and it said the odds were 1 in 60 million. lol. Not sure if that's right or not but it sure seems to be quite unlikely. That's 20 separate dice rolls with no 1!

On another note, this is why I hate playing with ""random"" number generators. Because they do this shit. :(

Thanks for your help!"
"CFA, AMOS, how to report it APA Style",5,0,False,False,False,statistics,1492669065,True,"Hi redditors! I kind of need a little help. 

I'm running a confirmatory factor analysis (CFA) to confirm my dataset with a Strength and Difficulties Questionnaire (SDQ by Goodman, 1997) which has 5 factors.  

Using AMOS, I ran a Model Fit 
(n = 478)
CMIN/DF = 1.640, p =.000 
GFI= .927
AGFI= .911
CFI= .922
PCFI= .814
PCLOSE= 1.000
RMSEA= .037

I ran this based on a print out that I found through google, and it would seem the number meant my data fit the model that I want it to fit into? 

Also, if anyone knows how to report this CFA in an APA style? 
"
Can't find Ci because n is not equal.,1,0,False,False,False,statistics,1492677758,False,
I can't solve this because n is not equal across all samples. What do I need to do?,2,0,False,False,False,statistics,1492678691,False,
[Article] In 2016 I applied to 1000 positions in software and received zero job offers. • r/jobs,1,0,False,False,False,statistics,1492680812,False,
Combinatorics Assignment Help,0,1,False,False,False,statistics,1492681965,True,[removed]
New Statistics Demonstrate Desperate Need to Increase Cigarette Tax,0,1,False,False,False,statistics,1492683680,False,
Devils 2016-17 Skaters’ Advanced Statistics Summary,0,1,False,False,False,statistics,1492683748,False,
Contingency Table Analysis homework help,0,1,False,False,False,statistics,1492684871,True,[removed]
Data Visualization homework help,0,1,False,False,False,statistics,1492685726,True,[removed]
Discrete Models in Applied Mathematics homework help,0,1,False,False,False,statistics,1492686475,True,[removed]
What statistical test to use if I want to know if an obtained value is close to actual value?,7,0,False,False,False,statistics,1492690734,True,[removed]
Method for calculating depreciation on cars,0,2,False,False,False,statistics,1492691267,True,"Hello! Om working on a project where we are developing an automated depreciation formula and im quite insecure of what method i should be using.

I've looked into some forecasting methods to forecast values with the older prices of the models, though this method could work it doesn't because quite often a car stands in the car hall for a while which makes the first year depreciation wrong (it somewhat does maintain its listprice since it didnt have any owners).

Of owned cars the value decrease is about 25-35 % but i do only have access to data from cars sold by a dealership (new and older). 

But i have access to a whole lot of data, variables are: Price, date listed, date removed, age of the car, mileage of the car, car segments, model, brand, hp(kW), listprice, well basically all data there is, except consumer transactions.

So basically im looking for a forcasting method that is adjustable by weights?

Thanks in advance!"
A bioinformatics StackExchange is being created. If you are a user of CrossValidated your expertise might be useful here.,4,53,False,False,False,statistics,1492692149,False,
D prime assumptions violated?,0,1,False,False,False,statistics,1492700407,True,"Hello so I just needed some quick clarification on what i was working on. Here is an extract from Stanislaw and Todorov
>""SDT states that d′ is unaffected by response bias (i.e.,
is a pure measure of sensitivity) if two assumptions are
met regarding the decision variable: (1) The signal and
noise distributions are both normal, and (2) the signal and
noise distributions have the same standard deviation. We
call these the d′ assumptions. The assumptions cannot actually
be tested in yes/no tasks; rating tasks are required
for this purpose. However, for some yes/no tasks, the d′
assumptions may not be tenable; the assumption regarding
the equality of the signal and the noise standard deviations
is particularly suspect (Swets, 1986).""<

So based on that I conducted a test of normality in SPSS, by checking the zHit and z False alarms, the z hit scores were statistically shown by shapiro wilk to not violate the normality assumption, where as zfalse alarms did violate this normality assumption p=.000. Am I right therefore in assuming that I must employ the use of A prime or Zhang& Muellers Corrected ""A"" non-parametric versions. [http://obereed.net/docs/ZhangMueller2005.pdf]

Also if anyone has a better under standing of the differences between Aprime and zhang's A is could they possibly explain it to me in a simple fashion because im struggling. I know it has something to do with the ROC curves but Im not an expert so i struggle with the compelx terminology, im also only an undergrad Psychology student so have basic statistics knowledge.
Thank you "
A statistical argument for why it's hard to prove discrimination from personal experience,2,19,False,False,False,statistics,1492707027,False,
Multiple Pairwise Comparisons,0,0,False,False,False,statistics,1492711974,True,[deleted]
"Can anyone tell me how to do this type analysis with SPSS? I want to use this analysis in my paper, but I don't know how to do.",0,0,False,False,False,statistics,1492713473,False,
Looking for advise on applied statistics programs,0,1,False,False,False,statistics,1492714905,True,[removed]
Forecasting Water Usage,3,1,False,False,False,statistics,1492715510,True,"Hey guys, I've been tasked with forecasting the water consumption for residential households in a city. I have three years of previous water usage, customer ids, zipcodes, and climate data from our local weather station.

I want to forecast water consumption based on days since last rain and maybe a similar day analysis. 

I've never done forecasting before and I'm unsure how my regression would look or what type of analysis I should be using.

I'd love any help you guys can offer. 

Thanks "
How to take a percentage of standard deviation,11,3,False,False,False,statistics,1492721418,True,"**Not a stats guy, trying to help company develop better product**

Hi, 

To begin with I am working for a company that is wanting to design methods to group consumer behavior into various ""buckets"". Each bucket getting a different marketing action. To do this they are constructing scores around buying behavior (can not get into what metrics are being used due non disclosure agreements), from here they are construction a curve of best fit to generate a standard deviation score. 

My question is, from the curve that is generated by the standard deviation, how can we mathematically pick the top 15%, 85-60%, and 60-0%? 

Please let me know if this question does not make sense. 

Thank you in advance. 

**not a stats guy**"
Compare variable between age groups over 10 year period.,0,1,False,False,False,statistics,1492730458,True,[removed]
SEM bars on a percentage change graph?,0,1,False,False,False,statistics,1492731502,True,[removed]
Compare different age groups with repeated measures over time?,0,1,False,False,False,statistics,1492731556,True,[removed]
Really need help about what statistical method to use on research,0,1,False,False,False,statistics,1492731590,True,[deleted]
Really need help about what statistical method to use on our research,13,0,False,False,False,statistics,1492732206,True,"So we have our research defense and I need help which method to use. I have no replicate and my classmates are saying replicates are needed for anova. 

So I have an obtained value which I need to compare to a hypothethical value but I have no replicates. So no mean and others. Just comparison whether my value is close to the hypothetical value.

Please and thank and you."
Appropriate Hypothesis Test,6,0,False,False,False,statistics,1492732675,True,"Basically question involves 3 groups of 15 rats that have been given a drug. Only focusing on Group 1 which is the controlled group that hasn't taken the drug.

The mean is expected to be 0.210 g/cm^2 when normally distributed throughout all the 45 rats.

If i'm given a standard deviation of 0.015 how can I create a null and alternative hypothesis to find out if their mean is above 0.210 g/cm^2.

It also needs to involve a power.
"
Is this a weighted sum and can i do it??,3,0,False,False,False,statistics,1492736472,True,"Hey all, medical student writing my research report. I have survey results about peoples level of confidence on a topic. I have given the answers numerical values 1-4. I have added the totals up for each particular question and used this as a measure of how confident people were in regard to that particular topic. i.e if a question had a total combined score of 10 and another had 5, this means people were more confident in this aspect that the other.
I have referred to this as a WEIGHTED SUM. 
Is this right/appropriate/does it even exist?
Thank you in advance statlords."
Has anyone ever calculated a Confidence Interval on a point of inflection?,6,1,False,False,False,statistics,1492740648,True,"Hi all,
I'm modeling some data that seems to fit a quadratic model. I have calculated the point of inflection but am lost as to how to calculate a 95% CI for that point of inflection (i'm trying to find an interval in which i could expect the point of inflection to occur given my data).

Do I run simulations with the data? If so what kind would be recommended.

Thanks again

"
Binary IV Continuous DV,0,1,False,False,False,statistics,1492745712,True,[removed]
Is Human Intelligence a Normal Distribution? [Discussion],12,0,False,False,False,statistics,1492745881,True,I'm having a debate with a friend on weather human intelligence is a normal distribution. 
Most Availed Statistics Assignment Writing Services,0,1,False,False,False,statistics,1492759488,False,
Wolfram Data Repository: Data Publishing that Really Works,2,11,False,False,False,statistics,1492760042,False,
Dynamical Systems homework help,0,1,False,False,False,statistics,1492766573,True,[removed]
Elementary Econometrics homework help,0,1,False,False,False,statistics,1492767752,True,[removed]
E-Views Homework help,0,1,False,False,False,statistics,1492768402,True,[removed]
Contingency Table Analysis Assignment Help,0,1,False,False,False,statistics,1492768743,True,[removed]
Some questions about MCMC and how things are done in a bayesian estimation of parameters,17,5,False,False,False,statistics,1492770982,True,"Hello, r/statistics ! First of all, thank you all for your incredible work. I've been browsing a lot lately, and your answers (and questions) are all really informative, even if sometime complex to me.

Basically, I'm still trying to globally understand what I'm doing when using a software which is using bayesian modelisation, and especially the parameter estimation part using MCMC simulation.

I've been reading for hours some courses found online about it and this subreddit, but haven't been able to properly find an answer to the questions i'm having on the whole process. Thus, here it goes :

-------------------------------------------------------------------

What I don't understand is the either the purpose of the MCMC, or how it is acted in the end in relation of the other steps of bayesian estimation.

I read multiple times that the MCMC was used to find informations about the posterior distribution, meaning the probability for each possible parameter value to be ""true"" while taking into account both our prior about those probabilities, and the likelihood of each possible parameter value knowing the data we gathered.
From what I still gather, we need MCMC because other methods to have informations about this posterior distribution can quickly get really, really complicated, especially when having multiple dimensions (multiple parameters ? I'm having trouble picturing what it means), and because those methods require the use of the normalizing constant, which is a constant multiplying the prior x likelihood product so that the posterior distribution sum up to one, as a PDF should. 

What I don't get is that in the explanation of how a MCMC work (especially with the Metropolis-Hasting Algorithm), it seems to be implied that we are able to reproduce the posterior distribution because the expression of the likelihood is introduced inside the ""choice"" we make at each ""step"" of the algorithm; this choice ultimatly determining how much time the MCMC is gonna pop out a certain value, thus making us able to make an histogram resembling the PDF of the posterior, and drawing conclusions about it.

...But then, where do we get the likelihood functions from our data ? That's something I cannot understand, and it seems glanced other in many of the manual/courses that I read. Or is the likelihood directly infered of the PDF we choosed at first which should imitate the data-generating process ?

I'm sorry if the questions are blurry. I think I'm having real trouble understanding this notion of ""choosing a PDF"" before getting into modelisation or estimation/hypothesis testing. It always seems arbitrary to me, and I imagine that I don't see how much it is used in the rest of the process.

-------------------------------------------------------------------

I'm sorry if the questions are a bit fuzzy, and I'll try to get the to be more precise if needed. In any case, thanks again for making an awesome and very, very usefull subreddit. 

Have a nice day !
"
Dynamic Optimization Methods Assignment Help,0,1,False,False,False,statistics,1492771722,True,[removed]
Inference and model selection with very large sample sizes,12,3,False,False,False,statistics,1492771842,True,"Or alternatively, ""inference without relying to p-values too much"".


I recently had to tackle what amounted to a standard logistic regression problem - what is the effect of this or that covariate on this binary outcome, which covariates are the best predictors of the outcome, etc. All very straightforward.


The ""problem"" is that there were several million samples in the data set, so after applying a generalised linear model absolutely everything appeared to be statistically significant just because (I assume) the sample size was so large. 


What is the preferred way to approach statistical inference in this scenario? "
China Statistics Agency Sets Up Special Arm to Combat Fake Data,0,1,False,False,False,statistics,1492773149,False,
Advanced statistics: can they measure my apathy?,0,1,False,False,False,statistics,1492773686,False,
Indirect Discrimination in UK – You Know What They Say About Statistics,1,1,False,False,False,statistics,1492773744,False,
Butte Interagency Narcotics Task Force releases 2016 statistics,0,1,False,False,False,statistics,1492773803,False,
Study finds startling distracted driving statistics,0,1,False,False,False,statistics,1492773881,False,
Business Opportunity using Multivariable Regression Analysis,0,1,False,False,False,statistics,1492783743,True,[removed]
What are my chances of conceiving - 40% Each IVF 2x?,9,1,False,False,False,statistics,1492796670,True,"Hi Everyone! 

My husband and I are going through infertility and it seems like our only hope is IVF. My RE (Reproductive Endo) gave us a 40-50% chance of conceiving. We'd do a max of two attempts. 

I'd like to be on the safe side of estimating - so lets say I have a 40% chance each try...what is the cumulative percentage this may actually work?

Trying to understand what my odds are here. Thank you so much! 

"
Newbie question: How long does it take for SPSS to do its damn thing?,7,3,False,False,False,statistics,1492808209,True,"I'm running an ANOVA. I got my one ratio DV, oxy use over a 12 month period. I got 7 number of categorical IVs; race, sex, income, age, ""region"" (divided into metropolitan, micropolitain, and other), over/under poverty level, and government assistance.

I run my test with all these bad boys, add some descriptive, added a test for homogeneity (or it might have been homoscedasticity, I'm fucking burnt) and now I've been waiting for a long time for it to spit out the results. It still says its running, the task manager still says the SPSS engine is working, but I have yet to see anything. Computer is new, a consumerist floor model that one might pick up at Walmart. Should I continue to leave it alone or x the program out and try again? I don't remember it taking this long on the computers at school or on my laptop, but on desktop it is taking forever."
EGARCH coefficient interpretation in EVIEWS,0,2,False,False,False,statistics,1492811937,True,"Hello reddit. Could you help me with EGARCH coefficients interpretation? I know it shall be easy, but I am afraid of making a mistake. I have the following EGARCH model:

LOG(GARCH) = C(4) + C(5)*ABS(RESID(-1)/@SQRT(GARCH(-1))) + C(6) *RESID(-1)/@SQRT(GARCH(-1)) + C(7)*LOG(GARCH(-1))

With the following coefficients:
Variable	Coefficient	Std. Error	z-Statistic	Prob.  



C(4)	-0.469408	0.061574	-7.623453	0.0000

C(5)	0.340783	0.031643	10.76968	0.0000

C(6)	-0.001430	0.025087	-0.056992	0.9546

C(7)	0.977235	0.004770	204.8704	0.0000
				
Can somebody tell me what do these c(4-7) mean?
Tnx!"
Atlantic Causal Inference Conference Data Analysis Challenge,0,1,False,False,False,statistics,1492821030,True,[removed]
Setting Bayesian Priors for Categorical Predictors,21,9,False,False,False,statistics,1492823421,True,"I've been reading documentation on different Bayesian regression packages in SAS, R and Python and most of the examples specify priors for continuous predictors. Is there an easy framework for setting priors for categorical variables with many levels, or would I just have to encode them to numeric and set the priors that way?

"
Difference between Hierarchal Bayes and Bayesian Networks?,3,1,False,False,False,statistics,1492837449,True,Can someone enlighten me about the differences between these two things? 
14 Awesome Social Media Facts and Statistics for 2017,0,1,False,False,False,statistics,1492844584,False,
Trends and statistics: Women in higher ed,3,0,False,False,False,statistics,1492845525,False,
Opinion Journal: How Government Twists Climate Statistics,3,0,False,False,False,statistics,1492845962,False,
Recent Pharmacy Robbery Statistics,0,1,False,False,False,statistics,1492846158,False,
New Statistics minor offers students a more focused area of study within the mathematics department,0,0,False,False,False,statistics,1492846251,False,
"Nigerian First-Quarter Output Improved, Statistics Body Says",0,0,False,False,False,statistics,1492848362,False,
Trump Says Dreamers Shouldnât Worry. Statistics Say Otherwise.,0,1,False,False,False,statistics,1492848429,False,
forcasting financial time series Assignment Help,0,1,False,False,False,statistics,1492854510,False,
Graph Theory and Combinatorics Homework help,0,1,False,False,False,statistics,1492857335,True,[removed]
Industrial Engineering and Management Homework help,0,1,False,False,False,statistics,1492858111,True,[removed]
Anyone know how to remove the intercept box on a scatter plot in SPSS?,2,1,False,False,False,statistics,1492868177,True,"I've tried Googling and asking FB, can't seem to come up with anything. Maybe it can't be done. "
Some targeted marketing directed at baseball fans in New York and DC. Maybe this is why SAS licenses are so expensive?,18,81,False,False,False,statistics,1492872446,False,
"How good is the textbook ""Probability and Statistical Inferences"" for an undergraduate CS major looking to get into machine learning?",6,1,False,False,False,statistics,1492872707,True,
Grad School Dilemma: stay home or go away? Please help me with some input about the programs.,0,1,False,False,False,statistics,1492893112,True,[removed]
How to deal with missing values and data merging [EXCEL/STATA]?,0,1,False,False,False,statistics,1492893473,True,[removed]
Can someone help me with SPSS data analysis for interviews?,5,0,False,False,False,statistics,1492919375,True,"hey guys,

I recently completed interviews with 12 students.

How would I go about entering the data obtained into SPSS, and how would I analyse the data? What analysis should be done?

Thanks."
Success on Tinder is just a geometric probability distribution with p > 0.,0,0,False,False,False,statistics,1492920244,True,[deleted]
Inference from Data and Models Assignment Help,0,1,False,False,False,statistics,1492941649,True,[removed]
Chi-Squared: Using Yates' Correction for Continuity giving a very low p-value for data which shouldn't produce this?,3,1,False,False,False,statistics,1492943005,True,"Hi all,

Having an issue with a lab report on ants falling into carnivorous pitcher plants.

We recorded the number of ants visiting a pitcher plant and the number of ants that subsequently fell in for a pitcher plant under 3 conditions: dry, wet and re-dry (basically after we wetted it we then dried the pitcher plant using towels). I collated class data into [**this table**](http://imgur.com/XxYdXyZ) and the bottom row is the average of each column.

After doing an initial Chi-Squared (which we found to be significant p<0.05) we did a ""Post-Hoc Test of Pairwise Comparisons"" to see which of the conditions were significant. We were told to use the Yate's correction for continuity (as without it, it would produce an X^2 value too high). We then applied a Bonferroni correction by multiplying the p-value we obtained by the number of comparisons, in this case 3, to get a corrected p-value.

The issue is with the yates correction the corrected p-value we get for dry vs re-dry is 0.019 which is significant, however, as can be seen in the data [**here (middle column)**](http://imgur.com/wPypKo2) the data for the dry and redry are very close (19.75 and 19.67 for not falls) (0.04 and 0.08 for falls) which is very similar to the expected frequencies calculated. Surely since it is so similar the p-value should be high in this case?

I ran a chi-squared without a Yates correction and got p-values which make more sense which can be seen in the data [**here**](http://imgur.com/863zDEH). In this case Dry vs Redry gives a corrected p-value of 2.7 which is non-significant.

[**Here**](http://imgur.com/1vrdCzt) is the formula we used with the Yates correction for calculating the weighted square differences (before summing them).

[**Here**](http://imgur.com/380GXkd) is the formula we used without the Yates correction.

The [**wikipedia**](https://en.wikipedia.org/wiki/Yates's_correction_for_continuity) page states that the Yates' correct may tend to overcorrect but I am not sure if this is happening here?

Should we use the Yates' correction for these data and if we shouldn't what should our reasoning be?

Any help on this would be greatly appreciated!"
Large Sample Statistical Methods Assignment Help,0,1,False,False,False,statistics,1492943540,True,[removed]
247Sports,1,0,False,False,False,statistics,1492944173,False,
Linear regression online help,0,1,False,False,False,statistics,1492947321,True,[removed]
Linear Transformation online help,0,1,False,False,False,statistics,1492948770,True,[removed]
Markov Chain online help,0,1,False,False,False,statistics,1492949635,True,[removed]
Finding 95% CI for linear regression,0,1,False,False,False,statistics,1492949839,True,[removed]
Simple Main effect Syntax SPSS,2,3,False,False,False,statistics,1492955041,True,"Hello, really desperate for some advice. Im doing a 2x2 mixed factor ANOVA, within variable-referent (self vs other) and between subject (group 1 vs group 2). I have an interaction effect of referent and group however Im not sure I go about looking for simple main effects, I read that I need to use a different syntax but im not sure how or what the right syntax should be. 

really appreciate the help in advanced"
How to report an F value of 0.000009 in APA??,0,1,False,False,False,statistics,1492960410,True,[deleted]
Course work for grad level stats - how important is real analysis especially?,3,1,False,False,False,statistics,1492979618,True,"I'm currently in an experimental psychology masters program. My original plan had been to get a Ph.D. in psych, but now I'm looking at stats graduate programs. I'm currently taking pre calc 1, and I'm thinking I can take up through multivariate calc and linear algebra before I graduate (thank god for free classes through my assistantship). 

I'm also planning on taking a year to do pre-rec course work before applying to programs. However, if I graduate in June 2018 and graduate applications are due that December 2018, that means I have one semester to get any extra course work onto my transcript. What are my chances of getting into a Ph.D. program if I do a semester at a state college (Washington State U) to take real analysis 1 (plus a stats course, programming course, and possibly linear algebra 2). 

I could also take everything BUT real analysis at a satellite campus, plus I wouldn't have to go 10k into debt to do that. However, because real analysis seems like one of the most important classes to take, my chances of getting into a program goes down. 

Plan B, I could try for a masters program (needs to have funding), take real analysis while in that program, and then go for a Ph.D. after the masters (What are my chances of getting funding with only the basic pre reqs for a stats program as opposed to an entire math degree?).

If I didn't get into any programs with funding, I could still go for a math bachelors at WSU and then try to get into a PhD program (Plan C).

I know there's a lot of content in this post but I'm mostly concerned about that real analysis class, given that I would have to do an unfunded semester away from home just to take it, plus it seems like the most advanced class I'll need to take."
How to decide between an imputed model and a model that deletes missing cases list-wise?,5,7,False,False,False,statistics,1492984515,True,"I am having trouble understanding the methodology for this. I am building a multivariate logistic model where I want to understand the role of gender while controlling for all other variables. For one variable, I have over 20% missing data.

I imputed the data (MI) through SPSS and then used the imputed values to create a logistic regression. I also have a regression model where I deleted cases listwise. How do I decide which one is ""better""?

Do I just compare the -2 log likelihood between models? Do I look at changes in the beta coefficient and SE?

Any explanation of this would be much appreciated! Thanks!

I suspect my data is MNAR or MAR, but I can't be sure."
Need help choosing statistical analysis,5,1,False,False,False,statistics,1493007304,True,"Control group (regular therapy)
Intervention group (+ 30min adjunct MT treatment)

Both group --> get baseline
Both group --> get postbaseline after 3 weeks

I want to see if there is a significant difference in the average jump between control and intervention group.

That is, is the average jump in intervention group significantly higher than the control group."
Is there a hippocratic oath type thing that statisticians need to take when they finish their program and or designation tests?,0,1,False,False,False,statistics,1493012189,True,[removed]
How to evaluate two time series?,6,9,False,False,False,statistics,1493023731,True,"I have two price indexes: the official one of my country and one I have created myself using scanner data. Each index starts at January 2015, and for each month I have an index number (like 101.02, 104.23) which is the change of the price for that month compared to January 2015.     
How can I compare my newly devised index to the official index in a sound statistical way, to see whether they are similar (in numbers or in trend)? "
I could use some feedback on my modeling methodology,0,2,False,False,False,statistics,1493024110,True,"Note: I am not looking for anyone to do my work for me, just looking for feedback on how to improve my process. 

basically I run predictive models on pricing experiment data. IE I run a experiment where customers get one of 4 prices. 25% get price 1, 25% get price 2, etc. Lets call this the treatment. The response variable is the amount purchased (revenue).

For my modeling, I first split my data into Training/Validation. 

In my training data, I separate the data based on the treatment, and then run the model on the 4 sets of data. So i would have 4 different models. 

I then run all 4 models on each observation in the validation data. Each prediction is the predicted revenue for that particular price. So for customer A, he would have 4 different predictions. I then determine which model predicted the highest revenue, and call that the predicted treatment. 

In order to calculate uplift, I take those customers in my validation data where the predicted treatment matches the actual treatment. I take the average revenue of all said customers to determine the model revenue. I compare that to the overall average revenue of the validation data to compute the uplift.

Visualization: http://i.imgur.com/KwDH7Jn.png

Issues:
1. The accuracy of my models is basically irrelevant to the resulting uplift. 
2. Secondary, hard to get consistent uplift. 

I am trying to think of alternative methods to achieve my desired result, IE find which price would be best for each customer. Limitations: I have to offer one of the 4 prices. 

any feedback is welcome"
Measures of Central Tendency online help,0,1,False,False,False,statistics,1493029709,True,[removed]
Microeconomics online help,0,1,False,False,False,statistics,1493030474,True,[removed]
Multivariate Quantitative Research Methods online help,0,1,False,False,False,statistics,1493031124,True,[removed]
help with SPSS - running mann-whitney on multiple variables?,0,1,False,False,False,statistics,1493033075,True,[removed]
8 Benefits of Statistics Tutor Help Service for Securing High Score,0,1,False,False,False,statistics,1493035380,False,
Linear Programming Assignment Help,0,1,False,False,False,statistics,1493035892,True,[removed]
Managerial Statistics Assignment Help,0,1,False,False,False,statistics,1493037312,True,[removed]
Need help in analyzing a nominal data and a ratio data,7,1,False,False,False,statistics,1493037321,True,"So I have been conducting a mini-study for a project in my undergrad class, and I need some help. I'm trying to find out the correlation of the time of a class (morning/afternoon) and the final grades of the students that took that class. I thought about using point-biserial correlation, but I'm not sure if my nominal data is a dichotomous varaible. Any suggestions or tips would be appreciated. Thanks."
Pitfalls of weighted decision matrix,0,1,False,False,False,statistics,1493040921,True,[removed]
Word association in literature with machine learning,0,1,False,False,False,statistics,1493043876,True,[removed]
Online Statistics Tutor from Statisticshelpdesk.com Help Has Created a Special Niche,0,1,False,False,False,statistics,1493047172,False,
Question about standard error and standard deviation,4,6,False,False,False,statistics,1493057789,True,"Couple of questions:
1. Can standard error of the mean be less than or greater than the standard deviation of the population? 
2. Can standard deviation of a sample be greater than or less than the standard deviation of a population?

And if someone could help explain reason why they can or can't be greater than or less than, I really struggle with the understanding behind these numbers and grasping their outcomes. Thanks for any help."
The Massed Pipes and Drums - Edinburgh Military Tattoo - BBC One,1,1,False,False,False,statistics,1493058155,False,
Dumb question but do you have to constantly determine what constitutes an outlier?,24,11,False,False,False,statistics,1493060111,True,[deleted]
"[Survey] Everyone is invited, and appreciated for participating! Perfectionism, Personality, Death Anxiety, and more for data mining (Everyone; 15-20 minutes)",0,0,False,False,False,statistics,1493063011,False,[deleted]
[Survey] Rick and Morty Survey for a Uni Project.,2,0,False,False,False,statistics,1493068229,False,
Looking for a Health Stats tutor (will pay),0,0,False,False,False,statistics,1493073129,True,[removed]
Deductibles and benefits paid help?,0,0,False,False,False,statistics,1493084113,True,[deleted]
Which charts/tabkes/graphs can be used to compared two categorical variables?,4,1,False,False,False,statistics,1493090924,True,"Also which can be used to compare two quantitative variables?

If i need to compare a quantitative and a categorical variable i am assuming i would make the quantitative variable a categorical one. "
Is this real?,14,0,False,False,False,statistics,1493094510,True,"I just want a definitive answer, are these statistics true?
http://imgur.com/8iskmuZ"
We are now closer to 8 Billion People than 7 Billion on Earth,5,54,False,False,False,statistics,1493104858,False,
Methods of Statistical Computing Assignment Help,0,1,False,False,False,statistics,1493113976,True,[removed]
Multivariate Methods Assignment Help,0,1,False,False,False,statistics,1493115207,True,[removed]
Non-Life Insurance Assignment Help,0,1,False,False,False,statistics,1493115896,True,[removed]
Accepted into MA Statistics program. Have some questions for you guys.,12,12,False,False,False,statistics,1493124379,True,"About two or three months ago, I made a post inquiring about graduate school in statistics. Well, I have good news. I was accepted into Rutgers University. I didn't think this would happen because my GRE score was around average. Well, the Statistics and BioStatistics department decided to accept me. This is very exciting because Rutgers University Math and Statistics departments are prestigious and well known. I will be doing a Masters (M.A.) in Statistics.
 
I just have a few questions for you guys. Here is the list of courses that Rutgers provides. My question is: Is there any logical order I should take these courses in which would complement my learning of these subjects? Also, which electives do you think are beneficial to take? I don't want to take random electives which won't be useful to me in the future. Thanks for your time.

Required Courses:

563 Regression Analysis

582 Introduction to Methods and Theory of Probability

583 Methods of Statistical Inference

586 Interpretation of Data I

590 Design of Experiments

Electives:

540-541 Quality Control I and II

542 Life Data Analysis

545 Statistical Practice

553 Categorical Data Analysis

554 Applied Stochastic Processes

555 Nonparametric Statistics

565 Applied Time Series Analysis

567 Applied Multivariate Analysis

575 Acceptance Sampling Theory

576 Survey Sampling

584-585 Biostatistics I and II

587 Interpretation of Data II

588 Data Mining

591 Advanced Design of Experiments

595 Intermediate Probability"
"Question about using one thing to determine a random number, compared to multiple things to determine a random number",4,1,False,False,False,statistics,1493128267,True,"I am *almost* 100% positive I am right on this, but I would like to have some education on it and maybe some advice. 

--------

Lets assume for the sake of it, that the whole thing is done in Excel and the numbers are generated perfectly randomly (which I think is impossible but ignore that). Lets assume we want a number between 1 and 1000. So, assuming a perfect world, you roll a 100 sided dice that is perfectly weighted and nothing ever affects the roll. 

Each number should have a perfectly random chance of happening, correct? What if I wanted to use **two** dice to represent this?

Instead I take:
    
    10 sided dice numbered 0, 9
    10 sided dice numbered 10, 00 (or 100) in 10 increments

While in theory I can generate every number using both of these dice if you use the second dice to roll the range of 10, 20, etc and the first dice to roll the individual in that range. So for example:
    
    3, 80 = 83
    0, 00 = 1
    0, 100 = 100

Would this have a chance to generate each number the same amount of times? Not compared to rolling a 100 sided dice, but compared to its own pool of potential outcomes?

If yes or no, could I get a bit of an explanation on why not? My guess is no because of the logic my brain comes up with, which is probably wrong."
- The Boston Globe,0,0,False,False,False,statistics,1493129087,False,
Statistics students compete at UNM DataFest,0,3,False,False,False,statistics,1493129145,False,
Springer book recommendations,3,2,False,False,False,statistics,1493131299,True,"I'm about to graduate in a little over a week. My school has a deal with Springer that allows me to download all sorts of books PDFs for free. Anyone have any recommendations? Statistics, math, programming, science, whatever. Just looking to hoard."
"Random question: does ""random"" imply equal likelihood to you?",0,1,False,False,False,statistics,1493134625,True,[removed]
How do you check that you cleaned your data correctly?,7,2,False,False,False,statistics,1493137733,True,"I often work with datasets containing millions of rows drawn from a variety of databases and APIs.  I pull and clean all of this data on my own.  Occasionally this data has some obvious errors even after I do some sanity checks.  For example, I may have forgotten a line of code somewhere that results in a wrong comparison, or aggregated something incorrectly.  A script may work well on a subset of data but may have weird behavior when applied to the entire set.

How do you check that your data is what it should be?  I feel like there has to be some way of doing automated tests for this in R and Python, but I have no idea what typical best practices are."
Looking for an introductory textbook,5,7,False,False,False,statistics,1493152581,True,"I am interested in learning basic statistical concepts. I do not know calculus as of now, hence, I am looking for an algebra-based introductory text. I have found a number on Amazon, but all of them get decent reviews. I do not know which one to look into."
Comparing two groups - can someone help?,7,4,False,False,False,statistics,1493153048,True,"I am completely new to stats, but I have had to run an experiment, and I am not sure how to interpret my results. I could really use some help.

There are two groups in my experiment. Each of them is shown a different prompt, and is asked the same yes/no question after seeing the prompt. I am using SPSS (pretty much the only stats tool that I am familiar with right now), and trying to see if there is a difference between how these two groups answer the yes/no question. My hypothesis is that seeing Prompt 1 will get people to answer 'yes', while seeing prompt 2 will get people to answer 'no'.

What sort of a test would I use to check this? So far I have only compared the percentages of people in each group who gave 'yes' answers, but I imagine that this can't be fully conclusive."
Can anyone help me,0,1,False,False,False,statistics,1493155160,True,[removed]
Help! Stats question for a project!,4,0,False,False,False,statistics,1493155838,True,"Hi all,

I could REALLY use your help. I did a project that I now have to write up, and naturally I am having difficulty with finding anyone to help me.  I need to know what test to use in SPSS, but I am so stats rusty it just can't come to me.

This is the study: 4 kids were given 4 measures across three time points. I don't even know where to begin, but if anyone could PLEASE suggest the best test to use I would really appreciate it!

Thank you in advance :)

Best,
Ariana"
How do you read the output from eacf function in r?,0,0,False,False,False,statistics,1493161028,True,Someone please explain or share a link that can help me understand.
Multiple Correspondence Analysis on SPSS- need help!,4,0,False,False,False,statistics,1493164262,True,"I'm new to using SPSS and trying to self-teach...I want to run a MCA for some survey data and am looking for some guidance in setting it up in SPSS. My variables are can be classified as nominal (although technically two of them really could be ordinal- I'm not sure if these matters- and if so any suggestions? Would a Non-linear Canonical Correlation Analysis be better with these variables??) 

I've tried running it a few times just to see where I get with trial and error but I'm just not confident I am doing it right. I'm also confused with the graphs SPSS produces because the axis are labeled as dimension 1 and dimension 2 and I'm not sure how to interpret that.

Any help or guidance would be much appreciated, thanks!"
Masters programs,10,1,False,False,False,statistics,1493176542,True,"Hi, I'm a senior at Carnegie Mellon studying statistics. I've gotten into both CMU's MSP program and NYU's data science program and I'm unsure of which one I want to do for a career in data science/statistics. If anyone has any experience with either program or any recommendations for resources to help make the decision, it would be appreciated.
Thank you"
How do i compare groups of data using a side by side histogram.,9,3,False,False,False,statistics,1493179311,True,I am comparing basketball turnovers in the NBA by position. I have made side by side histograms for each position but now i dont know what to say. How do I read them and compare?
What is a measure of the ability to detect a true effect called?,6,1,False,False,False,statistics,1493184588,True,[deleted]
Did I give bad advice? (GPA related),0,1,False,False,False,statistics,1493184792,True,[removed]
What is it called when you don't obtain significance but a true effect does not exist?,6,1,False,False,False,statistics,1493185188,True,[deleted]
Do researchers usually want a higher t value and a smaller p value?,1,0,False,False,False,statistics,1493186956,True,[deleted]
What test allows you to make specific predictions using correlational information?,0,0,False,False,False,statistics,1493187600,True,[deleted]
Reality Check - The Theory of Multiple Intelligences,10,14,False,False,False,statistics,1493188434,False,
"In a new edition of Chopped, the t.v. audience rates how good the food looks for all 4 contestants. You need to compare ratings across all 4 contestants. What statistical test do you use?",4,1,False,False,False,statistics,1493188867,True,[deleted]
Hey I need some stats help,0,1,False,False,False,statistics,1493193244,True,[removed]
How can I prove that there is a significant difference between my samples?,18,1,False,False,False,statistics,1493194885,True,"Hey r/statistics,  
  
So I made an international survey during the last few months and now I am at the interpretation part.  
You can see the graphs to one of the questions [right here](http://i.imgur.com/62lsVr6.jpg).  
So I made a bar diagram that shows that the samples are differing. On the right side the mean and the standard deviation are displayed.  
On the bottom you can see a column diagram that shows that there is no normal distribution at that the sample sizes are different.  
Now I'm not pretty good when it comes to statistics so I need some guidance.  
The samples are independent and ordinally scaled.  
So if I wanted to compare 2 samples I would use a Mann-Whitney-U Test, is that right?  
But because I have 4 samples I need to use a Kruskal-Wallis-Test (h-test)?  
  
I need some confirmation or clarification to know if what I'm doing is right or wrong.  

Edit: unwichtig means unimportant and sehr wichtig means very important. 18 bis 40 means 18 to 40.
  
*Thanks in advance,  
Pascal*"
Nonparametric test Assignment Help,0,1,False,False,False,statistics,1493199283,True,[removed]
Partial differential equation Assignment help,0,1,False,False,False,statistics,1493200509,True,[removed]
Prediction and Predictability in the Atmosphere and Oceans Assignment Help,0,1,False,False,False,statistics,1493201554,True,[removed]
Poisson Distributions online help,0,1,False,False,False,statistics,1493203366,True,[removed]
Quantitative Methods online help,0,1,False,False,False,statistics,1493204653,True,[removed]
SPSS Help,0,0,False,False,False,statistics,1493211023,True,[removed]
Six Sigma Black Belt,1,0,False,False,False,statistics,1493211026,False,
Confused about using chi-square - please help?,16,5,False,False,False,statistics,1493213919,True,"I am conducting my own research for the first time, and got really confused by something that my supervisor said to me today. He advised me to include a chi-square test for the following table, but given my limited knowledge of stats, I am really not sure what this adds to the interpretation of my data. The table presents answers to two questions with the same wording, but which ask about two different activities. The questions are (1) How happy are you with your fitness regime?, and (2) how happy are you with your budgeting regime?. The table is structured as follows:

Answer Category | Fitness Q | Budgeting Q
:--|:--|:--  
Very happy | 54.2% | 26.3%
Moderately happy | 34.1% | 46.8%
Moderately unhappy | 7.4% | 18.1%
I am very unhappy | 4.3% | 8.8%

To my knowledge, a chi-suqare test would make sense here if this was a crosstabs table, but it is not. It literally just lists answers to two questions side by side (so in the first row, it is 54.2% of all respondents who said 'very happy' to the fitness Q, and it is 26.3% of ALL respondents -- not the 54.2% -- who said very happy to the Budgeting Q).

I would be really grateful for an explanation, as this is driving me insane. Could it possibly be that it is not clear enough that this is not a crosstable, and that’s why my supervisor suggested chi-square?
"
need help with7 point likert scale interpretation,0,1,False,False,False,statistics,1493215799,True,[removed]
Nike EYBL Statistics Leaders: Session I,0,0,False,False,False,statistics,1493216124,False,
How statistical thinking should shape the courtroom,0,3,False,False,False,statistics,1493216271,False,
How Many Tetri Are in a Lari? The Importance of Municipal Statistics for Good Governance,0,1,False,False,False,statistics,1493216355,False,
China Indicts Former Statistics Bureau Director Wang for Bribery,0,1,False,False,False,statistics,1493216466,False,
“Smart Baseball” Is a Baseball Statistics Education in Three Parts â The Hardball Times,0,2,False,False,False,statistics,1493216523,False,
"Germany sees rise in crime committed by migrants, statistics show",4,0,False,False,False,statistics,1493216586,False,
Is it possible to generalize discriminability in terms of the type of transformation made on the data? Rather: does decreasing the standard deviation improve discriminability almost all of the time?,3,1,False,False,False,statistics,1493217583,True,"Can I say that discriminating between two populations (say, using a t-test) will be easier/harder after performing a certain type of transformation on the data?

For example, under certain conditions, will a log transformation always increase or decrease the t-score from a t-test?

Is describing the effect of a transformation on the standard deviation basically answer my question?"
[META] What's going on with the moderation?,8,46,False,False,False,statistics,1493218879,True,"Seems like the sub is starting to get overrun with spam posts that just have ""statistics"" somewhere in the title, or have a statistic in them (see rule 3), that are just sitting around on the front page for days.

This has been an ongoing problem for a while now, and I've tried to contact the mods privately to no avail. Then I started looking at the mods' activity histories and noticed something:

/u/koolkao: Last visible activity 2 years ago

/u/RA_Fisher: Last visible activity 7 months ago

/u/talgalili: User acct no longer seems to exist

So half the moderation staff of the sub has completely disengaged from reddit.

To the mods who are actually still here: I'd strongly recommend one of you go over to /r/redditrequest and request to be promoted to top mod so the inactives can get booted out and we can get some more actual active people in here to help get things cleaned up."
Looking for a statistical term to describe chance consideration thresholds.,7,1,False,False,False,statistics,1493223238,True,"I'm looking for a term/name that describes the threshold to which a value drops outside the realm of consideration. In context it might be used to describe someone's acceptance of risk; Once the chance of something happening drops below this threshold, there ceases to be consideration of that event. You walk down the street and never have the thought of an asteroid hitting you because it is so beyond this threshold the very idea is not even conceived. If someone asked if I was worried about an asteroid, I would be able to say that I don't because it is beyond my consideration threshold and don't see a benefit in taking precautions. I am looking for a word/name for that threshold, does one exist?"
Tools for Practicing Custom Contrast Development,0,2,False,False,False,statistics,1493235719,True,"I am having an incredibly hard time wrapping my mind around how to properly develop custom orthogonal contrasts. I know practicing would help, but it's hard to find examples to work with that are explained in detail. I'm trying super hard. I think the issue is that I don't know how to approach the problems and need more practice. When I look at a completed complex orthogonal contrasts, I just don't see how they address the questions at hand or how the researcher got to that point. It's like reading another language. 

There are a billion websites that will tell someone what an orthogonal contrast is, as well as simple things like comparing 2 levels of the same variable or doing simple polynomial contrasts on one variable (there are literal tables for those, which is nice). I got that. 

But, what about the hard ones? Testing cubic trends while simultaneously comparing the averages of groups of levels in 3x4 ANOVAs? Multiplying two complex contrasts to examine them both in combination? I don't even know if I'm phrasing that correctly because I have no idea what I'm doing.

Are there any websites, books, or resources that will guide someone through the logic of developing complex custom contrasts? Preferably something with practice problems and an accompanying answer key.  "
Statistically significant,9,0,False,False,False,statistics,1493244741,True,"If I have a p value 0f 0.015 with a r2 pseudo of 0.275, would that suggest the model describes the variance well and it being statistically significant?

Is that the right use of pseudo r2

*For clarity, this isn’t an actual statistical question that is being asked.  it is a question of term use. I am trying to understand an academic paper’s statistical analysis that says something similar without the phrase pseudo r2, it says 27% of the variance. 

I am trying to understand if I am understanding where the number came from when I look at the statistics table’

Edit: clarification. "
Confidence Interval,1,1,False,False,False,statistics,1493245672,True,[removed]
For me statistics is an art.,1,0,False,False,False,statistics,1493246755,True,[deleted]
What is the probability that this guy is my soul mate?,0,0,False,False,False,statistics,1493247529,True,[removed]
This is on my business statistics review and seems so simple yet I cannot figure it out. Could anyone help?,3,0,False,False,False,statistics,1493250755,False,
Scatter Plot,2,1,False,False,False,statistics,1493260111,True,"I coducted a research project where I had to collect survey results. I'm trying to determine if there's a correlation between time spent checking a smartphone vs GPA. 

I'm not quite sure how I would go about making a scatter plot. Because on the survey instead of asking for a specific numerical value, we gave a few interval options. I know how to make a scatter plot with numbers but not when the respondent has to select a particular range for an answer. "
Chances of getting into MS Stats with 2.9 GPA,21,5,False,False,False,statistics,1493261577,True,"So I did my undergrad in physics and battled depression my last two years of school. It was a combination of my father being in poor health and feeling that I wasn't learning anything relevant or useful in the real world from my major. I ended up getting a job in finance after graduating and am doing quite well now, but want to move into a more quantitative role. I'm very interested in data science/analytics and it looks like getting an MS in Stats is a good way to go about that. However, my undergraduate GPA was a 2.9.

Right now I'm interested in getting my MS from UCLA. On the site one of their requirements is a 3.0 GPA. Would I be able to make up my 2.9 with a stellar GRE score, solid work experience, and good letters of rec? My letters of rec would be from industry not academia so I'm not sure if that would also hurt my chances. 

Any insight is appreciated. In the meantime I plan to take up several MOOC courses and start building up a portfolio/blog but I'm worried none of that matters because of my GPA. "
Major in math or computer science to be a statistician?,0,1,False,False,False,statistics,1493266499,True,[removed]
statistics quickie,0,1,False,False,False,statistics,1493268292,True,[removed]
Fitting SARIMA-models with exogenous variables.,6,6,False,False,False,statistics,1493278955,True,"So I have a time series of electricity consumption data which I'm trying to model using a SARIMA-model with R. My problem is that during the observed period of time there seems to be a linear upward trend in the data, so according to my understanding I should apply differencing in order to stationarize the data before a SARIMA model can be fitted.

However I also have a time series of temperature data during this period which I can use as an exogenous variable in the model. The thing is, there is a clear downward trend in the temperature data, which I suspect is the cause for the apparent upward trend in the electricity consumption data.

If I difference the time series before fitting a model this in effect cancels out the effect the temperature has had on the data, and I end up with nonsensical estimates for the temperature coefficients even thought there is a strong negative correlation between these two time-series. The predictions I get are also bad. If I don't difference the electricity data, and fit the model on non-stationary data I get a clearly negative coefficient for the temperature and the predictions are way better. So it seems like I get better results by fitting the model for a non-stationary time series even though this is theoretically wrong according to my understanding. I'm just wondering if anyone can give me advice on what to do here."
Quality Control and Its Management Assignment Help,0,1,False,False,False,statistics,1493288035,True,[removed]
Statistics Homework Tutors,0,1,False,False,False,statistics,1493288644,True,[removed]
Statistics Homework Tutors,0,1,False,False,False,statistics,1493289249,True,[removed]
Random Walks and Diffusion online help,0,1,False,False,False,statistics,1493294835,True,[removed]
Sampling And Large Sample Tests online help,0,1,False,False,False,statistics,1493295829,True,[removed]
Scheduling Theory online help,0,1,False,False,False,statistics,1493296599,True,[removed]
Calculating the overlap area of two distributions,15,2,False,False,False,statistics,1493299623,True,"Hi! I'm wondering how to solve the following statistic problem:

Let's say I have two populations, A and B, being each one Poisson distributed. How could I calculate the overlapping area between these two populations?. It must be possible, but I don't figure out how.

PS: I'm not looking for statistic tools or R scripts, etc... but for any reference to a formal explanation or an expression that let do the calculus ""by hand"".

EDIT: Ok. So lets better talk not about ""area"". Les imagine two Poisson populations differing in their lambda parameter (ie. lamda = 4 for A, and lambda = 10 for B). If you plot the corresponding probability mass functions, these functions overlapp along k, but they will differ in every each P(k) as long as their lamba parameters are different. Now, the question, ihoy could I measure in terms of probability the overlapping in these two, not areas, but probability mass functions?"
Ranking scale help,4,4,False,False,False,statistics,1493306432,True,Looking to move a company ranking scale from a 3-star to a 4-star system. Anyone know of any useful academic literature to get us on that path? 
Looking for something to describe an incremental and meaningful change in utility,2,1,False,False,False,statistics,1493313482,True,"My stats background isn't that strong but I've taken several statistic classes ranging from undergrad to MBA program stuff (still I know that's only the very tip of the iceberg). I couldn't come up with anything that I know if that would quantify what I'm looking for.


In baseball there is a tradeoff in power (slugging%, home run, isolated slugging, etc...) for contact. Go look at any of the league laggers in contact% (percentage of the time they make contact when they swing) and they are almost always guys who hit for a lot of power. The inverse is true as well. Looking at a list of contact% league leaders usually yields players with little power. This is just the general way hitting works; there's a correlation between contact% and power and the guys who don't have to sacrifice contact% for power are usually really good hitters.


So what I'm looking something that answers ""what's the minimum threshold for contact% to make power useful"" or perhaps said differently ""when does a sacrifice in contact% not create a meaningful amount of power?"" 


In a way I'm looking for when does the delta of say 1% of contact% loss stop adding meaningful amount of power. When is the tradeoff between contact% for power not useful anymore. Something like the marginal utility of the drop.


I've got more than a decades worth of data so that won't be an issue, I'm just not sure what methodology to approach this with.


And FWIW I use [MegaStat for Excel](http://highered.mheducation.com/sites/0077425995/information_center_view0/index.html) primarily."
Handy statistical lexicon,0,20,False,False,False,statistics,1493316779,False,
PhD in Biostats:,2,0,False,False,False,statistics,1493317879,True,"All, 

I am working on applications for biostatistics PhD's in the US and abroad for next fall (2018). I am currently midway through my MsC in Epi/Biostat. I am in my Calculus series, as it seems recommended on all program requirements virtually across the board. However, this semester some extenuating circumstances in my family forced me to take a Withdrawal from Cal 2. I am panicking on my timeline and whether or not that leaves me the right amount of Calc coursework to justify my application (waiting until 2019 is simply not an option). Does anyone know from experience or through others that the completeness of Cal I-III and Linear Algebra and/or Real analysis should happen *before* applying? 
GRE scores, GPA and research experience are not a concern for me. 

**TL;DR** Can my calculus, linear alg, etc., series be ongoing as I apply to Biostat programs and still have legitimate chances for admission? 
 "
SurveyMonkey Data Analysis Challenge Re: Correlations Please help,11,1,False,False,False,statistics,1493318357,True,"Unfortunately SurveyMonkey won't do the analyses I'm needing to be done because of the structure of the survey. It is a paid account. Any suggestions about how to do this analysis simply? Frustrated and expect to have to export raw data for the first time (likely for Excel because I don't have a current copy of SPSS on this machine.)

The nature of the study compares various degrees of user participation with the design of a variable number of objects/changes in the home. Users have the choice to answer a short set of questions for any number of these devices 1-6. They can jump to the end of the survey after discussing any of these objects up to a maximum of 6. Ex. Thanks for discussing 1 object, would you like to discuss another? Yes=Go on No=Advance to end of survey. Ex. I believe I was listened to when product x was being designed (strongly agree-Strongly disagree) / I am satisfied with the final product (strongly agree-Strongly disagree) 

Essentially I'd like to combine all the answers given for all devices discussed (up to 6 per person- many only answered for 1 or 2 devices and jumped to the end) into a single data set. 

Any and all help greatly appreciated. Thank you!!!
"
Probability Modeling for an RNG event happening twice in a single event and then once more in an independent event,0,1,False,False,False,statistics,1493318440,True,"So the TLDR of a situation, A friend and I are trying to figure out the exact probability of this even happening twice for a League Of Legends. In a single game most times there are five dragons to spawn, with 4 different elements with equal probability (air, water, earth and fire). Assuming each team has equal chance to take the dragon(50/50), what is the likelihood of having TWO FIRE DRAKES to the SAME TEAM, TWO GAMES in a row? 
Wouldn't the model be similar to 
(.25^2 x 5) = spawning two fires
.5^2= the chance of taking the dragon to the same team twice
(((.25^2 x5))(.5^2 ))) ^2 = probability of two fires, same team taking, two games in a row? I feel like I am missing the probability for the ""in a row"" factor
"
Should I be using a stratified randomization model for my clinical trial?,0,1,False,False,False,statistics,1493320957,True,[removed]
Euclidian distance on multiple continous variables,13,10,False,False,False,statistics,1493322859,True,"Hello fellow stats-enthusiasts.

I have a problem that is a little out of what I  normally work with. This is a short example of my full problem, but I think if I get to solve this, I can figure out the rest. Let's say I have 20 variables (all continuous, all positive values) and I want to calculate one single measurement of distance (nearest neighbor) so that I can find observations that are most similar in regards to all of the variables. I have thought about clustering of sorts and also propensity scores, but since I do not have an outcome or treatment, I've ruled propensity scores out. Any thoughts?"
Introduction to Structural Equation Modeling book recommendations,4,8,False,False,False,statistics,1493332780,True,"Hi all, I'm working on my dissertation right now and am trying to employ a generalized structural equation model to fit a multi-level multinomial model in Stata. While I've read a lot about the ways to do this in Stata, I am looking for any suggestions for beginners guide books to SEM. Something with a social science lens would be preferred but not necessary. "
Quick question on Simple Random Sampling,1,5,False,False,False,statistics,1493366705,True,[deleted]
Stat Crunch online help,0,1,False,False,False,statistics,1493374932,True,[removed]
Statistical Simulation online help,0,1,False,False,False,statistics,1493376493,True,[removed]
Sampling Concepts And Methods Assignment help,0,1,False,False,False,statistics,1493377033,True,[removed]
Stochastic Processes online help,0,1,False,False,False,statistics,1493377257,True,[removed]
SPSS Assignment Help,0,1,False,False,False,statistics,1493379246,True,[removed]
Survey about concussions and sports (Spread Awareness!),1,0,False,False,False,statistics,1493386882,False,
Why is the distribution to the p-value then the null hypothesis is true uniform?,14,7,False,False,False,statistics,1493386933,True,I'l learned this in my stat class today and I'm really struggling to grasp why this is the case. So I'm wondering if there is a intuitive way to explain why this is true?
Statistical Consulting Services – Take Your Business To The Next Level,0,1,False,False,False,statistics,1493389423,False,
"If a mixed models fit to panel data requires a random intercept component, would a survival model fit to the same data require a frailty?",0,1,False,False,False,statistics,1493391706,True,"I have read a few texts on survival analysis, and while many of the examples are based on panel data, they never really mention the problem of heterogeneity in repeated measures designs, so it is not clear whether a frailty would be necessary for survival analysis based on panel data. 

Singer and Willett note that each wave is considered conditionally independent given the previous wave, iirc, which implies to me that heterogeneity is not an issue in this case. However, that would also seem to imply that it is never an issue, which is certainly not true. "
What are some techniques for examining suspicious selection of sampling sites in a survey?,1,10,False,False,False,statistics,1493392342,True,"For a massive project to be authorized (e.g. mining camp, highway, hydroelectric station), governments require an Environmental Impact Assessment to be conducted. Part of this assessment requires a survey of the area for the purpose of documenting all the species living therein which may be affected by the project.

I have reasons to suspect that the sites being picked for these surveys are selected in order to minimize the probability of finding anything of value. Now, because terrain conditions often can't be assessed before getting to the actual place, random selection of sites is not a viable approach. Additionally, there may be distinct microhabitats within the area which should constitute differentiated strata for sampling. 

I've been thinking about ways of uncovering intentional undersampling of the true number of species in a given area. Naturally, using outside information to compare if reports from other similar habitas provide similar information is a viable approach. But I was wondering if this type of problem might already have been tackled by someone in the past and if any well-established techniques to study the issue exist."
"Thesis help - what method to use to measure correlation, Pearson or bi-weight?",4,0,False,False,False,statistics,1493393485,True,"Hello,

I am writing my thesis and I want to find out the correlation between the index of the level of financial literacy and the household disposable income in a X country during a period of 10 years. 

I am struggling with the statistics part of it because I do not know what method is the best one to use for what I want to measure - if I shall use the Pearson correlation coefficient or the bi weight correlation - or perhaps even some other method! What about a simple t-test on Excel?

Thanks a lot in advance for any help!"
How to handle missing data in a repeated measures design?,7,15,False,False,False,statistics,1493395611,True,"When running a simple repeated measures design, I am familiar with data replacement techniques (such as imputation with multiple iteration ) but I have some other concerns. If a subject's first data point  (say on the first test or trial) is lost, say due to technology error,  or if they drop out a study say at test 3 of 6, is it valid to keep the remaining  data points  of said subject? 
"
"Which undergrad stats courses to take, not a stats major.",5,5,False,False,False,statistics,1493400710,True,"Hey rstats, I'm a computer science undergrad senior. I would like to work with data and statistics in some way. I gained an interest after spending a summer working on institutional effectiveness at my workplace. I work as a tutor and we decided to do a study about how our tutoring clients perform versus non-tutored students. The data scrubbing and stats work was really fun, and I'd like to do more. 

So far, I've only taken 2 courses, both intro stats courses, one geared towards science and the other geared towards psychology. I would like to look into getting an MAS in stats, and I want to know which of these undergrad courses I should take while I have the chance. I don't have a particularly strong desire to be an academic statistician, but would rather work in industry, hence the MAS. I can fit 1-2 of these in before I graduate: 

* Applied Regression Analysis
* Applied Experimental Design
* Nonparametric Statistics
* Computer Applications in Statistics
* Probability Theory
* Mathematical Statistics 1
* Mathematical Statistics 2

Are any of these particularly important? FWIW, I've taken 3 semesters of calculus and linear algebra. "
Help with Interpretation of Interaction Effect,2,1,False,False,False,statistics,1493413138,True,"Hello,

I am running a model in SAS and I wanted to confirm an idea.  I have many variables in a model, but two in particular (A and B) that I want to focus on.

A is a binary (0, 1) variable.  B is a categorical (0,1,2,...9) variable.

I wanted to run a model with all variables and an interaction of A and B, but then not include the main effects of A and B.  So, I would have:

response = all other variables + (AxB) + (random error)


Is there any meaningful interpretation of the interaction without the main effects?  My guess is no, but I wanted to see what others thought.

Any help is appreciated!"
"When an ARMA (1,1) model has a phi=theta, isn't that just white noise?",7,0,False,False,False,statistics,1493416680,True,
I need help/guidance for a problem I'm trying to solve...,0,0,False,False,False,statistics,1493431212,True,[removed]
Are there any sites where I can read stats related research papers? I wouldn't mind paying a little money if it's required.,7,7,False,False,False,statistics,1493453120,True,Thanks guys! I appreciate the replies. 
Statistical Methods for Analyzing Unbalanced Data,0,1,False,False,False,statistics,1493459284,True,[removed]
Statistical Problem Solving by Computer Assignment help,0,1,False,False,False,statistics,1493459945,True,[removed]
Stochastic Estimation and Control Assignment help,0,1,False,False,False,statistics,1493461339,True,[removed]
How do you subset data by a partial word in R?,3,10,False,False,False,statistics,1493461664,True,[deleted]
Black US New Born Babies Die more then White Babies,3,0,False,False,False,statistics,1493463165,False,
Time Series Analysis online help,0,1,False,False,False,statistics,1493463435,True,[removed]
Advanced Regression Analysis online help,0,1,False,False,False,statistics,1493464306,True,[removed]
Shop for Kaffemaskiner at Unikefunnno BigBuy Gourmet Kjkken Kaffemaskiner Sm Kjkkenapparater,0,1,False,False,False,statistics,1493467046,False,
Which test to use - pre&post-test scores with 2 groups?,3,2,False,False,False,statistics,1493475639,True,"I am looking at pre- and post-Intervention scores for students at one school, and am trying to compare scores against a comparison school too.

Which test? 2x2 factorial ANOVA? Any help appreciated!!!!"
Advice for university statistics,0,1,False,False,False,statistics,1493476466,True,[removed]
stochastic approximation using samples,0,0,False,False,False,statistics,1493477387,False,
what is an alternative to Multiple Regression?,0,1,False,False,False,statistics,1493484820,True,[removed]
Help with Power Analysis,5,1,False,False,False,statistics,1493485800,True,"So, I'm looking at several programs to use for conducting a post-hoc power analysis on data I've collected.  I've got two datasets that I would like to use and trying to determine the best way to approach it.

The first dataset contains data from 25 individual samples from two locations (50 data points).

The second dataset contains 5 samples of 5 bulked specimens (each data point is an average of 5 subsamples), from the same two locations (10 data points total).

The reason the second data is smaller was due to the length of time in measuring the response variable.

I want to make direct comparisons between the two datasets, so I believe I can just average each of 5 data points in the first dataset and arrive with 10 data points.  Is this correct?

Second, I want to use this data to determine an appropriate sample size for future assays of the response variable.  I can use the first dataset to identify the number of specimens needed, correct?  So  how can I use the second dataset in a similar manner?  Can I use it to determine the number of replicates (blocks) needed, as opposed to just evenly splitting the needed sample size from the first dataset?

Ex.  If from the first dataset I determine I need a sample of 50 specimens to have a power of .80, I can equally split 50 specimens in 5 groups of 10.  What I determine though, from the second dataset, is that it is more effective to have 10 groups of 5 specimens than the previous grouping method.

Does that make sense at all?
"
"Hierarchical Regression, supporting hypothesis!? help!",0,1,False,False,False,statistics,1493488643,True,[removed]
Paid tutoring,1,1,False,False,False,statistics,1493492848,True,[removed]
"Hi! I have an easy question for you, I need help with determine what kind of variable this is: ""Percentage of working age (aged 16-64) males claiming Key Benefits""",0,0,False,False,False,statistics,1493497764,True,[removed]
When can I sum dice probabilities?,5,2,False,False,False,statistics,1493499424,True,"Hello guys, I was trying to explain some friends the odds of either of two dice rolling a particular number and told'em that it would be 11/36, simply cause 1- (5/6*5/6) is 11/36. They insisted that it should be 1/3, since it would be 2*1/6, which I know it's wrong, but I couldn't point out in a simple way why it's wrong. I know it's a quite simple problem for the forum standards, but I just couldn't explain them why it's wrong to simply sum the probabilities. Could you guys give me a light?

- sorry for the poor English"
Is there a way to calculate with a degree of accuracy the initial ballots for an alternative vote ballot?,3,2,False,False,False,statistics,1493504480,True,"The question came to my mind when I was talking about recent Hugo Award scandals which nominate for awards using an alternative vote system and in reviewing past ballots  there was one in particular - the 2011 Best Novel award - which I found very perplexing because it was not as straightforwards as the others.

http://www.thehugoawards.org/content/pdf/2011%20Final%20Ballot.pdf

Can anyone help me figuring this out? Is there an algorithm or a formula to do it?
"
How to calculate if a test result is significant?,6,10,False,False,False,statistics,1493511389,True,"Say I have 1000 races.  Each race has 8 sprinters, and one winner at the end.  I want to calculate how good I am at guessing the winner for the race.

From those 1000, if I randomly choose a pool of 5 races to guess the winner, and I get it right 5 times. How would I calculate if that result is 'significant'?

Intuitively, the above example does not seem 'significant'...picking 5 out of 1000 and getting those right could be by random chance.  Whereas if I had a pool of 50 from that 1000, and got all 50 correct, that would be more 'significant'.  And If I only got 1 out of those 50 correct, that would also be significant...in telling me I'm rubbish at guessing the winner.

I hope my example makes sense..."
Average,0,1,False,False,False,statistics,1493531054,True,[removed]
Calculate probability of another positive/negative number appearing,0,1,False,False,False,statistics,1493542138,True,[removed]
Applied Multivariate Analysis online help,0,1,False,False,False,statistics,1493548433,True,[removed]
Binomial Distribution online help,0,1,False,False,False,statistics,1493549160,True,[removed]
Time Series Analysis Assignment Help,0,1,False,False,False,statistics,1493549645,True,[removed]
Census and Sampling online help,0,1,False,False,False,statistics,1493549942,True,[removed]
Weka Assignment Help,0,1,False,False,False,statistics,1493550371,True,[removed]
Double majoring?,16,1,False,False,False,statistics,1493555236,True,"If I had both a major and a MSc in stats and a minor in math, would it be any valuable also having a philosophy bachelor's on the side in the eyes of an employer?

Edit: In the case I don't end up majoring in philosophy I wouldn't just not double major, I'd get a bachelor's degree in either math or economics on top of the stats one. Explaining this as many people are assuming I'll either go for philosophy or not double major."
Accelerated Cohort Design: Growth on Growth Model,4,3,False,False,False,statistics,1493573019,True,"I am currently attempting to analyze a dataset using the COHORT package in MPlus to take full advantage of the accelerated cohort design. And I am running into some problems and would like some help. The help does not need to be in MPlus. 

A little background: 
The dataset under investigation includes participants in grade school with multiple different cohorts. Participants were assessed 13 times, with the final timepoint of one cohort, matching up with the first timepoint of another. This means that there is multiple years of data (developmentally speaking) that was measured only over a couple of years. 

I would like to run a growth on growth model, taking advantage of this accelerated cohort design and MPlus isn't cooperating with me. I wanted to reach out to this wonderful community to see if there were any other suggestions in other programs (possibly R). 

Thanks for reading! "
How can we apply statistics to this situation? In 2016 I applied to 1000 positions in software and received zero job offers. • r/jobs,5,0,False,False,False,statistics,1493595481,False,
STATISTICS PROBLEM PLEASE HELP!,0,1,False,False,False,statistics,1493604654,True,[removed]
Free program to chart normal distrubtion / skewness,4,1,False,False,False,statistics,1493607128,True,"Any recommended software out there to chart the normal distribution / skewness etc.

Preferably easy to copy excel cells into and preferably free. I have about 4000 data points."
Help with Conditional probability,2,0,False,False,False,statistics,1493608453,True,[deleted]
Post Traumatic Stress Disorder (PTSD) Marriage Statistics and Encouragement,0,0,False,False,False,statistics,1493617484,False,
What is the difference between statistics and machine learning?,31,22,False,False,False,statistics,1493618355,True,
Wilcoxon Mann Whitney Test - p-result differences,9,1,False,False,False,statistics,1493629912,True,"Hey r/statistics,  
to keep this short and simple:  
I calculated a p-value using the Wilcoxon Test for 2 samples (n1=95,n2=60) but the results are different depending on which programm I used:  
Excel result: p-value = 0.2046  
[Online calculator result](http://www.socscistatistics.com/tests/mannwhitney/Default2.aspx): = 0.2046  
SPSS result: = 0.1940  
R result: = 0.1940  
  
So there is a bit of a difference when I use my excel formulas and online calculator (both based on normal distribution z-scores) or if I use the statistical programs R and SPSS.  
My question is: What is the reason for this difference? Is it that R and SPSS are calculating an exact p-value instead of using the normal distribution approximation?   
I'm a total newbie to both R and SPSS but their results are certainly more trustworthy..  
  
Edit:  
This is what I found in the R help box: ""By default (if exact is not specified), an exact p-value is computed if the samples contain less than 50 finite values and there are no ties. Otherwise, a normal approximation is used.""  
So the difference should be somewhere else..."
How to remove some columns from a dataframe without having the other colums shift.,5,4,False,False,False,statistics,1493631849,True,"Hello! I have a dataframe containing the results from a survey I made. The dataframe contains 252 observations and 22 variables.

I was already succesfully able to separate the data into two dataframes, one containing the demographics data and another with the data for the factor analysis. 

    data = as_tibble(data)
    
    
    demographic = data[,1:6 ]
     
    factordata = data[, 7:22]
Now *factordata* has the remaining variables. However I have encountered a problem. For some reason an absurd amount of people haven't filled in questions ""ISP_1"",""ISP_2"",""ISP_3"",""ISP_4"". I would like to remove them from *factordata*, and do my analysis without them.

What I have tried and why it failed: 

    factordata$ISP_1 = NULL 
    etc...
When I do this the columns with the  data respective to ISP_1 : ISP_4 gets removed, but the columns containing the values of the variables adjacent to them are moved under the variables names ISP_1:ISP_4. 

I would like to be able to something like :

    datafactor = datafactor[,1:variablebeforeISP_1 & variableafterISP_4:end]

How would I do this? or is there a better way to do this?

Many thanks'"
http://statisticshomeworktutors.com/Algebra-and-Geometry.php,0,1,False,False,False,statistics,1493632912,False,
Applied probability models in marketing Assignment Help,0,1,False,False,False,statistics,1493633879,True,[removed]
Computational Statistics & Data Analysis online help,0,1,False,False,False,statistics,1493636015,True,[removed]
Continuous time optimization online help,0,1,False,False,False,statistics,1493637365,True,[removed]
Derivatives online help,0,1,False,False,False,statistics,1493638441,True,[removed]
Excel probability functions?,2,0,False,False,False,statistics,1493646145,True,[removed]
Which statistical test to use?,2,1,False,False,False,statistics,1493653124,True,"Hi everyone, I'm doing a study on deception detection across different modalities (audio only lies x audiovisual lies) and levels of training (training x no training). Aside from looking at the accuracy of their lie detection, participants have recorded how confident they are with each of their answers and I have took a mean of each participant's confidence scores.

I want to compare the mean confidence scores across all 4 conditions of my study (audio+no training, audiovisual+no training, audio+training, audiovisual+ training). Which statistical test should I use to see if there is a significant difference in participant confidence levels between these 4 conditions.

Any help will be greatly appreciated. Thanks in advance"
I need some simple guidance what test do i need to use in spss,4,2,False,False,False,statistics,1493657149,True,"So i am an undergraduate in psychology and need to do some statistical analysis for my study in spss. The thing is i have about a year to do anything related to spss and i forgot which test i need to use. Long story short, i gave the participants one test that counts scores for extraversion and neuroticism, and another test that counts the happiness in their lives. My independent variable is extraversion and neuroticism and the dependent the happiness scores. I know i need to use the explore option in spss but is it enough if i want to see if my results are statistically significant (and to check for normal distribution)? Or do i need to use also the t-test and which one of the options available.

ps. i really need some help cause the deadline is near and have yet to determine what to do with this one

ps2. pm me if you want , any help is greatly appreciated!!"
"How can I create an index ""score"" from 3 different variables?",6,5,False,False,False,statistics,1493657493,True,"I am trying to evaluate some campaigns using spend (in dollars), impressions (integer), and effectiveness of impressions (percentage from 0-100%).

Is there a way that I can use these three variables to create a sort of score to evaluate how well a specific campaign is doing relative to the others?

I'm thinking something along the lines of the human development index or others, where multiple data points are merged into one final score.

Thoughts? Any resources or ideas on where to create or learn how to create something like that would be great. Thank you!"
ELFI: Python engine for likelihood free inference,0,4,False,False,False,statistics,1493657847,False,
Regression question: adding insignificant variables v.s. improving fit,0,1,False,False,False,statistics,1493659020,True,[removed]
Should I Retake Courses for Better Grades?,2,0,False,False,False,statistics,1493659115,True,"I am a Business Analytics undergrad senior graduating in the fall. I have a 3.2 GPA and am strongly considering pursuing a Statistics M.S. I haven't been the best student due to a mix of my own fault, work, overloading myself, etc. I was planning on getting my applied mathematics minor but was wondering if I should let this minor go in exchange for upping my grades in a few important classes.


I am considering retaking two classes, probability + statistics and linear algebra. Would this be worth it, given my less than stellar GPA and that I have Bs in these courses? 


For more background into my situation: 

* I am also getting my computer science minor

* no formal research experience

* a couple of (BI centered) paid internships

* a couple of data analysis side projects on GitHub

* plan on having strong GRE scores, letters of recommendation, etc.

* Am looking for a more applied non-thesis M.S. program but I'm open to any program."
"Conditional Mutual Information, Chain Rule help",0,1,False,False,False,statistics,1493661088,True,[removed]
Best Flowers For Mom from 2016 statistics,0,1,False,False,False,statistics,1493662736,False,
"When using the least squares algorithm, what does the formula for the ""a"" variable represent?",3,0,False,False,False,statistics,1493663687,True,"When performing traditional least squares, you can use formulas to calculate the a/intercept variable and the various b's.

> b=∑ni=1xiwi−1n(∑ni=1xi)(∑ni=1wi)/∑ni=1x2i−1n(∑ni=1xi)2

and the formula for a
> a=1n∑i=1nwi−1nb∑i=1nxi

I know that the b is just the covariance, of the independent variable and that particular dependent variable, over the variance for that particular independent variable.

However, I'm not sure what the a variable would be called in simple terms.

Would you just say, the sum of the dependent data points minus the sum of the different coefficients? Is there a simpler name for this?"
XKCD Comic about Statistics,1,0,False,False,False,statistics,1493663989,False,
Perceptron - Visual artificial neural network builder,2,10,False,False,False,statistics,1493666888,False,[deleted]
Accounting for Within Subject Effects in Group Comparisons,0,2,False,False,False,statistics,1493670894,True,"This is a relatively basic statistics question for a immunology research publication I am writing up. Would appreciate any input!

As a general outline: The patient cohort I am working with was tested for a certain value over a period of 10 years, and each patient has 3-5 values each. For the statistics, I grouped them according into four categories (A, B, C, and D) to their response.

In some cases, when performing comparisons, Patient 1 timepoint 1 falls into group A whereas Patient 1 timepoint 2 may fall into group B. If I then compare Group A and Group B, how do I account for these within-subject effects? My study is based mostly on individual sample analysis, so it doesn't really matter whether a sample comes from which patient. But I do have to mention that this is a potential limitation in the discussion, so I want to make sure it is accounted for.

Also, is there something beyond this that I have to specify as a limitation in my study?

Thank you!"
Diversity in Stats (x-post from R/JMP),1,0,False,False,False,statistics,1493671772,False,[deleted]
Question on probability distribution.,1,1,False,False,False,statistics,1493671836,True,[removed]
Which method of analysis is appropriate here?,3,1,False,False,False,statistics,1493673376,True,[deleted]
"Are there expository journals like the Bulletin of the AMS or the American Mathematical Monthly, but for statistics?",3,22,False,False,False,statistics,1493675667,True,
Memoryless property,0,1,False,False,False,statistics,1493675718,True,[removed]
"Had a Google Survey run, need to know to what population does it apply.",10,1,False,False,False,statistics,1493679044,True,"Hi all:

I bought 750 respondents to a single question vía Google Surveys.

I would like you guys to help me understand to what size of the general population do my results apply to?

I've done the reverse in a few sample size calculators on the net, but would like to understand how to measure the population size my sample applies to.

Thanks! "
Could someone please help me finish this?,3,0,False,False,False,statistics,1493681379,False,[deleted]
"Is it true that given a run of x random numbers with mean 5 and a separate run of y random numbers with mean 4, the difference between the max number from the first run and the max number from the second run can be expected to exceed 5-4, if x>y and the stdev of the two runs is the same?",0,1,False,False,False,statistics,1493700430,True,[removed]
Quantum,0,1,False,False,False,statistics,1493707950,True,[removed]
Why the Gross Rental Yield is such a Misleading Statistic?,0,1,False,False,False,statistics,1493721214,False,
Environmental Informatics online help,0,1,False,False,False,statistics,1493722954,True,[removed]
Communication in Statistical Collaborations Assignment Help,0,1,False,False,False,statistics,1493723143,True,[removed]
Experimental Design and Analysis online help,0,1,False,False,False,statistics,1493723833,True,[removed]
Computational Modeling of Cellular Systems Assignment Help,0,1,False,False,False,statistics,1493724141,True,[removed]
Which test to use - Paired nominal data (2 categories; 4 samples),3,2,False,False,False,statistics,1493724405,True,"Hey r/statistics,  
I have another question for you, the relevant information is in the title but let me give you an example I just came up with.
So let's assume I asked **the same group of people** four different questions:  
1. Do you think Donald Trump is trustworthy? Y/N  
2. Do you think Ted Cruz is trustworthy? Y/N   
3. Do you think Hillary Clinton is trustworthy? Y/N   
4. Do you think Bernie Sanders is trustworthy? Y/N   
and I wanted to compare the outcomes to each other,  
which test do I need to use a. to compare all 4 samples b. to compare two samples each time afterwards.   
Thanks in advance.  
  
Edit: Well, maybe I just share with you which tests I would use: Cochran's Q test followed by 6 McNemar tests. Is that correct? "
Foundations Interest Rate Credit Risk Help online help,0,1,False,False,False,statistics,1493724458,True,[removed]
"Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing",0,10,False,False,False,statistics,1493737707,False,
Looking for reference for Variance Inflation Factor,15,11,False,False,False,statistics,1493739691,True,"I am having a really tough time trying to find a source to cite for VIF. All of the sources I've tracked down so far are either journal articles I don't have access too or books I can't get right now.

Does anyone happen to have some peer reviewed sources that explain the basics of VIF? Or the original paper that introduced VIF? "
"Introduction to statistics | A roadmap to Machine Learning, AI and Data Science",0,1,False,False,False,statistics,1493742584,False,
F Distribution Tables for huge degrees of freedom.,4,1,False,False,False,statistics,1493743568,True,"Hey everyone!

I've got a ""noobie"" question here: I'm helping my cousin with her statistics and she's doing a One-way ANOVA by hand and was asking me how to find a critical F in the F distribution Tables for degrees of freedom greater than the ones she got in her book (like F0.99[7, 1664]). I've been doing my ANOVAs with R for a long time and I can't remember the procedure I gotta confess...

Can someone help me (and her)?

Thanks :) "
Optimization involving parameters of a distribution,5,1,False,False,False,statistics,1493743989,True," I'm confused as to how to set up an optimization problem where the objective function is a parameter of a distribution. My objective is to minimize the variance of a distribution such that the probability of an event is above a certain threshold. An example to illustrate the problem is as follows:

I am running a simple test experiment, in which I have a country that has assets and debts.

I know that its debts will grow at 6% every year no matter what. I also know that its assets will grow at an average of 7% and are normally distributed, but with an unknown standard deviation of growth from year to year. As such, there is a probability that assets will be less than debts in a given year.

Given these growth rates, I want to find the lowest volatility of assets such that the probability of assets being less than debts is greater than 90% for 25 years.

Would most nonlinear optimization techniques be sufficient to solve a problem like this? If so, how would I set up this constrained optimization problem (for example in Matlab)?"
Frequentist or Bayesian AB Testing Methodology?,29,13,False,False,False,statistics,1493748717,True,"Hello everyone, stats noob here. My company uses a frequentist AB testing platform which requires a pre-determined sample size and generates confidence using a fixed horizon point. The problem we've been having is that many of our tests end up failing to reject the null hypothesis, but according to Bayesian calculators these tests have very high chances to result in actual uplift. 

I'm trying to get a better understanding of the downsides and upsides of frequentist v. bayesian testing methodologies. One that I keep running into and can't answer has to do with sample size. As an optimization marketer it's been beaten into our heads from day 1 that sample size is one of the most important requirements when determining a testing strategy. And yet most of the reading I find on bayesian testing says it's not necessary, or at least only necessary to the point the data forms a nice bell curve. Is this correct, or is there a formula that can be used to increase accuracy? Is sample size even relevant in bayesian testing?

Would really appreciate any input you all have on this."
Quick survey for my statistics class!,0,1,False,False,False,statistics,1493753687,True,[removed]
"Hi all, I am looking help for the attached question.Qn.11.17.",1,0,False,False,False,statistics,1493756938,False,[deleted]
Analysis of NBA players' height vs birth date?,3,1,False,False,False,statistics,1493768697,True,"I would like to see an analysis of the height of basketball players that currently are in the NBA vs their birth date (dd-mm-yyyy).

I would do so by myself if I knew where to get the data needed; can someone help me with that?"
Would it be beneficial for me to take calculus to understand statistics better?,0,1,False,False,False,statistics,1493773448,True,[removed]
Simple Slope Test for 3 way interaction -- What exactly am I looking at?,1,2,False,False,False,statistics,1493775183,True,"What does running this test tell me? I know I need to use it but I'm struggling with why I need to. I've used the Excel template located at www.jeremydawson.com/slopes for 3-way interactions (all options) to utilize the simple slopes test. When I did that for 2-way interaction, it produced two test results (you can see that in the templates from the web page) but 3-way only shows one result. In my case, my p-value for the test came out significant but I don't know what that tells me."
How to discover logistic regression classification significance between conditions?,0,4,False,False,False,statistics,1493775698,True,"Hi, I recently ran a logistic regression classification to determine whether a model that was built could predict an object from it's background. Here's half my data in a table showing the frequency my model classified an object to it's own condition vs other conditions:





 | 12mm | 3mm | 8mm
---|---|----|----
12mm | 45 | 7 | 31 
3mm| 12 | 25 | 41 
8mm| 26 | 33 | 34 

I want to know if the prediction was significant for a given condition or even between conditions to say that my model predicted one category better/worse than the other. Can anyone please suggest how I might go about this? 
"
TF-IDF for documents labelled with a score?,4,3,False,False,False,statistics,1493793018,True,"Suppose I have a set of documents, where each document is associated with a numeric score. I want to determine how important each word is, relative to this score (so an important word is one that has high influence on the score).

What statistical tools can I look into? The closest I can find is tf-idf, which finds the most important words for unlabeled documents."
Looking for help with R!!,4,0,False,False,False,statistics,1493794866,True,[deleted]
Practical challenges of clustering and setting up feature-space,1,1,False,False,False,statistics,1493800462,True,"Hey guys,

I'm looking to improve my knowledge on Clustering techniques in general. As part of this I'm taking a coursera course in the field, however I'm looking for some more in depth materials on practical challenges faced in clustering that I don't feel are going to be sufficiently covered in this course after having browsed the reading materials.

My application would be customer segmentation in an insurance and investment industry where customers typically only purchase 2 or 3 products with us out of say 15 product-offerings.
I'm looking for some materials on how to practically choose features to input into a cluster that will give meaningful output considering I have demographic data, data about what policies a customer owns, what premiums they pay, data that describes how similar the different product-offerings are to each other, underwriting data etc. 

What I'm looking for:
Some sources on how to approach practical clustering problems where I have a wealth of data available to me. I need guidance on how to practically choose inputs into a clustering algorithm so that my output doesn't give meaningless clusters, or end up clustering policies together instead of clustering customers together for example. I also have concerns about how meaningful the outputs of a clustering algorithm are if its half based on demographic data and half based on policy data. I haven't been able to find much online that deals with these practical issues so I'm hoping someone here can link me to a textbook or discussion or something helpful.

Thanks
"
Economicshelpdesk.com Offers Best Statistics Homework Help Service,0,1,False,False,False,statistics,1493803116,False,
Continuous Time Optimisation Assignment Help,0,1,False,False,False,statistics,1493805241,True,[removed]
Descriptive Statistics Assignment Help,0,1,False,False,False,statistics,1493805907,True,[removed]
2017 Mid-Year Statistics Point to Continued Rise in IPR Petitions | JD Supra,0,4,False,False,False,statistics,1493808335,False,
MarketAxess Announces Monthly Volume Statistics for April 2017 - EconoTimes,0,1,False,False,False,statistics,1493808398,False,
Change the Statistics Around Relationship Violence,2,0,False,False,False,statistics,1493808472,False,
Department of Justice Releases New Statistics On Illegal Alien Population in U.S. Prisons,0,1,False,False,False,statistics,1493808625,False,
Healthcare Statistics online help,0,1,False,False,False,statistics,1493809132,True,[removed]
One way ANOVA or paired TTest,8,8,False,False,False,statistics,1493809918,True,"Hello really quick question, my supervisor told me to conduct a one-way anova to check if my word lengths and syllabic lengths among 3 lists. 
so i have list 1 wl list 1 sl, list 2 wl, list 2 sl, list 3 wl, list 3 sl.

I was told to run these through a one-way anova to compare the means and check they are same.
However would it be better to run a ttest instead? please could you advise which way is better?
thanks in advanced"
Inference from Data and Models online help,0,1,False,False,False,statistics,1493810679,True,[removed]
Kalman Filter and Particle Filter online help,0,1,False,False,False,statistics,1493811328,True,[removed]
Problem with Foreach RandomForest,1,4,False,False,False,statistics,1493821629,True,"Hi All, 

Quick question. I'm using foreach to run my randomforest model in parallel (yes I'm aware of ranger and caret, but for various purposes I'm having to work with foreach). Anyway, when I run my piece of code, the foreach loop returns the model as a list of its output rather than the randomforest object. Here's the line of code I'm executing which is within a user-defined function calling another user-defined function

cl <- makeCluster(n_cores)
registerDoSNOW(cl)
model.fit <- foreach(ntrees = rep(round(500, 3), .combine = combine, .packages = ""randomForest"") %dopar% randomForest(Continuous_target~., data = model.data, ntree=ntrees, importance = T)
  

Also yes I know providing an example that can replicate the problem is desirable but currently I am having issues supplying a simplified version of the code to create the error using openly available data.

Thanks!"
Log-linear interpretation if the outcome is already on a % scale...,1,0,False,False,False,statistics,1493823336,True,[deleted]
"HELP ME, PLEASE!",0,1,False,False,False,statistics,1493828620,True,[removed]
What test can I do to compare four levels of a condition?,6,2,False,False,False,statistics,1493829513,True,"I want to do a test where I'm comparing L1 VS. L2 VS. L3 VS. L4.
Whatever is the greatest I will remove. For example, if it's L1.

Then I would do L2 VS. L3 VS. L4."
"Does it make logical/computational sense to transform an independent variable using the square root, and then include in the model a non-linear squared term of the same variable?",1,2,False,False,False,statistics,1493830314,True,"Basically, the model would include X^(1/2) and X. It seems to defeat the purpose of transforming the variable, in the first place, at least with regard to distributional considerations. 

The reason for the initial transformation is to address large right skew in a count variable (integer values, zero inclusive). "
Proving null hypothesis help,10,1,False,False,False,statistics,1493831494,True,"hey, sorry in advance I know very little about statistics... 

I'm trying to prove there is no difference between two samples, but my statistics knowledge is limited to doing hypothesis testing and saying there is no evidence to suggest they are different (failing to prove Ha). 

is there a way i can conclusively (or nearly) say that they are different? "
Can anyone tell me how to find CCCC?,0,1,False,False,False,statistics,1493831610,False,[deleted]
Review of mathematical statistics,16,13,False,False,False,statistics,1493831910,True,"Hello and thank you for looking at my question. 

I'm currently enrolled in probability and mathematical statistics in my university, and I have a final coming up. The textbook the class uses is Mathematical statistics with applications by Wackerly et al. It does a good job, however it leaves me confused in regards to functions pig random variables. What resources do you recommend to supplement the textbook?


Thank you!"
How would you explain stats to a 5 year old?,0,1,False,False,False,statistics,1493833947,True,[removed]
Guidance on Data Analysis Quality Control (xpost from r/consulting),5,2,False,False,False,statistics,1493835189,True,"I work for a boutique consulting firm where we do analysis with large data sets (often millions of rows) using statistical software packages (e.g. Stata, SAS, R). Recently, meaningful errors were identified in important deliverables that had already been submitted to our client.

Currently, we employ multi-stage review processes, where a peer reviewer reads over code line-by-line and a senior reviewer does benchmarking and ""sanity checks"" with the final results. Still, more errors are getting through the process than we are comfortable with.

We are trying to revise our internal quality control processes. In particular, we are interested in developing and incorporating checklists that could be used during review. However, thus far, I'm not convinced that the ideas we have developed internally will be that useful.

Does anyone know of any good resources available for developing internal protocols for ensuring data analysis quality control? Any advice or suggestions would be welcome.

Thanks!"
Multiple Regression Plot in Excel Help,0,1,False,False,False,statistics,1493836163,True,[removed]
Empirical Bayes for multiple sample sizes,4,14,False,False,False,statistics,1493848194,False,
URGENT: what test do I do if my data is normally distributed but doesn't have equal variance,6,1,False,False,False,statistics,1493849573,True,[deleted]
"In theory, what would the best odds be on buying 100 California Lottery Scratchers and winning?",0,1,False,False,False,statistics,1493849618,True,[removed]
Fit statistics for Confirmatory Factor Analysis,2,1,False,False,False,statistics,1493851611,True,"So for my university course, we have to do confirmatory factor analysis, and regarding the fit statistics for each factor. 

* I am unsure as to why the RMSEA does not fit the <.09 guideline, and are all above .09, yet the CFI, and TLI meet their criteria of fit, by being above .9.

* *Would i still assume fit due to CFI and TLI meeting their criteria, and assume there may be some issues with the RMSEA.

* *FYI - the sample size is 650. and although there may be better fit statistics, we've been told to use those 3 specifically 
"
How do I convert Movie Genres into a numbering system that I can use for a Regression Analysis,7,1,False,False,False,statistics,1493852181,True,"I am trying to see if the genre, year produced, IMDB rating and the number of Academy awards are correlated somehow. 

The genre is the only variable that I can not figure out how to turn into a numbering system so that my regression will understand the analysis. Any suggestions for how I can number it would be greatly welcome."
Question on health statistics,0,1,False,False,False,statistics,1493853348,False,[deleted]
I have the answer here but can someone explain how to look up the correct critical value? I know that the degree of freedom is 17 but not sure how he ends up with a t sub c of 2.110 when looking it up in the table,0,1,False,False,False,statistics,1493854413,False,[deleted]
I'm stuck on what analysis to use? ANOVA?,1,1,False,False,False,statistics,1493855482,True,[deleted]
Question on health statistics,0,1,False,False,False,statistics,1493856018,True,[deleted]
Question on health statistics.,0,1,False,False,False,statistics,1493858093,True,[deleted]
Advice on how to conduct a survey for game development,1,1,False,False,False,statistics,1493864633,True,"Hey everybody. I'm a game developer, and I wish to start doing statistical analyses on playtests of my game. Ultimately, I want as many people as possible to play my game once I reach certain milestones and then extract as much information as I can out of that data. What I'm confused about is how to keeping my costs at a minimum. My idea was to set up a booth somewhere in my town where people can come and play my game, which would programatically collect the data I need. I'm almost positive that this would introduce bias into my experiments (self selection bias). If this type of data collection is viable, how exactly would I conduct my calculations for things like hypothesis tests, confidence intervals, etc? (I've taken a statistics class in college, but all our examples have used simple random surveys). If this type of data collection will not work, is there any better, less costly, approach to this problem?"
Experiment Ideas!,0,0,False,False,False,statistics,1493866981,True,[deleted]
Chances of getting into a top Stats PhD program?,16,11,False,False,False,statistics,1493867210,True,"Hi all,

I'm an undergrad studying CS + Math. From the looks of it, it seems like I'll end up with a 3.35-3.4ish GPA, which is discouraging. However, I'll also have the following:

- research experience

- every course I've taken are hardcore math and CS classes (data structures and algorithms, algorithm analysis, PDE, advanced linear algebra, probability I and II, analysis, econometrics, modern/quantum physics as well as physics labs, to name a few), with some grad-level classes thrown in here and there

- extreme improvement in GPA from first two years to second two years (where all the hard classes are). I'm talking 2.9 to 3.6+.

Sorry for this extremely dull topic, I am stressed for my finals and compulsively calculating all permutations of possible final outcomes for this semester's gpa on collegesimply and just needed reassurance (or lack of it, so I can figure out plan B)"
ANOVA question!,4,0,False,False,False,statistics,1493872081,True,"I'm creating a study with 2 IVs (each with three levels) and 2 DVs.
Is this considered a 2 by 3? If not... what the heck is it?"
8 Benefits of Statistics Tutor Help Service for Securing High Score,0,1,False,False,False,statistics,1493879578,False,
Bayesian Programming HELP!,4,0,False,False,False,statistics,1493879724,True,"I am a mechanical engineer who has a nack for programming in java. I want to write a program about bayesian mcmc procedure. I need a guideline like this:

-Take data set,
-get first element,
-do this,
-Generate this,
-take the result,
-throw it into this function,
-burn in those results,

etc...

all the tutorials has enormous amount of mathematical notation which is very complicated.  if anyone can help me it would be appreciated. Thank you. 

ps: I generally program games and algorithms so I dont think this is too complex for me programmtically.

This could also be a beggining of a collobration academically my wife has a PhD in Statistics (i want to suprise her on our anniversary)"
StatCrunch Assignment Help by Statisticshelpdesk Is a Complete Package,0,1,False,False,False,statistics,1493882130,False,
Online Statistics Tutor from Statisticshelpdesk.com Help Has Created a Special Niche,0,1,False,False,False,statistics,1493889297,False,
"Statistically speaking, amount of kids of amount of sons is known",7,5,False,False,False,statistics,1493892964,True,"Greetings,

As you might know Ibn Saud was the so called ""godfather"" of what we currently know as Saudi-Arabia. As I can read on Wikipedia, he had 45 sons (36 survived) with 22 wives. It also states that he had ""many"" daughters, but I couldn't find how many exactly.
I had a heated discussion yesterday with some friends of mine and I stated that if he had 45 sons, he probibly had around 90 kids in total (seemed rather obvious to me) since this looks like a basic statistics problem where Pboy=51% and Pgirl=49% (birth statistics gathered from 6000 random births in the US).
So my question now is, how many kids did he have in total? Am I so retarded for presuming he had about 90 kids if he had 45 sons? N seems large enough to make a somewhat estimated guess I think. 

You've helped me a lot already by reading so far!

Edit: damm I made a mistake in the title, should be ""amount of kids if amount of sons is known"""
Fourier to Wavelets Assignment Help,0,1,False,False,False,statistics,1493893084,True,[removed]
Linear Models online help,0,1,False,False,False,statistics,1493894796,True,[removed]
Mann-Whitney U test with negative data in continuous scale?,0,1,False,False,False,statistics,1493895442,True,[removed]
Mathematical methods online help,0,1,False,False,False,statistics,1493895948,True,[removed]
Megastat online help,0,1,False,False,False,statistics,1493896911,True,[removed]
Structural equation model advice,2,6,False,False,False,statistics,1493904813,True,"I am looking to do (actually already have done) a structural equation model on time series data. The dataset is pretty extensive and so sample size isn't necessarily an issue, and the model is not overly complex. I am predicting lake chlorophyll concentrations using a variety of ecosystem characteristics and predictors. All predictors are measured and there are no latent variables in the model. The reason I chose to do an SEM and not just regression is the ability to observe the effect of more distal variables on the proximate drivers (if that make sense). 

My question is whether it is ""ok"" to do an SEM on time series data, and if it is ""ok"" to have an autoregressive term in the model (e.g. predicting chlorophyll with chlorophyll at t-1). I can't find a lot out there on doing SEMs on time series with ecological data. Thanks for the help!"
Mixed ANOVA confusion,2,2,False,False,False,statistics,1493921875,True,"Hi all!

I have a design with a N of 97 (55 people in one group and 42 in the other). I am interested in the effects of an intervention vs. an active control on reducing a measure of anxiety at 4 time points (baseline, post-test, 1 week follow-up, 2 week follow-up). Now, when I run two 2X2 Mixed ANOVAs (intervention X time for each group) I found significant main effects of time for both groups, and a significant interaction between intervention by time for one group but not the other. 

My question is would it be worth trying to do a 3 way ANOVA instead of the two 2-way ANOVAS? It would look like this: 2 (group) X 2 (intervention) x 4 (time) mixed ANOVA. 

I would love to hear your opinions on the pros/cons of these two ways. 

Thanks in advance for the help :-)  

(I am using STATA 14)"
Harsh Streaks in Residual Plot,14,4,False,False,False,statistics,1493928003,True,"I am running a log-log OLS regression. The dependent variable is the log of a discrete, integer value (1,2,3,4,5...). Have y'all seen anything like this before and do you have any idea how to fix it? I have tried to isolated the streaks in R using ggplot2 coloring, but so far, I have not seen anything that predicts the streaks.

http://imgur.com/5QK6xQW

EDIT: This is panel data FYI. Also, here is the plot of the residuals against the dependent variable.

http://imgur.com/QWDaeFJ

Any help is much appreciated."
Student Stats Project Tips!,0,1,False,False,False,statistics,1493931867,True,[deleted]
"What would you include in an ANOVA crash course? And by ""crash"" I mean Hindenburg.",11,39,False,False,False,statistics,1493936231,True,"I need to put together a really quick crash course on ANOVA for work.  I have about 30 minutes to talk about ANOVA and 30 minutes to demo stuff on JMP.

The assumption is that they will already understand hypothesis testing, and this is by no means meant to be academic, so I think it will be doable.   It's meant as a primer for the next class which will be a whopping 45 minutes on Design of Experiments.

Here's what I'm thinking of covering:

-analysis of ""variance"": is the difference explainable by the variance? Yadayada.

-Degrees of Freedom

-assumption of equal variance

-making levels wide enough to detect stuff (S/N), but not so wide you get screwed by curvature.  Also how it effects the assumption of equal variance

-you're never going to do a ""classical"" ANOVA just do a general linear model (engineering work/continuous variables)

-how to read the table (show how the degrees of freedom are assigned)

-dangers of multicolinearity

-if you get a multilevel categorical variable call an adult

-if you get a categorical response variable call an adult

-if there's a lot of money on the line call an adult.

Note: the main purpose of this is to promote a new stats department we are trying to build within the organization.  So we give people a quick rough outline of what they can do and how, while simultaneously sewing a bit of doubt in their minds about whether or not they are actually capable of doing it themselves.

Anyways, any other things you may think is worth covering.  I only get like 6 slides.
"
Looking for applications of the bispectrum,0,1,False,False,False,statistics,1493937157,True,"In cosmology it is well known that studying the bispectrum of the large scale structure of the universe is a powerful way to distinguish different models of cosmic initial conditions. I had assumed that the bispectrum had endless applications in many areas of science and data analysis more generally but I just tried to get my google on and hardly found anything. e.g. this is all the (Wikipedia page)[https://en.wikipedia.org/wiki/Bispectrum] has to say. 

I'm sure there must be interesting applications outside of cosmology, so why am I having trouble finding them? Perhaps I should be looking for applications of higher order statistics more generally? Is there some other terminology I could try searching? If anyone can point me in the direction of some interesting applications I would be very grateful."
Confidence Intervals Using Randomization Distribution,0,4,False,False,False,statistics,1493937315,True,"I am confused about how to generate a CI using the randomization distribution.

Suppose I am testing for the difference in means in a paired comparison experiment and say the sample size is 5.  If I wanted to compute a p-value using the randomization distribution, I would calculate all 32 permutations and then compute the proportion of randomized test values greater than the observed values and multiply by two, making easy intuition.

How do I compute a confidence interval?"
"MATLAB, Mplus and SPSS correlations",0,1,False,False,False,statistics,1493938818,True,[removed]
Data Advise,3,0,False,False,False,statistics,1493943724,True,"Hey everyone, I am working on a paper that is assessing the effect that incentives and costs have on residential solar panel adoption. The problem is that my data has several residential types one of which kind of mixes commercial and residential (apartments and condos is my guess) and I am not sure what would be the most appropriate way of splitting up the data. 

For reference here are the data categories I am working with:

[1] Single Family Residential            
[2] Commercial/Multifamily Residential   
[3] Low Income Multifamily Residential  
[4] Low Income Single Family Residential  
[5] Commercial Pools                                                         
[6] Commercial 

Thoughts? "
Do categories coincide with each other as expected?,3,1,False,False,False,statistics,1493944910,True,"I need to know what test to use: I have an area I've mapped in two different ways. I want to know whether the categories of one type of mapping coincide with the other more than expected, simply due to proportion of the total area covered by each category. I don't know if it's a problem, for finding the right test, but there's no real point of truth with this mapping, neither map is 'right', they're just the result of different methods."
Correlation HW Help!,1,1,False,False,False,statistics,1493945904,True,[deleted]
Measurement Precision in Dosage,0,1,False,False,False,statistics,1493947652,True,[removed]
[Request] Video Series For Developing An Intuition About Statistics,0,1,False,False,False,statistics,1493954244,True,[removed]
I need some help with a test. Trying to get better at statistics and I found this test.,0,1,False,False,False,statistics,1493970756,False,
A good ressource to learn statistics for market finance analysis and R from scratch? (Background: Major in CS with a minor in finance),0,1,False,False,False,statistics,1493973267,True,[removed]
Inference Fundamentals with Applications to Categorical Data Assignment Help,0,1,False,False,False,statistics,1493977692,True,[removed]
Interpreting a 4 point likert scale,0,1,False,False,False,statistics,1493977980,True,[removed]
Life Testing and Reliability Assignment Help,0,1,False,False,False,statistics,1493978508,True,[removed]
Monte Carlo simulation online help,0,1,False,False,False,statistics,1493981171,True,[removed]
Non Parametric Statistics online help,0,1,False,False,False,statistics,1493982008,True,[removed]
Normal Distribution online help,0,1,False,False,False,statistics,1493983656,True,[removed]
Which statistics to apply,0,1,False,False,False,statistics,1493993352,True,[removed]
"If entropy is a measure of disorder/uncertainty, how can it be a measure of complexity?",8,2,False,False,False,statistics,1493998056,True,"I've read papers that use entropy as a measure of complexity rather than uncertainty and disorder. How can we use the same measure to quantify two seemingly antithetical parameters?

Edit: paper that uses entropy as a measure of complexity - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3872328/"
STATS HW HELP ASAP,0,1,False,False,False,statistics,1494001449,True,[removed]
How do you do a path analysis power analysis?,0,1,False,False,False,statistics,1494003950,True,[removed]
What is the hardest statistics course in your opinion?,44,22,False,False,False,statistics,1494005544,True,
What is role of biostatistics in dentistry?,0,1,False,False,False,statistics,1494006378,False,
Needing some help figuring out how to put this into a TI-84,0,1,False,False,False,statistics,1494016356,True,[deleted]
Statistics Final Project (Help Appreciated!),7,1,False,False,False,statistics,1494031320,True,"Hey guys, I conducted a survey for 2 AP Stats classes with 56 people. My question was (Do you feel your High School Education has Prepared you for College.?) My responses were Yes: 57.14% No: 14.29% Neutral: 28.57% and I don't know what Test to use because there are 3 variables."
Stats Question Example,0,1,False,False,False,statistics,1494039316,True,[removed]
Comparing PCA clustering to a t-test?,0,1,False,False,False,statistics,1494040204,True,"Is there any way I can relate the degree of clustering between two features across multiple (3+) dimensions (reduced to 2 PC's) to the magnitude of a t-value from a two sample t-test?

For the data in question, I would like to posit that the difference between the amount of clustering and the magnitude of the two sample t-test is related to the number of dimensions used for PCA. 

Thus, when PCA is performed using only two dimensions, the metric I use to compare the clustering to the t-test should be zero. "
Data Analysis help,0,1,False,False,False,statistics,1494047656,True,[removed]
Data Analysis Help,0,1,False,False,False,statistics,1494048416,True,[removed]
Effectiveness of exam: did it achieve the goal?,4,15,False,False,False,statistics,1494075726,False,
Counterfactual-based Incrementality Measurement in a Digital Ad-Buying Platform,0,2,False,False,False,statistics,1494089644,False,
Question regarding conclusions reached from high NPV and low PPV,1,1,False,False,False,statistics,1494092765,True,I saw a study linked in r/science indicating a predictor of non-violence in schizophrenic populations with a high NPV and low PPV. As I'm not an expert in statistics it made me wonder. Does a high NPV and low PPV imply necessarily that the negative group will only be a small percentage of the population studied? This is a question about how to interpret the results of such studies. 
Am I skewing my model?,4,1,False,False,False,statistics,1494099233,True,[deleted]
Are there any statisticians out there that can help me with this problem?,0,1,False,False,False,statistics,1494103293,True,[removed]
Total distance from sample with lower bound. Please help.,0,1,False,False,False,statistics,1494103982,True,[removed]
"stats final/work project, need advice please",5,2,False,False,False,statistics,1494106664,True,"Hi all, I'm hoping to find a bit of help outside my less than desirable responsive teacher. 

I'm taking an elementary stats class and we of course have a final project. For this project I decided to actually do something that I could present to my boss if I find some good information, but it delves slightly deeper than elementary stats goes in my class. I only have access to excel, so I am limited a bit in what I can do. 

I work in the shipping department of a warehouse and have felt that we are experiencing too many instances of having extremely busy days followed by extremely light days. Now we produce all product in house, we aren't like amazon. We are still dependent orders, but there is more freedom of determining ship dates than warehouses that just store and ship. 

Now I've obtained data for all of March plus the last 2 days of February to give me 25 total days, with 5 of each weekday included (I have access to 3 yrs worth of data, but don't want to do that much for the school project unless more data would be better). I've done basic sorting, created a pivot table to do the sum, avg, count, and standard deviation of it all, tested for normal distribution, created box plots. Basically I've done just elementary stuff.

Now, my teacher recommended ANOVA testing, so I looked into that, did a one-way in excel, and now I don't know where to go or even if I'm headed in the right direction. 

I don't have a copy of my workbook with me (on my phone) but I can provide it later when I get back home. Off the top of my head though, my means range from ~40 to ~100. The variances given in the ANOVA test ranged from ~24 to ~792. My f value was 6.something and f-crit was 2.something, and I had a p value of .00something. 

Any guidance would be super grateful"
"An Intuitive Primer on Statistical Significance via Dice Rolling, Tire Evaluation and Other Gimmicks",26,0,False,False,False,statistics,1494112490,False,
Where do I post questions asking statisticians for advice as how to model a scientific dataset?,12,2,False,False,False,statistics,1494120906,True,"Is this the forum for a scientist to gain advice as to the best way to model a certain dataset with various properties?

I have a problem about how to statistically model a certain scientific dataset. I would describe the properties of this dataset, and explain what I'm trying to model. The idea would be to discuss the proper way to model this data.

What is the best reddit for this discussion? Are there other statistical forums where statisticians/machine learning/computational statistics gurus could help me out?"
I need a hero for homework,0,1,False,False,False,statistics,1494122178,True,[removed]
Partial Regression Coefficients,0,1,False,False,False,statistics,1494125134,True,[removed]
Using Graphpad Prism v6.0...any way to highlight cells so I can visually compare differences between analysis sheets,0,2,False,False,False,statistics,1494131963,True,"I'm running a bunch of Correlation Matrices on data sets that are comparing several morphology metrics of cells across different brain regions (e.g. area, perimeter, aspect ratio of microglia cells between CA1 vs CA2 regions).

I want to see if the directionality or magnitude of the correlation coefficient changes between regions but it seems like I can't highlight or mark cells in the sheet, so switching between the sheets manually and writing down if the r value direction changes or diminishes is becoming really cumbersome. Any tips? I don't want to pay to get Prism 7 since Prism 6 was free... :("
Reality Class - Correlation and Causation,0,12,False,False,False,statistics,1494144242,False,
I was looking for some data about games and I bumped into this great browser game. Test yourself,8,25,False,False,False,statistics,1494153655,False,
Could someone help me answer my question? I'm really stuck here.,0,1,False,False,False,statistics,1494155944,False,[deleted]
Could someone please help me answer this question on likelihood functions? I'm really stuck here.,3,4,False,False,False,statistics,1494158453,False,
How easy is it to find a job with a bachelor of statistics?,0,5,False,False,False,statistics,1494164635,True,"Hi Im studying a stats major in australia in my third and final year. I know that statistics and math are highly sought after skills however Im wondering if anyone who has experience can speak about the job market and its expectations

Currently I have knowledge of
R Programming (and packages like nlme, nlmeU, tidyverse, STAN, BRugs)
-analysis of variance
-logistic regression
-model selection methods based on AIC, BIC, likelihood
-intra and inter class correlation analysis
-time series analysis
-bayesian inference and markov chain monte carlo methods

As well as experience in a number of programs like Excel, Python, MATLAB and MySQL. However less developed.

I'm currently trying to find an internship in my final semester.

All the big jobs in data now seem to be as a ""Data Scientist"" developing algorithms mainly in Python to scrape websites, and model data with things like classifiers, nearest neighbours, decision trees and deep neural nets with backpropogation etc.

Do I need to learn advanced algorithms to get a decent paying secure job nowadays? Can I find a good entry level job that will train me up with my current knowledge or do they all expect me to be an algo master? 
"
"What is the ""L1"" statistic and how do I calculate it? Does this paper provide a good introduction to statistical matching?",6,5,False,False,False,statistics,1494175169,True,"I was looking over the [article reading list](http://thecasinstitute.org/wp-content/uploads/2016/09/iCAS-CSPA-PM-Reading-List-Articles.pdf) for part 3 of the CAS Institute predictive analytics exam.

It looks like a lot of the articles focus heavily on (quasi) experimental design and since I don't have a background in this I just picked a paper.

I'm currently reading [cem: Software for Coarsened Exact Matching](http://gking.harvard.edu/files/gking/files/jss-paper.pdf).

If you read the paper you see they mention calculating the ""L1 statistic"" of a data set. 

> Although CEM is MIB, the actual degree of imbalance achieved in the matched sample may be lower than the chosen maximum, and so we also introduce a simple and comprehensive multivariate imbalance measure (Iacus et al. 2008). The measure is based on the **L1 difference** between the multidimensional histogram of all pretreatment covariates in the treated group and that in the control group.

However, the paper just assumes that you know what the L1 statistic is. I tried looking up the term in some of my old stats/math books but I didn't find anything. When I tried looking the term up in Google, I got a lot of papers on L1 regularization.

Is calculating the L1 statistic the same thing as L1 regularization? If they are different, what paper/book can I read to find out what the L1 statistic is, what it is used for and how to calculate it.

 I also have no idea what statistical matching is. To get a general idea of how Coarsened Exact Matching differs from regular matching, I did a little searching and I found this paper ([Statistical Matching and Imputation of Survey Data with StatMatch](https://cran.r-project.org/web/packages/StatMatch/vignettes/Statistical_Matching_with_StatMatch.pdf)). Is this paper a good source to give me a basic overview of statistical matching?"
Worried about my progress in university,14,6,False,False,False,statistics,1494178953,True,"I have just recently finished my second year at a relatively high ranking public university and I feel like I'm not performing any where near to the standards I should be performing at.  After this semester I am holding a 2.8 as a Statistics major and I honest to god put it a lot of work into this major as I THOROUGHLY enjoy working on Statistics and the Calculus prerequisite courses.  I keep landing grades anywhere from B- to B+ but in math courses I can never really edge out an A or A-.  My father who pays for Uni says it's fine as long as I come out of the classes knowing things I didn't know before, but I feel like I'm still failing him and more importantly myself.  What I'm asking I guess is what steps do I need to take to get to that level because I do enjoy this major but I want to better myself to achieve even higher marks.  What steps do I need to take to get to that point?"
How to model the presence of count data in two spatial regions?,22,4,False,False,False,statistics,1494179815,True,"I feel like there is a straightforward way to model this dataset, but I'm a bit stuck. 

Let me give you a metaphor for the data first: 

Let's say that we are looking at a strip of land of fixed width. There are only two types of regions on this land: either there is sand or there is grass. The length of these patches are not equal: sometimes there are uniform patches of grass which stretch miles, over times there are uniform patches of grass only several feet. 

I'm interested in clusters of discrete entities. Let's think of them as trees. We are interested in clusters of trees. (Assume we know how to define these by location---if they're nearby one another within some distance, this is a cluster.) Five or more trees are a group.

The question is this: how statistically enriched are patches of sands with groups of trees? 

One must take into account that patches are of different lengths, and that the groups of trees are of different sizes. This is all count data though. 

I am able to measure everthing. 
    
    region     type    clusters    region_length
    region1    sand    4           10
    region2    grass   4           500
    region3    sand    4           25
    region4    sand    4           20


Further questions: one could include covariates, e.g. certain regions of sand also have snakes, we know rainfall levels, etc. How would I include these? 

Actual problem:

I have a genome with certain genes. These genes very in length (naturally). There are effects I am measuring of an enzyme. I want to quantify how enriched these genes are with this effect.

My answer:
So far, I think one could model this as using discrete count regression models, e.g. GLM Poisson/Binomial. However, this shows me a correlation e.g. correlation between sand and cluster size. I would prefer to quantify how statistically enriched sand is with clusters of trees versus grass regions with clusters of trees. 

This dataset also could use a hierarchical Bayesian approach possibly...but I don't see how that really helps me. 

What would be a good model for this data? 


"
Running into a small issue trying to apply a chi-square test to validate a hypothesis,8,3,False,False,False,statistics,1494186295,True,"Hi!

I'm trying to validate a hypothesis that gender does not have a significant impact on intent to purchase for a luxury good.

I made a document explaining the entire thing, here's a screenshot. Anybody got a tip?

http://imgur.com/a/ab24a

Thanks!!!"
T testing,7,4,False,False,False,statistics,1494187305,True,"I am performing a study on medical education.

I have a 5 point likert scale (1 completely unconfident, 5 fully confident) there are 4 groups. they are all of different size

for each respondent I have calculated a mean from their likert responses (all responses are regarding self-perception of surgical skill)

whilst i understand this approach is frowned on, it is frequently used in medical education.

my question is, is it appropriate to use t-testing on these means to compare significance between groups as the data are nominal (and not ordinal) when means are calculated

should mann-whitney testing be used instead?

does either approach have any benefit from a statistical viewpoint"
Developing a confidence interval for a classification model,0,1,False,False,False,statistics,1494198566,True,[removed]
What would be the Type I and Type II errors in this situation?,18,10,False,False,False,statistics,1494202444,False,
"Biomedical statisticians in industry, does the degree title (bio)statistics matter?",6,15,False,False,False,statistics,1494209665,True,[deleted]
"Given sample proportions, what statistical concept should I use to find a specific proportion in the future?",0,1,False,False,False,statistics,1494213487,True,[removed]
How to design a survey,0,0,False,False,False,statistics,1494227277,False,
Google TRIGNOSOURCE.,1,0,False,False,False,statistics,1494233512,False,
"On an examination, average grade of 200 students was 74 and standard deviation were 7. If 12% of class give A’s and the grade are curve to follow a normal distribution, what is the lowest possible A and highest possible B?",0,1,False,False,False,statistics,1494239625,True,[removed]
"Help, how do you solve the standard deviation in the Q2?",5,0,False,False,False,statistics,1494242050,False,
Where's my p-value??,0,1,False,False,False,statistics,1494242082,True,[removed]
Comparing results from two simulations with different simulators,2,1,False,False,False,statistics,1494251419,True,[deleted]
How do I statistically control for subject getting exhausted?,7,5,False,False,False,statistics,1494255734,True,"I have a data set with three measures of motivation. Subjects were divided into two groups at random and were then instructed to squeeze into a hand grip three times. A first time to establish a baseline. A second time after the doing the manipulation (group A vs B) and a third time after having a rest for 10 minutes.

Any idea on how to compare the results for group A vs group B? I'm trying to determine if the manipulation has an effect on the motivation. The problem I have is that almost all subject get a worse time the second and third time they squeeze because they are tired. How can I solve this?

Also if you can find any papers who had the same problem and found a way to solve this, it would be very helpful!

Thanks!"
Help me make my marginsplot look good (STATA),6,0,False,False,False,statistics,1494257232,True,"I'm trying to make my marginsplot look nicer. I'm using the recast function to change it up a bit, but i really can't get it to look good - the reason being, my x-axis is binary, so it looks a bit empty. What do you guys normally do (if anything)?"
"Statisticians of Reddit, what level of education do you have, and how much do you make?",93,60,False,False,False,statistics,1494270717,True,
Stat problem,0,1,False,False,False,statistics,1494278709,True,[removed]
"Macron Won, But The French Polls Were Way Off",0,8,False,False,False,statistics,1494281069,False,
Statistics degree: phD vs masters,0,1,False,False,False,statistics,1494286257,True,[removed]
"Girl, I'd love to be your generating function in an acceptance-rejection simulation",0,0,False,False,False,statistics,1494295011,True,[removed]
Question about what statistical method to use,4,2,False,False,False,statistics,1494300584,True,"So I have a research about extraction of a substance. The concentration is already known, I just want to know what method to use in determining whether the concentration of the extracted substance is close or equal to the actual concentration.

Do I need a statistical method, or do I just say the percent error?. Also how many replicates is necessary? 

Thankss

Edit: Another one. What if I have two different ways of extraction. Do I compare the two or simply use the answer to my first question twice? If I need to compare them, what statistical analysis should I use?
"
Statistics Help Online Service,0,1,False,False,False,statistics,1494317182,False,
Normal sampling theorem for one way anova and simple linear regression?,2,0,False,False,False,statistics,1494318408,True,"What is it?  Also for one way anova and simple linear regression, how could you prove the sum of squares total = sum of squares between +sum of squares within.

If you could answer any of this within the next 7 hours I'd be very grateful!"
pdpipe - Easy pipelines for pandas DataFrames.,0,1,False,False,False,statistics,1494321941,False,
Help regarding ANCOVA and regression,0,1,False,False,False,statistics,1494323560,True,[removed]
Comparing test-retest reliability,1,3,False,False,False,statistics,1494323672,True,"So the issue I have is this.

I administered a similar test at time 1 to everyone. A week later at time 2 I administered 1 of 2 versions of the original tests.

So I have:

Time 1

Time 2a

Time 2b


What I want to do is get the test-retest reliability of Time 1 -> Time 2a and compare this to see if it is significantly different to the test-retest reliability of Time 1 -> Time 2b.

I was wondering how I go about doing this (I use SPSS)."
13 Uplifting PPC Statistics You Should Know About,0,1,False,False,False,statistics,1494328319,False,
Which clustering algorithm is appropriate when?,2,7,False,False,False,statistics,1494336041,True,"Hey everyone.

Can anyone point me towards a flowchart or something of the sort that can inform my decisions on which type of clustering analysis would be appropriate for what kind of data? "
Reliability of IAT data,8,5,False,False,False,statistics,1494339115,True,"Hi all,
I recently did an experiment using inquisit software. I created an IAT. I would like to check the reliability of my IAT, but I haven't really found a way to do so in SPSS. Any pointers how to do this?

Thanks!
"
College statistics course,2,1,False,False,False,statistics,1494339976,True,"I am taking an elementary statistics course, and I want to make sure I am prepared, and get everything of value (I am going for my graduate degree in Data Science, and this course is one of three pre-requisites I need).

How should I best prepare? What take aways can I expect? Are there any terms, or formulas I should familiarize myself with now?

215 Elementary Statistical Analysis. 3 cr. U. Elementary probability theory; descriptive statistics; sampling distributions; basic problems of statistical inference including estimation; tests of statistical hypothesis in both one- and two- sample cases.|Prereq: satisfaction of Quantitative Literacy

https://www4.uwm.edu/academics/undergraduatecatalog.cfm?u=SC/C_601.html"
Calculating difference between causal mediation analyses?,2,3,False,False,False,statistics,1494346594,True,"If I calculate a causal mediation analysis for 'x' data rows, calculate a second analysis for data rows 'x+1', and then calculate the mediation results for '(x+1)-(x)', would the resulting difference between the two analyses = the ACME/ADE for that single extra row of data?"
How to obtain mean of a poisson distribution given the data set?,5,2,False,False,False,statistics,1494347780,True,So my prof says since the data set looks like a poisson distribution and not a standard one I have to calculate the mean using a different formula. I've looked online but all I can find on poisson distributions are how to get the data after you have the mean and the variance. I want the opposite so that doesn't help me.
Texas A&M Online Statistics Program,9,18,False,False,False,statistics,1494348706,True,"Anyone have experience with this program? I am thinking of doing it full time for a career switch. The grad department is decently ranked and from what it seems the online classes are identical to what the face to face masters students take,"
Three card prime,1,4,False,False,False,statistics,1494349792,True,"On a cruise ship and the have a poke game calldd three card prime. In it they have a side bet. Bet 5 and if your 3 cards are all red win 3x your bet. If you have three red and dealer has 3 red the  win 5x your bet. 
What are the odds of winning?"
Changes to /r/Statistics,25,57,False,False,True,statistics,1494352516,True,"Hello everyone, the mod team has gotten together and have talked about making some changes to the subreddit. About a week ago there was a post bringing attention to the current state of moderation. We have decided to try and move the sub in a direction to foster more of a community. We also want your input, because ultimately the community members is what makes the gears turn. 

1. **We have decided to change to a flair based system.** We hope to stimulate more focused discussion by allowing you guys to flair your posts. Here is the current running list for possible flairs:

        Article/Research
        Career/College Advice
        Statistics Question
        Software
        Other
        Meta
    Blatant homework posts and posts that can't [ask a good statistics question](http://www.statisticalanalysisconsulting.com/how-to-ask-a-statistics-question/) will be removed and directed to more appropriate subreddits. We want to open up to more questions but keep them at a high level of discussion. 

2. **We also are planning on updating the sidebar with fresh links.** Perhaps we can have a stickied College/Career advice post that we can point towards new comers. If you have anything that you think would be helpful in the sidebar please link. 

3. **Cutdown on self promotion and pointless links.** There has been a spree of people posting questionable material and links to their own personal website too much. People should be able to link to OC but it cannot be blatant or excessive. Also, we do our best to block spam but it looks rather suspicious when a user just posts a link with no discussion but yet they are an active user on other subreddits. As members please be on the lookout and report this kind of activity. 

As of right now a new CSS style is in the works but given the current issue with Reddit's changes it might not be done in time. Let's make this community an online benchmark and please leave your thoughts below!

Thanks!"
TV Viewership by episode,2,3,False,False,False,statistics,1494352838,True,"Hey, does anyone know where I can find the number of viewers for any show/season/episode? I feel like there's gotta be a database for this kind of info somewhere, but didn't have any luck on Google"
What are current methods for dealing with decision under uncertainty?,1,2,False,False,False,statistics,1494353545,True,I have a problem that deals with this and I'm using Choices (1987) for learning the material. It has some great ideas but have there been any new ways to handle this issue since this book came out? Thanks
Path Analysis of Likert Scale Data,0,1,False,False,False,statistics,1494359885,True,"Hi all!

I'm currently undertaking a university final project (undergrad). Unfortunately my supervisors aren't familiar with path analysis/path goal analysis - which is needed in order to test our model.
As a brief background, we have adapted a model found from literature. In literature, we found that researchers tested each connection within their model, by forming hypotheses, which were subsequently tested using path goal analysis. For our project, we wanted to follow a similar statistical approach, and as such, have created 3 different questions to test each hypothesis, using a likert (5 point) style questionnaire. I was hoping if anyone within the community knew of any good resources which would simplify the process of path goal analysis. Additionally, if anyone is familiar with STATA - is path goal analysis possible via STATA?

Thank you!"
Where to find listed companies basic financial statements ?,12,4,False,False,False,statistics,1494368658,True,"Hi all.

I'm looking for annual data on listed companies all over the world, like net profit, revenu, equity, ... i.e. financial statements. I've found a lot of limited or non-free resource and I cannot admit that there is no mean to access public data online. If anyone know a way or faced the same issue, I'll be pleased to discuss that.

Thanks in advance !

PS : sorry for my bad english."
Looking for datasets that come from wearable technology?,7,6,False,False,False,statistics,1494368959,True,"Hey guys, I was on the lookout for data that has been collected from any kind of wearable technology. This can be from a smart watch or a fitbit or heart rate monitor, etc.

Any links you might have or any resources that are out there for me to use that might have that kind of data will be greatly appreciated.

Thanks."
Name for a seperated/double (?) relationship,4,3,False,False,False,statistics,1494371051,True,"I'm investigating the bivariate relationship between car engine size and fuel efficiency. I have noticed that fuel efficiency and annual cost also have a very strong relationship. As fuel efficiency increases, annual cost decreases. What is the name for the relationship between engine size and annual cost? (As engine size increases, annual cost increases.) I don't mean ""positive"" but am looking for a word that describes the cross relationship. Apologies for poor description."
Job Posting in Southeast Michigan,0,0,False,False,False,statistics,1494377292,True,"Hello all,

I’m hoping I’m not breaking any rules posting a job opening, if I am, moderators feel free to take it down.

The company I work for (www.gongos.com) is currently looking for someone to lead our analytics team and I felt it worth a shot to post here.  In short we’re looking for someone with a strong stats background, minimum of 6-8 years’ experience.  The ideal person will have a passion for growing a team of passionate analysts and data scientists and is either located in southeast Michigan or is willing to relocate.

A complete job posting can be found here:

https://gongos.hua.hrsmart.com/hr/ats/Posting/view/17

If you are interested contact me and I’ll start you down the path!
"
How to explain this gaming problem with statistics,6,1,False,False,False,statistics,1494379288,True,"Hi all!  Hoping you can help me out.  I'm playing a mobile game, and one of the activities you can do in the game is to modify a weapon.  When you modify the weapon, you can add (hard to find) optional parts. One of these optional parts increases the chance of successfully modifying the weapon.  The other part increases the chance of a critical success, which results in a more powerful weapon. For the purposes of this example, let's say that using the first part increases the chance of success from 45% to 99%. Using the second item increases the chance of critical success from 5% to 45%.  

Using these estimates I have estimated this distribution:  


Distribution | Crafting FAIL | Normal Mod | Critical success 
---------:|:----------:|:----------:|:----------:
No parts | 55% | 42.75% | 2.25%
Only success item | 1% | 94.05% | 4.95%
Only critical item | 55% | 24.75% | 20.25% 
Both items | 1% | 54.45% | 44.55%


Now, in the game there are a lot of people who want to use only the critical part because they're not interested in a 'normal' weapon mod. They have hundreds of them. They're ONLY interested in the critical mod. They had vehemently argued that they should use ONLY the critical part then, to get the critical mod.  With the above table, I've managed to convince them they're still better off using both items because they will succeed more often if they use both items.  

Here comes the complication.  You can get one of a dozen different critical mods, and one of these is much, much more highly desirable.  It's common for someone to reset the weapon after attempting the modification if they don't get the 'good' modification.  You can modify a weapon up to three times, and there's a group of people who believe that once you get the good mod, you should not use the success item for the second and third modifications because they see 2 acceptable outcomes: failure and critical success. Critical success is obviously the desired outcome. Failure is acceptable because you get to try again. A normal modification without critical success is seen as the 'worst' outcome, because you can't reset the weapon without losing the good critical modification, too.  

So, how do I explain to them that the chance of critical success is 45% whether they use the success item or not, and they're better off using both items for all three modifications?"
Overbooking,0,1,False,False,False,statistics,1494399534,True,[removed]
Continuous Bottom-Hole Pressure (BHP) data comparison,2,3,False,False,False,statistics,1494417565,True,"I have simulated the same reservoir twice, with two different simulators. The picture is an example of how the BHP pressure varies through time (every equal report step, between the two simulators).

My question, is how would I say something about the difference in terms of a statistical analysis? I am trying to check the validity of the results of the second simulator (orange in the picture). I would like to be able to get a standard deviation, mean and even a confidence interval. I am a complete beginner so if there are more things that should be added, I would love to hear your opinion! I have tried to search the internet for a while, with little success. An indication of where to look would be greatly appreciated. 

Edit1. added a picture: https://reddit-uploaded-media.s3-accelerate.amazonaws.com/images%2Ft2_ehrnc%2F9pcavj2cwnwy

Edit2. some more context "
How to prove a proficiency in statistics,6,6,False,False,False,statistics,1494417662,True,"This could be the wrong sub. If so, just let me know; I don't mean to be breaking any rules.


I majored in art history and marketing in college, but by year 2 decided I was leaning more toward market research/statistics. I graduated without changing my major. However, I was able to find a part time job that dealt heavily with digital marketing, and my job involved analysis and data visualisation on a daily basis. During my last semester of college, I worked as a teaching assistant on a statistics course. I also know Python and JavaScript.


I applied to a data science masters program that I believe I'm qualified for, and was sort of wait-listed because the department is waiting for proof of my statistics proficiency. I really really want to get into this program, but I don't know what to show them. I never saved any Google AdWords or Analytics documentation because it was considered confidential company data. I can prove that I worked as a TA and I can show them stuff I've programmed, but that's not exactly statistics. 


If you were me, how would you handle this? What could I show them that would prove my proficiency? They have already talked to my past professors and bosses who could vouch for my skills in this area, but they want to *see* something. I'm at a complete loss. Any help would be appreciated."
Is anyone able to read through my survey and provide feedback? Its in relation to nomophobia.,0,1,False,False,False,statistics,1494417873,True,[removed]
Why do so many organizations use SAS?,84,50,False,False,False,statistics,1494420801,True,"It's clunky, difficult to read, and feels so archiac compared to other languages like R and Python. I've been avoiding it for years now because it feels like every time I have to perform even the simplest of tasks, I have to read a 10 page manual. I'm finally going to learn it just to keep my job opportunities open. I would love to hear some reasons why I shouldn't hate it, but so far, I don't have any. "
Does anyone have a link to good former regression papers/final projects for undergraduate/graduate level?,1,2,False,False,False,statistics,1494435128,True,I have to do a regression final project for my stats class and it would be helpful to have some former reports to get a better understanding/basis on doing my report.
Test Post,0,1,False,False,False,statistics,1494435730,True,[deleted]
Test Post,0,1,False,False,False,statistics,1494437684,True,[deleted]
Test Post,0,1,False,False,False,statistics,1494438100,True,[deleted]
test,0,1,False,False,False,statistics,1494438159,True,[deleted]
Generating cross-references in R for a statistical thesaurus,0,1,False,False,False,statistics,1494438516,False,
"Possible to do a blocked ANCOVA with a model II, non-linear covariate?",2,4,False,False,False,statistics,1494439563,True,"So, I've got some a block design that I'd like to do an ANCOVA with. The problem is that the covariate (and subsequent regression) is non-linear and there is considerable error in the X variable. Is it possible to do a blocked ANCOVA with a model II, non-linear covariate?"
What are some good videos on probability that are not lectures or Khan Academy?,6,4,False,False,False,statistics,1494440097,True,I want to boost my conceptual knowledge of probability; what are some good videos in going about this that are not OCW type things or Khan Academy?
What are your requirements for upvoting something?,0,1,False,False,False,statistics,1494444163,True,[deleted]
Power calculation for a large scale factorial design,8,3,False,False,False,statistics,1494445195,True,"Hi,

I was wondering if anyone can point me to any resource that can help me figure out the appropriate way of determining the sample size required for a large scale factorical design that I'm planning to run? Thank you"
Evaluating Alpha and Beta of a binomial distribution problem.,0,1,False,False,False,statistics,1494448160,True,[removed]
Adding Avg Forecast Error into future Predictions?,2,3,False,False,False,statistics,1494449483,True,"So I'm a bit of Stats novice and wondering if this is a statistically valid method to use, for a very basic predictive exercise. 

Say you have 10 months worth of predictions and their actual amounts. You calculate all 10 of the (Predictions - Actuals), add them up, and divide by 10 to get your average forecast error. 

Would it make sense to then ADD this average error to future predictions? So I'd allow my current projection model to produce an estimate, and then add this avg error in an attempt to make it more accurate. I'd then keep adding to the 10 datapoints as each month passed by, gathering more history for calc'ing the Avg Error (and hopefully decreasing it). 
"
Best practice for examining relationship between a scale that uses standard scores vs scale that uses T-scores.,1,1,False,False,False,statistics,1494450869,True,"I'm currently trying to figure out if I need to do some converting of my scores. I have one scale that utilizes standard scores for its data output, and another that utilizes T-scores. I'm trying to find correlation between the 2 scales and I don't know if I should leave the data: as is, convert to a z-score, or convert to a percentile rank? Any help on this matter would be much appreciated. "
How valuable are academic certificates?,7,4,False,False,False,statistics,1494452077,True,"Someone posted something abou A&Ms online stats program a bit ago and it got me thinking about going back. Now, I *just* finished a Masters in Industrial Engineering so I'm thinking that going back for another Masters might lead to a divorce, and it might be hard to get my work to fund it, but I noticed that they also offer ""certificates"", which I assume are like ""mini-degrees"".

I'm having some difficulty finding details on them, but I'm curious if you ever see these in folks and do you think they actually carry any weight?  The weight is a significant part of the equation for me as my current degree isn't opening the doors I want it to.  

Ed:  sorry, here's a link to one of the certificate programs.  Can't find the actual plan for it though.

http://online.stat.tamu.edu/degree-plan/applied-statistics/

Ed2:  found the degree plans.  12 hour certificates

http://online.stat.tamu.edu/degree-plan/

"
"Single Z-test, Double Z-test, T-test, and chi squared tests.",4,1,False,False,False,statistics,1494471485,True,I'm currently trying to understand when to use each one of the above tests in the context of a problem/scenario. For example what words or characteristics in a chart or question should I look for to know which test to use?
Is cointegration necessary for Granger Causality to be significant?,0,1,False,False,False,statistics,1494498064,True,[removed]
Time invarying covariates in a fixed-effects regression?,3,3,False,False,False,statistics,1494508977,True,[removed]
Bidirectional Causality in Granger Causality,1,1,False,False,False,statistics,1494516300,True,"Hi, I'm not sure if this is the right place to post this but I've been having a lot of confusion here. 

Basically, my professor and I disagree about bidirectional Causality. I'm running some Granger Causality tests into the relationships between different types of infrastructure and economic growth. 

Could somebody explain bidirectional Causality? How are two variables acting on each other in the same time period? How does that interaction work?"
That last question (4) is annoying me!,5,0,False,False,False,statistics,1494517616,False,
Demographic Survey for /r/Statistics [05/11 - 05/25],14,37,False,False,False,statistics,1494525453,True,"Hello, 

Yesterday, /u/qlikers suggested to run a survey for the users of /r/statistics to find out more about the people using this sub. It only seems appropriate given we are a community of people obsessed with data. 

The survey is distributed through Qualtrics and will run for two weeks from **May 11th to May 25th.** After that the results will be shared with the sub. Individual responses will be kept private.

You can take the survey here:
https://byui.az1.qualtrics.com/jfe/form/SV_cYmCsCKOyh6cGeF"
"I'm having trouble finding a good resource that explains what a mixture model is, to someone who is an absolute beginner. A scarcity of formulas would be nice too.",21,4,False,False,False,statistics,1494525999,True,
Help with Chi-Square and biology,3,1,False,False,False,statistics,1494527740,True,"Hi everyone. I'm a Zoology student with zero experience in statistics. 

Here's the issue:
I'm trying to compare frequencies and number of appearances of behavioral patterns between two ages in elephant seals.

My idea was to use chi-square between each pattern.

My data looks like this. For each pattern, one table:

For example, Pattern Nº1

Juveniles|Subadults
:--|:--
113|50

Pattern Nº2

Juveniles|Subadults
:--|:--
85|23


being 113 and 50 the number of times pattern Nº1 appeared in the whole sample.
and so on. With a total of 20 patterns.


The idea was to use chi-square to see if the frequency of each pattern is related to the age of the animal.
But don't understand how to make the chi-square table because it should be 2x2.

If you could please help me, I would appreciate it a lot."
Question 6 #bastard,0,1,False,False,False,statistics,1494532719,True,[removed]
What is the appropriate statistical test for this type of data?,0,1,False,False,False,statistics,1494532802,True,[removed]
Does anyone know of a video that shows a matched pairs experiment?,0,1,False,False,False,statistics,1494534448,True,[removed]
ELI5 Fisher scoring for generalized linear models?,0,1,False,False,False,statistics,1494538479,True,[deleted]
538 feature on the perils of small subgroup analysis in presence of measurement error: The Tangled Story Behind Trump's False Claims Of Voter Fraud,4,9,False,False,False,statistics,1494543576,False,
hat's a good way to validate a linear model predicting revneue for vendors?,1,1,False,False,False,statistics,1494557685,True,Is R2 good enough? That's what I usually use but I wonder if an interviewer could be more aggressive. I guess I can say I can keep some data to test on for bias variance trade off and go from there. Is this the typical route?
How common is it to use data on computers with no internet access?,7,2,False,False,False,statistics,1494562585,True,"At my work, all computers with data have no internet access. This is to prevent confidential breaches. Has anyone else come across this? Seems like a pretty old-school solution.

So I have 2 computers: one with data connected to an intrAnet, and another, completely separate machine with internet access. "
Would you take a 20k job or wait for 60k job you have a 50% chance of getting (both jobs are the same in every other aspect)?,5,0,False,False,False,statistics,1494564811,True,[deleted]
Benchmarks for adjusted R-squared,2,1,False,False,False,statistics,1494570658,True,"I have a survey dataset where customer satisfaction is the dependent variable. I have 5 independent variables, and my adjusted r-squared is .45. Wondering what benchmarks there are for r-squared for this type of dataset? The industry is retail."
What test do I use for two variables that are ordinal and seem to have a non monotonic relationship?,0,1,False,False,False,statistics,1494574103,True,[removed]
"Here is a survey about password security. It's for my mathematics research. I will post the results here, if there is atleast 100 answers.",16,51,False,False,False,statistics,1494574591,False,[deleted]
How do I write the following results in my Results section?,0,1,False,False,False,statistics,1494582944,True,[removed]
Anyone here attend Rice University for a M.S. in Statistics?,0,1,False,False,False,statistics,1494598560,True,[removed]
College student looking for recommendations on a good textbook,0,1,False,False,False,statistics,1494599679,True,[removed]
"SPSS allows you to save 95% confidence interval values from a linear regression, what is the difference between doing this by means or individuals?",0,1,False,False,False,statistics,1494610741,True,[deleted]
Finding Scale Factor,0,1,False,False,False,statistics,1494623573,True,[removed]
Improving SEM model fit: removing items or adding posthoc covariances?,7,2,False,False,False,statistics,1494634433,True,"Howdy folks. I am trying to fit a fairly complex multi-level SEM (200+ variables). This is known to be low quality data, and I have to perform posthoc manipulations to reach a reasonable fit (collection more data is not an option). 

I understand that I can either correlate between specific items or just remove misbehaving items. My question is, what is the preferable option, given that the end-product will be not empirically validated (but I still want to get the most accurate results possible)? 

Also, given that I have 300 subjects, is it possible to perfom a 200/100 train/test split?

Thanks!"
Luck finally explained.,4,2,False,False,False,statistics,1494640860,False,[deleted]
"I just dont understand arch & garch, can anyone give me a hand?",4,11,False,False,False,statistics,1494655238,True,"I'm trying to understand it but i just cant make sense of it, even after reading it several times. From what i can understand about it, arch and garch model the conditional variance of a time series. Where the variance of the next period depends on the variance of the previous one. To prepare a garch/arch model, one would have to first prepare a regression on a time series(which must be stationary?), then say, a garch(1,1) model would regress the conditional variance of the residuals of the first regression, on the lags of itself and on the lags of the variance of the squared residuals. Am i misunderstanding something? Can anyone correct me or fill me in anything im not following? 

Any help is appreciated."
R - How to self-teach?,32,59,False,False,False,statistics,1494665306,True,"I have a professor with over 30 years of educational research that believes R is the best statistical software available due to its extensive community of users. 

I would like to teach myself how to use this program so I am prepared for grad school. Are there any good guides you would recommend for a beginner?

Edit: Thank you for the suggestions everyone! This should keep me busy for a while."
This is how many tweets will be possible using Unicode 10.0.,0,0,False,False,False,statistics,1494678728,True,[removed]
Basic stats for clinical research project,2,0,False,False,False,statistics,1494679324,True,[removed]
"saw this, thought it was coolest use of statistics ever.",14,122,False,False,False,statistics,1494679449,False,
The Office Tableau Viz,0,1,False,False,False,statistics,1494712443,False,
Normal Distributions -- review of basic properties with derivations,6,11,False,False,False,statistics,1494738005,False,
"Correlation, Causation and Machine Learning",18,16,False,False,False,statistics,1494749658,False,
Question: convergence of Iterated Conditional Modes (ICM) for MAP inference,6,2,False,False,False,statistics,1494767657,True,"Hi everyone,
Please forgive if I'm not posting in the right place.
ICM is very fast for solving MAP inference but I could not find any references that contain a detailed analysis on its convergence (e.g. rate of convergence).
Any suggestions please? Thanks a lot for your help!"
Marginal/partial (APE) effects in an ordered logit model,1,3,False,False,False,statistics,1494782912,True,[removed]
"How bad did i fuck up ? (K-means clustering, hierarchical clustering)",1,2,False,False,False,statistics,1494788437,True,[removed]
Equivalent chart for inferential statistics?,7,52,False,False,False,statistics,1494792297,False,
"Can someone help me with binning data, and explanations of various distributions (example included)",0,1,False,False,False,statistics,1494797410,True,[removed]
One of my clients is a music record label. I'm trying to build a predictive model to forecast ticket sale performance on one of their recurring concert events. Could use some help!,6,3,False,False,False,statistics,1494797740,True,"I'm an ecommerce business development consultant. One of my clients is a relatively small-sized record label. I'm most definitely not a stats guy but have a grasp on the basics (with a lot of information gaps since college). Could use some help and direction for creating a data forecasting model. This isn't really part of my paid scope with them, but wanted to go above and beyond to help them out.../r/statistics has come in the clutch before.

Here is the scenario I'm trying to model:
The record label is responsible for planning, promoting, and executing many concerts throughout the year for their artists. 

Last year they started a touring concert series. Its essentially a 1 day mini-festival they throw in various cities around the US under the same brand name.  They've thrown 5 of these so far, and have several more planned for this year already on sale now or about to go on sale.

While on sale, its important for the label to be able to gauge sales performance and get an estimate of the final attendance.

Now, the actual number of sales each day can be dramatically different from show to show depending on the size of the market, competing events, and a myriad of other variables. The total attendances range anywhere from 2,000 - 4,000+ people. But, I have a theory that changes in daily ticket sales volume for these events still follow a general trend from the time the event is announced until the day of the show. 

I want to accomplish 3 things:

 1. Model and compare the data from each of the 5 previous events to see if they do in fact follow a similar trend.

 2. If they do follow a similar trend, I want to create a regression model for this trend.

 3. I want to use this model to evaluate and forecast sales performance of this year's events. (for example, determine whether or not a show is currently underperforming or over-performing)

This trend would not be totally linear. Typically tickets go on sale anywhere from 2 - 3 months prior to the event. When the show is announced there is very high volume (hundreds of tickets daily) for several days while they sell reduced-priced ""early bird"" tickets. Then it drops off to between 5- 20 per day until ~2 weeks before the event. Daily sales then gradually raise, until a final big spike in the few days leading up to event day.  

The data I have available: 
1. Daily Ticket Sale Quantities For All 5 Past Events
2. For each day of sales, I can calculate the number of days remaining until the event
3. Daily Cummulative Sales Volume

I have an idea in my mind how to do this, but stuck on specifics of execution and which type of analysis I should be running.

Some questions:

1. Tickets are on sale for different lengths of time for each event depending on how soon before the event they announce. How can I normalize this? Was thinking maybe turn days remaining into a % of time remaining and use data buckets.

2. What test can I run to see if the 5 data sets do in fact follow a similar pattern....not in terms of specific volumes, but in the up and down trends based on time remaining to the show?

3. Once I run those tests, whats the best way to accurately model the trend across 5 data sets? Should I map the actual data points, or an average variance from the mean?

4. I need to be able to judge if an event is under/over performing, and accurately predict what the final sales volume will be using the trend model. What is the best method to accomplish this?

I have excel at my disposal, and might be able to snag a trial version of Minitab. If you can give me some pointers on general direction you would take this I can do additional research on the concepts.
"
ANOVA-Analysis of Variance,0,1,False,False,False,statistics,1494819076,True,[removed]
How to plot multiple lines from different data sets GGplot2?,0,1,False,False,False,statistics,1494842856,True,[deleted]
How to combine two linear regression models?,12,8,False,False,False,statistics,1494845524,True,"i want to try model mineral consumption in two different places but try to combine them into one equation so i can model it in stata. for example, consumption1=b+b1*factor1B + b2*factor2B + b3*factorK, with consumption2=a+a1*factor1A + a2*factor2A + a3*factorK, both have a common explanatory variable factorK, how do i combine both equations into 1 equation? thanks stats gods :)

.
factor1A and factor1B explain the same thing but just for different places so they have different values , just like factor2A and factor2B"
Re: How do I formally state that the highest random number chosen from a distribution is higher the more numbers I choose?,0,1,False,False,False,statistics,1494847083,True,[removed]
How accurate is a linear regression when using ordinal dependent variables such as Likert scales?,5,1,False,False,False,statistics,1494851518,True,
Embarrassing T-test Question,0,1,False,False,False,statistics,1494861010,True,[removed]
Minitab Training Course Suggestions?,0,4,False,False,False,statistics,1494865521,True,[removed]
"I need help with my statistics homework in psychology,i am really desperate pls",0,1,False,False,False,statistics,1494868893,True,[removed]
Statistics methods for four point likert scale,0,1,False,False,False,statistics,1494871170,True,[removed]
Question about formula for sample autocovariance,0,1,False,False,False,statistics,1494874192,True,[deleted]
Average salaries,0,1,False,False,False,statistics,1494876093,True,[removed]
"desctable, an elegant and powerful package for creating statistical summary tables is now available on CRAN. Feedback appreciated!",9,32,False,False,False,statistics,1494881811,False,
Ap Statistics Project,2,0,False,False,False,statistics,1494885414,True,[removed]
Is a 2 proportion z test suitable for our observational study?,0,1,False,False,False,statistics,1494886059,True,[removed]
How to get into big data,0,1,False,False,False,statistics,1494889950,True,[removed]
Could a statistics pro give me some insights to catch a cheating significant other?,0,1,False,False,False,statistics,1494903471,True,[removed]
Ratio of percentiles of a Gamma distribution depends only on shape not rate. Why?,3,0,False,False,False,statistics,1494904250,True,"I learned recently that this is true, and verified it for a few values using R, but would like to prove it for myself. Searching on Google did not get me to see this property anywhere, so I though I would post it here to see how I could prove it to myself or see a proof online. Thanks!"
How does one model discrete count regression with a binary output?,6,1,False,False,False,statistics,1494904818,True,"For discrete count data, I would normally use Poisson regression or negative-binomial regression, implemented with `glm()` and `glm.nb()` in R. 

What if the dependent variable y_i is binary, 0 or 1? "
Order of analysing simple regression models,1,3,False,False,False,statistics,1494906057,True,"I have a simple regression model to analyse 
Consumption=b+b1factor+b2factor+b3timedummy

I want to do some analysis which include heteroscedasticity check, testing for structural break, adding interaction term, adding high power factors for functional form RESET testing, adding in lag variables. 
Problem is I don't know in what order to do the tests in. Do I add the interaction terms in then check heteroscedasticity? Do i do heteroscedasticity first then test structural break? Do I create a simple model first and add in the test one by one? Do I include everything first and remove the terms one by one? Can i add in the interaction term and the high power terms and regress them all together then do heteroskedasticity test? etc.etc.

I've tried to model it as usual and a factor is statistically significant, but when I add in the same but squared factor, both the squared and non squared coefficient became insignificant. I'm not sure what this means and I'm confused lol. 
Thanks stats Gods!!

Edit: im modeling consumption data in two different places and I have a few variables to build up the best model."
What are some interesting case studies you've read where statistics was used to solve a problem?,15,46,False,False,False,statistics,1494940480,True,
Am I understanding inflation correctly?,1,0,False,False,False,statistics,1494944190,True,[removed]
How easy is it to find work with a bsc in statistics?,4,2,False,False,False,statistics,1494947705,True,"I'll preface this by saying Im from canada but chose to go to university in a pretty isolated part of australia. Perth, where I live is at least 20-30 years behind the rest of the world in most ways and as such I am struggling/unable to find much work opportunities or internships here.

I feel like the course Im taking in stats is good, our head professor is german and he knows his stuff. However I'm wondering how easy it will be to find work as a data analyst/data scientist with a Bsc. in statistics and possibly no internship experience. Im considering in interning for a not for profit with minimal statistical requirement just to get a decent reference

My grades are all high, I know my shit and I do really good in interviews. I can program in R, ANOVA, logistic regression, longitudinal analysis, time series, correlation analysis, heteroscedastic variance. And I can do clustering, association, classification etc. 

I honestly feel capable of doing high level statistics/data science for a company however when I look on job sites they all seem to be looking for overly qualified people with 2-10 years of experience. Thanks for the help"
What are some data points in customer issue tickets you analyse? Which one has the biggest impact?,0,1,False,False,False,statistics,1494949633,True,[removed]
Looking for Coaching/Support in Stata against Payment,1,1,False,False,False,statistics,1494954256,True,[removed]
Help with understanding chances for graduate school admissions!,1,0,False,False,False,statistics,1494955272,True,[deleted]
Best Way to Compare 2 Data sets With same Y axis,0,1,False,False,False,statistics,1494957136,True,[removed]
How would I graph survey data?,3,1,False,False,False,statistics,1494972531,True,"I recently did a survey and I want to put the results into a visually appealing form like a graph but I'm confused as to how I would do so? They survey was 2 questions long with 12 responses for each question, the respondent was only allowed to select 1 response for each question. I had a total of 19 participants in the survey who answered both questions. Any help with this would be greatly appreciated."
Data collection - Does anyone collect survey data in the 'field' using technology like tablets?,10,10,False,False,False,statistics,1494977798,True,"Hi All, hoping some folks here might have some experience with using tablets to collect data in the field.

We currently do our evaluating efforts using scannable printed surveys due tech capabilities of our subjects. Its suboptimal for many reasons including the obscene prices we get charged for printing the scannable formats and the gargantuan data cleaning efforts that follow including massive man-hours hand entering crumpled surveys that won't scan.

Some sites will have internet access and can use online surveys, but usually the tech to subject ratio isn't fantastic. Many will not have any tech that can be relied upon. What we would like to do is use tablets that we can hand out at survey sites and get returned afterwards, maybe pre-loaded with the surveys or maybe in support with WiFi hotspots (WiFi is also no guarantee).

I wondered if anyone else had experience moving from a really low tech data collection methodology to something quicker/better, what challenges and solutions you came up with, etc. I know this isn't 100% statistics but I feel it might be a problem other data anaylsts in this sub may have encountered and might have some experience with. Mods please remove if our think it is inappropriate."
Simple question to settle and argument:,5,0,False,False,False,statistics,1494979436,True,[deleted]
Log-Level Interpretation? (x-post from Stata),2,3,False,False,False,statistics,1494984392,True,"My friend is trying to interpret a coefficient and our answer seems a bit off-base. The Y is a log of population growth rate (a ratio) and the X is per-capita GDP. If her coefficient is 31.04, is it correct to say that a $1 increase in per-capita GDP is associated with a 3104% increase in the population growth rate, on average?

We're not sure if this is a regression/data issue or an interpretation issue. As far as I know, the level-log increase requires a x100 in the interpretation but then it's just crazy... Even 31% seems a bit much, even if it's on average for per-capita."
Where to begin with analysis,0,1,False,False,False,statistics,1494984811,True,[removed]
Statistics Question (Need Help ASAP),0,1,False,False,False,statistics,1494988088,True,[removed]
Question about logistic regression for determining which factors/variables predict treatment selection,18,4,False,False,False,statistics,1494990858,True,"Hi everyone. I am conducting a project where I have to conduct regression on a data set with rather large amount of variables and am in need of some help. 

To give some context, we are looking at whether certain prognostic variables make individuals more likely to undergo surgical or non-surgical treatment. There are a variety of variables, which are grouped into different prognostic categories (each category has 3 to 20 different variables). Categories include past medical history, which may include the presence of hypertension, obese, etc. These prognostic variables are usually binary, but can also be categorical with >2 categories or continuous. 

From my understanding, I would conduct some sort of multinomial logistic regression, where the outcome is surgical or non-surgical treatment (categorical). However, I am uncertain whether I should include all variables in this model or conduct several logistic regressions for the prognostic categories. I also am wondering what other considerations I should have when conducting this analysis. 

I would like to use either R or SPSS to conduct this analysis.

Thank you!"
Do dice multiply?,3,0,False,False,False,statistics,1495002787,True,[removed]
Quick question about transforming data to normal distribution,0,1,False,False,False,statistics,1495004601,True,"Hello,

I have a dataset for cookies. In this data I have the number of cookies and Price. But there is some stupid data points where people want for 2 person except for 2 cookies. And I don't and can't know if data point says number of person,  instead of number of cookies, how many cookies have been made and sent.
I have 53 data points
I want to get rid of outliers as someone might ""price"" the cookies overvalued sometimes.

To get rid of outliers, afaik, my data should distributed as normal distribution. But it is not and I could not transform it to normal distribution. 

My question is, is it legit to try average prices per cookie to normal distribution?

Data is like:
number / average price
2	50


1	30


2	25


2	25


2	15


2	15


4	10


2	10


5	8


20	7


6	6,666666667


5	6


30	5


12	5


10	5


10	5


6	5


5	5


30	4


20	3,75


20	3,5


35	3,142857143


40	3,125


100	3


20	3


15	3


10	3


10	3


10	3


50	2,7


50	2,6


50	2,6


25	2,6


30	2,5


20	2,5


10	2,5


10	2,5


150	2,333333333


15	2,333333333




20	2,25


14	2,142857143


30	2


30	2


30	2


20	2


20	2


15	2


13	2


100	1,5


20	1,5


75	1,466666667


200	1

thanks in advance. this is a pricing case study and the only info I have is this data."
"Lead time bias, length time bias and Will Rogers effect in cancer diagnostic",1,8,False,False,False,statistics,1495005845,False,
SPSS For Research,0,1,False,False,False,statistics,1495013516,False,
Beta distribution with multiple (5) shape parameters,5,1,False,False,False,statistics,1495018989,True,"Hi everyone,

Is this even possible? I have a series of questions to ask my client where he can only answer between 1 and 5. (1-best; 5-worst). The questions have different sizes of impact on my overall measurement.

Let's say question 1 weighs:

1-1

2-4

3-7

4-11

5-16

and question 2 weighs:

1-1

2-5

3-11

4-18

5-28

I have the help of computing all this in excel. Though I do not know how to shape each curve according to it's weight.

Thanks in advance redditors!

Edit: In the end I would want a % in which each question could impact my scenario."
7 Awesome Qualities Of Statistics Homework Help Service From Tutorhelpdesk,0,1,False,False,False,statistics,1495020286,False,
Lost Matching Criteria - Matched Pairs Alternatives?,2,2,False,False,False,statistics,1495033294,True,"Hey All, 

I just got a new GA position at my university and had run into a unique problem I haven't encountered before. The criteria meant to match two records (for a matched pairs t-test) was collected on one form and not the other and because of that we have no way to patch the pre-post records. I know that a two sample t-test would be out because the records aren't independent of each other (being the same person doing it pre&post) I was wondering if you all could knew any alternatives for this data so that it's not just completely thrown out? And that way more than just descriptives are reported on for it. "
Seeking help planning a self-study curriculum that focuses on time-series analysis and modeling,6,15,False,False,False,statistics,1495038690,True,"I apologize if this isn't appropriate for this subreddit. I did do a reddit search, and saw that other self study posts were reasonably well received so I thought I would post my own topic.

A bit of background: I have a bachelor's degree in Computer Science, and a decent background in math. In college I took

* Calculus 1, 2, and 3 (multivariate)
* Differential and Partial Differential Equations
* Calculus based statistics (but didn't retain much)
* Linear Algebra (but I do not consider myself to have a good understanding; I struggled really badly in this class)

My goal is to design a self-study path that sets me with a solid statistical background but focuses primarily on time series analysis and forecasting. To that end, I am trying to design a plan of study that uses online classes. At some point I may consider going back to school to acquire a Master's degree, but for the near future (4-5 years) I would like to spend that time on self study. 

Right now my current plan is to start with the MIT OCW course [Probabilistic Systems Analysis and Applied Probability] (https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/index.htm) to establish a baseline, then work through [Statistical Analysis of Financial Data in R] (https://www.amazon.com/Statistical-Analysis-Financial-Springer-Statistics/dp/1461487870) to learn to use R and gain more specific experience.

Does this seem like a good plan? Should I try and get a more solid background before I try and tackle the Statistical Analysis book? If so, any suggestions on what topics specifically I should focus on? I did find [this helpful topic] (https://stats.stackexchange.com/questions/20514/books-for-self-studying-time-series-analysis) on Stack Overflow for books on time series analysis. I just want to make sure I do my due diligence in establishing a foundation before I branch into a more focused topic.
"
Can any regression analyses be run as a model II regression?,1,1,False,False,False,statistics,1495040903,True,"I've done model II linear regressions but what if I wanted to do a piecewise regression, or a quantile regression, or an ANCOVA? Would I be able to make the regression (components) of these analyses model II (i.e., there is error in Y *and* X)?"
Working out the S.E.M without the sample number,0,2,False,False,False,statistics,1495045061,True,[removed]
please consider taking my survey for my statistics project if you have a spare second,0,1,False,False,False,statistics,1495051031,True,[removed]
please consider taking my survey for my statistics class if you have a spare second!,0,1,False,False,False,statistics,1495052222,True,[removed]
How to correct for multiple comparisons in dependent data?,4,10,False,False,False,statistics,1495056798,True,"I have two time series data (of length N) from two measures respectively, and I did a t-test time point by time point to compare the difference between the two measures. So I have a p-value for each time point (so N p-values).

Obviously there's a multiple comparisons problem that I need to correct for. I understand the standard procedure is FDR (or the stricter Bonferroni). However, please correct me if I misunderstand, both FDR and Bonferroni assumes each data point is independent from the other. Wouldn't both be too strict a test for time series data because time points are dependent on each other?

Is there a more suitable correction for multiple comparisons that take into account the dependence between data points (i.e. tests)? Would appreciate any help!"
Question on how to model a poisson process,0,1,False,False,False,statistics,1495057020,True,[removed]
Best order to take courses in Swirl?,1,0,False,False,False,statistics,1495063720,True,[removed]
Question regarding a 2x2 design,2,1,False,False,False,statistics,1495066131,True,"Hello everyone. I have a question for you all regarding some comments a reviewer made on one of my manuscripts regarding my stats. Let me give you a quick description of my experimental setup.

Basically, I was looking at how many individuals in a colony attack simulated prey under 4 different conditions: (1) indirect cues from a predator, (2) direct cues from a predator, (3) direct + indirect cues from a predator, and (4) a control group that received no predator cues whatsoever. 

The way I analyzed differences between these groups at certain time points was to do Tukey's post-hoc tests between treatments. There were substantial differences. The reviewer, however, suggested doing a 2x2 analysis. Here are their brief comments:

> Why include ""treatment"" as a one-way variable? The perk of your 2x2 design (Direct cues: present/absent; indirect cues: present/absent) would be to included it as 2 separate factors. That would allow you to detect if there were a significant interaction, and would more precisely identify which factor (direct cues or indirect cues) best predicts your response variable.

I am a little confused at how to go about doing this. Is my study even a 2x2 design? Does he want me to do a 2-way ANOVA setup with # of attackers as my response variable and 

1. Direct
1. Indirect
1. Direct*Indirect (interaction)

As my predictor variables? 

But what about my group that received both direct and indirect cues, and my control group? Is his suggestion appropriate? How would I go about doing what they asked me to do?

Any info you guys can offer me would be very helpful. I feel like I'm being an idiot here, as the request seems so simple, but I just don't see how to set this up with the treatments I have.

Thanks!"
"[Academic] Statistics project (need answers) ""United states, highschool graduates""",0,1,False,False,False,statistics,1495071757,True,[removed]
Best Statistics Assignment Help for your Project in Australia,0,0,False,False,False,statistics,1495100960,False,
"Junk in, Junk out: significance testing",0,1,False,False,False,statistics,1495111073,True,[removed]
Formulating my own curve,12,2,False,False,False,statistics,1495111076,True,"http://imgur.com/g9WURCj

Hi all,

I posted on here yesterday with a vague description on what I am trying to achieve. 
The link shows a graph in which I have plotted my data. Each line represents a question I will be asking. The user can answer anything from 1 to 5. 1 - best; 5 - worst. Each question has a different 'weight'/probability. (and an impact on the total, not displayed here).

Now, I want to know how to draw a curve according to each 'question's' possible answers/weight/probability. The idea is that, when a user answers, let's say, 2, that I will be able to say.. OK, so this question's weight/probability when 2 is chosen is 12%. Now I will multiply this 12% with the total impact. Finally, I can say, that having a rating of 2 for this question, it will impact the final total by a factor of 0.24=(12%x2%).

Please ask more if you do not understand, as this is also new to me and the model is not set in stone yet, (so I am open to any suggestions).

Also, the probability will be calculated from the weight (y-axis). There is leeway on the top and bottom, as answering 1 to all questions should still impact the final total, and not impact it by zero percent. The same with answering 5 to all questions, it should not impact it with exactly 100%, could be more, could be less for the top border."
Top 15 Python Libraries for Data Science in 2017,0,21,False,False,False,statistics,1495111227,False,[deleted]
How to fairly select teams where noone is favored?,7,1,False,False,False,statistics,1495127282,True,"I have virtually no experience in statistics, and if this is the wrong place to put this questions, I apologize and could you redirect me to a better subreddit for my problem.

I am the President of a local theater organization. Frequently, we have an event where 5 team captains select their teams from 20 or so people. How do I make this selection the most fair?

Every time I try a different method, one captain always ends up with the best people! Its frustrating.


I have tried having every captain number who they want in order, and directly comparing them, giving each participant to the captain who put them highest. This means that if their are 4 people that no one wants, then one captain who has them slightly higher up than the rest will get all four.


Tl;dr What is the best way to let 5 captains choose team members fairly, where each captain gets an even amount of people they prefer?"
Is that possible to adjust for covariates for correlation test in SAS?,0,1,False,False,False,statistics,1495127812,True,[removed]
How can I adjust for covariates in using correlation test in SAS?,0,1,False,False,False,statistics,1495130278,True,[removed]
I need ideas for a stats project,4,1,False,False,False,statistics,1495139680,True,I'm in Ap stats. We're doing projects. I need an idea. It has to have a good design for collecting data and displaying it. Thoughts?
Any help with finding right type of stat test to run for this set of data?,0,1,False,False,False,statistics,1495141404,True,[removed]
Should I take Discrete Math/Graph Theory as a stats major?,11,2,False,False,False,statistics,1495141612,True,"Hi, I was not sure if this would be necessary or recommended if I am looking for a masters in statistics somewhere down the line."
How would I find the confidence level for cost-per-click of Facebook ads?,14,20,False,False,False,statistics,1495143653,True,"I've got about STAT 101 knowledge and my job has given me a statistics question that I don't know how to solve. 

We're testing different Facebook ads with the goal of figuring out which ads have the lowest cost per conversion (Each conversion in this instance would be someone clicking on our ad). We A/B test ads against one another to hone in on this - which of two images performs better, which of two target audiences performs better, and so on. 

I'll use an example to get at exactly what I'm looking for. We've got three pieces of data for each ad: Reach (how many people saw the ad - effectively the sample size), Conversions (how many people clicked it), and Cost-per-Click. 

Ad A: 
Reach - 5438
Conversions - 164
Cost per Click - $.41

Ad B: 
Reach - 1194
Conversions - 82
Cost per Click - $.61

I've been using [this calculator](http://getdatadriven.com/ab-significance-test) to find the confidence level for the conversion rate, but that isn't directly related to cost. 

Which brings me to my question: **How can I determine the confidence level that Ad A has a cheaper cost per click than Ad B?** My goal is to be able to say with 95% confidence that one ad is cheaper than the other so I can toss out the more expensive one and continue to refine. 

More broadly, I suppose I'm asking how to find the confidence level for a rate (cost per click) as opposed to a number (conversions). "
Data for several instances of one measured result with random changes to inputs. How can I best quantify the effect of each?,4,7,False,False,False,statistics,1495160628,True,"My brain is sort of fried right now so apologies in advance if this is a sloppy explanation.

My situation is sort of analogous to having data on the throughput of many different manufacturing plants, for each, before and after a handful of maintenance and repairs were performed. The repair measures overlap for different plants but not in any systematic way. The data sets look something like:

**Plant A**:

First year throughput: $5 million of product

Second year throughput: $9 million of product

           Measures performed between 1st and 2nd year:

-Replace motors

-Extra employee training

-New conveyor belt

**Plant B**:

First year throughput: $7 million of product

Second year throughput: $6 million of product

           Measures performed between 1st and 2nd year:

-Change in employee schedule

-New automatic assembly process

-Replace motors

And repeat maybe 50 times. I want to take a ""best guess"" at what the change in throughput will be if measure X is implemented.

Of course, all this data is in a million different spreadsheets right now, so it would also have to go into some kind of database."
Error in R-code for evaluation of optimal design,0,1,False,False,False,statistics,1495182856,True,[removed]
Understanding the practical application of statistics?,0,1,False,False,False,statistics,1495188353,False,
"Hi, is there something called a ""disruptive mthod/technique"" in statistics?",10,1,False,False,False,statistics,1495190335,True,"EDIT: ""Disruptive method/technique"". That is a verbatim translation from Japanese. I was wondering how it would be translated into English."
Wouldn't AI take over eventually?,8,0,False,False,False,statistics,1495191025,True,[deleted]
"Simulations, distributions and monte carlo. Help :)",2,2,False,False,False,statistics,1495193069,True,"Hey guys i don't know if this is the correct place to ask this but here it goes:

I work at a textile company analysing data and working it out.

I'm trying to make something to preview, for exemple the number of defect meters of a certain article.

I have a table with the month/year and the defect meters of the last 8 years. Would i be able to ""try to guess"" the number of meters for, let's say, may/2017 with a uniform distribution+monte  carlo simulation?

I'll delete this is i'm asking in the wrong section.

Need helping putting this up :)

Thanks"
Top 15 Python Libraries for Data Science in 2017,1,43,False,False,False,statistics,1495196963,False,
Can someone help me with this R question?,0,1,False,False,False,statistics,1495214905,True,[removed]
Logic/Knowledge check for a statistics novice.,5,8,False,False,False,statistics,1495223527,True,[removed]
Multilevel models multicollinearity problem x-post from r/AskStatistics #desperateAndInHurry,0,4,False,False,False,statistics,1495226187,False,
"I have a sample of 30, I know they're exponentially/gamma distributed.. How do I find a 95% confidence interval in R?",11,9,False,False,False,statistics,1495227042,True,"To be clear, I'm looking for a 95% confidence interval of potential future observations.. 

I think I've found a 95% confidence interval for the rate parameter using this code when x is my vector of 30 observations:
      
      mean(x)
      qgamma(c(.025,.975), 30, 30)/mean(x)
I get:
      
    [1] 0.0001420522 0.0002922951
But I dont know how to use that yet..

Is it as simple as using the calculated rate parameter within the pexp function like so?:
      
    > qexp(0.95, rate = 0.0001420522)
    [1] 21088.95
    > qexp(0.95, rate = 0.0002922951)
    [1] 10249
"
Confused on how to best present this data and what tests to use.,1,4,False,False,False,statistics,1495231391,True,"I wanted to see if there was any difference in issues with an apple after applying a sealant using two different techniques.  The first technique I took n=102 apples (I) and applied the sealant immediately after washing it.  The second technique I took n=136 (II) apples and applied the same sealant 24 hours after washing it. 


I placed both apples outside and after a week, I observed both groups to see if which apples had one or more issues being: Discoloration, bugs, mold, shrinkage.


I recorded the instance of each issue that presented itself, recorded the total number of apples which presented one or more issues, and I came up with the following table:

[Table of issues](https://i.stack.imgur.com/CESaf.png)

Which statistical test would be best to determine if there is a significant difference between the method of the two groups in terms of preventing issues or having fewer 'bad' apples?


I'm not sure if it is correct, but I ran a two proportion test in minitab comparing the number of apples with one or more issues using the pooled estimate of the proportion and a 95% CI with a hypothesized difference of '0'. Minitab came back with a p-value = 0.046 but also showed a Fisher's exact test p-value = 0.059.


Which, if any, do I use? Is using a two proportion test even correct to get my point across?  This method seems to test whether one bunch has less 'bad' apples but it doesn't seem to show whether either method has fewer issues with the bad apples (e.g. bad apples having multiple issues vs only one issue). 


If I wanted to say that the method in group (I) yielded fewer issues, how should I present it? Average issue per apple for those apples that had issues? Average issue per apple for all apples in the study? or just the total number of apples with one or more issues? 


Also, which type of test should I use for the best choice?


Thank you so much for your time and any assistance you may provide!"
Should I use KNN?,1,5,False,False,False,statistics,1495235114,True,[deleted]
Biostats or epi Jobs and tattoos,10,6,False,False,False,statistics,1495235944,True,"Im starting up my ms in biostats and epi this fall.
I have a collarbone tattoo that is easily covered; however, I wish to get a wrist tattoo. Is this looked down upon by most biostats or epi employers?
Id rather be able to secure a job than gat a tattoo; but would love both, if possible."
Advice on simple feature recommendation program?,0,1,False,False,False,statistics,1495237837,True,[removed]
Questions about where to apply to MS in Stat.,0,3,False,False,False,statistics,1495251436,True,"How accurate are the US rankings for best programs in statistics? I know it's self-ranked, but is it reflective of general quality? Say after top 5, is there a real difference between 6 vs 10 or 10 vs 20? At which point does it become not bad, to just average, to mediocre? Schools to look at? Schools to avoid?

I'm trying to find a program to do nondegree first then apply for masters (sub 3.0 gpa). I'm trying to figure out if it's worth it to relocate in order to make a better program possible and which ones are a solid no. I see that Stanford #1 on rankings has a non-matriculated option, but the possibility seems unlikely compared to trying at a state school. 

I'd also love to hear from people who have gone/are going to ms in stats about any generalizations about their schools/cohort. Do they tend to be younger/older? Professionals vs. academia? Exit ops?"
"Tectbooks/Tutorials for Key Driver Analysis, Penalty Analysis, Holt-Winters smoothing method (econometrics),",2,2,False,False,False,statistics,1495277589,True,"I am diving into market research and these are subjects of interest for the latest projects we're preparing but I haven't got an opportunity to learn this in college so I'd appreciate if you could provide me with source of learning of any kind for aforementioned tests/methods. I have good knowledge of SPSS, R, Python, Wolfram and EViews, usually don't do Stata."
How to explain two-way ANOVA as univariate ANOVA?,4,4,False,False,False,statistics,1495292351,True,"In SPSS, two-way ANOVA is called univariate ANOVA. So, following that logic, I labeled my table of two-way ANOVA as univariate on my research paper. Now when I have to explain it in my defense, to teachers, What can I say that is correct for labeling it as such?"
Trying to come up with a mixed design for categorical data analysis,0,1,False,False,False,statistics,1495313424,True,[removed]
Masters in Statistics with little to no background - give it to me raw,13,22,False,False,False,statistics,1495324836,True,"So I have a Bachelors degree in an unrelated discipline but I'm considering a enrolling in a Masters program in Statistics as part of a career change to (probably) data science. I'm trying to get a realistic handle on how to fill in the gaps to get a solid background before I start applying to programs.
 

From what I've seen, most Statistics Masters programs require Calc I, II, and III, Linear Algebra, and an Intro Statistics course. As an undergrad I took Calc I and an intro statistics course that was required for basically all science/social science majors. I got an A in both courses, but that was probably 6 to 7 years ago at this point. Should I retake Calc I and Intro statistics? What type of background do I need for Linear Algebra? Are there any other courses I should take before starting the Masters program? I also took an intro python class as an undergraduate which is evidently very useful for data science these days. Will I need anything like an algorithms or data structures course?
 

Thanks in advance for any insight!"
"How to ""calibrate"" a model?",0,1,False,False,False,statistics,1495329137,True,[deleted]
Would statistics be a good field for someone with ADD and not much of a statistical background?,5,2,False,False,False,statistics,1495329528,True,"I'm a college graduate with a BA who majored (and had two minors) in social science. A couple years after graduating, my career has gone nowhere, since I feel that I made some mistakes with my career decisions, as for most of my time in college I was distracted by taking care of a parent dying of cancer.

I've decided I need to go back to college, and I'm wondering if statistics might be a good field. I've heard great things about careers like statisticians. I have no coursework with statistics, and not a lot of math coursework. However, I had an internship for the last couple years of college in education research, where I had to do a lot of quantitative and qualitative research. The director of the department was my supervisor, and he taught statistics at the university.

The other issue though is that I have ADD. It's never been an issue with my classes as I tended to always get at least a B+, but college had much more variety than most of my jobs, and I've noticed that I'm more prone to mistakes than the average person at work. Is this a field someone with ADD can excel at, and how can I know before jumping in that it's something that I could truly be passionate about?

Would it make more sense to go back and get a Bachelor's in Statistics, or go straight for a Master's? Are there other careers besides Statistician that I might want to consider?

Sorry about the wall of text, I tend to ramble when I'm talking about something important. I appreciate any help."
"""Calibrating"" a model",0,1,False,False,False,statistics,1495330785,True,[removed]
Reality Ring - Responding to an advocate of multiple intelligences,0,0,False,False,False,statistics,1495351497,False,
Picking the best weighting technique,0,1,False,False,False,statistics,1495397114,True,[removed]
What is this sort of distribution called? Exponential decay with a constant maybe?,7,3,False,False,False,statistics,1495397774,True,"Here is the shape of the distribution I have in mind:

http://imgur.com/a/n8wbK

I'm sure there's a name for this. What is this? At first sight, I was thinking this is related to exponential decay with a constant, e.g. exp(-/alpha*x), where /alpha is the constant. The distribution above looks like ""inverted"" exponential decay...almost.

Is there a name for this in the literature?"
What statistical test should I use? The data is too few. Data is posted,10,1,False,False,False,statistics,1495407254,True,"
I want to test if the training provided by a volunteer organization to a certain hosptial resulted in significantly lower intraoperative complication rates.

Below is the data. Also, the volunteer organization provided the training in the hospital in the year 2010.

(Year, Complication rate)
2008, 11.11%.
2009, 8.00%.
2010, 9.52%.
2011, 6.58%.
2012, 6.25%.
2013, 5.63%.
2014, 4.07%.
2015, 2.15%.
2016, 1.89%.

Would I be comparing the data from years 2008 and 2009 with years 2010-2016? That would mean comparing only 2 data points before the intervention vs 7 data points after the intervention.
Doesn't look like a normal distribution at all. What kind of testing can I possibly do? 

I've got other data points as well, such as number of surgeries performed, number of certain types of surgeries performed, etc. I definitely see trends and have done linear regression on this data already. I'm just hoping to perform another test comparing the means before vs. after the volunteer intervention to do more analysis on the data.

This is my first research project so I'm new at this any help is greatly appreciated!"
Stuck with 3 group x 12 measures,3,5,False,False,False,statistics,1495414808,True,"Hello,

I am struggling pretty hard trying to find the best way to analyze data from 3 groups and 12 measures. The 12 measures can be segmented 3 x 4 - distance and metric. I am using SPSS at the moment and so far have tried linear mixed-model and repeated measures. I can't seem to find a way not to perform multiple ANOVAs + posthocs. Would appreciate any help, open to using R or MATLAB as well. 

Thank you"
Number assigned to letter,0,1,False,False,False,statistics,1495418916,True,[removed]
Regression model with time series transformation,8,8,False,False,False,statistics,1495427841,True,"Not sure if ""transformation"" is the right term but I've been working with regression models between digital marketing data as my independent variables vs my dependent variable of sales. The issue is that some customers might take 1 day to purchase while others might take up to 7 days. Out of curiosity I experimented with transforming all the data into moving sums of multiple days (let's say 3 days). 
Just to make it clearer, I'd transform the data from the date from 05/01 to a sum of 05/01-05/03 as one data point, then 05/02-05/04 as the next data point, etc. I'm trying to improve the model to better account for longer purchase times. Just looking at my adjusted R^2, it does hit a peak at 4 days.
My questions are the following:
1. Am I committing some stupid mathematical mistake that makes this approach useless?
2. If the approach works, how do I transform it back to better understand the effect each independent variable has on sales? Is it as simple as dividing by 3?

Thanks!"
Regression Method,0,1,False,False,False,statistics,1495458207,True,[removed]
Logistic regression with uncertainty in outcome measurement,6,2,False,False,False,statistics,1495464706,True,"Hi all,

Briefly: I have a dataset with a binary outcome (dead/not dead) and a number of other covariates.  For roughly 80% of my population, I know with certainty whether the person is dead/not dead that is, the field is 0 or 1.  For the remaining 20%, I have a probability that the person is dead.  This probability has been estimated via model with known sensitivity and specificity.  I would like to model dead/not dead using a logistic regression in order to understand the influence of the other covariates

My question is: how would I setup this model incorporating those patients for whom I have a probability of death?  My first thought was to weight the observations somehow.  Since probabilities close to 0 and 1 represent the most certainty, and probabilities close to 0.5 represent the lowest certainty.  So I could formulate my weights according to: wt(Pr) = abs(2 Pr - 1)

Which would transforms a probability of death from 0..0.5..1 to a weight of 1..0..1.

Is this a reasonable approach?  Are there others I should be considering?
"
"Stats 101 Q : Where the distribution of a selected attribute is not known in comparison to others, how do you account for the distribution of attributes between maximum and minimum variability?",10,0,False,False,False,statistics,1495469801,True,"For example...

If we know that 

* 14.5% of men in the United States are over 6'
* 6% of men in the United States make $100,000 or more

It would be erroneous to say that less than 1% are over 6' and make $100,000 per year.  That would be the product of the two.  We could say that ""no more than 6% of men are over 6' and make $100,000 per year.""  But that wouldn't be very accurate.

So we might be able to control for some of the occurrences of this, but as the number of variables increase, it becomes more difficult to control for.

It's probably not difficult to find a study on income and height.  But now, what if I have to find a study on income distributions for people who have gotten a DUI?

I guess I'm looking for advice on smoothing distributions so that I can somewhat roughly but fairly bridge the gap between minimum and maximum distributions and product distributions.

For instance.  We know that height can be a strong predictor of income, or at least, additional income.  And low income can be a strong predictor of getting a DUI.

What smoothing methods would someone use in absence of data of the correlation of these effects to bridge the gap?

OR

What would be a fair way to INCREASE the minimum population and DECREASE the maximum population to create a fair range of distribution?

So if we know that a 

* MAXIMUM of 6% of men could be 6' tall w/ an income of $100,000 or more
* The PRODUCT of these two is ~.9%

What way could we smooth the data to indicate stances that 

* Men that make over $100,000 are MOSTLY tall
* Men that make over $100,000 tend to be tall
* Men that make over $100,000 are slightly taller
* Men that make over $100,000 are even distributed (PRODUCT)
* Men that make over $100,000 tend to be shorter
* Men that make over $100,000 are slightly shorter
* Men that make over $100,000 are MOSTLY short

If you only knew these stats, how would you most honestly present the answer to the question if we know that roughly 3% of people have DUIs (but no knowledge of the effect of height on income, or income on DUI, if we assume that height has no effect on drinking and driving).

""What percent of men over 6' tall, who make $100,000 a year or more would have a DUI?""

It should be clear this is not a homework problem ;)"
Why is a fully marginalized likelihood so much more difficult to computer than a partially marginalized posterior?,6,8,False,False,False,statistics,1495471781,True,"I have a question in terms of Bayesian computation.

Let's contrast parameter estimation and model selection. When performing a Bayesian hypothesis test, this involves computing fully marginalized likelihoods. When performing (Bayesian) parameter estimation, one computes partially marginalized posteriors.

The latter is normally far more easy to actually compute than the former. Why exactly is that? How could it be so much easier to computer the marginalization of all of your parameters except one versus marginalizing all of the parameters?

Is there a computational intuition behind why this is?"
Factor Analysis and comparison,0,1,False,False,False,statistics,1495474690,True,[removed]
Cox proportional hazard model (PWP gap-time) - need to know if I'm using it correctly,4,0,False,False,False,statistics,1495476151,True,"This is kind of a strange situation, but I'll try to be succinct. 

I am trying to determine if a number of predictors influence the periodicity of high-crime street segments (""hot spots""). I have 14,000 subjects, each with 60 months' of observations.

My data are structured according to the Prentice-Williams-Peterson gap-time (time since last event) model in Stata 13:

stset time, fail (failure) enter(time0) exit(ftime) 

ID  time0  time  failure  strata 

1  0  1  0  1

1  0  2  0  1

1  0  3  0  1

1  0  4  1  1

1  0  1  0  2

...

2  0  1  0  1

2  0  2  0  1

2  0  3  1  1

2  0  1  0  2

...

k...

(using stcox with cluster[id] strata[risk event])

My question is whether I leave in all of the observations that do not have an event. With all observations, I have nearly 850,000 subjects (14,000*60). Without them, it drops to around 1,200 (the number of failures in the data set). 

I get *very* different results running the data both ways, so it's critical to my project that I get this right...

My independent variables are not time-varying (census data), I just want to know if they influence event periodicity, that is, whether an increase in a variable (e.g. poverty) is associated with an increasing likelihood of failure (and thus, a shorter interval between events). 

Edit: not all subjects experience failure...the vast majority never fail. "
Career Advice: starting MS in Stats next year; considering software development instead. WWJD?,9,2,False,False,False,statistics,1495486885,True,"Longtime listener, first-time caller.  I've read through many of the career advice threads here over the last year, and this sub was very helpful to me in figuring out how to get into a good grad program for Stats - so thank you!

 

--- QUESTION (or TL;DR) ---

Knowing what you know now as an experienced statistician, if you were in my position,  would you do the MS or go for a coding bootcamp to become a software developer?  Do you think your quality of life as a statistician compares favorably to that of a software developer?

 
 
I've done a lot of research online, but I'm not sure how the information I've found stacks up against your real-life experience as statisticians.  So I hope you'll have advice for me now too - either about what you would do in my position, if I have terrible misconceptions about the work, or about perspectives I should take on this question.  

 
 
--- BACKGROUND ---

Sorry, this is long. 

 
 
Now that I've been accepted to an MS in Stats, a bit of buyers' remorse has started to edge in.  If I were fresh out of college, I wouldn't have these questions, but I'm changing careers after many years in an unrelated field, I'm in my mid-30s and have a family, and so I'm a bit worried about cost/student debt, quality of life, and career trajectory and timeline.

 
 
In addition, while taking an intro programming course, I found a kind of satisfaction in programming, like woodworking or finishing a project around the house.  I haven't found this, at least yet, while taking undergrad statistical theory courses - but again, they're just the theory courses.

 
 
I've also recently run across coding bootcamps, like Hack Reactor or App Academy, which take 3 months and advertise a high placement rate.  I don't think I'd have trouble getting into a reputable one.  This seems like a considerably faster and cheaper way to enter the workforce at a comparable entry-level salary (~70k).  Essentially, considering tuition + time working/not working, doing the MS in Stats will be around $75k (best case scenario: full funding + stipend, somewhat likely) to $150k more expensive (no funding).  The other major consideration is that the coding bootcamp route seems like it will only be stressful financially and personally for my family for 3 months as opposed to for 2 years of grad school.  I can weather the cost and stress if the long-term outlook seems worth it.

 
 
So, basically, do you think it's worth it?  I'm hoping the MS in Stats will lead to higher quality of life both at work and at home, greater intellectual challenge, increasing demand for my skillset, and higher eventual salaries.  In terms of personal qualities, my reasons for wanting to do Stats are:

+ I find almost anything interesting to learn about
* Being the content-area expert whom experts in other fields need seems like it would lead to more interesting, varied work with the opportunity to learn about everyone else's work
* I'm not bad at or afraid of math
* I like having relative autonomy in my work
* I want to contribute to scientific research but not spend years of my life doing the research
* whenever I talk about stats my wife groans and walks away =)

 
 
Please correct me if it seems that I am misguided about what working as a statistician is actually like in the day-to-day - I worry that I'm being overly optimistic about it, especially if I only do the MS and not a PhD.  Last, in case it helps, in terms of content area, I think I'm most interested in working in biostatistics, environmental statistics, or ""data science"" (whatever that is =).  Lately the ""data science for the public good"" stuff has seemed pretty cool too. 

 
 
Many, many thanks to anyone who takes the time to read and respond to this.

"
t-distribution and inverse t-distribution tables. When to use which?,3,3,False,False,False,statistics,1495487306,True,"Hello,
just a small question form a statistic nood who is going to a rough time.

What is the difference between these two distributions? 

I need to calculate confidence intervals, one professors told me tu use the inverse t-distribution for that purpose but in ebvery book I can only find information about  that t-distribution and that it is use to calculate the CI.

I really hope that it makes sense what I wrote and I will be very grateful if somebody could help me out.



"
How do I test for differences between groups with bimodal distributions?,10,12,False,False,False,statistics,1495491871,True,"Hey all,

I'm trying to figure out how to test whether three groups differ from each other. I have three groups of animals, homozygotic, heterozygotic and wild-type. For each animal I have analyzed a number of vocalizations (generally in the hundreds). I want to know if the frequency of the vocalizations differ between the groups. To check whether the animals differ, I'd usually calculate the mean value for each mouse from their datapoints (in this case the mean frequency of their vocalizations), and compare them in an ANOVA. However, I found that the vocalizations are distributed in a bimodal fashion. My intuition is that just getting the mean frequency for each animal would not be correct, as you are then ignoring the bimodality of the vocalizations. I'm considering taking either the median and doing my ANOVA on that, or calculating the number of vocalizations < 75000.0 and above 75000.0 and looking at the ratio of those. Neither of those options seem ideal though.

Does anyone here have a good suggestion as to how I can work with this?
"
Need Help ASAP Please n Thank you,0,1,False,False,False,statistics,1495501330,True,[removed]
Family Statistics Visualization for a Tattoo,2,1,False,False,False,statistics,1495522315,True,[removed]
How are multiple models handled in the real world? (Example in description),3,1,False,False,False,statistics,1495525116,True,I'm newish to stats but not math. Have only started looking at basic distributions and stuff but say there's a restaurant or something with guests coming in Poisson distributed with a mean x1 most of the day but mean x2 at lunch. Not looking to solve any specific problem more just looking for discussion to read while I'm at work. Thanks for anything remotely related!
Advice regarding analysis on population literacy datasets,0,1,False,False,False,statistics,1495531928,True,[removed]
Standard errors and parameters calculated after NLLS fitting.,3,5,False,False,False,statistics,1495535469,True,"I have fitted an equation to data using nonlinear least squares regression and have extracted estimates of its 4 parameters, along with standard errors for each one. There are a couple extra parameters whose values I can calculate after fitting, through equations that include the 4 main parameters.

What would be the best way to get standard errors for these two extra parameters? My first hunch would be via bootstrapping, but I am not entirely sure if it would be the best way."
Five Key Statistics from New York City FC’s 3-0 win over Orlando City,0,1,False,False,False,statistics,1495545235,False,
Prep softball statistics through May 22,0,1,False,False,False,statistics,1495545703,False,
Standard Errors using a bootstrapped data set,3,2,False,False,False,statistics,1495549523,True,"So I have a bootstrapped data set of estimates of density. I want to see what the standard error is, but since SE=sd/sqr(N) is the SE really accurate? Since it tends to zero as N grows, isn't the SE being heavily influenced by my number of bootstrapping replicates?

Thanks"
Which statistical method to compare two genetic tests?,0,6,False,False,False,statistics,1495555214,True,[removed]
Assumed multicollinearity in multiple regression,9,3,False,False,False,statistics,1495565657,True,"Hi,

I have a multiple regression model that includes independent variables that are closely related. They are biological markers at time 1 and time 2. Is there any problem with including both into a regression to predict memory at time 3? I am also including age in the model. The VIF values for the biological variables are <2. I had thought there would be more multicollinearity than that. Would it be wrong to remove either bio time 1 or 2 based on their correlation with the y? 

Thank you for your thoughts. I read through told posts on multicollinearity but didn't find my answer. "
Understanding median weighted by sample size,6,1,False,False,False,statistics,1495567153,True,"Hello,
I was reading a meta-analysis which combined 50 individual studies, each of which calculated the recovery rate for a mental health problem. Each individual study had a different number of participants, going from 20 up to about 3000. The meta-analysis took the recovery rates from each of these studies and then calculated the ""median weighted by sample size"". I hope to understand what this means. I've taken basic college statistics, and have already researched this question on Google (to no avail), but am not very knowledgeable. 

I believe the median weighted by sample size means that studies with a greater sample size were weighted more heavily when determining the median, and ""pulled"" that value down or up depending on their recovery rate - is that essentially correct? I would be grateful if anyone could give me a brief example.

I imagine that if a sample had, say, 200 patients, rather than 100, that it would essentially be counted twice or have an extra instance of its recovery rate inserted (compared to the 100 patient sample) when laying out the numbers in a row in order to determine the median. Please let me know if this is (at least simplistically) correct. 

Here is the source meta-analysis: https://www.yellowbrickprogram.com/ArticlePDF/Jaaskelainen-2013-A-systematic-review-Bull.pdf

It's far too long to read in full; but the relevant question is based on Table 1 on page 8 (page 1303 of the journal article). You can see that it says, a ""Median weighted by sample size"" beneath the table reporting the median recovery rates for different criteria.

Thank you to anyone who can offer help or explanation."
Dealing with,0,1,False,False,False,statistics,1495569896,True,[deleted]
What does this mean?,1,0,False,False,False,statistics,1495571331,True,"Hey everyone, I'm a high school Stats student and I'm doing a project on spending per student and various possible correlations. The one I'm having trouble with is SAT scores due to the fact that not all states require or even encourage it. However I found a bunch of ""adjusted data"" that adjusts the scores. 
""The full method I used was linear regression on states as observations (N=51). The regression was of both SAT scores onto participation rates on both the SAT and ACT, their interactions, and their second powers, as well as a constant. An analogous regression was made for SAT scores. The SAT score residual from this regression was added to the statewide average SAT score to get predicted SAT scores of all states if they had the same participation rate -- namely that of the average state.""
http://blog.prepscholar.com/average-sat-and-act-scores-by-stated-adjusted-for-participation-rate

I'd  like to be able to perform this myself so if someone could make this easier to understand or even give me an equation I could follow, it would be much appreciated."
I have a set of technical support cases that I want to experiment with. What kind of studies would be interesting?,2,7,False,False,False,statistics,1495577179,True,"I have about 5000 cases of technical support incidents that I can use as data. What can I do with them to get some meaningful insights? What type of study would be useful? Is there some type of modeling that I could potentially do?
This is more of a data-science question I see, but they very often overlap.
Thanks in advance."
(Magic: The Gathering) What is the ideal mana curve?,5,1,False,False,False,statistics,1495579490,True,[deleted]
How to assess normality of distribution between two datasets?,11,6,False,False,False,statistics,1495590429,True,"I have 2 related datasets: 1) measured values under a control setting, and 2) measured values under a test setting. 

I want to run statistics regarding the differences between the 2 datasets, but first need to know if my data is normally distributed.

I'm confused as to if I should run the Kolmogorov-Smirnov test (n=200 for each dataset, so Shapiro-Wilk is not suitable) for normality on the datasets themselves, or on the **differences** between the datasets. 

My end goal is to run either a paired sample t-test (if data is normal) or a Wilcoxon signed-rank test (if data is not normal) to compare the values between the datasets for significant differences.

The main problem I'm running into is that, for example, dataset 1 is normally distributed per K-S test, but dataset 2 is not. How do I decide between paired t-test or Wilcoxon in this case? Or am I supposed to run the K-S on the differences between the datasets, and if that's normal --> t-test and if not --> Wilcoxon? 

Thanks for your help!"
Why is it useful to sample probability distributions models?,4,3,False,False,False,statistics,1495595640,False,
Statistics Homework help,0,1,False,False,False,statistics,1495610192,True,[removed]
Best Correlation test,4,5,False,False,False,statistics,1495612988,True,"I work at a research funding agency and have recently been asked to look at the relationship between the institution the proposal came from and the institution of the Panel that judge them (peer review process) So basically looking whether it seems there is a bias but I was thinking of presenting this in table and chart form and perhaps looking into a correlation analysis but unsure whether  the data set is suitable. The data i have gathered is the number of grants funded from panel member institutions, other institutions and unfunded panel institutions and other."
[Question] Pooling and comparing confidence intervals from different publications,0,1,False,False,False,statistics,1495633834,True,[removed]
Problem with log. regression with interaction in SAS,2,4,False,False,False,statistics,1495634815,True,[removed]
How to know when a regression is no longer relevant?,28,8,False,False,False,statistics,1495648656,True,"I have a simple linear regression with a R^2 of .972 and n=253. The out-of-sample predictions were holding pretty much between +/- 2.58 Standard errors. Then the outlier came at over 3 standard errors and subsequent points (3) remains there... Should I dump my regression? How to test for regime changes with a short number of elements?

Thanks"
Toward tidy analysis · Simply Statistics,0,5,False,False,False,statistics,1495648875,False,
Margin of Error for a Proportion,0,1,False,False,False,statistics,1495665305,True,[removed]
Bayesian Statistic & Bayesian Graphical Model book suggestions?,3,15,False,False,False,statistics,1495666792,True,"You guys were right.

Bayesian Data Analysis 3rd edition by Andrew Gelman was the best textbook. I should have started this first over statistical rethinking and that one puppy book.

It's a really good book and very comprehensive. I've taken statistical inference and computational statistic so this book is perfect for me. 

The stat/math is strong in this book but nothing too bad for an master of applied stat student I guess. I'm very much applied and only theory class is stat inference really. 

Currently doing this for internship.

Was curious if you guys have any Bayesian Network book recommendation?

I currently have

* Adnan_Darwiche_Modeling_and_Reasoning_with_Bayesian_Networks
* Bayesian_Reasoning_and_Machine_Learning_David_Barber
* Building Probabilistic Graphical Models with Python
* Denis,_Jean-Baptiste;_Scutari,_Marco_Bayesian_Networks_With_Examples_in_R
* Daphne_Koller,_Nir_Friedman_Probabilistic_Graphical_Models_Principles_and_Techniques

It seems like Dr. Adnan Darwiche is good from the rating. Dr. Koller is more overall. The python one seems too applied. Likewise with the R one. Would love comments on these books including the R one. I need to do one eventually for my internship."
Behavioral Economics - Studying Risk analysis of Etherium and underlying Opportunity Cost. Anyone open for discussion?,0,1,False,False,False,statistics,1495671073,True,[removed]
Why do we do importance sampling,0,0,False,False,False,statistics,1495672916,False,
Cardiovascular Disease,0,1,False,False,False,statistics,1495674726,True,"Hi all, I've recently been offered to work as a statistician on several research projects regarding Cardiovascular Events.  I was wondering if any of ya'll had some recommendations on a primer textbook regarding cardiovascular health primarily to be used to some of the verbiage in current research.

Thanks mates"
Why is it infeasible to take samples from the optimum distribution when doing importance sampling?,8,0,False,False,False,statistics,1495674975,False,
What is this graph and how can I make it? More in comments,5,2,False,False,False,statistics,1495701723,False,
SPSS Assignment Writing Help Online,0,1,False,False,False,statistics,1495703845,False,
Interpreting F-Test Results,5,5,False,False,False,statistics,1495706547,True,"I cannot seem to find this answer ANYWHERE on the web or in my texts, everyone can explain what an F value and significance is, and how they are calculated, but not how to interpret them!

Given an F value and significance with an independent samples T test, how do you decide whether you are going to reject assumption of equal variance or accept assumption of equal variance.

Here are some examples 1) F=.61, Sig =0.44, 2) F=0.07, Sig = 0.78 and 3)F=4.21, Sig = 0.05

Do I consider both the F value and significance when making my decision, or is one more important than the other when making this choice, do you have a personal cutoff where you reject assumptions?"
polls on education level vs income,0,1,False,False,False,statistics,1495709756,True,[removed]
Are the bounds of a CI that we calculate mere estimations of the quantiles just like the sample mean is an estimation of the mean?,7,1,False,False,False,statistics,1495714991,True,[deleted]
A comprehensive beginners guide to Linear Algebra for Data Scientists,20,66,False,False,False,statistics,1495715823,False,
Cuyahoga County medical examiner releases final report on 2016 overdose death statistics,0,0,False,False,False,statistics,1495721634,False,
"Pacific Power: Call 8-1-1 before you dig, dig-in statistics on the rise",0,0,False,False,False,statistics,1495721724,False,
Significant rise in paramilitary activity in County Derry according to PSNI statistics,0,0,False,False,False,statistics,1495721821,False,
Opioid overdose statistics show decline since Louisville's record-high spike in February,0,0,False,False,False,statistics,1495721911,False,
How do I solve a test statistic when the standard deviation is unknown,0,0,False,False,False,statistics,1495729021,True,[removed]
How are Rutgers stats (960) courses. Need advice.,1,0,False,False,False,statistics,1495739974,True,"Hi, 
I'm a new graduate student in  Rutgers CS and I'm thinking of taking interpretation of data (960:586) and bayesian data analysis (960:668) for fall 2017. I've a machine learning background and I want to do a deeper study of some topics. Any feedback on these courses? What topics should I prepare beforehand to get most out of these courses. 
Thanks in advance. "
Emergency advise!,1,0,False,False,False,statistics,1495747267,True,"Ok so I've done 4 tests to show that A is bigger than B. In all 5, A was 3 to 5 fold larger. However, There was a large difference between values for A between tests. How do I show that the increase is significant?"
Using ratios as x variables in regression?,11,4,False,False,False,statistics,1495749070,True,"Hello,

Originally I had an x variable as the differential between two different levels (US inflation rate - Canadian inflation rate). The problem with this is I wanted to find the effects of how this differential changes over time. A level change isn't valid, a log doesnt work with negative values, and if my differential is 0 I can't use a percentage change.

I think that a possible solution would be to use ratios instead (USinflation/CADinflation). Would using ratios bring any sort of complications to this model? 

I am considering then logging this ratio to regress the estimated effect of a percentage change in X on Y. Is this valid, or should I format the data as a percent change instead of taking the log?

Thanks!"
Why would we ever want to confront the Partition function in Machine Learning?,1,0,False,False,False,statistics,1495750292,False,
What kind of statistics background is necessary to do serious research in machine learning and AI?,7,11,False,False,False,statistics,1495752803,True,"I feel as if I'm taking too many courses that emphasize the computational aspect but my statistics background may be too weak. Can anyone give any suggestions?

Thanks in advance."
Is there a term for the mean of the mean+median?,8,1,False,False,False,statistics,1495754116,True,"The mean is often useful for determining raw averages. The median is useful to look at if there are extreme outliers. Considering both are useful, I wonder if the average of the mean and the median is ever a represented value? Is there a term for it?"
Need help with correct statistical test to run,2,1,False,False,False,statistics,1495761607,True,"I have two groups (obese and nonobese) that I want to compare their preoperative outcome scores and post operative outcomes scores.  I know that the pre and post scores overall are sig diff.  What I am trying to do is compare the two groups (obese vs. nonobese) at two times points (pre and post operatively).  

What test(s) do I need to run for this?  I am using SPSS.  My question is whether or not obesity affects the subjective post operative outcome."
Inference from covariate-free models,3,1,False,False,False,statistics,1495767397,True,"Hey all, I wanted to get some thoughts on an analysis I'm trying to run. Say I have a dependent variable Y I want to predict,  an explanatory variable X, and a potential confounding covariate C. 

Say I run the model Y ~ X, and I get a significant result for X.

If C is not correlated (pairwise) with Y, but is highly correlated with X, can I satisfactorily infer from my first model that it is truly X that is influencing Y?

I know I should include C in the model, but I am wondering from a theoretical perspective if C could be excluded."
Would you trust a shapiro/wilk test (p<.0005) indicating abnormal distribution of residuals when histogram is clearly normal (with slight positive skew)?,0,1,False,False,False,statistics,1495776992,True,[removed]
Learn the basic approaches about business statistics.,0,0,False,False,False,statistics,1495782490,False,
Proper test for,1,2,False,False,False,statistics,1495794244,True,[removed]
Statistics help (psychology),15,2,False,False,False,statistics,1495794668,True,[removed]
[Academic] Looking for small business owners to fill out a quick survey! (All welcome),0,1,False,False,False,statistics,1495803136,True,[removed]
Regression help for product prices across time.,1,1,False,False,False,statistics,1495814168,True,"I was asked to help with a forecasting problem. We have historical data for a lot of different models of a certain type of product. For each model, we have prices from a lot of different sources. For each type of product, the price over time seems to look like a Gamma-type distribution a*t^(b)*exp(-ct). From fitting a simple linear regression on a few types after log-transforming the Gamma-like equation, it seems to fit pretty well.

However, they want a all-encompassing model that can describe all models with many other variables (i.e. size, manufacturer, etc). My co-worker that has been working on this as well thinks that we should do a pooled regression across the variables at different points in time. I have some experience with this type of technique, but it completely ignores the Gamma function over the time variable. His model is basically a linear regression at each point in time.

My idea would be to just regress a function of the type a(x)*t^(b)*exp(-ct) where x=variables other than time. Though that would force me to use a nonlinear model to fit it, the data is not too massive.

Any thoughts on what type of model that I can use that can use the Gamma-type function? Another analyst mentioned using time-series, but I do not have much experience in that."
PCI Statistics: A preprint review peer community in (computational) statistics,0,18,False,False,False,statistics,1495815630,False,
Help: How do I transform my data to be between 0 and 1?,1,1,False,False,False,statistics,1495818431,True,[deleted]
Request: The Most Boring Science/Statistic Research Papers You Know,1,6,False,False,False,statistics,1495822010,True,"Hi,

Can you recommend (With PDF link if possible) research papers that:

* are really boring,
* have unnecessary verbiage,
* have lots of long run-on sentences
* have minimal pictures
* display over complication of a simple procedure


 etc.. Ideally it would cover statistics, or a field of science using statistic to study an incredibly boring phenomenon in society or nature.


and also specifically, I would like a link to a Paper using Chi-Squared methods.'



Thanks Statistic / Science Redditors!"
What are the statistics for cops in the US shooting black men versus black women?,1,0,False,False,False,statistics,1495828995,True,And is there a bigger backlash against the shootings against women? Are the cops who shoot women more likely to be prosecuted?
How do I compare proportions between groups over time?,5,3,False,False,False,statistics,1495829213,True,"Hi all,

I'm a bit unsure about how I should tackle this particular problem. I have three groups of animals, measured on 4 different days, and I'm looking at the behaviours they are exhibiting. Specifically, they can exhibit 8 different behaviours, and they generally do show each on of those in a five minute period, with some variability in how often they show the different behaviours. So for example, animal 1 on day 1 does action x 15 times, action y 40 times, etc. I now want to know whether there is a difference over time, and whether there is a difference between groups between the behaviours exhibited. 

I was considering a mixed model ANOVA on the proportions of the actions (e.g. action x/sum of all actions), but I do know that proportions generally don't play nice with ANOVA. I also considered 4 different Chi-squared tests, one for each day, but I'm unsure whether this would account for the differences in group sizes (and thus the increase in actions due to the group being larger). 

Could anyone point me in a good direction?"
Some good statistics book for a non-statistician?,7,1,False,False,False,statistics,1495829341,True,
What are some important published papers in that are important for any PhD student in statistics to thoroughly understand?,0,1,False,False,False,statistics,1495844252,True,[removed]
What are some published papers that are important for any PhD student in statistics to thoroughly understand?,0,1,False,False,False,statistics,1495845680,True,[removed]
Statistics: an introduction to the discipline,0,46,False,False,False,statistics,1495854068,False,[deleted]
What are some published papers that are important for any PhD student in statistics to thoroughly understand?,0,1,False,False,False,statistics,1495869630,True,[removed]
What are some published papers that are important for any PhD student in statistics to thoroughly understand?,7,60,False,False,False,statistics,1495870239,True,"Basically, I'll have some time this summer during my PhD program (I'm a first year student not yet doing any research). One option suggested to me was as a project, take an academic paper and slowly work through it, and understand it thoroughly. I was looking for suggestions for any papers that people think might be suitable. I'm looking for a paper that I would benefit substantially by working through slowly, so likely the method of getting the result is somehow significant (and not some niche trick that isn't used anywhere else). I'm also looking for something that is at a graduate student level (most papers tend to satisfy this), while not requiring too vast an amount of background material (although I fully expect to have to spend some time to get the right background, in fact that's basically the point!)

Any thoughts or suggestions? What papers do you think it's particularly important for PhD students to understand on a deep level, rather than just reading the result and moving on? (I don't yet have any sort of specialty, just building maturity with the general material, that's why I'm asking here rather than asking an advisor).
"
Thoughts on Data Science certificate programs in the US,3,18,False,False,False,statistics,1495889176,True,"Hi everyone,

How are certificate programs viewed by data science professionals? Database and Data Analytics certificate of UCSC Extension at Silicon Valley seems like a good programme with good internship and employment opportunities for an international.

Also, are they a good step towards a masters or PhD degree? If the certificates are not worth doing, what else is there for an international recent-graduate looking to get into data science in the US? 

Any comment and recommendation is appreciated, thanks!"
Determine Contamination in Sample - Population and Sample Standard Deviations,3,2,False,False,False,statistics,1495895435,True,"Let's say I have an image database with 50,000 pictures of dogs. I had an algorithm find them for me, but I know it's not perfect. I know that there are some pictures of cats in there instead. If I pick a random subsample of size 1000 of these 50,000 pictures and find that there are 3 cats in there, what can I infer on the parameters of the whole dataset (number of cats plus minus error in the whole database)?

My first intuition is that I have to scale the number of cats I found by the factor between my sample size and the whole database (50), so there are probably 150 cats in the database.

Now I'm unsure about the error. What is the error on these 150 cats in my sample? 

Any hints will be appreciated!
"
Is the mean of medians meaningful?,1,1,False,False,False,statistics,1495904201,True,[removed]
Quantifying the extremity of an outlier?,4,1,False,False,False,statistics,1495912680,True,"I've got a ton of small datasets, each with about ten numbers. Most of them have an outlier that is significantly higher than the rest. I want to rank them by the extremity of that outlier. What would be the best way to quantify that?  
  
I thought about taking the ratio of the mean and median, but that would return a high number even if there were two high numbers, in which case I'd want the returned value to be low because the top value isn't much of an outlier.

The magnitude of the numbers also differs pretty greatly, and I don't want that to affect the result. (eg. 1, 2, 3 and 10, 20, 30 should have a similar result)  
  
Would taking the ratio of the highest number and the second highest be sound?   "
What tests should I use to interpret this data?,4,0,False,False,False,statistics,1495916228,False,[deleted]
dataanalysisclassroom – making data analysis easy,0,2,False,False,False,statistics,1495937180,False,[deleted]
Need some help with intepreting the outcome of this logistic regression model. My own thought is in the comments. Thanks in advance 😊,19,14,False,False,False,statistics,1495958611,False,
"Video reviewing Spatial Correlation, Spatial Weights, with interactive spreadsheet",3,16,False,False,False,statistics,1495979335,False,
What do I say about the SEMs in a psych report?,1,0,False,False,False,statistics,1495985059,True,[deleted]
Statistics for Business Question: Is there a downside to overpowering experiments?,21,7,False,False,False,statistics,1495988338,True,"Quick question from a stats noob: At my business we regularly run AB tests on our website to determine what content impacts customer behavior more. In the stats/scientific community there seems to be an aversion to overpowering tests, however the only good reason I can find is opportunity cost. In the event that we can get millions of test subjects in a matter of days and achieve statistical power of 95%+ relatively easy, is there a good reason not do this?"
How important is Stochastic Processes in Machine Learning?,0,1,False,False,False,statistics,1495998151,True,[removed]
"How are trading cards (pokemon, yu gi oh, mtg, etc) packaged to make sure the distribution of cards is balanced between common and rare?",4,23,False,False,False,statistics,1496000096,True,
Should I include the bar plot with the error bar with CIs if the error bars are very wide and overlapping? I want to show that they are statistically not significant.,4,5,False,False,False,statistics,1496001714,True,"I am doing Chi-square tests for my experiment in R. I want to show that they are not statistically significant. The chi-square test says that. For better visual representation should I show bar graphs with the wide overlapping error bars of observed and expected?  
I have used binom.test for calculating confidence intervals. 

For saying that results are significant, one need to show error bar I understand. Does one need to show for not statistically significant result?

Thanks"
Multinomial logistic regression vs Canonical discriminant analysis,0,1,False,False,False,statistics,1496009822,True,[removed]
CFA question,2,5,False,False,False,statistics,1496022213,True,[removed]
Mean or median of absolute delta values?,13,4,False,False,False,statistics,1496059486,True,"**update:** What the question basically comes down to is whether I can look at the Mean Absolute Error (MAE) for strongly non-gaussian data or should I report the ""Median absolute error"" if this even is a thing (Not to be confused with median absolute deviation).


*original post:*

*For a research project I'm comparing multiple testing methods with an outcome ranging from 1 to 16. In addition to comparing test-retest reliability (through Pearson's R), I wanted to take a look how much the within subject values differed.*

*The delta values (test 1 - test 2) are normally distributed around 0, and thus average out to nothing. Consequently I'm now working with the absolute values. The problem now is that the distribution of these scores is now extremely right tailed.*

*Usually I would advice running non-parametric tests and reporting median values but I'm a bit unsure now since it's my own ""modification"" that made the data non-gaussian.*

*Any insights?*"
How MattsenKumar radically changed our Client Services model,0,1,False,False,False,statistics,1496059875,False,
What test can I use for this situation?,5,1,False,False,False,statistics,1496075508,True,"I have two columns of data. 

First, a value between 5 and 23 based on an observation.

Then, a yes/no.

I want to show that there is a strong correlation (or not) between the observation value and the binary value. What test can I use for this?"
Finding The Sample Size With Known 'n' and Unknown Standard Deviation,8,2,False,False,False,statistics,1496077023,True,"I am conducting an experiment where I must estimate a population mean. First, I must find an appropriate size sample with a given confidence level of 95%

I am estimating the mean/average height of each tenant from my apartment building. There are a total of 350 people living at this apartment.

I also must conduct a preliminary test which I know nothing about. If you can help me out in any way, it would be very appreciated. Thank you!"
"TIL that Ronald Fisher, who invented the p-value,, discouraged it's use as a definitive test-- intended it to be only an informal test of whether it's worth a second look and to be used in conjunction with other tests",29,223,False,False,False,statistics,1496079293,False,
Predictive Model Power Analysis,2,1,False,False,False,statistics,1496079328,True,[removed]
Combining variables in SPSS,0,1,False,False,False,statistics,1496079764,True,[removed]
Determining confidence intervals on predictions for mixed models (using R),0,2,False,False,False,statistics,1496083780,True,"Hey Reddit,

I have a mixed model and am having trouble determining confidence intervals on each prediction. 

As an example, my model is something like:

> dev=lmer(development ~ temperature + month + (temperature|year))

And it was made using the *lmer* function in R. 

Currently, I am using the *confint* function provided by the lme4 package, with the default profile confidence interval method (my n is very large, well over 900, which should essentially approach infinity; data are more or less normally distributed).

When I run 

> confidence.intervals=confint(dev)

I get several ""sig01"", ""sig02"", etc. parameters as well as a ""sigma"" parameter. 

I have two main questions:

1. I am having trouble finding out the meaning of these sig(ma) parameters - it seems like they're just there to tell me about the variance (?) of interactions in my model? None flank 0, and the range of each is small, which I think means my parameters were good. If anyone can tell me more about what they mean that would be great.

2. This sounds foolish, but I don't actually know how to apply them to my predictions to get predicted confidence intervals. I also don't understand what is going on with the random effects here.

To add, I won't be doing much analysis on the confidence intervals, just reporting them.

Thank you!"
Micro-level data to contingency tables and back again?,2,1,False,False,False,statistics,1496087152,True,"This might be a ""dumb"" question, but I'm a bit stumped (I think I'm too sleep deprived) and Googling isn't helping.

I have dataset X, which I generate K many 2-way contingency tables. From these K contingency tables, is it possible to recreate dataset X? If so, how?

Thank you in advance."
"My treatment zones are statistically significant, but I want to figure out which variables in my set are contributing most to this difference. Can someone recommend a test?",1,0,False,False,False,statistics,1496098406,True,"For context, I have four 'zones' of different topographical features, 10 points in each zone with 20 different soil attributes at each point. I standardized the variables (using field mean and SD) and used a Tukey test to see which zones had the greatest contrast to one another, but I also want to see which of the soil attributes change the most from zone to zone. Any recommendations on how to do this. PCA keeps coming up in my search, but it doesn't seem quite appropriate. "
Probability for how long human race has?,0,1,False,False,False,statistics,1496098421,True,[removed]
Help me please? Working out how to get a representative sample of two population subgroups across several districts.,0,1,False,False,False,statistics,1496142726,True,[removed]
"How to infer ""groupings"" of data points separated alone one dimension?",16,2,False,False,False,statistics,1496145860,True,"I have the following data:

          type    distance
    0      X      12572
    1      X      11229
    2      Y      14144
    3      A      15781
    4      A      15486
    5      B      461
    6      X      328
    7      X      23
    8      X      50
    9      A      45
    10     A      231
    11     A      10779
    12     X      11433
    ...      .....

`type` refers to the data points category. `distance` is the distance between each data point. That is, the difference between X index 0 and X index 1 is 12572, the difference between the second and third datapoint is 11229, etc. 

One can think of this set of datapoints as being along one dimension. The identity (i.e. `type`) of the datapoint is irrelevant to this problem. I am interested somehow inferring the ""clusters"" of data points which occurs when datapoints are spaced closely together. In this case, it looks clear that the datapoints from index 5-11 consist of one grouping. 

One-dimensional clustering algorithms come to mind. However, there is a natural structure to this dataset; if the distances are less than 10,000, normally there's a cluster. Simply binning by hand might be more important. 

Is there a method for this problem based in probabilistic inference? Either there could be a way to infer the ""natural"" clustering within a given dataset (though that's ill-defined) or perhaps use part of the dataset as a training set?"
Your favourite graph-making/chart software?,27,23,False,False,False,statistics,1496146799,True,"Currently writing my BSC thesis in economics, and I do not want to use Excel or sheets etc for graphs, because I think they always turn out amateur-ish looking. Tips on good and preferably free software for this? Thanks in advance!"
List of violent events since 2010,0,1,False,False,False,statistics,1496149852,False,[deleted]
Using measurements on 19 pictures to detect the best location to take a photo at the Leaning Tower of Pisa,0,1,False,False,False,statistics,1496151588,False,[deleted]
Using measurements on 19 pictures to detect the best location to take a photo of the Leaning Tower of Pisa,0,5,False,False,False,statistics,1496151850,False,
Writing up linear mixed effects models? (Multilevel Modelling) R-Statistics,2,1,False,False,False,statistics,1496171370,True,"Hi all

I'm quite confused right now as to how I should correctly interpret a linear mixed effects model for a thesis I'm writing. 

I'm using the lme4 package with the lmerTest package in R right now.

I know that lme4 fits a linear model with a varying-intercept group effect using the variable GROUP.

In my case the state of residence would be the random intercept (GROUP variable). I use this since the responents are ""nested"" into different states.

For the regression table, when I write up the results, how do I go about including the random intercept into the paper? Do I always have to mention that the results vary according to the state that the respondent lives in?

I'm sorry if I'm being unclear here but I don't quite understand how to interpret these multilevel models.

Here's my r-code if this would help:

 model2 <- lmer(clintontrump ~ mobility_agg + mobility + age + ownedu  + race + health + gender + partyid + (1|GROUP), data = data2)

 > summary(model2)

"
What are some good resources to practice clustering and classification techniques using R or Python? (x-post with r/AskStatistics),6,1,False,False,False,statistics,1496171784,True,[deleted]
Am I going crazy? I get a different value when I run a simulation vs expected value [Question].,4,1,False,False,False,statistics,1496171899,True,"I play a simple tabletop wargame called Warhammer 40k. In it, you roll dice to determine whether an attacking units hits and wounds an opponent. The game functions like a boardgame version of xcom (if anyone's played it).



So, let's say a character wants to shoot a tank with a cannon. He has to roll greater than 2 on a D6, roll greater than 2 again, roll less than 6, and then he rolls a D6 to determine how much damage he inflicts. I wanted to calculate how many shots would be needed on average to kill the tank with the soldier on average. Using expected value, R returns ~9.5 from the expression below

    12/((2/3)*(2/3)*(5/6)*((1+2+3+4+5+6)/6))

But, if I simulate the data using the code below, I get a value of ~10.5. Can anyone explain what's happening? I feel like the simulation should match the expected value.

    roll <- function(){
      die <- c(1,2,3,4,5,6)
          return(sample(die, 1))
    }

    average = 0.0

    for (i in 1:1000){
      wounds <- 12
      shots <- 0
      while(wounds > 0){
        shots <- shots + 1
        if(roll() > 2 & roll() > 2 & roll() < 6){
          wounds <- wounds - roll()
        }
      }
      average <- average + shots
    }

    how_many <- average/1000.0"
If I have to study 7 topics for an exam and I have a choice of 4 Questions of which I have to answer 2. How many topics would it be safe to study?,0,1,False,False,False,statistics,1496172299,True,[removed]
"In a hierarchical regression model, is r squared delta equal to the semipartial r squared?",2,1,False,False,False,statistics,1496175655,True,"I'm predicting variable Z with variables X and Y. X is entered in the first block, Y is entered in the second block. I want to know if Y adds meaningful prediction above and beyond X.

I've been looking at r squared change for effect size and the statistical significance of the associated f change.

Would the semipartial R squared be different from the r squared delta in this case? (I get that it wouldn't be equal if they were in the same block.)"
How would I use standardized coefficients to create a model? I think that's the right question to ask..,1,1,False,False,False,statistics,1496178620,True,[removed]
tools that approximate the shape of a plot,0,1,False,False,False,statistics,1496187538,True,[removed]
Question about the percentage of overlap between two normal distributions,13,4,False,False,False,statistics,1496187885,True,"As someone with a very rusty understanding of statistics, I don't know what to make of this:

http://rpsychologist.com/cohen-d-proportion-overlap

The essence of the article is that when Cohen gave the percentage of overlap between the two curves, he doesn't count the area of *both* curves that overlap. The percentage of overlap would be higher if you base it on the combined area of both distributions (unless they overlap completely). Here are a couple of snippets:

>Now, let's plot Cohen's interpretation of overlap, which is the proportion of shared area covered by both population. Since we're now thinking about areas, and not frequencies, we'll have to remove half of the observations from the overlapping region. 

And:

>I believe this interpretation to be a bit strange (since I'm not using it on my site). To me it's more intuitive to think: if we have two populations, one that received an active treatment, and one that received a control, then 80 % tells us that 80 % of the total observations actually overlap. I believe that's more intuitive than reporting that the shared area is 67 %.

**Is there a consensus in the field of statistics on how the overlap between two distributions should be handled? The blogger has a source, but one publication is not a good indication of the mainstream thought on an issue.**  "
PSU online vs Columbia?,7,9,False,False,False,statistics,1496194127,True,[deleted]
"Linear, Quadratic, Cubic and Quartic fit to Multiple Regression Approach",4,1,False,False,False,statistics,1496195991,True,[removed]
The reusable holdout: Preserving validity in adaptive data analysis,1,5,False,False,False,statistics,1496200497,False,
Conditional Probability explained,0,6,False,False,False,statistics,1496214343,False,[deleted]
Idea for my incoming thesis,2,0,False,False,False,statistics,1496214599,True,"Hi! I'm an incoming 4th year bachelor of science major in Applied Statistics student. I'm just wondering what best possible topic that is appropriate to my course. Also, I want to apply a complicated statistical technique to my study. Thankyou!"
Is there a specific term to differentiate between these interaction effects?,8,2,False,False,False,statistics,1496231268,False,
Underpredicting mortality on intensive care,0,1,False,False,False,statistics,1496238816,True,[removed]
"Why are control variables said to be most influential on the outcome of the target variable? What are regressand, regressor, regressed?",3,8,False,False,False,statistics,1496245898,True,[removed]
Confused about the best tests to use for my medical project,0,1,False,False,False,statistics,1496249905,True,[deleted]
Should I be using a multilevel linear model?,2,1,False,False,False,statistics,1496250920,True,"I'm working through the *Discovering Statistics Using R* chapter on multilevel linear models and applying it to a data set I have. The data set contains measures of viral load over time in two groups of monkeys (one group got a treatment, one didn't).

Something like:

Subject | Group | Time.Point | Viral.Load

1 | 1 | 1 | 8.69e+04

1 | 1 | 2 | 3.23e+03

1 | 1 | 3 | 2.25e+02

2 | 2 | 1 | 3.74e+04

2 | 2 | 2 | 6.61e+03

2 | 2 | 3 | 2.14e+01

In the book, it talks about verifying the need for a multilevel model by comparing a model with a fixed intercept to one in which intercepts vary over contexts. It uses the 'nlme' package for this. When looking at time course data, it suggests using the 'optim' option. Like so:

$ interceptOnly <- gls(value ~ 1, data = subDf, method='ML')

$ randomInterceptOnly <- lme(Viral.Load ~ 1, data = subDf, random = ~1|Subject, method='ML', control = list(opt='optim'))

Then using *anova* for the comparison:

$ anova(interceptOnly, randomInterceptOnly)

>                    Model df      AIC      BIC    logLik   Test    L.Ratio p-value
>      interceptOnly           1    2    1738.133     1742.419 -867.0666                          
>      randomInterceptOnly     2   3     1740.105    1746.535 -867.0527    1 vs 2    0.02792943     0.8673

Obviously the p-value is non-significant at 0.8673. However, if I ignore the dire warning to not continue with multilevel modeling and do it anyway:

$ randomInterceptGroup <- update(randomInterceptOnly, .~. + Group)

$ timeRI <- update(randomInterceptGroup, .~. + Time.Point)

$ timeRS <- update(timeRI, random = ~Time.Point|Subject)

$ randomInterceptGroup <- update(randomInterceptOnly, .~. + Group)

$ ARModel <- update(timeRS, correlation = corAR1(0, form = ~Time.Point|Subject))

I get significant p-values:

>                     Model df      AIC      BIC    logLik   Test   L.Ratio p-value
>     interceptOnly            1   2   1738.133    1742.419 -867.0666                         
>     randomInterceptOnly      2   3   1740.105    1746.535 -867.0527    1 vs 2    0.027929     0.8673
>     randomInterceptGroup     3   4   1734.430    1743.003 -863.2152    2 vs 3    7.675021     0.0056
>     timeRI                   4   5   1727.098    1737.813 -858.5489    3 vs 4     9.332517    0.0023
>     timeRS                   5   7   1722.831    1737.833 -854.4157    4 vs 5     8.266431     0.0160
>     ARModel                  6   8   1711.731    1728.876 -847.8655    5 vs 6    13.100416     0.0003

So, should I be heading the advice of the book and not using a multilevel model or is it worth my time to further pursue it? Our biggest question is whether treatment has an effect on viral load.

(I have a Biology degree and haven't taken a true stats class since high school, so I'd appreciate explanations of anything that seems stupidly obvious)"
Don't use deep learning your data isn't that big · Simply Statistics,17,73,False,False,False,statistics,1496251267,False,
"When specifying the interaction between a continuous variables and a three-level categorical variable, should the dummy variables used to represent the direct effects of the categorical variable still be coded in the usual 0/1 scheme?",1,2,False,False,False,statistics,1496252365,True,"To create the interaction, the continuous variable is multiplied by each of the dummy variables. If both were continuous, the variables used to create the interaction would be centered to address multicollinearity. Is there something analogous that should be done when one of the variables is a multilevel categorical variable?"
"[Hiring] RStudio tutoring, preferably face to face",4,0,False,False,False,statistics,1496253433,True,"Hi all, I would be grateful for some R script tutoring. If possible, I'd like one-on-one classes. Its for introduction to RStudio. Are there any statistics wizards who are in London/SE England who are interested? 

Sorry to the mods of this isn't allowed"
Question About Evaluating Tagged Content,0,1,False,False,False,statistics,1496253913,True,"I’m working with a relatively small *data set that consists of several hundred social media posts, key engagement metrics and up to 10 content “tags” that describe the image of each post*. We leveraged Google Vision API along with a manual review to construct the tags. I’ve linked to an example of what we’re working with [here](http://imgur.com/vcZkWi9).

**What I’m trying to do:** I would like to leverage a statistically valid methodology to identify which one or more (in combination) of tags tend to perform the best across the data set. It’s easy enough to look at an individual tag and calculate the mean of the KPI, but any suggestions on how to evaluate combinations of tags that yield high performance? It wouldn’t necessarily need to be all tags in combination, but could be 3 out of the 10 perform the best.

Just looking for help on thinking through the approach to the math."
Various plots to describe one discrete variable.,0,1,False,False,False,statistics,1496266069,True,[removed]
How busy is statistics grad school for you?,3,10,False,False,False,statistics,1496271611,True,"Have a BSc in statistics and considering an MSc in statistics as well. I was wondering how busy each course can get compared to undergrad studies so I know how much hours I can work a week while going to school/paying bills. 

So how many classes are you taking, hours of study for each class/week, and how much you work?

I was able to get through undergrad taking 4 to 5 classes per semester and working about 20 hours a week just fine, even though I sort of had no social life."
Is it possible to self-teach yourself statistics?,7,2,False,False,False,statistics,1496285673,True,"I'm a biochemistry major, statistics/biomathematics minor, and I'll be finishing up my undergrad career next year. Over the past few years I've been reading about all of the really cool research that's being done with applied statistics and bio stats, and I've made it a personal goal to pursue an MD/PhD to interrelate medicine and research. 

I've been subscribed to this page for a while, and from reading some of these posts and trying to understand other aspects of statistics, I'm coming to realize that I don't know much at all. I'm really passionate and excited about the study, and I find myself reading ""trendy"" articles in data science, predictive analysis, and big data all the time, but it's not really helping me understand the meat and bones of statistics. 

I have a relatively weak math background, which I've been attempting to strengthen. Next semester I plan on taking linear algebra and an introductory computer science class. But I haven't taken a single theoretical class yet, and the only experience with statistics I have is the research I do in my lab, coding with R and STATA, and a couple of applied statistics classes where we learned about different tests. 

I guess my question is whether or not it's possible to learn some of this stuff on my own. If so, where do I start? Thanks!"
Need help determining where to take my stats education (current undergrad),3,0,False,False,False,statistics,1496291450,True,"Just to start off, please feel free to delete this if this isn't the right sub...

But to sum up, I need some help. I've been strongly considering getting a statistics PhD, but the thought literally popped up out of the blue a few weeks ago, and I'm not sure I have the necessary qualifications. Do I have any chance of getting into a decent Stats PhD?

I'm a junior attending an ""elite"" US institution (top 15), but don't have a solid GPA (3.54). I'm majoring in Stats and Economics, and my major GPA is around a 3.6. I've taken calculus and linear algebra, but have no advanced math classes, although I do plan on taking real analysis in the Fall. I have no research experience. I don't have a great relationship with any of my professors.

...All that said, how screwed am I? I literally never considered a PhD before this month, so I never bothered to research or do anything. At this point, I'm left coasting on my university's name, which I'm still not sure will get me anywhere. If anyone could chance me in general and recommend me schools that I could actually get into, I would be highly appreciative. My top two schools are Rice and UTAustin at this point, but I'm not sure I could get into either...

I also haven't taken the GRE yet, but I'm expecting a 90% percentile+ in the Quantitative section...I feel like the only thing I can do is standardized tests. I'll probably look into the Math GRE subject test as well and try to study over the summer..."
Structural equation modelling meltdown,0,1,False,False,False,statistics,1496299038,True,[removed]
Just a three question survey asking about what you went to school for and how you paid for it. Did Uncle Sam fund your Liberal Arts degree?,1,0,False,False,False,statistics,1496317732,False,
Entering a Quantitative Methods-based Master's program. Where do you think this will take me?,11,8,False,False,False,statistics,1496322969,True,"Hi all,

I have a psych undergrad degree and a master's degree in mental health counseling. About a year ago, I realized that I needed to make a major change in my career or I was going to be unhappy for a very long time. I have always been fascinated by the research process and the use of statistical analysis in research, so I decided that I wanted to pursue that as my career. I applied to, and was accepted into an MA program at a state school. The program: Educational Psychology and Quantitative Methods, is intended to folks who want to become education researchers. However, it seems the bulk of the course work is the Quantitative Methods stuff, which I find very exciting. 

Truth be told, I am kind of unclear where I want to go with my life. I know that I want to work with data in some capacity. I find that working with numbers and information is my favorite part of my current job (I work in higher ed). I also very much enjoyed my course work in math and statistics as both an under grad and grad student. I took calculus I and stats for psychology as an undergrad, and I did pretty well without really applying myself. I also took an educational research class as a grad student, but that class was a joke. I love the idea of being a researcher in some capacity, but over the past year, I have become enthralled by the idea of working in statistics. I consider myself to be something of a life-long learner, and I am amazed at just how much there is to learn about statistics. I know that there is something of a demand for people who understand statistics, so I would really like to get a sense for what sorts of opportunities are out there for a degree such as this one. 

These are some of the quantitative methods courses I would be taking:

* Statistical Methods: Inference I
* Statistical Methods: Inference II
* Applied Multivariate Analysis
* Structural Equation Modeling (SEM)
* Analysis of Large-Scale Data Bases
* Hierarchical Linear Modeling: Multilevel and Longitudinal Data Analysis (HLM)
* Applied Regression Analysis

As far as I can tell, all of these courses are offered in the Educational Research context. While I am not opposed to that, I want to know if these skills would be applicable in other settings. If anyone could share their thoughts about what I have written, or about some of these courses and what they could be used for in a future career, it would be very much appreciated. 

I know I probably sound like a complete layman, and truthfully, I am. However, I am certain that this is the direction that I would like to go, and I am hoping to gain some valuable insight from people who are already out there. Thanks!!!!!!!!!

"
Regression with binary response variable,0,1,False,False,False,statistics,1496326718,True,[removed]
Help: Control Variable as Predictor Variable,0,0,False,False,False,statistics,1496329182,True,When control variable work as the predictor and when not? 
Help with linear mixed model (preferably SPSS),0,1,False,False,False,statistics,1496330078,True,[removed]
Propensity Matching,9,15,False,False,False,statistics,1496333719,True,"My company is contractually obligated to evaluate outcomes using Diff-in-Diff based on 1-1 PSM; estimating savings over 1000 bootstrap iterations.

My issue is that I'm having to create propensity scores using only claims data and a few demographic variables (age, gender) - there simply isn't enough discriminating signal to ""predict"" enrollment into our program.  To give an idea of performance, 15% of members enroll - the mean PS for control is .145 and the mean PS for treatment is .165. with scores heavily clustered around the mean for both groups.

My hypothesis is that if my best effort to ""predict"" enrollment is terrible then so to will be my effort to mitigate selection bias.  Is this a fair assumption?

I'm using King's matchIT package in R.  I don't believe there's an option for penalized regression so I used Lasso on a large(er) variable set of reasonable options to choose a more optimal subset of predictors to feed to matchIT.  Anyone have a different take on approach here?

Is my best bet here going to be pushing for exact matching within PSM?  I'm trying to cut down on the likelihood of getting terrible bootstrap draws since PSM just ends up with similarly matched cohorts vs. similarly match cohorts as a result of combining good individual matches.

"
Fundamental of Statistics,0,1,False,False,False,statistics,1496337940,False,
What kind of computer monitor do you have?,0,1,False,False,False,statistics,1496344981,True,[removed]
Converting Shapley Values to Regression Coefficients,2,1,False,False,False,statistics,1496346278,True,"Hi all,

I have a set of data that includes DV: y and IVs: x1-x8. I've run through all the iterations of possible models and calculated the associated Shapley Values according to [Lipovestky,2006](http://digitalcommons.wayne.edu/cgi/viewcontent.cgi?article=1263&context=jmasm) (figure 30). But when moving to convert these values in the estimation of new Betas (figs. 32-34), I fall flat on my face - matrix algebra was never a strong suit of mine. Does anyone know of a way to achieve this in SAS without IML or Excel (perhaps with the use of Solver)? Other methods are also appreciated, of course!

I know this is a pretty niche question but I'm thankful for any feedback!"
Erm.... What is a unit increase?,4,0,False,False,False,statistics,1496348428,True,"I'm sorry that this may be among the dumbest questions you've read in here. On interpreting coefficients in a regression, I know that the coefficient value is the change in the dependent variable when x+1. I now have run a regression with a variable that is public support in % for a certain policy. The possible values are thus bound between 0 and 1 (i.e. 100%). In Stata, some of the observations could look like this

    .3
    .234
    .67263
    .2
    .988

So what IS a unit change here? .00001? i.e. taking the value with the most decimal points? Or does it have to do something with variable type set in Stata or anything? I'm quite embarrassed to ask!"
Principle Component Analysis; what units are the components in?,14,17,False,False,False,statistics,1496362051,True,"Hi,

I've had a professor talking about PCA, and it's ability to reduce multiple measures of something into a smaller number of components. 

She gave the example of measuring the wingspan of a bird, using different kinds of measurements (devices). Imagine rulers, tape measures, laser stuff,.. 

Say you use 10 different devices to measure 1 bird, you end up with 10 measurements, all in centimeter. If you now do PCA on these measurements... What unit will the component(s) be in? Is this even a sensible question?

Does using a covariance or correlation matrix have any influence in interpretation? 

Please excuse the error in my title :p"
bayesian finite mixture,1,2,False,False,False,statistics,1496370373,True,"I'm reading BDA 3 and I'm having a tough time on Bayesian Finite Mixture. 

I was curious if anybody know of a different resource or book for explaining bayesian finite mixture?

Also I'm going back and forth, in term of reading, between BDA3 and Kruschke's Doing BDA puppy book.

Thanks"
Why third moment measure the skewness of a distribution?,3,1,False,False,False,statistics,1496371262,True,"I am studying the first course on prabability. My professor explaned to us intuition behind the third moment. However, i want to know why. I have searched on the internet  but they seem not to explain why, just talk about the meaning of third moment"
In what situations would one use Approximate Bayesian Computation instead of Bayesian inference?,1,6,False,False,False,statistics,1496375895,True,I'm not sure why one would use ABC instead of standard Bayesian methods. Any concrete examples? Perhaps I'm having conceptual problems? 
Misleading Statistics,0,1,False,False,False,statistics,1496385866,False,
Why we should love null results,2,2,False,False,False,statistics,1496390171,False,
The concept of Independent Events explained.,1,7,False,False,False,statistics,1496399642,False,[deleted]
Repeated measures ANCOVA - how to deal with RM covariates,3,8,False,False,False,statistics,1496416670,True,"I'm looking to run a RM ANOVA to investigate changes in cortisol levels at 2 time points. I'm planning to include variables such as age and sex as covariates, but how do I hand covariates that themselves are repeated (e.g., sleep quality at time 1 and sleep quality at time 2). The quality of sleep could affect cortisol the next day, but unsure how to include this variable as a covariate. Would I simply calculate a difference score?"
Is the expected maximum higher with more choices?,5,1,False,False,False,statistics,1496422225,True,"Am I justified in making the statement that given a choice of x random numbers from a distribution with the same mean and standard deviation and a choice of y random numbers from the same distribution, where x>y, the expected highest number out of the x choices is higher than the expected highest number out of the y choices?  If so, how do I say this formally?  
"
What kind of ANOVA to use if I was unable to start all factors at the same time.,0,4,False,False,False,statistics,1496426028,True,"I am running a two factor experiment that has exposure to 4 treatments at 4 time frames(not repeated measure). I wasn't able to start all my time frames simultaneously. I was able to start all level of treatments for 96h, 72h, and 48, on the first day and had to wait until day two to start my 24h treatments. What would be the correct methods for analyzing this data? I had a small replicate number (n=5). Can I just run a two-way ANOVA since the 24h exposure still overlapped with the other exposures times or should I use some sort of blocking?
"
"What is the distinction between ""empirical null"" and ""theoretical null"" in the context of multiple testing?",0,0,False,False,False,statistics,1496431371,True,[deleted]
Help an engineering undergrad choose the right MSc Program. (Please),0,2,False,False,False,statistics,1496437477,True,"I'm an agronomic engineering undergrad student and I have been studying some basic statistics for about 2 years now. Problem is, I graduate in December and do not know what MSc program to choose. My current math/stats background is as follows:

-- Calculus, but no Real Analysis or Adv. Calculus;
-- Matrix Algebra and almost done self-teaching Linear Algebra;
-- undergrad Statistics and MSc level Probability Theory;
-- Experimental Statistics (applied to agronomy); and
-- working knowledge of the R language.

At my university, the MS in Statistics degree is heavily focused on the Design and Analysis of Experiments. The other two programs I'm interested at the moment are a lot more theoretical. I must say I have always loved the theoretical development of science in general, but I am unsure if I would be able to catch up to math and statistics majors on the theoretical matters of a Dissertation or even a PhD Thesis in the future.  At the same time, I'm deeply afraid that too much applied stats will eventually making me regret never having learned and worked on the abstract topics of statistics and probability.
Finally, my questions are:

1) Is it possible for me to enroll in an Applied Statistics program and still have enough theoretical research to keep me busy for the rest of my life?
2) If I decide to pursue a theoretical program, what advice would you give me? (courses to take, hopes, mentality etc.)

Thank you all very much! o/"
"If A is 60% to win a frame of snooker, what probability he win 4 out of the next 9 frames?",5,0,False,False,False,statistics,1496439069,True,"Hi - I'm not looking for an answer to this specific question, but for the formula for questions of this nature. I saw it somewhere before, it was first discovered by a famous mathematician of yesteryear (someone like Fermat or Pascal). Now I can't find it. Thanks!!"
Logistic regression in Stata,6,7,False,False,False,statistics,1496441031,True,"I'm working on a study focusing on a binary variable that I'll call ""health"" for simplicity's sake. The two conditions of this variable are poor health and good health. I'm looking to examine the associations of various variables with poor health using logistic regressions in Stata.  Since I'm looking at the predictors of poor health, would poor health be coded as 1 and good health coded as 0? 

I know this is a really simple question but I don't really know what I'm doing. Thanks in advance!"
How can you tell the standard deviation of a data set by looking at a histogram?,5,5,False,False,False,statistics,1496443342,False,
How to determine confidence interval in excel. What equations do I plug in?,0,1,False,False,False,statistics,1496443716,True,[removed]
I think this problem is bugged,1,1,False,False,False,statistics,1496445837,True,[deleted]
How much larger is mathematics than statistics as a field?,0,1,False,False,False,statistics,1496456277,True,[removed]
Question about curve fitting/trend analysis,3,5,False,False,False,statistics,1496458365,True,"I have a time series data set of a group of subjects responding to a task through time (so a #subjects x #time points matrix). There's an interesting shape/trend along time. I would like to quantify this shape and its significance, but I'm not exactly sure how?

From what I understand, you can do a repeated measures anova, and/or a polynomial regression. In the repeated measures anova, I get a significant F statistics, which indicates that some time points are significantly higher than other time points. But how do I quantify exactly which time points are significantly higher than other time points? Does a usual post hoc test for one way anova work here?

Secondly, I also fitted my time series data to a polynomial (i.e. y = b0 + b1*x + b2*x^2 ...) And the cubic polynomial showed significant parameters (i.e. b0, b1, b2). Does that mean that the shape of the time series is cubic?

Thanks in advance!"
Need help regarding combining variables with different scales,0,1,False,False,False,statistics,1496478359,True,[removed]
"Do regression coefficients have one definite meaning when treatments are each a combination of 2 treatments among A, B and C?",0,1,False,False,False,statistics,1496493889,True,[removed]
Re-Learning Stats: Where Should I Start?,13,35,False,False,False,statistics,1496495365,True,"I'm in a unique position.  I've completed a Ph.D. in Higher Ed, and finished numerous applied stats classes, such as those that would be offered in social sciences.  I did very well on all of those classes, but the interesting thing was that many of them were taught with an emphasis on formulas and the underlying mathematics behind stats.

Professors would say things like ""oh, yeah, you'd totally use SPSS for this, but I want you to work through the data set by hand so you learn the way the math works.""

Ironically, the previous stats course I had was almost 15 years earlier.

That said, I never quite learned, I think, about exactly WHEN to use particular stats.  So, for example, if a person says, here's what we want to find out, what process would we use?  I find myself completely befuddled.

Are there any books/lectures/primers, etc. out there that can help me fill in this gap in my knowledge?  Many of my texts were either too basic or focused too heavily on mathematics to give me the practicality of what I was learning..."
Probability help!! Thanks!!,0,1,False,False,False,statistics,1496511562,True,[removed]
Matlab vs R vs Python,72,24,False,False,False,statistics,1496526756,True,"I'm graduating in Economics. Till now I have the knowledge of SPSS and STATA, however to increase my value as a potential job applicant I would like to learn a couple of more programming languages or extra skills. 

I'm very new to statistics software and am wondering what the main differences (advantages/disadvantages) between Python, R, and Matlab are so I can choose the most appropriate platform to use. Does anybody have advice on these 3 platforms?

Additional info: I got Matlab for free from my university (because I am still a student)."
Jobs for Applied Stat Grad Certs?,15,2,False,False,False,statistics,1496527448,True,"I was wondering if I could get some advice from whoever is willing to help. I'm 29 years old seeking a career change, I'm a Police Officer, I have a degree in Criminal Justice, and I just started my first class in pursuit of a Grad Cert in Applied Statistics. I do intend to eventually finish up with a Master's in Applied Statistics, but my question is about the certificate.

Would someone like me be able to find a job with literally no experience other than a graduate certificate, or will I most likely have to wait until I go all the way through and complete the Master's program?

If a grad cert can get my foot in the door, any suggestions on where to start? Internships, part-time work, etc. Any help would be appreciated, Thanks!

Edit: BTW, I am willing to work in any capacity, I just want to get started at somewhere, my long term goal is unlikely and considered by most just a hobby."
Graduate stats and medical technology,4,6,False,False,False,statistics,1496533148,True,"I've recently posted on this subreddit, but have another career-related question. I've been interested in pursuing an MD/PhD in biostatistics, and delving deeper into clinical trials. However, a big goal of mine is to use my analytical knowledge and understanding of medicine to create a health technology business or clinical trials organization of my own. As an undergraduate and a first-generation URM student, it's been hard finding direction with my goals, as the advisors at my school don't know much and are hard to reach. I have competitive grades and I've been enjoying the research I've been doing (epidemiology and longitudinal data analysis), but have no real understanding of the academic pathway. 

My question is whether or not this is a logical plan and what schools/what kind of research I should understand to gain the most. There's a lot of research being done in sensor technology, and I think it'd be cool to see how that interrelates with big data analytics, but that often seems to fall under bioengineering. Is it possible to be interdisciplinary in a field like this? Any advice will be greatly appreciated!"
Can someone tell me whether I'm using the generalized linear mixed model correctly?,0,1,False,False,False,statistics,1496543200,True,"A bit silly maybe, but I would like to know whether I'm doing things completely wrong or not. 

I've got a repeated measures factor (4 levels) and a between subjects factor (3 levels). My dependent variable is a count, and the number of observations is variable between measurements (e.g. behavior x happens 10 out of 50 observations). I use the count as my ""target"" and use the total observations as the denominator. I use the binary probit link, as that yields a better model (lower information criterion (approximately 280 for the Aikake Corrected IC). My fixed effects are the group variable and the day variable. For my build options, I'm using varied degrees of freedom across tests (Satterthwaite approximation) as my design is unbalanced (one of the 3 groups is approximately twice the size of the others). For the test of fixed effects and coefficients, I've selected the option for robust estimation, as I'd prefer to be on the conservative side with this test. Lastly, I opt in to the pairwise estimated mean contrasts, as this is pretty much what I really want to know; are there differences between the groups at different time points. 

The results show high accuracy in the model summary, and significant fixed effects. 

Is this a reasonable approach? Am I missing something important? Cheers."
Are The U.K. Polls Skewed?,0,5,False,False,False,statistics,1496543410,False,
Statistics,0,1,False,False,False,statistics,1496599986,True,[removed]
Is there any justification to treat a sum of ordinal scores as a ratio score?,0,1,False,False,False,statistics,1496609614,True,[removed]
Is invariance test (Multigroup Analysis in SmartPLS) and ANOVA the same thing?,0,3,False,False,False,statistics,1496611680,True,"I have 3 PLS models (SEM) of the same 8 latent variables for 3 different grocery chains. The aim is to compare these models. Thus, I also have 3 different groups (the customers whom prefers one of these chains). My understanding is that an invariance test shows if the paths (constructs) between the latent variables (i.e. price--image) are being measured the same way across the chains?

Additionally, ANOVA will show if the concepts have equal or unequal variance (i.e. is the price concept the same for all the 3 chains, or are they different?). 

If I am correct, isn't this just the same test? Is an invariance test = ANOVA test? Thanks!

"
Should mathematicians teach statistics?,0,1,False,False,False,statistics,1496617517,False,[deleted]
MANCOVA vs ANCOVA?,2,1,False,False,False,statistics,1496620562,True,[removed]
[X-post r/askscience] Can academic papers performing statistical analysis with small samples still be of value?,8,14,False,False,False,statistics,1496622792,False,
I need to predict price using all categorical variables,4,3,False,False,False,statistics,1496633917,True,"I have less than 10 variables that I need to use to predict a price. The categorical variables have thousands of levels. One has about 4K levels. I'm not sure how to tackle this problem so if some could link me to some good reads or give me some advice, that'd be great! I plan on using R to tackle this. "
Lecture notes/tutorial on Bayesian hierarchical modeling/regression?,2,6,False,False,False,statistics,1496637606,True,"I am applying a Bayesian hierarchical model to some data. I need to convince my fellow researcher colleagues (who are not entirely ignorant of statistics) that this is the correct approach.
Besides recommending reading Gelman & Hill, are there resources I could offer which concisely explain how hierarchical regression works, why one would use it, etc.?

Thanks for any recommendations"
"SOME TERRIFYING FACTS About Debt On Pakistan ""PAKISTAN PAYS 52,314 PKR EVERY SECOND INTEREST ON DEBTS.""",0,0,False,False,False,statistics,1496647274,False,[deleted]
Get the Best Biostatistics Assignment Anytime You Need It,0,1,False,False,False,statistics,1496656415,False,
Pert Distribution,3,5,False,False,False,statistics,1496657680,True,"Hi everyone,

If there is someone who could help me use Pert in Excel with a twist. I know Pert uses a min, most likely, and a max, for example[(=min + 4most likely + max)/6]; but I want to change this setting to use 5 values; let's say [(a + 2b + 4c + 2d +e)/10].

The reason I want to do this is because I want to alter this formula for different scenarios within the same project. Sometimes I want to change the formula to say [(5a + 2b + c + d + e)/10].

My reasoning is that changing to for example this last formula is that the probability of scenario A to occur is 5 times the probability/weight of the other scenarios. If this principle/reasoning is wrong, could you please explain why?

Thanks to all you redditors!"
statistics finals tomorrow. i am dead.,1,1,False,False,False,statistics,1496662125,True,[removed]
ELI5: Test Statistics,0,1,False,False,False,statistics,1496663952,True,[removed]
ELI5: Test statistics,1,2,False,False,False,statistics,1496664580,True,"I'm getting back to statistics on Andy Field's manual, since italian school lacks practical use of it and I forgot all I studied two years ago. In fact I'll study psychology at Leiden University and they'll test my ability to attend this master (this is to say that if you also have specific information about their method, I'll be really thankful if you'd share it). But my question is about the confusing stuff I'm studying. It was all clear until the book started talking about test statistics, from that point I blanked out. Got an easy way to put it? I'm pretty frustrated! "
What do you guys think of this debts pay off stats?,1,0,False,False,False,statistics,1496677891,False,[deleted]
Four characterizations of the normal distribution,3,15,False,False,False,statistics,1496679340,False,
How to determine probability of something happening on one try if you know the probability on multiple tries,8,3,False,False,False,statistics,1496681885,True,"So I've thoroughly confused myself here and there is probably an easy answer that I'm missing.

My question is, if you know the probability of something happening over 4 tries, can you use that to find the probability of it happening in a single try.

For example, you have a black box that when you press a button it gives generates four colored lights, each light can be one of four colors call them grey, blue purple and orange, but the colors don't appear with the same frequency.

After testing you find that, on average, 7.4% of the time you press the button you get *at least one* orange light.

What would the probability, per button press, of getting an orange result if you covered up 3 of the four lights?"
Comparing 3 different scores,4,3,False,False,False,statistics,1496692371,True,"I have a cross-sectional study and I am trying to think about what can I do with the data. Participants took 3 questionnaires and each of these questionnaire computes a health status score (high scores=better health). Some of the scores ranges from -1 (bad health state) to 1 (good health state) and some scores range from 0 (poor health state) to 100 (good health state)

One of the analysis I am looking as predictors that influences these scores using linear regression models. But I assume in order to compare the 3 linear regression models (one for each score), I would have to standardize my variables and scores right? (e.g do a Z-transformation on my predictor and response variables and run linear regression using the Z-transformed variables?)"
Arrhenius HTOL Model,1,2,False,False,False,statistics,1496703662,True,"Can anyone describe to me, in really basic terms, what the Arrhenius HTOL Model is and what exactly it does? From what I gathered, it's a way to estimate the failure rate of a component at its end of life. But I could use a little more description."
How to determine if d20 is fair or biased - using stata14?,25,11,False,False,False,statistics,1496714691,True,"Hi! this is my first post on this sub. 

I have some d20 i want to examine and have accurate response whether they are fair or not.
some options:

1) using a high sensitivity caliper to determine if sizes are all the same and there is no shaving.

2) using a glass of water and salt to make the die float on it and determine whether or not it has a evenly distributed mass.

3) using statistics to prove it.

The first 2 options are interesting, but 1: i don't own a caliper.
2: i am using dices that are too heavy, i can't make them float in salty water, no matter how hard i try. (besides: one of these have inner electronic parts - i am concerned water may enter the die and destroy its electronic components.)


and so i tried to go for statistics.
So let's try this way:
Ho: One of the sides has increased chance of being on top. (die is biased)
Ha: All sides have equal chance of being on top (die is fair)


In a d20, as in any other dice, I understand each side represents one categorical variable. and i want to count the prevalence of each side.
I read somewhere that to do this, i should roll the die a total of 5 times the number of sides (Sorry - i cannot tell you where this data came from! it may be just a random assumption). that would make something around 100 rolls (but this is not a problem! i have the die with me, i can roll it further and add to the data!). 
after i got the data set, i could simply analyse the data with the summarize var1, detail (this would give me the mean, sd, and quartiles values. ) and maybe a histogram of the data set would be helpful (histogram var1, d).
so far, it went ok. but i wanted to have a correlation proved or rejected by a statistical test. My readings of statistics showed me that i should either use Fisher's exact test, or Pearson's chi squared test for this kind of variable.
therefore, i should be able to build a contingency table, and then find a X2 value, that i would compare with a table of critical values of Chi squared, and determine the chances of rejecting or not the null hypothesis.
that's when things got complicated.
i managed to do the calculations manually with excel. 
i used this wikipedia article to help me: https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test#Fairness_of_dice
manually i could calculate a value.

ok... but i am using Stata for a course in clinical research I'm currently enrolled into, so i decided i wanted to do the same calculations using stata. the issue arrises when i try to tabulate the data! how should i do it? i can't simply go like: tabulate var1, chi2 -- it simply doesn't work. do you guys know how should i do it? 
i used var1 here, but if you want to have some sample of 100 random rolls of a d20, you can simply try this link: https://www.random.org/integers/?num=100&min=1&max=20&col=1&base=10&format=html&rnd=new
some help with this topic would help a lot my understanding of statistics! thanks a lot in advance for your contributions. cheers!

EDIT: i just formated the text a little bit and corrected some little details. Thanks for your wonderful feedback so far!

EDIT2: this seems to be the answer to my question: (thanks a lot /u/ivansml) 
>According to [these notes](https://www3.nd.edu/~rwilliam/stats1/Categorical-Stata.pdf), you can use chitest command from tab_chi package by N. Cox (install with 
    ssc install tab_chi
).
"
please help,0,1,False,False,False,statistics,1496721824,True,[removed]
How to combine and divide data points with margins of error?,3,1,False,False,False,statistics,1496726742,True,"Hey there,

I'm researching a foreign group using census data.  I have two questions on multiple data points.

1. How do you combine data points?  For example, lets say a group has 15.0+/-0.1% of its members with a bachelor's degree and 5.0+/-0.1% with professional degrees.  How would you combine statistics to get the percentage with a bachelor's degree or higher, with a margin of error?  Is it combined (20.0+/-.2%?)

2. How do you divide data points?  For example, lets say 50,000+/-4,000 people are naturalized citizens out of 90,000+/-5,000 people in an immigrant group.  How would you divide these to acquire a percentage of the group that are naturalized with a margin of error?  My instinct is to say its 55.5% and get a margin of error by averaging the largest and smallest possible values for each point (|.555...-(54000/85000)|=8.0%, |555...-(46000/95000)|==7.1%, avg = 7.6 <- margin), but that seems wrong as can be.

This is all 90% Confidence interval data.  Thank you so much."
Your Negotiation Style Survey,0,1,False,False,False,statistics,1496727029,False,[deleted]
[Academic] Your negotiation Survey ( American Respondents only please),0,1,False,False,False,statistics,1496727665,True,[removed]
Applications of Random Matrix Theory in statistics,2,2,False,False,False,statistics,1496730511,True,"Hello, I was wondering if random matrix theory is of any use in statistics. I'm interested in both theoretical and practical usage, though I think the latter would be a bit more interesting. Any resources (books/articles) would be welcomed. Thank you!"
Applying Z-score to non-normal distribution,0,1,False,False,False,statistics,1496737494,True,[removed]
"Mutually Exclusive Events and ""Or""",5,1,False,False,False,statistics,1496740955,True,"I've been wondering about this particular instance:

Say there's a video game. By killing a boss, it has a 10% chance to drop some piece of loot X. And a character can kill this boss once a day. Since each character is different/""unique,"" one character killing the boss shouldn't affect the drop chance for other characters (mutual exclusivity). If I have 10 characters, and kill the boss on all 10, wouldn't I technically have 10*10 = 100% chance of getting said loot, since it's ""item drops on Char. 1 OR Char.2 OR... Char 10."" But, isn't there still the possibility of the item not dropping at all? Where in my logic/thinking did I go wrong? Thank you."
Can R^2 and SSE values be biased towards smaller data sets?,0,1,False,False,False,statistics,1496744955,True,[removed]
Identifying areas of improvement using software such as Minitab or JMP,4,3,False,False,False,statistics,1496745628,True,"So i'm a current intern working on a project to identify areas and ways at which we can improve our estimating abilities. What are some good statistical methods to do this? I have spreadsheets of the different areas we estimated, person who estimated, years, etc. but I really don't have much of a background in statistics. The software I have available is JMP"
BRIEF-CIFI Holdings Group updates on operating statistics for May,0,3,False,False,False,statistics,1496745712,False,
- The Boston Globe,1,0,False,False,False,statistics,1496745770,False,
Can I regress a z-score as a dependent variable?,3,12,False,False,False,statistics,1496751318,True,"Essentially I'm looking to see a slope change at a certain z score. If it gets large enough, does the slope change to a negative.

I'm wanting to test mean reversion properties of a certain series based on how far it is away from another series. Is this possible with regression?

Thanks"
Interview help- clinical trials statistician,3,6,False,False,False,statistics,1496760025,True,"I have an upcoming interview for a master's level Statistician position at a large organization for cancer research. I just graduated with an MS in Applied Statistics and have never had an interview for a statistician/data analyst job, so I am quite anxious thinking about what they could possibly drill me on. 

During the initial phone screening they asked me about any work I've done with clinical trials as well as my experience with SAS and R, so I am anticipating questions about those.

I read on glassdoor that in a previous interview someone was asked about LASSO, random forests, and sufficient statistics. I know random forests inside and out, I know the gist of LASSO, and I could give the definition of a sufficient statistic, but I shoved most of the theoretical stats stuff to the back of my head when I was done with the theoretical courses. 

The reason I bring this up is because I'm wondering if there are algorithms and concepts I should just ""know"" before an interview, because I would have frozen up if the aforementioned interview questions were asked. I'd also like to know if anyone who works in a medical research/clinical trials position has any general advice for me. 

"
Interesting Data-Driven News Site. They have an interactive visual search which is quite interesting.,6,0,False,False,False,statistics,1496770526,False,
"Noob Here, Need Help With SPSS Analysis!",0,1,False,False,False,statistics,1496770857,True,[removed]
"To me, results of these 2 scenarios should have same odds (colleagues disagree). Currently melting my brain. HELP!",11,10,False,False,False,statistics,1496778182,True,"I’m trying to determine the odds of selecting sweepstakes winners via 2 different processes.  Process A results in a lot more work than Process B, but my colleagues believe the extra work will result in better odds of winning for customers.  Our goal is to pick the process that yields the best odds of customers winning.  Examples:

PROCESS A (validating customers BEFORE drawing):
10,000 Entries are received in a sweepstakes, (8,000 from customers, 2,000 from non-customers)
The 8,000 customer entries are validated and only 5,000 are found to be valid (i.e., 3,000 invalid)
The drawing is then conducted from among the 7,000 valid entries (2,000 non-customers + 5,000 valid customers) to pick 10 winners. 

PROCESS B (validating customers AFTER drawing)
10,000 Entries are received in a sweepstakes, (8,000 from customers, 2,000 from non-customers)
A drawing is conducted from among all 10,000 entries to pick 10 potential winners.  All of those 10 potential winners that are customers are then validated.  Similar to Process A, **IF** all 8,000 customers were to be validated, 3,000 would have been invalid.  

Which process would maximize the odds of customers being among the 10 winners?  My gut tells me the odds should be the same.
"
Summing Ranks from Different Variables,0,2,False,False,False,statistics,1496789818,True,"I have an example similar to the example in this link : https://en.wikipedia.org/wiki/Weighted_sum_model

I have a n x 3 data set (rows x columns). Each row contains information about a school. There are 3 columns, each column is a variable that has ordinal ranks (1,2,3,4.....n; 1 =best and n=worst)....the ranks are unique and do not repeat. It is also roughly known the weight for each variable (e.g. 0.25, 0.5, 0.25). Does anyone know how to calculate a summed rank for each row? Thank you."
Applying optimal stopping to wedding dress shopping.,2,11,False,False,False,statistics,1496790095,False,
(how to measure) learning bounds of a model,0,8,False,False,False,statistics,1496792742,False,
Research Question,0,1,False,False,False,statistics,1496800697,True,[removed]
Company Contact Numbers At Your Fingertips With Flaptor - Flaptor,0,1,False,False,False,statistics,1496801948,False,
Economicshelpdesk.com Hiring Online Statistics Tutor Service,0,1,False,False,False,statistics,1496820617,False,
best online masters degree in data analysis?,3,1,False,False,False,statistics,1496827547,True,"I'd like to pursue an M.S. in Data Analytics to improve my skills in statistics, Data Analysis and Statistical Software. 

Here's why I want to do it:
-Enjoyed statistics during undergrad
-I'm working overseas with a social enterprise that support farmers. My contributions/impact would be leaps and bounds higher if I was more competent at monitoring and evaluation.

Here's why I think it will help me in the future:
-High demand for M&E in development projects. 
-Could lead to interesting opportunities in for-profit sector (taking advantage of rapid mobile data that is becoming available and using it to learn about new populations which have traditionally been difficult to study)

I know there are a lot of free online courses (currently enrolled in a Stanford Course on Data Analysis), but I would like to pursue a degree. 

Any suggestions? "
Extracting mathematical probabilities from actual results with given side-conditions.. I think I need a hint.,14,7,False,False,False,statistics,1496837971,True,"Hello everyone, 

I hope this thread meets the requirements, I just found this subreddit. I came across a problem I couldn't really solve today, and nothing I could remember in my statistics courses (a while ago) gave me any idea how to continue. 

So I was working on a simulator for Hearthstone (a digital card game) pack openings. HS has 4 rarities: Common, Rare, Epic, Legendary. A pack contains 5 cards and is guaranteed to have at least one rare or better card. To make sure people don't go on horrible streaks of bad luck, you'll get a guaranteed epic every 10 packs and a guaranteed legendary every 40 packs. (""Pity timers"". So if you open 39 packs without a legendary, the next one will 100% contain one.)

Now, for the simulation, I initially used the probabilities that showed up in two recent mass openings, ~2500 packs total, that should be good enough for my purposes. But now here's the problem, when (for example) 70%, 25%, 4%, 1% of all opened cards in the reference sample were commons, rares etc. and I just use these probabilities (+ pity timer) for determining 5 randomly generated cards while afterwards making sure the whole 5-card pack contains at least a rare (if not, reroll whole pack), my distribution will obviously be skewed due to said ""pity timers"" plus the ""contains at least one rare or better"" condition.

So my model/probabilities will need to be adjusted, but what is the feasible way to approach that? How can I extract the percentages for determining a single card's rarity from the actual results with the given side-conditions? Or should I just revamp the whole pack generation process completely (somehow)?

Any help would be appreciated. In any case, thanks for reading."
Can I apply resampling to my imaging data to create independent-ish samples?,7,10,False,False,False,statistics,1496846848,True,"Hello! I work with an imaging modality where a machine measures the number of counts that each one of many sensors record during the acquisition period. It's a process that it's as Poisson as it could. From these data an algorithm reconstructs an image.

Now, I need to ""simulate"" the acquisition of a large number of such images (to study how the noise proprieties of the images impact the analysis that is performed on them).

I need to simulate acquisitions 30 s long. What I was thinking was to acquire data for 1 hour. And then I'd take the stream of counting events and split it up in, let's say, 100 ms intervals. Than pick up 300 random intervals (100 ms long) to simulate a 30 s acquisition. There are bilions of bilions of possible combinations. So I could possibly generate many bilions of ""independent"" acquisitions.

Is my way of reasoning correct? Are there any cons in this way of doing? Am I risking some error? Is there any potential problem with how long I make the intervals compared to the scan duration (i.e.: any difference in taking 300 intervals 100 ms long vs 3,000 10 ms long?)"
Use of ANOVA to analyze percent correct data,0,1,False,False,False,statistics,1496847474,True,[removed]
Why does the Yale conditional probability formula differ from others?,12,3,False,False,False,statistics,1496852023,True,"The conditional probability formula described on the Yale web site differs from that of every book and web site I've checked. Below is an excerpt from Yale:


""Suppose an individual applying to a college determines that he has an 80% chance of being accepted, and he knows that dormitory housing will only be provided for 60% of all of the accepted students. The chance of the student being accepted and receiving dormitory housing is defined by
P(Accepted and Dormitory Housing) = P(Dormitory Housing|Accepted)P(Accepted) = (0.60)*(0.80) = 0.48.""

Yet all other sources on conditional probability state that the answer should be .75 not .48

It seems that they mistakenly wrote the formula for A GIVEN B...

P(Dormitory Housing|Accepted)


When it should be P(A*B)...

No?

Link to site:

http://www.stat.yale.edu/Courses/1997-98/101/condprob.htm"
Please help with a survey for my statistics class,2,0,False,False,False,statistics,1496853378,True,[deleted]
Can someone explain to me the difference between a statistician and a data scientist?,27,19,False,False,False,statistics,1496856430,True,"Hi all. I am entering a quantitative methods-based masters program soon. I am very interested in research and statistics, but I have yet to narrow down what my focus will be, and more so, what type of work I would like to do. I have been reading as much as I can about various jobs. I see a lot of terms that I assume mean the same thing, but the more I read on this sub, the more I begin to feel that I am mistaken. 

If you would be so kind, could you please explain to me the difference between a statistician and a data scientist? Thanks!"
"New to this, can someone help me with understanding MANOVA/ANOVA?",3,12,False,False,False,statistics,1496857468,True,"Thanks in advance to anyone who can help me understand my assignment. I am writing about a hypothetical data set so I don't have any real data to experiment with and I just need to be able to explain how I could analyse this type of data . Basically it's a clinical trial, three treatment groups, assessed at 4 time points with 3 dependent variables. I understand that I would start with a MANOVA, if there were sig. differences I would use an ANOVA for each DV, then within each condition I could run paired-sample t-test to determine which time spans had the greatest difference but I've confused myself as to which test would determine which treatment is most effective... Any guidance is so appreciated. Thanks all!"
Where can I find the ebook of Levine's Business Statistics,1,1,False,False,False,statistics,1496857703,True,"https://www.amazon.com/Business-Statistics-First-Course-7th/dp/032197901X

Searched in library genesis and few other websites. I couldn't find the book. Edition doesn't matter if its 5 or above.
"
"I want to do summary statistics on a small data set, looking for resources",7,7,False,False,False,statistics,1496860258,True,"I don't have much stats knowledge;

I have a data set of 22 individuals and some metrics pertaining to each individual. I've been asked to graph all the metrics separately and come up with some normalized ""score"" based on them.

I'm wondering how I go about doing this. I have read that a two sample t test is appropriate for small sets but I don't see how it applies. I have essentially a collection of 22 name/number pairs. I've calculated the standard deviation for each set but graphing it seems pointless since there is no repetition. Any help is appreciated, really just looking for resources on methods I can use (that a semi-layman can understand, I'm definitely willing to get my hands dirty and read some technical information).

e: if I subtract the mean from a number in the data set and divide by the standard deviation (essentially how many standard deviations from the mean) is this number valuable at all? Or am I just doing useless math?"
Why use conditional probability when the probability is the same?,10,0,False,False,False,statistics,1496860895,True,"What is the point of the A|B (A given B) calculation if the probability just comes out the same as the individual event?

Example:

Event A is acceptance rate at a University, which is .6

Even B is graduation rate given you were accepted; said graduation rate is .4

I want to know the chances of of graduation given your acceptance into the school.

So P(A and B) / P(B) = .24/.6 = .4

.4 is the known graduation rate without factoring in acceptance rate into the University. What is the point of even calculating conditional probability in such a case?

Seems like it would make more sense to just calculate the probability of A AND B, which is .24, so you have a .24 chance of getting accepted AND graduating. Or am I looking at the problem wrong?


"
Recruiting help for DJ Patil project,0,1,False,False,False,statistics,1496862032,True,[removed]
Interpreting first differences on logged variables?,1,2,False,False,False,statistics,1496877096,True,"Hello, after logging my time series variables, I then differenced them to make them stationary for OLS.

Is the interpretation the same as a regular logged variable? Could not find a direct answer using google."
How competitive am I for statistics graduate school? What schools do you recommend I apply to? I want to get into the best school possible,0,1,False,False,False,statistics,1496883348,True,[removed]
statistics question,2,0,False,False,False,statistics,1496885312,True,[deleted]
Hypothesis testing of linear regression model with two variables (including a slope dummy variable),5,4,False,False,False,statistics,1496888759,True,">A researcher wants to explore how people perceive distances in comparison to the actual distance, and see how the presence of contact lenses/no contact lenses affected their performances. The following model was created, where z=1 with contact lenses and z=0 without:

>ŷ = [1.05 + (z)(0.21)] * x

>The standard errors for the estimated coefficients are 0.357 and 0.032, respectively.

>Using this model, test the hypothesis that subjects who wear contact lenses would overestimate the distance between the objects. 

I know I have to test for null hypothesis: slope = 1 and alternate hypothesis: slope < 1, but I don't know how to set this up with a variable and a slope dummy variable as well, and also take into account the standard errors of both of the coefficients. Thanks in advance for any help!"
Reoccurring Probability of Outcome in time series,0,1,False,False,False,statistics,1496889467,True,[removed]
What should the value of a dependent variable be?,8,7,False,False,False,statistics,1496907914,True,"Hi all,

I'm currently conducting regression analysis and I have all of my independent variables. I'm currently transforming all my data from Excel to my Statistics software (Gretl). I know that a dependent variable is explained by the independent variables, but I have to give it a value and I'm not quite sure what the value should be?"
"Can you transform part of a dataset but not others, or use seperate transformations?",12,4,False,False,False,statistics,1496908089,True,"My question is essentially the title, but i'll elaborate in case it helps:

I have a data set including three variables and two factors (I think) - plant cover, occurrence and number of species all split by exotic/native across a bunch of transects. I just can't get all of the exotic variables to be normally distributed with any transformation. I can get one or two normal-ish with some transformations, but there's always one that won't cooperate. The native variables are fine with any or no transformation. I couldn't get a clear answer on this from teaching staff, and I just don't want to use non-parametric tests.

Is it valid to do, say, a log transformation on exotic cover, a cube transformation on the exotic species/occurrence and leave the three native variables untransformed (or do one of those two to simplify)? I feel like this is a no, but I'm curious."
interpreting average partial effects (marginal effects) in ordered and multinomial logit models,1,3,False,False,False,statistics,1496918593,True,"MLOGIT:


The way I am interpreting the mlogit is that for a 1 unit increase in the price variable the individual is 0.003 percentage points less likely to be in category 4 (P=0.0960), and 0.002 percentage points more likely to be in category 1 (P= 0.045).  



Similar logic follows for the OLOGIT: for a 1 unit increase in the currency the individual is 0.003 percentage points less likely to be in category 4 (P=0.029), while it is 0.001 percentage points more likely of being in any of the three categories (P=0.026 to 0.033)


Can you confirm that this is correct?  I'm slightly worried that this isn't percentage points, and just percent.


Code in stata:
mlogit category determinants....   , cluster(hhid) baseoutcome(1)
margins, dydx(*) post 







Formatting is terrible, but here is a few lines of the output:


MLOGIT:

price (t-1, real 2010 Ksh/kg).       APE.       Sig.       P-val.

category 1                                  0.002.     **         0.045

category 2                                -0.001                   0.114

category 3                                 0.003                   0.144

category 4                                -0.003       *          0.096


OLOGIT:
 
price (t-1, real 2010 Ksh/kg).       APE.       Sig.       P-val.

category 1                                  0.001.      **.       0.033

category 2                                  0.001.      **        0.030

category 3.                                 0.001.      **        0.026

category 4                                 -0.003       **.       0.029
"
TMX Group Equity Financing Statistics - May 2017,0,4,False,False,False,statistics,1496926602,False,
Questions on Confidence and Risk,4,4,False,False,False,statistics,1496928868,True,"Here's a hypothetical situation a colleague and I were discussing, and I'd like your opinion(s):

A ""good"" sample (sufficient for the half-width desired from a relatively stable process) is taken, and sample mean & sample standard deviation computed.  Initially, a 98% confidence interval for both is computed as well.  Here begins the discussion:  

Since a confidence interval expresses a probability that repeated executions of that sample would encompass the true population parameter, a 98% CI represents a 2% risk that any sample actually taken did not encompass the true parameter (µ, σ).

*Q1*) For mound-shaped (normal or very near, AD normality test p-value of no worse than .05 let's say) data, is that CI risk *equally distributed on both sides of the CI interval*, i.e. a 2% risk is approximated equally distributed as marginal risks of (a) 1% chance of the true parameter being below your computed CI and (b) 1% chance of the true parameter being above your computed CI?  I would think it would be, but I've never analyzed it.

Continuing the discussion:  
For considering the case that *both* the CI of the mean and the CI of the standard deviation are wrong, and both are wrong on the *low side*, so (b) of Q1 above for both statistics (call this b1 and b2), leads to Q2 and Q3:

*Q2*) Would the chance of that event, i.e. the marginal probability of having underestimated **µ** via **x-bar** and *simultaneously* having underestimated **σ** with **s**, be p(b1)\*p(b2)?  
*Q3*) Are those exclusive events?  The estimations are possibly not independent, since **s** is computed via **x-bar**, even using unbiased estimators, so is that treatment of these probabilities of occurrence a violation of the assumptions of independence?

Finishing the discussion:  
Assuming that the probabilities of *underestimating* both parameters simultaneously can be treated as independent events, then it follows that the singular risk of *underestimating* one parameter via a 98% confidence interval is the same as the combined risk of *underestimating both parameters* via an 80% confidence interval, as (1-.98)/2 = 0.01 and [(1-0.8)/2]*[(1-0.8)/2] = 0.01.

Thoughts?

 ^edit: ^moar ^flair"
What statistical method should I use? Sales data compared with geographical data.,0,1,False,False,False,statistics,1496930108,True,[removed]
Сurrent state of anomaly detection algorithms for time series,9,21,False,False,False,statistics,1496930456,False,
Dear reddit! What type of analysis do I use? Greatly appreciate help!,4,1,False,False,False,statistics,1496932625,True,"Dear /statistics/. I am hoping you will embark with me upon a quest towards my MSc. I'm researching how types of work motivation relate to work engagement and how this relationship is moderated by gender stereotyping at work. Furthermore, I am looking at how work engagement relates to work performance and/or turnover intention. I've surveyed women in IT with the appropriate (Likert) scales. In the past I've worked with multiple regression analysis. However, Likert is ordinal, right? I am a nobody when it comes to SPSS (at least, I need freshing up big time!) and was wondering if you guys could help me out with some starting-out questions:

1) Which type of statistical analysis do I even use?
2) How many models (if multiple) do I use? Do I start off with WORK ENGAGAMENT as the IV, INTRINSIC/EXTRINSIC MOTIVATION as DVs and GENDER STEREOTYPING as moderation variable as MODEL 1? And subsequently WORK ENGAGEMENT as IV; PERFORMANCE and TURNOVER INTENTION as MODEL 2? And then an all-encompassing MODEL 3 with all variables somehow? I've added a process model of my research for better understanding. 

I thoroughly hope you guys can help me in the right direction here as far as my approach. It's been ages since I've last used SPSS (or any statistical analysis for that matter), and I figured perhaps reddit's cool and kind enough to share some of its' wisdom! 

[Imgur](http://i.imgur.com/8uFkKmH.png)
"
Finding maximum of multiple means,0,1,False,False,False,statistics,1496936159,True,[removed]
How should I track my life in numbers?,2,4,False,False,False,statistics,1496936546,True,"Hi!

Saw a post a while back, some dude was tracking his life, how much alcohol he drank, how much cocaine he took, how many poops he made etc etc. I was thinking of doing the same (I don't do any drugs). But how should I design the table? One row with date, and a bunch of columns of specific tasks or how?"
Fairly simple stat question,0,1,False,False,False,statistics,1496942738,True,[removed]
Where to go from here?,5,1,False,False,False,statistics,1496943247,True,[deleted]
What is the relation between the interitem correlation and Cronbach's alpha?,1,2,False,False,False,statistics,1496944106,True,"I'm currently reading a meta-analysis in which the authors state

> Because score reliability is influenced by both interitem correlations and the number of items in a measure (with higher interitem correlations and longer tests producing more reliable results), **we also calculated the average interitem correlation from the average reliability coefficient**.

What I'm trying to understand is how this is done. I could make sense of it if the actual scale items were available, but I'm confused as to how they computed the interitem *r* **from** Cronbach's alpha. "
"What is the ""inclusive"" standard deviation?",2,2,False,False,False,statistics,1496950616,True,"I'm following some guidlines to compute a score (*D-score*):

> ""*D* is computed as the differences between mean latencies of the two BIAT blocks divided by the *inclusive* (not pooled) standard deviation of latencies in the two blocks""

I'm not sure what is the difference between ""inclusive"" and ""pooled"" SD. I would appriciate any input!"
UK Election 2017: The statistician predicting winners,2,4,False,False,False,statistics,1496951552,False,
When the guys discover statistics from a simple questionnaire about age...,0,2,False,False,False,statistics,1496953859,False,
"How do I make my Statistics MS ""worth it""?",12,19,False,False,False,statistics,1496954761,True,"Hi /r/statistics. I am starting a 1-year Statistics MS program at a decent reputation school next year. I am a bit concerned about the difficulty of finding non-trivial employment after graduation, as it appears that I have not been able to secure an internship this summer. I am worried that even after I complete my MS, my lack of internship experience will make my MS not very good at starting me at a better position compared to someone who did a BS. Is this true? Is there anything I can do during the school year to increase my chances of getting a job that is generally what is offered to new BS level stats graduates?"
should i pursue an undergrad in statistics?,12,8,False,False,False,statistics,1496960166,True,"Recently graduated with a major in business administration and a concentration in MIS. I did not like any of my undergrad courses until I took one class in business analytics. There I really loved learning about cluster analysis, discriminant analysis, and basic monte carlos simulations. After talking with my professor, he mentioned that either I can pursue a masters in Data science or a degree in statistics. What should I do? I really loved the predictive analytics part of the business analytics course and searching through reddit, it seems a degree in statistics is much better than a degree in business analytics. Please help"
Readings on statistics in political campaigns?,2,10,False,False,False,statistics,1496972796,True,"Hi all,

I'm currently volunteering for a state senate election campaign (that's for the state legislature) for a candidate that doesn't have much machine politics on their side or a professional campaign structure.

I think I'm the only one in the inner circle with a solid statistics background (BS level, currently working in public health statistics). I have a solid intuition, but I'm pretty ignorant of specific techniques and strategies which are currently employed in modern elections.

Can you help me put together a reading list? I can probably provide some value by winging it, but I want to rely on more than raw intuition and foundational statistics knowledge.

If there's a better sub for this request please let me know."
How to isolate a dependent variable from its independent variable?,1,1,False,False,False,statistics,1496975691,True,"Sorry that the title is vague, I'm not sure how to word this question.

I'm looking at rebounding in basketball, and was thinking about how a player's offensive rebounding statistics can be negatively affected if that player shoots a lot of three pointers (more shots from outside = less time near the basket = less opportunities for offensive rebounds), which could lead to misinterpreting a player as being worse (or better) at offensive rebounding than they actually are. I thought it would be interesting and informative to figure out a way to isolate offensive rebounding stats from a players tendency to shoot 3s.

If basketball isn't your thing, I have two variables, A & B. A influences B in an inverse relationship. I have a large dataset of (A, B) pairs and I want to determine which pairs B value is the highest because of non-A factors.

I suppose in a sense I'm trying to determine the contributions to a players offensive rebounding from variables that aren't three point shooting; namely the contributions from that players ability as an offensive rebounder. I've considered finding a regression model using historical data and then looking at the residual values to see which players over-/under-perform the historical mean.

I'm not too well-versed with stats - I took an AP statistics course in high school a few years ago, but haven't done much since - so it's highly possible that I'm really overthinking this. That said, don't worry about dumbing stuff down, I really enjoy statistics and love to dig into new topics. Thanks for your help!"
How to turn negative results into good looking graphs?,3,0,False,False,False,statistics,1496980057,True,"Let's say that I am conducting a survey for a company, which asks people about the companies new product (let's say new flavour of cookies), I have collected some results (most of them are negative, people didn't like the new cookies), but I still NEED to present the company some graphs, which HAVE show that people in fact did like the new cookies. How would I do that? What are some of the techniques I can use?


This is not a true example, I am not actually going to lie to a company, this is just an example I am interested in."
SOS- Campbell and Stanley's notation,1,1,False,False,False,statistics,1496980743,True,[removed]
Is there a modification to ANOVA that is robust for non-normal distributions that have heterogenous variances?,7,4,False,False,False,statistics,1496985769,True,"Hello!

I am analyzing 5 sets of distributions, and would like to test for between-group variation. However, the data is not ideally distributed, which is causing me problems in selecting the appropriate tests. 

Basic one-way ANOVA testing requires that group distributions be normally distributed with similar variances. If the data does not satisfy these assumptions, modified analyses must be used. To my understanding, when variance homogeneity is violated, the Welch test is performed, which only assumes normality. However, when normality is violated, the non-parametric Kruskal-Wallis test can be used, but this still assumes similar distribution shapes (a weak form of variance homogeneity).

I have tested my data for normality using the Shapiro-Wilk test, and have tested for variance homogeneity using Levene's test, and have found both of these assumptions are not met. Is there any test I can use that is robust when both of these assumptions fail? 

Thanks in advance for any insight, and please let me know if any more information is needed."
feature selection with k-means,3,1,False,False,False,statistics,1496998997,True,"Does it actually make sense to manually remove feature and to look at the (between sum of square / total sum of square) ratio to tell if the new classification is ""better"" (bigger betweenSS/totSS) ? Should i use another tool ?
"
"Calculating Cronbach's Alpha with missing Values/""I dont know"" answers",0,1,False,False,False,statistics,1497000939,True,[removed]
Craigslist Subaru Legacy Costs,0,1,False,False,False,statistics,1497013962,True,[deleted]
Economics Student with a Question on Critical Values of F,11,1,False,False,False,statistics,1497019072,True,"I am in an econometrics class and we are doing regressions and hypothesis testing. I have found my regression and the information I need to make my hypothesis, however I can't find the critical value for F(3,118). Can anyone help explain to me how to find such a number because most answers I have found explain how to read an F Table, however most F Tables don't have degrees of freedom as high as 118, and if they do it is in intervals that goes from 100 to 120 in the next row.

Thanks for any insight you may be able to offer!"
Interpretation of cumulative hazard in the setting of Cox regression of repeated events,0,7,False,False,False,statistics,1497024490,True,"Essentially a X-post from Cross Validated ([link](https://stats.stackexchange.com/questions/284450/cumulative-hazard-in-the-setting-of-cox-regression-of-repeated-events)). 

I'm having trouble conceptualising the cumulative hazard. In short, I would like to present the results of a repeated event Cox regression by showing the expected number of events given the covariates. My feeling is that the cumulative hazard represents this statistic.

If that is a valid interpretation of the cumulative hazard I am also curious how time-dependant predictors change this relationship. A cumulative hazard given a time-dependant predictor cannot represent a cumulative event rate as it doesn't contain information on events an individual experienced before the current value of the time-dependant predictor. Is there anyway to rectify this?
"
Interpreting positive and negative interaction coefficients in regression,9,3,False,False,False,statistics,1497025649,True,"I'm trying to make sense on how to interpret regression output containing two main effects (X1 and X2) and their interaction (X1*X2) just by sight instead of having to plot. All of the resources I can find online are super confusing because they use the terms positive and negative to refer to both the direction of the effects and to also mean things like good and bad. 

Let's assume that X1 and X2 are both continuous. 

If I have the following output: 

* a negative coefficient for X1
* a positive coefficient for X2
* a negative coefficient for X1*X2

Does that mean that as X2 increases, X1 becomes MORE negative in its relationship with Y?

In the same example above, if X1*X2 was positive, does it mean that as X2 increases, the relationship X1 has with Y becomes more positive (i.e., less negative, closer to 0)?

"
Unsure of how to graph this data,0,1,False,False,False,statistics,1497033641,True,[deleted]
"How do you generate ""novelty statistics"" as commonly used by sports announcers?",0,1,False,False,False,statistics,1497033904,True,[removed]
Weighted average of Standard Deviations?,0,1,False,False,False,statistics,1497033999,True,[removed]
I need to choose a program in [Applied] Statistics based on hire-ability post-graduation,3,3,False,False,False,statistics,1497035009,True,"I'm not going to mince words; my top choices are Rutgers MS in Statistics and Villanova MS in Applied Statistics, and I need to choose. My most recent LinkedIn searching isn't helping me find out where grads of either program end up, though I know the student body of each program is very different. I know Villanova at the undergrad level is more prestigious than Rutgers, but at the MS level I may be delusional in thinking that's still the case. My goal is to head into industry, perhaps in a pharma role, though data science interests me as well. If all else fails I'll try to find work in a bank, as I know someone with a stats MS who did just that. If anyone has any opinions, I'd like to hear!"
I can't decide what time of normalization to use,0,1,False,False,False,statistics,1497038118,True,[deleted]
What do you guys think about this statistical breakdown?,2,2,False,False,False,statistics,1497045508,False,
How should I normalize these data?!,3,4,False,False,False,statistics,1497045597,True,"Hi, I'm trying to classify using k-NN and can't use z-score normalization because some of the data are not at all normal. I can't use min-max because there is a lot of information in the outliers of some of my features.
I have eight features all together. How should I normalize them? Some are 10^-1, some are 10^2, some are 10^6."
Question regarding Likelihood,11,4,False,False,False,statistics,1497077802,True,"https://prnt.sc/fi3dhx

My question is, what exactly is the likelihood? What is it doing? Initially I thought it meant ""The probability of individuals failing at time t"" but with the texts wording of ""contributions from time t to the partial likelihood"" it makes me feel like I'm completely off.

Can anyone guide me on what exactly is the likelihood and what is it used for?"
Get Help When You Face Problem on Statistics Homework/Assignment - Classified Ad,0,1,False,False,False,statistics,1497078459,False,
Statistics Assignment Problems,0,0,False,False,False,statistics,1497084594,False,
"If increasing the amount of training data in a binomial classification task decreases the accuracy, that means the model is overfitting, doesn't it?",3,7,False,False,False,statistics,1497095623,True,"I'm switching from logistic regression to a 4 layer by 16 unit DNN with softmax activation on the inputs and outputs, with the ADAM optimizer, if that makes any difference. Classification accuracy jumped from 74% for linear logistic regression to 90%, but then back to 87% when I added 40 examples to the original 170 data rows.

I've never used dropout in NNs, but I am willing to try if I have to. I just want to be sure whether I should or not. Thanks in advance.

edited to add: each of nine models (all with n=170 to 210 now) have between 10 and 30 features, about a fifth of which are normally distributed, a fifth are sharp halves of normal distribution (i.e., no values above the modal mean, essentially the negative of absolute values of standard scores) and the rest are boolean categories.

I guess I should also say that 75% is vastly larger than manual agreement on the same task, so we're in the superhuman realm."
Reality Check - Racism and Police Shootings in America,11,0,False,False,False,statistics,1497107866,False,
"What is, and when is, something Significant?",1,1,False,False,False,statistics,1497108999,True,[removed]
Predicting a Normal Distribution,0,1,False,False,False,statistics,1497112722,True,[removed]
How can I apply statistics at my job?,9,21,False,False,False,statistics,1497115948,True,"Hello,


I realize this is extremely open ended, but hopefully you can understand that I'm looking for a place to start. I have virtually no knowledge of statistics other than some intro courses during my undergrad, but I recognize that it is very useful and I find it interesting, so I’d like to study it. 


I would like to go back to school to earn a M.S in Statistics with a concentration on Biostatistics. However, in order to get my company to help me pay for it, I need to make a case as to why it's useful. The reason I am getting this degree is because I'd like to apply it in a somewhat clinical setting down the line.
I work in the pharmaceutical industry as a Microbiologist. My company specifically works on filling drugs into syringes and vials under aseptic conditions for other clinical research companies. Thus, we don’t generate any clinical data per se, but we do generate a lot of environmental data. We monitor the areas that we fill drugs in for microorganisms. 


We do perform very crude statistical analysis using our proprietary enterprise resource planning software SAP, but it’s not very meaningful. Worse yet, it’s unreliable. SAP requires several bizarre parameters to query data properly, and is notorious for constantly omitting pieces of data. I can present this information to my higher-ups, but I don’t have a solution to the problem!


We make line graphs of the incidence of organisms detected in each room, but nothing more than that. Surely there must be a deeper way to analyze this data? I’m convinced that there’s more that can be done- I just don’t have the education to figure out what that would be. Off the top of my head, I’d like to see ways we can follow the incidence of species through several rooms in our production areas- perhaps relate it to the frequency of cleaning, the amount of time since the last cleaning was performed, or even the person who performed the cleaning. Or perhaps there’s something I’m missing altogether.


Again, I apologize for how vague I am being- I’ll accept any resources. If anyone knows of a good place to start researching, knows of any common tests or methods for the work I’m interested in, or has a shred of knowledge on the application of statistics in pharmaceutical (particularly aseptic manufacturing) microbiology, I’d be extremely grateful.



Sincerely,
Not at all a statistician. 
"
Use Your Superior Stat Skills to Set the Budget for the 2046 US Navy. (x-post /r/Submarines),1,3,False,False,False,statistics,1497116233,False,
"Will someone here please explain why, in the game of baccarat, banker has a slight statistical advantage over player?",6,3,False,False,False,statistics,1497118290,True,It would appear that one should be equally as likely as the other.
[2 questions] Finding the maximum deviation percentage from a value / what do I use to detect outliers in a data set?,7,4,False,False,False,statistics,1497122286,True,"I'm not great at using statistics terminology outside of my native language, so I hope I've worded this correctly.

***

I'm comparing two sets of data and I've calculated the deviations between every data points.
I'd like to have a maximum deviation of 10% in both directions, and I've got a maximum of +6.5% and a minimum of -4,9%.

The question is: what is the (absolute) maximum variance in both directions? Is it the maximum of 6.5%? Do I average out?(don't think so, but just asking) Is there some other trick to this?
I'd like to know.

In the end I'd like be able to say something akin to""value x is within ±y% from value z.""

***

Second question(regarding the same data set):

I've got a data set with values that change regularly. Samples are measured daily and the data set is fairly constant but does change over time.
The average is around 5130 and stdev is a rather high ~350.
The data is fairly stable but sometimes peaks out.

Now the question is: what would be a suitable method to detect outliers and remove those from the data set?
I don't know too much about statistics and as such don't really know when to use what outlier detection method.
It's a reasonably large data set(75 entries) and I have no idea how many outliers to expect.
Should I use a Dixon's test for this?

Right now I just determined a max and minimum value consisting on average ±2σ, everything outside those tresholds is removed from the dataset. Is that an okay method to use, or does this make statisticians cry?

Edit: Data set appears to be normalized according to a quick 'n dirty q-q plot. However, considering this is a changing data set depending on the concentration of x in an effluent sample, would an outlier test that depends on a normalized set be of any use?
"
"Critique me, please. Self-taught, don't hold back just trying to learn.",0,1,False,False,False,statistics,1497126135,True,[removed]
Question on data analysis method,6,2,False,False,False,statistics,1497134045,True,"Hello /r/statistics,

I have a set of data that I am unsure how to explore for statistical significance. I data from 3 independently collected groups comparing the proportion of subjects in each group interacting with a problem divided into discrete 5 second chunks. For example, in the first 5 second, 0.06 subjects were active; at 10 seconds, 0.058 subjects were active, etc. How can I compare the distribution of these 3 groups against each other to look for statistically significant differences in the proportion of subjects interacting? The distributions are not normal, and all data sets are positively skewed. 

TL;DR: How do I compare 3 groups with proportional data collected over discrete time increments"
undergrad stat major here need help!,2,0,False,False,False,statistics,1497153511,True,"hello i need some Thesis ideas for our group thesis since all our ideas given are rejected. 

and also if some can share me a guide on how to use sas and where to get it :x"
Storing Text Data,8,8,False,False,False,statistics,1497154153,True,"Not sure if this is the best place to post this, but, I'm going to be doing a project involving text analysis on news articles, and I'm not sure what the best format to store the data would be. 

I guess, ideally, I'd like to keep them all in one file, as keeping thousands of text files seems like it'd be a hassle to deal with. However, the only way I've really stored data in the past is a csv, and that doesn't seem super appropriate for long strings of text. 

So, any suggestions on the best way to store text data?"
"Simple but.. is it possible to calculate stdev of a fold, fold change?",5,4,False,False,False,statistics,1497159646,True,"For example, values below are levels of gene expression in 2 groups: control and treatment.


Control: 4,6,5 (mean= 5.3, stdev= 1.5)

Treatment: 7,9,7 (mean = 7.6, stdev= 1.2)


I want to express my data as fold treatment/control, i.e. 7.6/5.3=**1.4**.

Is there anyway to derive a stdev value for this?"
How to measure goodness of fit for binomial classification from disagreeing observations?,0,1,False,False,False,statistics,1497163529,True,"The training data for my binomial classification task includes multiple observations of identical feature vectors, typically 3-5 observations of the same instance with identical features, which often aren't always classified into the same category. They usually are, but often there will be one and sometimes two instances in the other category than the rest of the observations. So it's impossible for me to obtain 100% accuracy.

How should I handle this situation when measuring goodness of fit for the purposes of selecting a classifier algorithm?

Should I treat five feature_vectors -> {a, a, a, a, b} as feature_vector -> 0.8 as the proportion of each, for logistic regression against fractional dependent variables? If so, should I do anything different for a specific set of features with different numbers of observations, i.e., should I treat a single observation of the same features differently than five observations of the same features?

How can I do the analogous thing with a DNN -- just one output unit, or two with (p, 1-p) training values for the expected output?"
Looking for books on Statistics with tonnes of questions (solved/ unsolved). Any recommendations?,8,19,False,False,False,statistics,1497177443,True,"For topics like The Central Limit theorem, Chi Square  Tests, ANOVA, Estimation etc... "
Finance undergrad considering pursuing a Statistics graduate degree,0,1,False,False,False,statistics,1497209937,True,[removed]
Explained Sum of Squares,6,6,False,False,False,statistics,1497232309,True,"Hey Guys,

I'm an economics student in an econometrics class. I'm learning about Total Sum of Squares and I'm trying to understand what the Explained Sum of Squares or ESS is. Can anyone help explain the concept and how to calculate it?"
"Regression analysis: How can I find the probability of a certain y value given an x value, a trendline formula, and r-squared?",0,1,False,False,False,statistics,1497239064,True,[removed]
Calculating Cronbach's Alpha with missing Values,0,1,False,False,False,statistics,1497254117,True,[removed]
Delta method to derive Fj(t)?,0,1,False,False,False,statistics,1497273926,True,[removed]
ama from someone who did the statistics courses on coursera from duke?,5,4,False,False,False,statistics,1497274755,True,"what was the experience like? was it difficult? were things described in a way that clicked? were there enough practical assignments provided? what are you focusing on now having done the specialization?

im thinking about doing it, but i might skip the first one."
Question: Relation between skewness and correlation between Mean and Standard Deviation,7,2,False,False,False,statistics,1497284845,True,"I'm having trouble seeing this.

Plotting the skewness on the x axis, and the correlation between mean and sd on the y axis (where the mid points are 0) would bring about a sigmoid curve? Where a zero on both x and y would define a normal distribution. 

Can someone try and intuitively (so, not too mathematically) tell me how skew is related to this correlation?"
"When to use mean & median to find a ""typical"" element of a data set",9,7,False,False,False,statistics,1497286486,True,"This question has come up several times over the course of my academic career, and I struggle with it more than most questions like that.  Do you have any thoughts?  Do you think the answer changes based on the distribution or clustering or data points?

Ex: Based on the class' test scores, what did a typical student get on the test?

Thanks!"
Looking for place to talk about data analysis. Got any suggestions?,8,7,False,False,False,statistics,1497294817,True,"Hi all, 
    I'm looking to learn the tools to store data in a scalable framework (like hadoop or mysql), analyze it using R (and maybe python), and present it efficiently using something like Shiny or other packages. Mainly I am looking for a Discord channel or somewhere else I can just drop in and ask question and chat with people who are knowledgable about these tools and are willing to help n00bs like me. I know there's stackexchange, but i'm looking for more real-time back/forth.

Do you have any suggestions?

 many many thanks. "
Worried about Choice of masters. (UK),0,1,False,False,False,statistics,1497300212,True,[removed]
Help: what is the best way to identify outliers and how to handle them? Excel/access,9,0,False,False,False,statistics,1497302804,True,"I have this problem at work. I need to prepare some basic analysis on sales data, but I can only use MS Access and MS Excel to do it.

However, I am not exactly sure how to detect outliers and how to handle them. I firstly removed top and bottom 1% clients by the average number of transactions - but that was almost 30% of all clients (a lot of them had just 1 transaction). This method didn't affect mean or standard deviation too much. 
Then I tried something else - the rule that outliers are above mean + 2 standard deviations (or less than mean - 2 std dev). However, this affected the mean more than a first try  and the value of new standard deviation was twice lower than original. 

We've never had such cases in work, so nobody has an expirience in that field. I'm in charge of reporting, that's why it landed on me.
Additionaly, the dataset has too much rows for the excel, so I can only import it by power query. As it turned out, because of that I can't even do a histogram to look at the data.. 

Apart from my problem, I've read about different ways to remove outliers, that I am curious which reddit thinks is the best one.
"
Concerned about choice of masters.,13,12,False,False,False,statistics,1497304701,True,"I have a choice of 2 masters, and honestly, after hours of discussion with many people, I still have no idea which one I should do. I have the options of UCL for Data Science and Machine Learning or Statistics at Nottingham. I think I'll probably want to go onto a PhD afterwards, so I need to think about how these courses look to Universities.

UCL provides a new, applied, interesting course. It also provides top lecturers, a great department and its' reputation as a top 5 uni. However, its' course is not tried and tested, perhaps employers might not regard it very well, it's going to cost a lot of money, the dissertation is 10000 words. Meaning I don't get to show a lot of research capabilities. This risk could pay off, if everything goes well then I get a good backing from a great university and a job search should be easier.

Nottingham provides a risk-free statistics course. It's half the price of UCL, it's tried and tested, it is a good university with good research. It contains good theoretical modules with a few applied modules, it also contains a 25000-word dissertation. This is obviously far more substantial, giving me a chance to show off my research! The aim of doing this is to find a job and learn on the job in either data science or finance, this gives some flexibility in job prospects by allowing me to specialise on the job rather than during the course. Then after 4-5 years in industry head back to university for a PhD. But I lose UCL written on my CV, which might be worth more than all of it.

As you can tell from the mess above, I have no idea what I should do. I've discussed with the course directors and neither of them inspired me, I have discussed with my previous university, I'm tempted to write to companies in this field to ask which is more desirable. I think in the long run, Nottingham will prove better, but maybe not.


Any help or tips are greatly appreciated.


Thanks."
Is my grps design a 3x1 stroop task?,0,1,False,False,False,statistics,1497305423,True,[deleted]
How do I create two columns of random numbers with a targeted correlation coefficient?,6,1,False,False,False,statistics,1497317355,True,"Think of this maybe as prices of two different equity prices. Looking at each one, the daily change in price might be completely random. But the correlation coefficient between the two is perhaps 0.5. How do I model that?

I'd like to model two columns with random instances between 0 and 1 with (target) Pearson's correlation coefficient of 0.6. I'm using Excel, but I would begrudgingly use R if required. It seems like the formula should be simple enough, but it's not coming to me. My first thought was to make column A simply randbetween(0,1), and column B = 0.6 * A+0.4 * randbetween(0,1). But that doesn't work. It shouldn't be *impossible* for column B to stray from column A by at most 0.4, and the correl is too high. Thoughts?"
Simple question about clarifying the 'sample mean',16,5,False,False,False,statistics,1497318954,True,"Hello all, this is something that has been bothering me for a while, but I wanted to get a clear idea of the use of 'sample mean' as a term.

As I initially understood it, if you took, say, 10 different samples, calculated their means, then calculated the mean of *those* means, you would then get a sample mean. 

However, I think I've heard 'sample mean' being used simply to refer to the mean of a single sample, which admittedly is more intuitive for me. 

Which one is correct? Are they used interchangeably? If so, how can I tell the difference between the two? Am I gravely misunderstanding something?

Thank you in advance for any and all replies and please forgive the silly question~"
Multi-Variable Regression Question.,0,1,False,False,False,statistics,1497319174,True,"I run a business and use a lot of different suppliers. A friend suggested that if i took the two costs of my business, A suppliers and B suppliers, and put them into a multi-variable regression I could find trends and relationships between the cost of A/B to my output. Knowing this would be invaluable to my business as each of my suppliers quotes me differently and the outputs are hard to ascertain. 

Is this possible? Below is an example of some of the data that i have. Any help would be very welcome. 


		
Input A 126632	Input B 1406.4 Output 392173
"
Help interpreting Correspondence Analysis (CA) and key things to mention,1,5,False,False,False,statistics,1497326812,True,"So I'm doing an ecology research project and was advised to do a correspondence analysis (CA) on my data. I looked at the presence a few different organisms at different sites (which have different levels of pollution).  I have very very very limited multivariate analysis experience so I've been reading around online about CA's on how to do them and interpret them. My final CAs show some nice patterns (useful organisms around sites with low pollution, and vice versa). But from what I read online, I can only see people interpreting the results by saying things like ''Those organisms are more related to that site, and those other to that other site''. Is that kind of interpretation enough? Is there something I should write in addition to that?, Like when people say an ANOVA was statistically significant and you include the F ratio and P value and DF, Is there any technical stuff like that that I should include when I'm writing my results section? Like my CA graph from R shows Dim1: 90% and Dim2: 7.2%... is just showing the graph and simply talking about the patterns enough? Thanks!!"
"Pearson's ""family"" of correlations?",0,2,False,False,False,statistics,1497327651,True,"How to determine which correlation is part of the Pearson's ""family""? I know that Pearsons, serial, point-biserial, partial and multiple all belong to it, but what about phi for example?"
"Is this MCAR, MAR or NMAR?",3,1,False,False,False,statistics,1497344774,False,
Determining correlation within a single population - help!,0,1,False,False,False,statistics,1497358078,True,[removed]
How To Write a Persuasive Speech (Years 3-10),1,0,False,False,False,statistics,1497359892,False,
Question about probability in game,11,15,False,False,False,statistics,1497362043,True,"So I have game I'm designing where you get an item 1 in x times. But because of bad luck you may not get the item in x times and if you don't the next roll I automatically give it to you. This will throw off the probability as I still want it to be 1 in x but it will be slightly higher as I give it to you automatically if you don't get it. How would I calculate the probability to get a proper X?

For example lets say you get a gold coin 1 in 6 times. But after 6 rolls you haven't gotten a coin. So the next roll I automatically give you a coin. But I still want the probability to be 1 in 6. So in order for it to be 1 in 6 with ""auto give"" roll I would have to adjust the initial probability to be like 1 in 6.5."
Difference between uses of ANOVA and Holm-Bonferroni Adjustment in AB Testing?,5,9,False,False,False,statistics,1497371816,True,"Hi all, 

Stats noob here back at it again. I've been trying to understand how to apply ANOVA to AB testing within ecommerce. I've been able to grasp that ANOVA is used to compare significance of variance within groups larger than two. However, in my search to figure out the best method for countering alpha inflation pre-test (or, looking for a Bonferroni model to better calculate necessary sample size) I stumbled upon the Holm-Bonferroni adjustment, which it seems is applied POST test and seems to calculate the significance of variance between variants. Hopefully that makes sense. If I'm way off base about any of the above or missing a key step, please let me know. 

To make the question more concrete, let's say I run an A/B/n test with 8 variants all being compared to the default. After the test ends, I'd have 8 different sets of uplift to different p values. To counter alpha inflation & understand how each version of lift compares to the other, would I use ANOVA, Holm-Bonferroni, or both? And why?

"
Question on Optimizing Weights for a Protein Energy Function,4,3,False,False,False,statistics,1497376263,True,"I'm trying to optimize a set of weights for a protein energy scoring function. I am developing a scoring function of the form: score = w_a *a + w_b *b + w_c *c where a,b,c are different values that are specific to a given protein structure and w_a,w_b,w_c are their corresponding weights. I have a data set of the score calculated for 20000 protein conformations and their corresponding RMSD. Plotting score v. rmsd should give a ""funnel"" plot. I am attempting to find the optimal weights to produce the most ""funnel"" like plot. To quantify the ""funnel-ness"" of the plot, I'm using a simple summed Boltzmann like distribution function that produces a single value for the entire ""funnel"" that represents the goodness-of-fit. The goal would be to identify what values for w_a,w_b,w_c for the given data give the best goodness-of-fit value.

I've been beating my head off of the wall for the past few days trying to come up with an approach to solving this problem. I have a feeling that I am over thinking things and the solution is much simpler than what I am thinking. Regardless, I am stuck. Any help or points into the right direction would be greatly appreciated!"
Question about odds of poker deck / shuffling,16,7,False,False,False,statistics,1497378044,True,"Hi, I observed some shady (it appeared) shuffling/dealing in a game the other day. 

standard 52 card deck.  5 players. No Limit Hold Em. 

the *9 of diamonds* appeared on the flop 5 **consecutive** hands out of the 29 dealt. 


Odds (assuming truly random shuffled deck)?  I don't remember how to approach this with analytical mathematics.  Thanks in advance for your help. 

"
Where can I work with lots of young fun coworkers?,2,0,False,False,False,statistics,1497380375,True,[deleted]
How to choose best regression model for dataset with plethora of features?,6,5,False,False,False,statistics,1497382980,True,[deleted]
Question about strange regression results.,10,4,False,False,False,statistics,1497387565,True,"Hi! I have 4 values variables that I got from a force sum ipsative questionnaire. 

When i put the 4 variables in a regression, spss threw away one of them so I transformed the data by using geometric means instead. I can put all four variables inside now. 

The results shows that alla variables are non significant when i run a standard ENTER regression. For some reason the Anova shows significant F 3.95. When 

When I instead do a stepwise, 1 variable becomes significant and the rest are dropped. 

And when i do a backwards regression, 2 different variables become significant and the rest are dropped.

I know that stepwise regression are invalid, but that made me also try to put 2 variables at a time or 3, that also gave me different significance results. I know some previous studying with the same variables i have that put them one by one to avoid collinearity and not getting one of the variables thrown out. Is my issue still related to this even though transformed?

What is going on and what is the right step to take? Not sure what to even google.

"
Can R^2 be greater than 1 when randomly searching for coefficients?,5,2,False,False,False,statistics,1497402817,True,[deleted]
Question about time series,2,6,False,False,False,statistics,1497404481,True,"For a project I am working on I am looking at money earned in period 0 and and in period 1 and then computing a growth rate. This growth rate is what matters and what I am regressing all of my independent variables against. However, I am concerned that there might be a time trend between earnings in different periods but it doesn't really make sense to add a time period factor because the growth rates only come to exist in period 1. Could this potentially cause problems? Should I rethink by dependent variable? If you need more info about what I am doing to answer this question, I am happy to provide it. Thanks "
(X-Post) Solving Single-Player Qwixx,0,2,False,False,False,statistics,1497408672,True,"This is a cross-post from [here](https://redd.it/6h4qpi)....

I had [previously posted](https://www.reddit.com/r/boardgames/comments/5l62f6/qwixx_analysis_and_strategy/) some statistics analysis of the dice game Qwixx. My previous post was geared toward developing a multi-player strategy that ""races to lock"" the colors as fast as possible. Well now I've analyzed the single-player version of the game to try to maximize the number of points you can get in a solitaire version of the game. The new analysis is in the form of an informal paper, titled [Solving Single-Player Qwixx](https://drive.google.com/open?id=0B0E4VFlFjnCuME9sZGhrbGRIWXc). Using an optimal strategy outlined in the paper, a player is able to achieve an average of 115.48 points, which is significantly higher than scores you would see in most multi-player games. However, just because you can achieve a high score in a single-player version of Qwixx doesn't necessarily mean that you can win multi-player games with that same strategy. I present some head-to-head results in the paper between the ""optimal single player"" strategy vs the ""race to lock"" strategy. Anyway, I hope some folks find it interesting!"
How would I resolve the following question in R?,0,1,False,False,False,statistics,1497419205,True,[removed]
Question about using age range in research,5,5,False,False,False,statistics,1497442586,True,"I'm currently in the process of creating a survey for a research. One of the questions is regarding age, and I was thinking of using age ranges to display the data.

I think I remember from my stats classes in university that if you're doing something with intervals like that, you should make all the intervals the same size. However, it is important for me to have a clear separation between certain ages (meaning more tight ""chunks"") while still being able to support a large range of ages. This means that each age group varies pretty wildly. As it stands now my age division is as follows:

<18 / 18-21 / 22-24 / 25-29 / 30-40 / >40

Is this an okay thing to do? I expect there to be few recipients (probably under 30) so would just using absolute age be better in that case?"
Help with Normal Distribution help,0,1,False,False,False,statistics,1497445500,True,[removed]
How to perform a Repeated-measures 3-Way MANOVA on SPSS?,0,0,False,False,False,statistics,1497449401,True,"I tried a bunch of two ways with RM but can't figure out how to do a 3way. I have 3 IVs(one is an RM with 3 levels that are categorical) and 6 DS. 


My DVs are:

Brain Region (3 levels)
Cell Size (small/large)
Treatment (1,2,3)

My IVs are:
Perimeter
Area
Density
Roundness
Compactness
Aspect Ratio

The way my data sheet is organized is that the first 3 columns are my DV, which each cell is codified with a 1,2 or 3 representing the level. The last 5 columns are the columns with the IVs. Each row is a subject, there are 3 for each region, so 3 treatments, 3 regions and 2 sizes. So about 50ish cells in the matrix. This was the format I used for a non RM ANOVA but when I try to do an RM it asks me to define which columns correspond to my 3 levels of my RM variable. But problem is I don't know how to transpose my data to isolate it into 3 rows. 

Someone told me you can only do one IV at a time with an RM, in which case I would just consolidate the column headers to combine variables( e.g. X1,Y1,Z1; X2,Y1,Z1, etc). 

Any tips? I'm so frustrated I wonder if there's a site where I can pay someone to just run it for me. "
There are more signs that Britain's roaring housing market is slowing down,0,0,False,False,False,statistics,1497452481,False,
"Statistics on hate crimes fluctuate in Toronto, but numbers show problem not going away - Toronto",0,2,False,False,False,statistics,1497452620,False,
Cox PH Regression Sigmaplot,0,0,False,False,False,statistics,1497452642,True,Version 13. Does anyone know how to set the reference for categorical variables with 2 or >2 categories?
Pair wise interactions in logistic regression,4,0,False,False,False,statistics,1497452662,True,"I have a binomial logistic model, and I run an multivariate analysis in R (using glmulti) I got an advise for ""running the models with pair-wise interactions change from level=1 to level=2""
I get loss with this! I think I'm not getting the point of it. 
Any help?
 "
Colombia Defense Minister Offers Dubious Statistics on ELN Weakening,0,0,False,False,False,statistics,1497452728,False,
How to quickly revise major stats concepts?,0,1,False,False,False,statistics,1497452819,True,[deleted]
7 key statistics on sports injuries,0,1,False,False,False,statistics,1497452824,False,
Ontario County sheriff releases May statistics,0,1,False,False,False,statistics,1497452951,False,
How to quickly revise major stats concepts?,7,2,False,False,False,statistics,1497453495,True,"I'm gonna soon have an interview and some of the questions will be on stats. I don't have any formal stats training, so I was wondering if there's a good resource (short book maybe) that walks me through concepts such as the use of parametric/nonparameteric tests, confidence intervals, significance testing (fisher vs other methods).

Basically stuff that can come up in quiz-like interviews. I already know things like probability, regression, bayesian stats (""machine learning stats"") but not the more traditional ones.

At the moment I'm just reading the Wikipedia pages, but I feel like there must be a more systematic approach to this. Also, ideally, I could revise all these in a short span of time (few days at most).

Any recommendations?
"
Question about finding correlation while using logistic regression?,10,11,False,False,False,statistics,1497457652,True,"I'm trying to run a regression on a binary dependent variable. However, I'm having trouble choosing the appropriate independent variables for this. Usually I would create a scatter plot for each candidate variable and the dependent variable and determine the correlations. Then I would choose my variables based off of these findings. However, I don't think this is possible with a binary dependent variable.

Are there any other techniques that I can use to sift through a large selection of potential independent variables?

Also, how should I decide how many independent variables to include? Are there any methods for doing this with a logit regression? 

Background information: My y variable is whether or not someone defaults on a loan. I have quite a few x variables to choose from. I imagine some will be continuous and others nominal."
Significance Test for Improvement,7,9,False,False,False,statistics,1497467535,True,"So to start, assume I am a beginner to Statistics in general. I minored in college, but this is my first time working with statistics in close to 5 years. I have a basic understanding, but definitely would need things spelled out to me.

I'm looking at health data at time 1 and then again at a future date. I want to determine if improvement between these two time periods is significant. As reference, there are about 300 members included in the data, and I've determined whether each member has improved, worsened, or remained the same across different measurements. I'm interested in the significance for individual measurements (ie BMI, blood pressure, HDL/LDL, etc.). The little bit of research I've done so far suggested a McNemar test, but I wasn't entirely sure if this is the right approach. I have access to SPSS if that makes anything easier. What is suggested?"
"If you have an MS in stats with no experience, how easy will it be to find a job in the US (as an American)?",11,4,False,False,False,statistics,1497471949,True,[deleted]
Handling Bias in a Gage study,0,2,False,False,False,statistics,1497478955,True,"I've recently started working on a fairly large gage analysis project.   Part of this project involves reviewing the current setup, and there are a few things being done that don't pass my smell test, but I am not a great statistician by any means, and I wanted to bounce one of them off everyone.  I'm going to generalize as much as possible.

Ok, so we have some gage that measures the output of some process.   There are 8 of these gages (8 sites), and each gage is assigned a few shift operators (its almost like a textbook nested ANOVA...which, its gage r&R so it should be).  There is no NIST traceable universal control, so what is done is a sample of the process is taken and distributed between the sites.  This sample is then used as a control/standard that is checked from time to time to monitor the gage.  For the sake of this discussion let's just assume that control sample is infinitely re-usable and doesn't degrade.

Since there is no ""true"" value the different sites each submit their recorded value and from that a guess at the mean comes out.  This is a straight up guess, the true value simply can't be known. 

Now here is where I get squirelly.   They also come up with a global variance.  From the global ""mean"" and the variance each site's local mean is assigned a z-score saying how far it is away from the global ""mean"".  If a sample is more than x-sigma (can't remember) then this throws a flag.

Here's my issues/logic.  First off, statistical process control isn't concerned with bias.  Bias is handled with calibration.  Determine the bias and then calibrate to it.  Shit, assume a mean of zero, who cares?  it doesn't matter for gage monitoring.  You are looking for *change* in bias and changes in variance, that's it.  

The second thing is that this z-score doesn't even make sense statistically.  Are these 8 groups part of the same population in a way that would make these z-scores even meaningful?  Are they actually random variables if they are all constantly biased due to subtle but specific and constant variations in how the test is being run?  This is the part I'm most concerned with.  Is there even any underlying logic in this?   Is Bias itself even a statistic?  If that makes sense.

Also I don't have any idea how this global variance is even being identified.  But that seems like a smaller issue at the moment because I'm not even sure that's a real thing here.

Am I way out of line to just say ""calibrate the bias and be done with it""?  "
Jobs for someone minoring in stats?,0,1,False,False,False,statistics,1497483682,True,[removed]
Statistical tests in ecology,5,3,False,False,False,statistics,1497485430,True,"Hello everyone. I am still new in statistics. I am learning statistic applied in ecology and environmental. I have a huge report to make about the statistical tests in ecology, like One Way Anova, T test, Chi Squared. I can do this things in SPSS and Excell with no problem but i need to make a theoretical report of: why/how to use them, what conditions you need to utilise etc and i really don't know what to write... Some tips anyone?"
Windshield Broken by Hail - Twice in one month,9,0,False,False,False,statistics,1497487850,True,What are the odds of this happening to an individual? My windshield was broken twice within one month by hail... can anyone crunch these numbers?
Moderation in SEM,0,1,False,False,False,statistics,1497489935,True,"Hi All,


I need some guidance on how to check for moderation effect using AMOS.


My study has 3 independent variables, 1 dependent variable and 2 moderators.


Earlier I have used IA terms and regression (SPSS) for checking moderation. Lately I have realized that this method yields lesser power and is not well accepted in good research journals. So for this study , I want to run CFA and create a model using SEM with AMOS software.


Any help is much appreciated."
How do you identify someone who's just following a script?,29,34,False,False,False,statistics,1497501043,True,"Something that's been bothering me more and more as I work is that there are a lot of people who claim to know a lot about stats who really don't.  I myself am someone who knows enough to know just how little I know, which is cliche, but true.   But I do know most of the fundamentals.

This isn't about testing someone's expertise, because there are so many levels of it out there.  I mean reading this subreddit is straight up daunting most days.   Its more about figuring out if they even understand the fundamentals.  There are a lot more ""cookbook statisticians"" floating around out there these days thanks to powerful software and the Data Science craze, people that figured out what buttons to press in JMP or what script to use in Python to run some fancy test, but have zero clue about the fundamental assumptions in play.   Like they could run a PCA but couldn't tell you what orthogonality meant (a bit of an extreme example).  Or they could run some ugly assed non-orthogonal/optimal design but couldn't read an aliasing map.  No joke I've seen the latter.  Turns out you only need 13 samples for 10 factors....  Or they don't know what the acronym NID means.  Whatever.

So what I'm trying to figure out is what are some simple questions you could ask that anyone at any level of real expertise with a solid fundamentals in stats should know, but someone who took a quick seminar or two would have no idea on.

Understand that this is 100% a ""shower arguments"" thing for me. I'm not going to take this list and go storm off and confront someone nor am I going to use it to feel smug and superior (like I said, I am *barely* middling *at best*).   I've just been dealing with a lot of cookbook dudes recently and it's been kind of grinding my gears.  And what scares me is that it's really hard to tell who's who.  I've taken directions from people before that I later found out didn't have a damn clue what they were really doing.  I'm legitimately concerned that very expensive decisions are being made at my company based on info these folks are giving.   I mean shit, maybe that's life?  These days though there's so much of this out there and I wonder how interviewers even ferret these guys out.  If they even do.

Anyways, what are some simple questions you could ask to test someone on fundamentals?   Here are a few I could think of:

-Name a biased and unbiased point estimator

-Given a distribution function f(x), how would you solve the Beta/Type 2 error risk, and what other info do you need?

-what's the difference between a binomial and hypergeometric distribution

-What's Bayes theorem, and could you write it out using Set Theory notation (intersections and unions only, admittedly this one would take me a minute)

Pretty much all of these are freshman-ish level, but I would like to think that most statisticians could answer these.  What are some other simple questions like this?

Or how else would you identify someone with weak fundamentals?

I feel weird asking this question, because it's going to sound really judgey and condescending, and I really don't want to act like I'm some kind of hot shit.  I am not, I'm straight up not very good.  But, as a young(ish) practitioner at the beginning of my career I'm getting very concerned with what I am seeing at my company.  I legitimately only have this sub to go to for help because I don't trust a lot of my coworkers skills.  Which is disturbing for where I work."
"Crime statistics for Melbourne, Victoria released",0,0,False,False,False,statistics,1497523470,False,
Question on binomial distribution,1,0,False,False,False,statistics,1497523584,True,"Question: An ASQ Six Sigma Green Belt exam has 100 questions and four choices per question.Assuming the exam requires 80 right answers, what is the probability of a student passing the exam if he/she randomly chose from the four choices for all 100 questions (let us believe that this student doesn’t have an iota of a clue about any question—no knowledge bias).

My approach:
100C80(0.25)^80 (.75)^(100-80) +100C81(0.25)^81 (0.75)^(100-81) +.........+100C100(0.25)^100 (0.75)^0 

Second part of the question, which I don't know how to solve:
>Also find out up to how many questions on
which the student may get lucky with maximum binomial probability.

"
National Bureau of Statistics report shows Lagos and Abuja have the highest crime rates - Ventures Africa,0,0,False,False,False,statistics,1497523590,False,
"The Source Weekly - Bend, Oregon",0,0,False,False,False,statistics,1497523910,False,
How do you go about building a new formula?,0,1,False,False,False,statistics,1497526665,True,[removed]
Are there any defined negatively skewed distributions? I.e. with an actual name.,10,4,False,False,False,statistics,1497527197,True,"Or do people just use 1-(Some positively skewed distribution) to fit to data?

Can have finite or infinite support."
Statistical Tests for GPA,5,3,False,False,False,statistics,1497539169,True,"Hello all, I'm doing a work project centered around first-year college students and freshman seminars. I have a spreadsheet with their fall term GPAs, spring term GPAs, and their cumulative GPAs - and I'd like to do some sort of statistical analysis to help the department. My stats knowledge is very basic (T-tests, Anova, regression, etc.) but this data isn't extensive enough, nor does it provide the conditions to conduct one of those tests. 

So - what can I do with fall GPA, spring GPA, and cumulative GPA for students who have taken a freshman seminar? "
Statistical test for one continuous variable and one ordinal variable?,0,1,False,False,False,statistics,1497542104,True,[removed]
What are some PhD programs in stats that lean more on the applied side?,26,13,False,False,False,statistics,1497542415,True,Any help is greatly appreciated. Looking specifically for applied with statistical learning and less probability theory work.
Studying statistics but i have a question.,1,1,False,False,False,statistics,1497543722,True,Can someone explain me why the approach of Dickey Fuller brings an improvement on the estimation of parameters? Couldnt find info anywhere
What type of graph to show people's change in interest over time?,0,1,False,False,False,statistics,1497545478,True,[removed]
Question about Blotto Game,6,5,False,False,False,statistics,1497546604,True,"Hi everyone. I hope this isn't a dumb question. I would like to know how to determine how many combinations are possible in a given blotto game.

For example, 100 units in 10 fields. 

Thank you in advance."
What type of graph to show people's change in interest over time?,9,7,False,False,False,statistics,1497546659,True,"My company asked the following multiple choice question to ~350 respondents over 5 different surveys: ""What industry are you most interested in working in?"" They were allowed to pick 1 of 15 industries.

So I have 350 respondents, and most of them completed all 5 surveys, which were sent out over a period of several months. My question is, is there a clever way I can visualize how John Doe's industry interest changed from:

Consulting -> Consulting -> Technology -> Technology -> Retail,

or how Jane Doe's industry interest went from:

Financial Services -> Media/Entertainment -> [BLANK] -> Media/Entertainment -> Financial Services?"
Stats,2,0,False,False,False,statistics,1497547021,True,"Why is this average of the two means not equal to the mean of all respondents? 
Students in a math class were asked their height. Then it was split into male and female averages. Also the total mean of both groups was taken."
question about combining two types of tests to form a single model,10,5,False,False,False,statistics,1497563650,True,"Is it possible to combine a one-sample t-test (comparing vector of values to zero) and a two-sample t-test (comparing two vectors of values to one another) to compose a single ~~statistical~~ mixed model? Thanks in advance for any feedback!

Edit: I have accuracy values for two conditions. Originally, I compared the distribution of values for each condition to zero, and then I compared values between conditions. A reviewer thinks that I can create a mixed model to account for both of these steps, but I'm unsure of its implementation (or if it's even possible). My data are quantitative & continuous and range from 0-1 (worst to best performance). They come from two independent conditions with a single value per participant. I'd like to establish (1) whether the data from each condition are statistically significant from zero and (2) whether they differ significantly between conditions. My null hypotheses would be (1) the distribution of accuracy values is not significantly different from zero and (2) there is no difference in accuracy values between conditions. "
Artificial Intelligence applied to Statistics,9,1,False,False,False,statistics,1497573730,True,"I'm currently studing a career on computer engineering, and I'm about to do my dissertation next semester. I passed the AI classes on my school (Pattern recognition, Image Analysis and Genetic Algorithms), and I would like to know what uses are there for AI in statistics.

Thank you in advance."
Monte Carlo Sampling,4,9,False,False,False,statistics,1497576433,True,Looking to get a better understanding of how Monte Carlo sampling is used. Was hoping for some suggestions of texts that have a good technical description and texts that provide a lot of sample problems. Any suggestions?
Clinical Trials Book Suggestions,3,1,False,False,False,statistics,1497588400,True,I am having trouble finding a clinical trials book to complement a course I am taking. I am looking for a clinical trials book that covers early stopping and has a slight bayesian touch.
Example of Autocovariance,0,1,False,False,False,statistics,1497601934,True,[removed]
Developers Who Use Spaces Make More Money Than Those Who Use Tabs - Stack Overflow Blog,23,94,False,False,False,statistics,1497615405,False,
"Hey, I'm starting a PhD in PKPD, but my background is in experimental statistics. Hence, I can't read this page. Given some of the models presented on this page, is there a good resource for learning this information?",5,1,False,False,False,statistics,1497627479,False,
Understanding P Values,3,0,False,False,False,statistics,1497629565,True,[deleted]
How can I test whether a die is fair? - Pearson's chi-squared test explained very nicely.,6,4,False,False,False,statistics,1497630596,False,
How can I tell whether or not this particular methodology is defensible?,4,6,False,False,False,statistics,1497651860,True,"This is not a homework question.  This is a real-life question.  I need to figure out whether or not a methodology is defensible, or if not, how to attack it, and what other data I should try to obtain to poke holes in it.

Okay- so there is a finite population of 80,000.  There was a sample taken of 300.  89.4% of the sampled items were found to contain a numerical cost inaccuracy.  The average numerical cost inaccuracy is 32.

Basically, they just took the average inaccuracy of $32 and multiplied it across the whole population, and said that the total extrapolated cost error was 2,560,000.

Is this a defensible methodology?  What other data should I try to get to poke holes in this?  For example, if there are a few statistical outliers that should be excluded, the average error should be lower.  What else can I try to take into account to argue against this methodology?   "
Can some one assist me with this problem?,0,1,False,False,False,statistics,1497654926,True,[removed]
What do differing crude and standardized death rates between two sets of data tell us about the proportion of the old-age groups?,1,0,False,False,False,statistics,1497696530,False,[deleted]
SPSS,1,0,False,False,False,statistics,1497706932,True,"When using SPSS, is there a way of checking a sample has been chosen at random?"
R or Python? R not the best choice anymore?,54,36,False,False,False,statistics,1497714726,True,"R was the only suggestion 2 years ago. Why are some people starting to recommend Python instead of R for novices? 

Python is much easier to learn. It's a big advantage. It's more flexible and forgiving. It takes less time to master it. Are these enough to overcome the disadvantages it has compared to R? R has amazing libraries but is harder to learn and takes longer.

Are visualisations better in Python or R? R is difficult to use working with very big data samples. Is Python better? 

Is it easier to run ten million of scenarios in R or Python? Are Python libraries good enough for heavy stats analysis?

Can Python be better for a person to learn? Is it sufficient to learn Python and never learn R?

I've been asked for advice and realised I don't know which is better.

"
The Only Guide You Need For Statistics Assignment,0,1,False,False,False,statistics,1497718329,False,[deleted]
Why doesn't VIF use adjusted R-squared?,2,9,False,False,False,statistics,1497722962,True,"So it appears that VIF uses standard R-squared instead of adjusted. Why? Doesn't this mean that VIF can be misguiding if I test multicollinearity in a model that contains multiple control variables, because standard r-squared is boosted following multiple control variables?

(Im calculating VIF using Stata) "
"I need to come up with a multiple linear regression research question with at least 1 qualitative variable, 1 quantitative variable, a dummy variable and at least 50 observations. Also need an accessible data set to easily transfer into excel which I will then import into R. Any ideas?",0,0,False,False,False,statistics,1497727041,True,[deleted]
"b1 as intercept, not b0?",1,1,False,False,False,statistics,1497736852,True,[removed]
Should I code race as a dummy variable for multiple linear regression in R?,4,0,False,False,False,statistics,1497743898,True,[deleted]
The Only Guide You Need For Statistics,0,1,False,False,False,statistics,1497759957,False,
Which test?,3,2,False,False,False,statistics,1497764279,True,"Last March the proportion of Lilytown households with unregistered pets was 0.24. Researchers suggested that the proportion of households with unregistered pets has decreased since last March. They took a random sample of 120 households and recorded whether each household had any unregistered pets.
Which analysis would be required in order to test the research hypothesis?
Would be I be correct in thinking binomial test for this?"
Clinical Research: unsure what statistical analysis is appropriate,23,12,False,False,False,statistics,1497791443,True,"I'll try and make this as brief as possible.  I'm doing clinical research at a hospital and I have all my data collected (nearly), but I am unsure what statistical analysis is appropriate.

The data consists of all patients who had ultrasounds done for suspected appendicitis: this was categorized into 3 groups, positive, negative, inconclusive.


Some of the patients in the inconclusive went on to get cat-scans to further evaluate for appendicitis (divided into positive or negative).


The majority of the patients in the inconclusive ultrasound group that went on to get cat-scans came back as negative, however a few were positive.  I want to know what stat analysis should be done to show that an inconclusive ultrasound tends to result in a negative catscan.  Later, the inconclusive ultrasound group will be stratified based off clinical information (i.e. fever, elevated white blood cell, etc.).  

So which statistical analysis would be best for this, chi square? linear regression? Those are the only two that come to mind that may apply, but it's been a LONG time since I did statistics. 


My general premise or hypothesis is that: an inconclusive ultrasound for appendicitis is equivalent to a negative study because if the appendix was inflamed and diseased, it would be obvious and seen.


I left out a lot of information regarding the study and data in the hopes of making this a simpler question, but if there is any other info needed to answer my question, just let me know and i'll add.

"
Ohio State Highway Patrol reports vehicle collision statistics,1,0,False,False,False,statistics,1497793926,False,
Opioid Addiction Statistics A Sobering Fact,0,0,False,False,False,statistics,1497794136,False,
This week in science: statistics and lies,2,0,False,False,False,statistics,1497794220,False,
Request (Not for Homework): statistics of anti-LGBT hatecrimes by race,2,0,False,False,False,statistics,1497799627,True,[deleted]
Ggplot2 is 10 years old: The program that brought data visualization to the masses,7,146,False,False,False,statistics,1497800444,False,
Best analysis options for fish data?,0,1,False,False,False,statistics,1497803159,True,[removed]
"Can I run a pairwise comparison after a 2-way ANOVA that was not significant for an interaction, but was significant for a main effect and group effect?",8,2,False,False,False,statistics,1497806417,False,
Side Jobs in Statistics,5,5,False,False,False,statistics,1497825555,True,"Do any of you know any side jobs in statistics or any data related field? I currently have analytics job that I love, but am looking to do something on the side. I've looked into freelancing but the freelance sites I've been on don't have too many data related gigs aside from people asking for homework solutions. Are there any other options that you might know of?"
"How would I find the median of 3 numbers x, y, and z using an if construct in R?",2,0,False,False,False,statistics,1497837274,True,
How To Get Online Help For Statistics Homework,0,1,False,False,False,statistics,1497859516,False,
Material as pre for Machine Learning,0,1,False,False,False,statistics,1497864954,True,[removed]
Understanding the fundamental theory and philosophy underpinning statistics,14,4,False,False,False,statistics,1497867239,True,"So I'm about to start my psychology degree and up until now, I've had a bad experience with maths and math teachers. However, I want to keep an open mindset and start to delve into statistics and the paradigm that underpins it so that, when I do begin learning about psychological research methods, I can have some semblance of understanding as to what statistics is about.

So, my question to the statisticians of r/statistics:

What is the philosophy/underlying theory of statistics?

I really do appreciate any feedback from you guys and I'm confident that I'll get it eventually! Just gotta get those neurons connecting..."
Big data analysis with SPSS,8,0,False,False,False,statistics,1497870718,True,[removed]
Stats short project,4,3,False,False,False,statistics,1497881397,True,"As part of a short project for one of my courses, we've been given some data on tomato yield and asked to use multiple linear regression to fit a model and see what causes tomatoes to have a higher yield. I've been running the diagnostics and I have an outlier, that also has a high cooks distance so has an unduly large influence on the model. I did diagnostics after writing up the results from the model and given the marks available I don't think they expect us to do two models (one without the outlier). Have I made a school boy error in running my diagnostics last as I would ideally like my model to exclude the outlier (I would explain my decision as to why) but as I say I'm not sure they want us to do two models. So I am unsure what to do now as it seems pointless me pointing out there being an outlier and doing nothing about it, so should I get rid of my original model and findings and start again with a new model that excludes the outlier? 

Edit: I've quickly ran the model again excluding the outlier and I still draw the same conclusion. Should I just mention this in the report rather than re-writing the whole thing for the new dataset? "
[Question] Noob Question: How do I calculate this average?,19,0,False,False,False,statistics,1497881875,True,[removed]
Help finding age distribution,0,0,False,False,False,statistics,1497891808,True,"I'm doing a simulation about disease caused death, where there are two exponential variables - time to diagnosis, and survival past diagnosis which are added together to get age of death. Additionally, each person is given a uniformly distributed birth time (eg between time -200 and 200). I found an article which talks about the PDF of the sum of two exponential RVs (http://www.eajournals.org/wp-content/uploads/On-the-Sum-of-Exponentially-Distributed-Random-Variables-A-Convolution-Approach1.pdf). Now, I'm trying to do various things with the simulation and I want to think of how I can calculate the theoretical age distribution of the population at any given time. As of yet I am not considering other cause death. Can anyone help me? Thank you"
Multiple questions,0,1,False,False,False,statistics,1497892630,True,[deleted]
Average differential trend line,0,1,False,False,False,statistics,1497893702,True,"I have a set of data points over time with the average for each day for a group, and the average for a subset of the group (the subset consists of data larger than the whole average). I want to look at how the subset relates to the total over time, and use it to predict if and when the subset might return to the overall average. I made an attempt but not sure if I'm going about it the right way. I took the difference between the two averages for each day, then divided by the daily averages of the whole group. I then averaged these numbers over time (day 2 is avg of day 1 and day 2, day 3 is avg of d3, d2, d1, etc. ) The graph this produces gives me an interesting result. Am I going about this the correct way, and if I'm not, is there a name for the analysis I just attempted? 

Thanks for the help 

"
Name the top 10 countries with the highest obesity rate,0,0,False,False,False,statistics,1497897270,False,
"Given a random integer generator in range -z to z, looking for any set that sums to 0, choosing strategicly which combinations to try next, on average how many sums do you try?",0,0,False,False,False,statistics,1497899075,True,[deleted]
Estimating Optimization,1,3,False,False,False,statistics,1497905890,True,"Im currently an intern working in process controls. My big project for the term is to analyze the company project estimations and figure out ways that they can get more projects within a +-10% range. Another thing to look at is to recommend ways that they can prevent from going over 15% of their estimation, where they would require requesting more money. So far its been 4 weeks and I haven't found much of any type of trend I could recommend or hone in on. I have been performing ANOVA's and t-test's for a variety of categories that the estimations are divided up into such as equipment, material, labor, manager, etc..but it all seems random. The software I am using is JMP but Im familiar with Minitab as well. Does anyone have any suggestions on where to go from here?"
revised question about mixed modeling,0,3,False,False,False,statistics,1497905926,True,"I have a task in which each participant has to solve a unique series of rotations (so each participant has a different sequence). Further, I have the participants in Condition A report their strategy while the participants in Condition B do not. Originally, I created a simple regression model for each participant, regressing the response on the unique solution sequence for each participant to extract a slope  (where perfect learning would result in a slope of 1). I wanted to know whether the distribution of slopes within each condition were significantly different from zero and whether the slopes were different between groups, so I conducted one- and two-sample t-tests.

However, someone suggested a mixed model to account for both of these steps. Is it possible to create a mixed model to do this, given that each participant has a unique sequence of rotations? I'm not sure of how to account for this degree of complexity within a mixed model in R. This is what I've started with: 

E1_fitMdl <- lmer(response ~ report + solution* + (1|subject), data = E1_data)

*Can I use the solution as a predictor, given that it isn't a categorical variable but continuous? The solution and the response are continuous, quantitative data. 

I'm a complete novice with mixed models, so I appreciate any help that you're willing to give. Thanks! 

"
Question about chi-squared test,2,8,False,False,False,statistics,1497906954,True,"When performing a chi-squared goodness of fit test, the final step is to use a lookup table , and depending on degrees of freedom and the computed chi-squared score, you will get a p-value. If the p-value is less than some chosen value such as 0.05, then statistically speaking, the observed categorical distribution is not likely to be governed by the expected categorical distribution.

My understanding is that the lookup table is derived from the chi-square distribution which is just a standard normal variable squared for a single degree of freedom, and the sum of multiple standard normal variables squared in the case of multiple degrees, one variable for each degree.

My question is: how can a standard normal random variable (or the addition of such for more degrees of freedom) account for the variance of an arbitrary expected categorical distribution?

My thought is that an arbitrary categorical distribution could naturally have a little variance, or a lot of variance. As such, it's hurting my head why this is always a useful test for any given expected distribution."
1/4th of Americans are retarded.,0,0,False,False,False,statistics,1497912787,False,
Explaining Confidence Interval for interview question,0,1,False,False,False,statistics,1497915631,True,[deleted]
Best way to analyze these data?,8,3,False,False,False,statistics,1497917340,True,"We collected these four measures for each participant, repeated at four times, under two different conditions (it's a within-subjects design):

-Three measures of fatigue (subjective and objective)

-One subjective measure of workload

Normally we make comparisons between control and experimental conditions, but in this case I've been tasked with determining the relationship (if any) between the three measures of fatigue and the measure of workload. It's also worth noting that we previously established that there was no statistically significant difference between the two conditions. 

What kind of analysis is appropriate here? My first thought was a multiple regression using the three fatigue measures as predictors and workload as the dependent variable."
Major in B.S. Applied/Comp Stats? (x-post w/ r/AskStatistics),4,2,False,False,False,statistics,1497928853,True,[deleted]
Need an appropriate model/test for medical research data,0,1,False,False,False,statistics,1497946719,True,[deleted]
Help modelling/testing mixed effects data for a longitudinal medical study,4,6,False,False,False,statistics,1497947711,True,"**Overview**

I'm trying to find a model to test data for subjects at several time points across a longitudinal medical study, while also integrating between-subjects factors. Not every subject has data at every time point, so I'm hoping a brighter mind than me can suggest a model/way to statistically test this data.

I've taken relatively basic undergrad stats courses covering topics up to the level of between- and within-subjects ANOVAs, as well as mixed-model ANOVAs, all done using R. I've also done very basic research into how to perform MANOVAs, ANCOVAs, MANCOVAs, etc., as well as the basics of multilevel/hierarchical models. I realize the answer to the question I'm asking may be complex, but if someone can point me in the right direction, I can figure it out from there. Somehow I'm the most knowledgeable about stats within this research program, but I'm in over my head here.

**Variables**

There are several different DV measures, all of which can be tested separately. These are things like angle of curvature of the spine, height of the spine, etc.; since I imagine whatever method I use can be generalized and done for each response variable, let's assume here that the DV is ""spine height"" measured in millimeters.

Now things get more complicated:

* The first IV is how many implants the subject had. This is a between-subjects categorical variable with 3 levels: 1 implant (""Single""), 2 implants (""Double""), or 1 implant, but later a second implant was added (""S/D""). This variable can be called ""Progression"".

* The second IV is the time point at which spine height was measured. It has 4 levels: prior to any implants (""Pre""), after receiving implants (""Post-1""), after receiving a second implant where applicable (""Post-2""), and a final followup done a few years later (""Final""). This is the crux of the issue: every subject has data at Pre, Post-1, and Final time points, but only those patients that originally received a single implant, but later received a second implant have values at the Post-2 time. For a multitude of reasons, there isn't really a way around this (e.g. ""simulate"" spine height at the post-2 time for the other subjects).

 Here's an example of what my data table looks like in its current form:

Subject | Progression | Time | Spine Height
---------|--------------|--------|----
s01 |	Single |	Pre |	150
s01	| Single	| Post-1	| 176
s01	| Single	| Post-2	| *NA*
s01	| Single	| Final	| 185
s02	| S/D	| Pre	| 187
s02	| S/D	| Post-1	| 196
s02	| S/D	| Post-2	| 326
s02	| S/D	| Final	| 357
s03	| Double	| Pre	| 182
s03	| Double	| Post-1	| 227
s03	| Double	| Post-2	| *NA*
s03	| Double	| Final	| 275

**Needed Tests**

If I wanted to test the differences in spine height at each time, ignoring progression, a 1-way Repeated-Measures ANOVA is invalid since not all subjects have spine height values at the Post-2 time point. The real test I'd like to run though, is how spine height at each time point differed, and how spine height at each time differed in each progression type. I originally was going to run a mixed-model ANOVA, but again this is invalid due to some subjects necessarily lacking ""Post-2"" measurements.

If this was a normal mixed-model ANOVA where every subject had data at every time point, Subject is nested within Progression while Time & Progression are crossed, as are Patient & Time. Then I could just do:

    > mod1 <- lmer(spineheight ~ progression * time + (1 | subject), data = mydata)
    > anova(mod1)
           
alternatively

    > mod2 <- aov(spineheight ~ (progression * time) + Error(subject / time), data = mydata)
    > summary(mod2)

Since that isn't possible in my case (perhaps time point is actually also nested within progression?), how can I work around this? From what I've read, I may have to do a multilevel model, but I have no idea of how to properly run that model for this type of data in R. Alternatively, a brighter mind than me might be able to suggest a way to restructure the data altogether and run it using a simpler, but I'm reaching the limits of what my formal stats knowledge taught me due to the missing data points.

If anybody needs any elaboration or has any questions I'm happy to answer them. Thanks in advance for anyone that can help or offer suggestions, because I'm at a pretty significant impasse."
Stuck at a statistical problem,6,7,False,False,False,statistics,1497953890,True,"Dear Reddit,
I am stuck on a statistical problem in one of my research projects. I hope to find some help through this subreddit. Let me explain.

In our experiments, we have 16 participants that we combine to form 8 pairs of 2 participants. The amount of unique combinations of pairings (i.e. a set of 8 pairs) is roughly 2 million possible pairings. Note that 2 sets of pairings can contain 6 similar pairs and 2 different ones. For example:

[1 2] [3 4] [5 6] [7 8] [9 10] [11 12] [13 14] [15 16]  --  
[1 3] [2 4] [5 6] [7 8] [9 10] [11 12] [13 14] [15 16]

are perfectly correct samples of what we label “unique” pairings. As said, one can create roughly 2 million pairings like that that are all slightly different from each other. What we than do is to create these 2 million sets of pairs and rank-order them by their averaged correlation on a given task (Task 1). To do this, we calculate the correlations in performance between the two observers in each pair and then take the average correlation of those 8 pairs. By rank ordering the 2 million pairings you basically get a list of pairings that ranks from highly correlated to highly decorrelated.

Now we want to see if there is any difference in performance for the different sets of pairings on a different task (Task 2) as a function of the degree of correlation on the original Task 1. To do this, we take the top 100 (out of a two million) pairings (most correlated), the bottom 100 (out of two million) pairings (most decorrelated) and the middle 100 pairings (medium correlated) out of the 2 million pairings sorted by their degree of correlation on Task 1. We now compare the Task 2 performance of these three groups against each other, and observe that decorrelated (more independent) pairings result in significantly better performance as compared to correlated pairings (numerically). That’s good. It fits our hypothesis.

Now here is the problem. By taking each of the 100 pairings as individual data points (essentially doing a 100 pairings x 3 correlation types (group) ANOVA, we use a lot of replicated data in this analysis. Take a look again at the above pairings: 6 of the pairs in those pairings are identical. Now obviously, this has the result that within a group of 100 pairings a lot of data is replicated and the variance within this data set is therefore much smaller than it would be if every pairing were independent from each other. As a result, the variance within a group is severely and unnaturally reduced. Any type of analysis of variance seems therefore not correct as the assumption of independence is violated. The main question is now how we can best compare our correlated, Middle and decorrelated data against each other.

What we have tried is the following: If we take the data for the 2 million + sets of pairings for performance on Task 2 sorted by correlation (from least to greatest) on Task 1 and average over performance in bins of 100, then plot the distribution of performance on Task 2 in a histogram you can see nice normal distribution (see Imgur link).  If we then plot the average of the Task 2 data for the top 100 (decorrelated), middle 100, and bottom 100 (correlated) pairs as three separate vertical lines, we see the decorrelated line (red) in the far-right tail of the distribution, the middle line (green) in the middle of the distribution, and the correlated line (black) in the far-left tail of the distribution. So now the main question is, is there a way to compare these 3 points in a single distribution of d’ primes?

Any help would be greatly appreciated.

Link to distribution: http://imgur.com/a/KS6BX

------
edit and follow-up:

Thanks everybody for their comments (and their willingness and time to answer)! 

A lot of questions clearly remain, so let me elaborate a bit more why we do what we do so you can have a better idea of the problems we’re facing (if you are still interested).

The main premise/hypothesis of our research is that strategically pairing people on a simple Task 1 leads to improvement on a Complex Task 2, when using the same pairings for the latter task. We’ve already established this, but now we would like to know whether people that are most inherently different on Task 1 (by directly calculating the correlation between two people, based on both of them doing 100s of iterations of simple Task 1) yield a higher overall performance on Task 2 compared to paired people that are more similar.

Our main research question is whether people who correlate low (i.e. most decorrelated pair) in their performance on Task 1, outperform those pairs that are highly correlated on Task 1, on a novel more complex task (Task 2). The idea here is that decorrelated people may use different strategies or heuristics, leading to a more optimized and diverse way of information accrual and hence better performance.

A number of important things to realize:
-	Highest correlated / decorrelated pairs on Task 1 does not equal highest / lowest performance on Task 1. It just means that their response patterns are similar.
-	Pairing occurs off line for both Task 1 and Task 2. People do the task individually and only after they have completed the tasks do we pair them up with other people.

So, the reason we have created all 2+ million possible pairings is to find some way to pair up all people in our experiment in such a way to be able to say whether a group of decorrelated paired people perform Task 2 better than a correlated group of pairs (As such we are not interested in individual pairs, but in a groups of pairs). This is the main reason we are interested in the tail ends of our distribution (as they contain the most correlated and decorrelated pairings).

Now, we fully understand that creating 2+ million pairs out of 16 people may lead to all sorts of problems with interdependence of the data, but I honestly do not see another way of getting to where we want to be. Obviously, we were hoping that the distribution shown earlier would yield some sort of analysis where we can conclude that yes, indeed decorrelated pairs on Task 1 lead to a better performance on Task 2, compared to pairings of correlated pairs.

As a small follow up, we have binned the 2 million pairings in to smaller bins (around 10.000) and directly looked at the correlation between paired correlation in Task 1 and paired performance in Task 2 and we find a nice negative correlation, showing that indeed the less correlated pairs lead to higher performance, as compared to correlated pairs. However, I am unsure whether any significance testing is meaningful here, cause a small trend in the data may become very significant due to the (still) high number of samples.

"
Best way to find loop lengths in data?,2,5,False,False,False,statistics,1497954400,True,"Hi everyone!

I have data for 100 000 machines which tend to work better for some time and then worse for a small time and then repeat.

I want to calculate ""good time"" length for every machine.

What would be a good approach to solve this problem?

Thanks!"
Salvaging the Pie,0,0,False,False,False,statistics,1497954982,False,
Statistical analysis of multiple similar datasets,1,2,False,False,False,statistics,1497956130,True,"So I'm getting my life back together after some hard times and I'm currently trying to finish up my bachelor thesis in which I made a certain simulation. I have everything working etc, all that's left is finishing up the results. Unfortunately I'm quite overwhelmed and I'm not sure how I would properly report on my data. I feel like this is quite a simple question that I should already know and me being overwhelmed is making me brainfart, but here we go:

I have 20 large datasets of simulations with 1 million timesteps in each. Of these 20 datasets half are simulations with setting 1 and the other half is without setting 1. Now I know how to analyze 1 single simulation in R and / or excel, but how do I do this over those 10 datasets at once? Each of those datasets has the exact same variables etc, but there is a clear difference in results between some of them. Do I just take the average of all 10 of them and then run a statistical analysis on that? Or is there some other way to do this?

I know how to do a statistical test in general and the dataset already includes data such as standard deviation per timestep etc. I just don't know how I would properly combine the 10 datasets into 1.

I sincerely hope someone could give me some advice / point me in the right direction :)

P.S. I also posted this on /r/HomeworkHelp ([link](https://www.reddit.com/r/HomeworkHelp/comments/6id7bi/bachelor_thesis_statistical_analysis_of_multiple/)) since I'm not sure in which reddit I should post this question, I hope that's okay!

**TL;DR - I have 10 simulations with the same settings, how do I analyze all 10 properly? Do I take the average of all 10 simulations and statistically test that or something else?**"
Tutorhelpdesk.com Offers Best Biostatistics Homework Service,0,1,False,False,False,statistics,1497957174,False,
Help please,1,0,False,False,False,statistics,1497957526,True,[deleted]
[Question] Pairwise deletion. Is it possible to adjust n in any kind of meaningful way.,0,3,False,False,False,statistics,1497963252,True,"I am using pairwise deletion to compute the correlation matrix of a data set. I think this approach is appropriate because:

* I have well under 10% missing values (~2%)
* I have only around 50% complete cases (so casewise deletion disregards too much data)
* Missing values are distributed evenly across cases and evenly across variables. (I have had difficulty running a proper test for MCAR as I have too many variables)

I am using the correlation matrix to perform a PCA and while I know there are no massive issues with the results, I am concerned that running significant tests based on the original n is not correct. Also I feel like I should be reporting some sort of adjusted 'post-deletion' n.

Is there any way to measure how much ""information"" (for want of a better term) I lose by using pairwise deletion compared to if I had a complete data set? In my case it does not really effect my result but I would like to know for in the future if I have data that has maybe 9-10% missing values and is MCAR. Should I be looking at some kind of imputation based method? Is there industry standards or rules of thumb?

Happy to hear opinions or be referred to papers/textbooks on this topic."
50 Statistics on the Latest Marketing Trends and Strategies for 2017,0,0,False,False,False,statistics,1497963774,False,
"Divorce rates continue slow rise, say Rabbinical Courts statistics - Israel News",0,0,False,False,False,statistics,1497964026,False,
Tricky stats problem,1,1,False,False,False,statistics,1497964478,True,[removed]
"Tabs, spaces and your salary",7,17,False,False,False,statistics,1497976437,False,
One-fourth,1,0,False,False,False,statistics,1497976828,False,
Calculate fits of Markov Switching AR (1) model,0,1,False,False,False,statistics,1497977529,True,[removed]
Comparing Results of Multiple Tests,2,0,False,False,False,statistics,1497978427,True,"Hello friends! I'm looking for material on how to compare something. Let's say I performed two tests on the same population about totally different subjects, like ""how much do you like fruit"" and ""how much do you like vegetables"". The goal is to match people who answered similarly in BOTH tests as accurately as possible. Example:

.

Fruit Quiz Z-Score for test takers 1-5:

1:  0.50
2:  0.52
3:  -1.56
4: -0.15
5: -1.00

.

Vegetable Quiz Z-Score for test takers 1-5:

1:  0.50
2:  0.48
3:  -2.56
4: -1.15
5: -2.00

.

In this example test takers 1 & 2 are very similar. But how might you determine this mathematically? What if I wanted to find the ""most similar user"" to test taker #5, who is different than his peers?"
calculate odds of being chosen,7,2,False,False,False,statistics,1497986772,True,"If i'm in a pool of 200 people and the police conduct 5 searches, what are my chances of being selected? Can someone help me walk through this? No, this is not for school, class, grades, or anything else."
Spearman's ranking scatter chart / normal standard deviation band?,0,1,False,False,False,statistics,1497989496,True,[removed]
Simplified definition of confidence interval,6,12,False,False,False,statistics,1497991272,True,"In ""Statistics in Plain English"", Timothy C. Urdan describes the concept of confidence interval to ""provide a range of values that we are confident, to a certain degree of probability, contains the population parameter (e.g., the population mean)""

Now, whenever CI comes up, if I recall correctly, there's often a debate along the lines of the idea that a population parameter is fixed so the probability is either 0 or 1, and a more elaborate definition is given; something like ""the confidence level indicates what percentage of confidence intervals are likely to contain the true population of the parameter when applying repeated sampling"" (I may be wrong)

Is Urdan's definition technically correct or simplified?"
Understand Propensity Score methods? Want to publish a paper with me? Medical related,0,1,False,False,False,statistics,1497991321,True,[deleted]
# of tests (samples) to increase reliability (software) with same confidence level?,0,1,False,False,False,statistics,1497996076,True,[removed]
Masters In Statistics but not sure if good enough,9,3,False,False,False,statistics,1498008985,True,"Hello, I wrote a similar post about grad school on the datascience page. I want to become a data scientist and I'm pretty sure I want to pursue a masters in Statistics since I want to study something more quantitative and apply it to data science. My concerns are I did not do well in some math classes that are prerequisites, I passed them, but not well, getting a C and C+ in calculus 2 and linear algebra. My cumulative GPA is also not outstanding (3.18). My plan is to take the GRE in 2 months and then see where I stand from there. I have a feeling I can find good recommendations since I know my linear algebra professor really well. My only concern is I haven't taken a lot of statistics classes (took probability in statistics with math department, got a B) and that I might not be good enough for other schools.One thing I thought about is taking the subject MATH GRE. I feel that if I do really well on that and start reviewing ahead of time I can overcompensate for some iffy math grades. I don't dislike math but I just had a rough finish to some classes I started well in. Also as far as looking for a school. I have narrowed down elite schools because I don't like my chances but are there other ways of finding programs that I would potentially be good enough for? If any questions are needed to fill in some context I'd be glad to answer."
Which statistical test should I apply in this biological experiment?,0,1,False,False,False,statistics,1498023080,True,[removed]
Explaining Confidence Interval for interview?,11,16,False,False,False,statistics,1498024641,True,[deleted]
Finding e text book,0,1,False,False,False,statistics,1498028910,True,[removed]
"In an econometrics/ststistical point of view, how does stable interest rates affect the biasedness of the estimates of the money demand equation?",0,3,False,False,False,statistics,1498033290,True,"so during monetary stability, interest rates will be somewhat constant, how does it affect econometric estimates of the money demand equation? does it affect the biasedness estimates at all? thx  "
"If a broken analog clock (stopped) is certain to be correct twice in a day, what is the probability that a broken digital clock (random change every minute) will be right once in a day? Twice in a day?",7,18,False,False,False,statistics,1498057835,True,
Looking for help,0,0,False,False,False,statistics,1498058757,True,"I was wondering if anyone would have time to give me expert advice on how they would go about analyzing a small dataset that I have. I have a new job and would like to know if I'm dropping the ball on my work and if it's something I can easily improve. It's truly basic- Small Population by a demographic, 10 year trends..."
Learning R as a Computer Scientist,0,3,False,False,False,statistics,1498062704,False,
Multiple choice test theory,10,1,False,False,False,statistics,1498065880,True,"So I was wondering if it is possible to apply statistics to boost a test score. I tried google but all I found was psychological. Let me give you a simple example: I answer 15 questions out of 20 and count the answers: 9A, 3B, 3C. Each right answer is worth 5 points, each wrong answer is worth -2.5 and non answered questions are worth 0 points. I know absolutely nothing about the 5 remaining questions, but I decide to answer anyway.

My question: how safe it is to assume that there won't be another A? I would like to think that every question is independent of the others, but since they all form part of the same test I would expect them to be ""generated"" at once, thus following a normal(?) distribution. Obviously I assume the answers are randomized by a computer, not a human. What do you think? Should I answer B and C to the remaining 5 questions, should I check my A answers because some of them may be wrong, or should I assume the questions are independent of each other?"
"Five dice, one roll probabilities",6,1,False,False,False,statistics,1498066032,True,"At my local pub, we have a dice game in which you put $1 into a jar, and you get one roll with five six-sided dice.  The goal is to exactly match the number on the side of the jar.  If your five dice match the number, you win 80% of the pot. (10% for bartender, 10% re-seeds the jar)

My question is, are there different probabilities of rolling different sets of numbers with a single roll?  Is rolling 1-2-3-4-5 a different probability then 6-6-6-6-6?  What are the probabilities?

Note: The number on the side of the jar is determined by the previous winner.  They roll the five dice to create the new set.  "
What is enough of a difference in LOO CV to justify keeping the more complex model?,12,2,False,False,False,statistics,1498076547,True,"I am comparing two multinomial logistic regressions with random subject and item intercepts, whose parameters were determined using a Bayesian approach. There are about 5000 data points, and the models I am comparing have one vs two fixed effects, and both have two random effects.

From my reading, it seems that leave one out cross validation is an acceptable way of comparing these models. However, I'm not sure what amount of difference in their standard error justifies retaining the more complex model.

Thanks for any help/pointers you can provide!"
How difficult is elementary statistics,7,2,False,False,False,statistics,1498098908,True,I'm not the best at math and need to take statistics for my social work major and an kinda terrified and want to know if I should be completely panicking or not. I've never taken stats or calculus either.
Preparing for a career as a statistician,0,1,False,False,False,statistics,1498102182,True,[removed]
"Don't say ""improper prior."" Say ""non-generative model.""",1,25,False,False,False,statistics,1498110377,False,
Histogram bins with different sizes?,0,1,False,False,False,statistics,1498120168,True,[deleted]
Why you should never use the Hodrick-Prescott filter,0,5,False,False,False,statistics,1498123971,False,
Calculating a weighted scale reliability?,0,0,False,False,False,statistics,1498125563,True,"I have a variable that is calculated by weighted variables V(AF) =
(3 x VA) + (2 x VB) + (1 x VC) + (-1 x. VD) + (-2 x. VE) + (-3 x. VF).

Each V(A-F) has it own alpha, A=.8 + B=.8 + C=.7 + D=.6 + E=.7 + F=.7.

How can i calculate alpha for V(AF)?

Thanks !
"
Data Scientist Resume Projects,1,27,False,False,False,statistics,1498138818,False,
Guys I really need some advice for my thesis on t-testing different data sets? What test do I use?,0,1,False,False,False,statistics,1498139929,True,[removed]
Really silly statistics question on T-tests vs ANOVA,22,16,False,False,False,statistics,1498141708,True,"Hey all,

So I have two groups: A group of high performers and a group of low performers.

Each of the groups completed a test that measures 52 different things. I am comparing each of these 52 things between the high and low performers.

So the data looks like this:

Performance | Score 1 | Score 2 | ... | Score 52

I'm running a T-test on each of the comparisons, but I'm worried I'm causing the possibility of an error. My thinking is, and I could be wrong, each time you run a t-test you increase the likelihood of an error. I'm effectively running 52 t-tests, fishing for which of the 52 tests comes out as significant.

I feel like I should be using an ANOVA or MANOVA  or some kind of correction, or perhaps I'm not using the right test at all.

Any help would be greatly appreciated!

"
Visualizing why we can't use a normal distribution for small sample size [OC],0,4,False,False,False,statistics,1498145094,True,"http://imgur.com/a/Z3m3P

Method: I generated a normally distributed population with some mean and variance. Starting with sample size of 2, I took random samples and computed the confidence interval using both a normal distribution and T distribution. I repeated this several times for each sample size and plotted each confidence interval, if the interval contains the population mean, then it is colored blue, otherwise it is red. For each sample size, the percentage of times the interval contained the mean is recorded. This recorded percentage should match the level of confidence we choose. As you can see from the intervals derived from the normal distribution, there is much error with small sample size.

[SourceCode](http://www.r-fiddle.org/#/fiddle?id=kcGi4aZ2)"
Help needed designing a statistical analysis problem,1,2,False,False,False,statistics,1498151890,True,"Dear Reddit,
My supervisors are away for a few weeks, and I am at a loss how to analyze a set of data. 
Basically, in my study, many listeners rated sound quality of recordings of music processed through multiple hearing aids (and different settings within each hearing aid).
There are differences between hearing aids and I already figured that out using an RM-ANOVA...easy peasy. 

Now, I am curious as to what electroacoustic factors about the hearing aids drove the sound quality ratings.
I have taken many electroacoustic measures of each recording. There are about 10 different measurements per recording. I suppose each of these measurements could be treated as a covariate to the DV (sound quality rating). 
I would like to determine which of the covariates explain most of the variance seen in the DV. 
Some of the measures produced values that fall on different scales. 
I've looked into PCA, but not sure how I can apply that to the DV. 

Any help it immensely appreciated!"
Market Correlations different in different currencies???,0,1,False,False,False,statistics,1498153993,True,[removed]
Can ANOVA/Dunnett's Correction be used with percentages?,0,1,False,False,False,statistics,1498154702,True,[deleted]
Creating groups to avoid confounding variables?,1,1,False,False,False,statistics,1498156799,True,"Hi, I've got an experiment that I'll be running soon and I'd like to try and minimize the effects of confounding variables as much as possible. One of the main factors I'm worried about is the size of the subjects, so I wanted to know, is it a good idea to create groups of test subjects that minimize differences in specific premeasured variables? I know confounding variables can be dealt with later during analysis, but if it doesn't cause issues with analyzing the data further down the line, it makes more sense to me to deal with the issue before gathering data rather than afterwards. If this isn't an issue, is there a framework already set up for determining the groups?"
Suggested Resources and Path to Build Foundation of Statistics?,0,0,False,False,False,statistics,1498157969,True,[deleted]
Performing treatment-control comparison within independent groups of a factor,1,0,False,False,False,statistics,1498160893,True,"Hi everyone!

For my analysis I've performed RT-qPCR and measured fold change of genes as log2. I've been trying to compare gene expression of treated (25 nm PSNP) and control samples across multiple independent genes. Since the genes are independent groups, I wasn't sure whether it would be accurate to perform an ANOVA, but I performed one anyway so I could still compare treatment and control for each independent gene at the post hoc analysis, and also include bonferroni correction for multiple testing.

So for the post hoc analysis I constructed the following formula:

TukeyHSD(aov(Log2~Treatment*Gene))

However, next to the Treated (25 nm PSNP) and Control comparisons, this formula also generates all possible combinations of genes and treatments which I don't want to include in my analysis.

Thus my 2 questions are: 1. Is it right to perform an ANOVA followed by a post hoc analysis to compare treatment and control groups of independent genes (groups)? I figured I could also perform an independent t test for each gene, but I didn't know how to correct for all independent tests afterwards in R. 2. How do I perform a post hoc statistical analysis in R where it only compares Treatment and Control for each indepent gene, instead of going through all possible combinations?

I've included a sneak peak of my datafile so you guys can perhaps get a better grip on my analysis.

https://i.stack.imgur.com/Kz81K.png

Thanks in advance!
"
What kind of courses do statistics majors take in the United States?,8,16,False,False,False,statistics,1498161330,True,
Help with this basic high school statistical question,1,0,False,False,False,statistics,1498161729,True,"It's a very basic high school statistical question, but I'm struggling to solve it.
 
Suppose I have a school with 287 students and each one made a test with 50 questions (multiple choice questions with 5 items each, they have to choose one item in each question). These questions are divided into the following subjects:

>Subject A - 8 questions

>Subject B - 6 questions

>Subject C - $10$ questions

>Subject D - 6 questions

>Subject E - 6 questions

>Subject F - 14 questions

We say a student fail the test when he doesn't solve any question. 

Then we have the following result:

>Subject A - 3 students failed

>Subject B - 16 students failed

>Subject C - 1 students failed

>Subject D - 1 students failed

>Subject E - 8 students failed

>Subject F - 0 students failed

So how can I compare the performance of the students? In another words, which subject were they best and which one were they worse?

**Remark:** Each student has only two options: successful or failure, so in this case the overall score in each subject is not important."
Has anyone ever seen binary logistic regression with -1 or 1 as the independent variable inputs?,12,12,False,False,False,statistics,1498162892,True,"Someone just asked me to interpret a binary logistic regression model where instead of using 0 and 1 as the input of the binary variables, they used -1 or 1.  Has anyone seen or used this before?  If so, does it have a name and what is the purpose?  If not, do you believe it's a mistake?"
"""Subtracting"" two distributions",2,1,False,False,False,statistics,1498163974,True,"So I have two data sets.  Both are binned velocities obtained from videos of worm movement, using movement tracking software (I'm doing behavioral analysis the model organism C. elegans).  I obtained graphs of the distribution by binning the velocities and plotting this as a histogram.  The distributions are approximately normal with a left skew.  One of them has been treated with a drug, resulting in a depressive effect on the mean velocity compared to the untreated group.  The depressive effect is very clear and well characterized, but data from out other experiments seems to suggest the presence of a slight activating effect of this drug.  The histogram of the treated group does suggest that there might be a slight locomotive activation, but its not exactly clear/difficult to discern.  What would be the best way to go forward?"
Level of measurement,0,1,False,False,False,statistics,1498182218,True,[removed]
Alternatives to Bonferroni Correction?,12,5,False,False,False,statistics,1498183553,True,"My company uses a frequentist testing tool to run AB tests. In order to account for family wise errors we've been using the Sidak-Dunn Bonferroni correction, but I feel like there could be a better solution. 

Does anyone have any recommendations?"
"Different formulas for finding the median, Q1 and Q3?",7,1,False,False,False,statistics,1498192744,True,"I'm doing the stanford online stats course and saw this passage in the section talking about interquartile ranges 

>Software packages use different formulas to calculate the quartiles Q1 and Q3. This should not worry you, as long as you understand the idea behind these concepts. Note that Q1 and Q3 as reported by the various software packages differ from each other and are also slightly different from the ones we found here. There are different acceptable ways to find the median and the quartiles. These can give different results occasionally, especially for datasets where n (the number of observations) is fairly small. As long as you know what the numbers mean, and how to interpret them in context, it doesn't really matter much what method you use to find them, since the differences are really negligible.

What are these other different even if negligibly so ways of finding those figures? Where can I read about them at any level of difficulty?"
What does variability data tell about a data set?,4,2,False,False,False,statistics,1498203060,True,"I calculated the range, standard deviation, and six sigma boundaries 

Upper boundary=mean+3(standard deviation)

lowerboundary= mean-3(standard deviation)

That data contained 60 observations of how long it took a bank teller to complete a process. With what I have found what generally will the range, standard deviation, or six sigma boundaries tell about a set?"
I'm quitting my current position (as the only statistician in my company) for another job. What steps should I take to make sure that my replacement (which I probably won't be able to meet in-person) hits the ground running?,20,56,False,False,False,statistics,1498216747,True,"I've been working on a research project for about a year now, but I've realized that it's time for me to move on. I informed the management when I got another job offer, and they were disappointed but took it well. I repeatedly stressed the importance that there is some overlap between me and my replacement, but they have decided to wait with new offers until after the summer, by which time I'll be out the door.

So given that I probably won't be able to communicate face to face with my replacement, what sort of steps should I take to make sure that he/she doesn't start from scratch? Here's my current to-do list:

* Finish up all of my current work

* Thoroughly document all the important ""home made"" R functions that are used in my workflow

* Give an overview of the most relevant R libraries that are used in my workflow

* Write a document detailing the results from the project so far and feasible paths forward. Include some domain knowledge/intuitions about the data that is generated in our lab. The domain knowledge I'm unsure about because we have a large volume of material that is not yet analyzed, so my ""sample size"" so to say is not that large.

* Create a library with some statistical theory that might be unfamiliar to the new person 

* Be available through e-mail and possibly skype, and also offer some external assistance with the work itself in the ""transitionary"" period.

Any other suggestions? "
Multiple regression question,0,1,False,False,False,statistics,1498222675,True,[deleted]
How to Determine Significance of a Simple Linear Regression Model,5,2,False,False,False,statistics,1498224299,True,"Let's suppose I have a bunch of data points in a vector Y. These data points are given by the equation Y = m*X + b + e, where m, b are known constants, X is the vector of integers from 1 to 10 (inclusive), and e is a vector of normally distributed errors (mean=0, sd=1). Now, I use some statistical software and create a linear model for Y. My question is this: what should I do to show whether my linear model is significant?

My statistics knowledge is pretty limited, as I'm only in my second university-level stats course. I am familiar with simple and multiple linear regression, ANOVA tables, confidence levels and intervals, f- and t-tests, and p-values.

So far, I've created the linear model with statistical software and looked at a statistical summary of the model (the p-value here is 3.79e-9, but I'm not sure what the p-value is doing here) as well as an ANOVA table. I've also done a paired t-test with Y and m*X+b (the p-value here is .014, which is not good enough for a confidence of alpha = 0.01, if I'm not mistaken). Additionally, I've done a two-sided t-test of the vector Y - (m*X+b) with mean 0. Doing that, I get the same p-value as the paired t-test (as I probably should have expected). I was told that with a vector of randomly generated errors (mean=0, sd=1), I should see that my regression slope is significant with a confidence of alpha=0.01. The t-tests don't indicate this, but the statistical summary does indicate significance (judging by p-value). What do I use to show the significance (or lack thereof) of my regression slope at the confidence level alpha=0.01?

Thanks in advance."
Help with Autocorrelation and Partial Autocorrelation,1,0,False,False,False,statistics,1498229594,True,"I'm trying to plot an autocorrelation and a partial autocorrelation graph in the free software Gretl for my econometrics class. I am incredibly confused on what these do and how to actually graph them. I understand that an autocorrelation is the correlation of the X variables on the Lag variables, but I'm kind of foggy on the rest. 

Here are some basic questions I have pertaining to a data set of around 1400 data points given for the nominal retail price of gasoline between 1991 & 2017:

1. When running a basic correlogram, if I have a total of 1,369 points, do i need to run it to have 1,368 lag points?

2. Can someone please explain what a partial autocorrelation does and what I need to do to interpret how many lags I need to run the function? I am incredibly confused around the whole idea of partial autocorrelation.

Thank you all so much for your help, I am an economics student who is trying to learn as much as he can in school. You all have always been kind and generous to me in giving me incite into statistics which I truly don't have a huge understanding of at this point in my college career."
"When presenting log-transformed data, how does one handle (show) the error?",1,3,False,False,False,statistics,1498234449,True,"I'm working with a dataset that shows the ratio of up- and down-regulation in genes across differently treated samples and their controls. I have an excel spreadsheet that compiles all the raw data (including several replicates) for a single treatment into a mean value that shows the increased or decreased (or unchanged) gene expression. It also propagates error throughout to accommodate the whole calculation. Resulting datapoints might be something like 4.0+SE0.3 or 0.25+SE0.01. 

Usually these data are presented in a bar graph on a log2 scale to show fold-changes, but if I similarly transform the error, I end up with very large error bars that don't present well on the graph. In the above example I'd have two points at +2 and -2, with error bars of heights of +/-1.74 and  +/-6.64. That seems crazy.

I actually don't like this particular calculation or data presentation, but it's sort of the standard among makers of quantitative PCR data. Data is usually presented as a bar graph (or sometimes lines for timepoints) with error bars. It's a frequent question/argument (how to present the error) among users generating this kind of data, and I'm trying to figure out why this particular calculation is the accepted norm, and how other people handle presenting this data. It's not always obvious from their published data how they arrived at their results. I rarely ever see huge errors that imply that they have actually log-transformed those numbers, so maybe they just don't, and present them as-is on top of the log2 means? That also seems hinky. 

Thanks for any insight."
Regression Model - 2 similar variables,4,1,False,False,False,statistics,1498240282,True,"Hello all,

I have run a poisson regression model with multiple independent variables.  Two of those variables are a numerical indicator (1,2,...,50) for states, and then I created a new variables (1,2,3,4) for Census Regions.  When including both variables in my model, the second variable listed is all zeroed out.  I understand this happens because it is basically a reformatted version of the first variable is presenting the same information in a different way.  But, I was hoping someone could explain the math behind why this happens.

Thanks!"
Sample Size Question For Audit,2,1,False,False,False,statistics,1498241606,True,"Here is my dilemma.  Lets say I work for a company that sells juice.  We are in about 500 locations on the East Coast.  Sales are bad and I don't know why.  I would like to audit my pricing, placement, stock, etc. within the store using a third party service.

What is reliable sample size for a project of this nature?  I can't afford to do everything, so what percentage of stores makes sense to look at?"
Regression Analysis of Time Series(RATS) software help needed,0,1,False,False,False,statistics,1498241921,True,[removed]
Multilevel Model with fNIRS (brain scan) data. Help!,0,1,False,False,False,statistics,1498243627,True,[removed]
Is this possible.,10,0,False,False,False,statistics,1498254173,True,Assuming you have the time to do so is it possible to roll a 1 on any dice an infinite number of times in a row? if so what are the odds.
Individual predictors and dyad outcome?,1,2,False,False,False,statistics,1498255321,True,"Hi all,

I'm looking for help on analyzing a set of data that has 112 couples. Each of the 2 people in the couple has 3 individual measurements (Depression, age, and optimism). Each dyad has one outcome measurement (Marital satisfaction). I want to measure the relative influence (explanation of variance) of each individual variable on the dyad outcome for the 112 dyads.


Any help is appreciated! I'm happy to provide more detail too."
What do you use to organize unwieldy datasets?,6,1,False,False,False,statistics,1498261798,True,"I joined a lab a few months ago and am in a position to make a positive impact. 

To put it bluntly, the datasets are a nightmare. Each study has an Excel spreadsheet has a row for each participant and roughly 1,500 columns worth of data related to that participant. You have to scroll horizontally for a decade to get to the end. I frequently lose track of where I am and have to double and quadruple check that things are in the right place. We have a policy that you have to do data entry in pairs to cut down on entry mistakes. 

I'm hoping that there's a software solution that can make this less of a nightmare. It doesn't have to be free, but it can't be an enterprise solution that costs $999/mo. Something simple enough so that if I leave, someone else can use it without much training. 

Any ideas? "
What is more important -- Knowledge of Bayesian statistics or Big Data methods?,0,1,False,False,False,statistics,1498273178,True,[removed]
How To Get Help with Statistics,0,1,False,False,False,statistics,1498283921,False,
Should I stay or should I go? What to do if you lose your friends at Glastonbury.,6,32,False,False,False,statistics,1498301419,False,
Difference in Differences Estimator Evaluation,0,1,False,False,False,statistics,1498305111,True,[removed]
Do you agree with this WLS regression video (7 min) for SPSS?,4,0,False,False,False,statistics,1498305623,True,"I'd like to perform a WLS regression, would you say this is an acceptable method?

https://www.youtube.com/watch?v=enPK_SXILnA&t=5s"
FIFA Ranking Algorithim!,0,1,False,False,False,statistics,1498330075,True,[removed]
What do I need to apply for PhD in biostatistics?,0,1,False,False,False,statistics,1498374510,True,[removed]
Multiple Regression: How many dummy variables should I create per factor levels?,0,1,False,False,False,statistics,1498401890,True,[removed]
Discontinuity analysis on stata?,1,0,False,False,False,statistics,1498423102,True,"Is there a way to do this analysis using stata? If not does anyone know a program that can do this? I'm hoping for a ""plug in the numbers and out goes the results"" approach since I'm a newbie in statistics. Thanks guys"
PhD in Statistics Advice,0,1,False,False,False,statistics,1498428498,True,[removed]
Binomial forecasting,0,1,False,False,False,statistics,1498429633,True,[removed]
Help to study these,3,1,False,False,False,statistics,1498432527,True,"I have 5 full days to  study for an exams with 
chapters as follows: 
Introduction to Statistics; Type of data; Measurement and scaling techniques; Data collection 
and data preparation; Graphical representation of data; Measures of location and dispersion; 
Basic probability; Distribution of random variables: Binominal and Normal distribution; 
Sampling distribution and interval estimation.

Can you please suggest me on which Website to go? And anyother tips in regards to these chapters?

"
How to model this type of data? Please help!,5,9,False,False,False,statistics,1498436993,True,"I have been wracking my brain for several weeks now, trying to figure out the best way to model the following type of data:

I am helping to set up a clinical trial. We are testing a new type of sealant for sealing wounds after lung surgery, and comparing it to an existing sealant on the market. Since this is a lung surgery, residual air may be leaking out after the surgery is over, even after applying a sealant. Patients can experience an air leak after surgery (this is fairly common), but they also might not. The **duration of this air leak** is our endpoint of interest. 

An air leak is a negative symptom, therefore a sealant is better if the air leak duration is shorter. Intuitively, a lower incidence of air leak is also better, though a simple analysis (e.g t-test) of our current chosen endpoint (duration of air leak) may not reflect this additional information.

Additionally, patients may be sent home while still experiencing an air leak. Therefore, some observations may be censored.

As you can see, it's not a traditional 'time-to-event' analysis, since the duration itself is conditional on whether patients experience the air leak. It's almost like we have a double time-to-event analysis... 'time to air leak' (And corresponding censoring if no air leak occurs) and then 'duration of air leak'.

Also, I think it's unwise to treat as a simple continuous endpoint (as most papers have done in their analysis) as you lose the information regarding the amount of patients actually experiencing the event. 

One last thing - I would like to present the result as a difference in median duration and corresponding 95% confidence interval (rather than hazard ratio or whatever), since this 'difference in duration' is what I based my sample size off of. 

Do you guys have any suggestions on how to best model this endpoint? Are recurrent event analysis methods appropriate here (e.g Anderson-Gill)? Is there some weird zero-inflated survival analysis? Am I overthinking this, and should I just treat it as continuous?

If you have any ideas or suggestions I would REALLY appreciate to hear them :) Thank you!"
Amateur here. Please help me analyse exam results from a cohort of students.,2,1,False,False,False,statistics,1498437958,True,"Hi everyone!

I'm writing a simple calculator to determine the required exam score for a certain subject score in my state's high school education system. The data I have at my disposal is something like this:

External Assessment X
Grade range: UG E E+ ... A A+
Mark: 2 5 10 ... 95 98
Percentile: 0.2 0.9 2.1  ... 91.6 98.4

The assessment authority also provide a mean and standard distribution for the data, but this seems useless as the graph is often very skewed.

This data is provided for each external assessment for each subject. This is fine for most subjects, as they only have one external assessment. I've implemented cubic spline interpolation on the data using Python, so  I can take any exam score or subject score and find the other.

My issue is with subjects that have more than one external assessment. Let's say a subject has two, and I know the data for each assessment. A student uses my calculator and indicates that in the first exam (worth A% of the subject) they were in the Xth percentile, and in the second (worth B% of the subject) they were in the Yth percentile. How can I determine what percentile of the overall cohort for that subject they belong to? Is it as simple as aX+bY? Is that at least a good approximation?"
"[Crosspost r/suggestalaptop] Dual- or quad-core CPU for ""light"" statistical computing in R",7,4,False,False,False,statistics,1498443991,True,"As the title suggests, I am wondering about whether or not it is necessary to get a quad-core CPU if I will not be handling extremely large datasets (< 1GB). Any information would be greatly appreciated. FWIW, I have been looking into the XPS 15 9560 (I7-7700HQ) vs. MacBook Pro retina 15"" (mid-2015; I7-4980HQ) OR XPS 13 9360 (I7-7560U) vs. MacBook Pro 13"" 2016 (the one with no touch bar; I7-6660U).

My apologies in advance if this is an inappropriate post for this sub.
"
Tutorial: How We Productized Bayesian Revenue Estimation with Stan,0,27,False,False,False,statistics,1498461039,False,
Are they any shortcuts to calculating degrees of freedom of a structural equation model?,2,2,False,False,False,statistics,1498496281,True,"I'm reviewing a bunch of SEM models for a class I'm TAing, and I'm having to individually count all of the paths and variables and such to figure out if they calculated the df correctly. Does anyone know of a way to speed this up?"
Variance calculation question..,0,1,False,False,False,statistics,1498497019,True,[removed]
How to test that g groups of exponentially distributed random variables have the same rate?,2,5,False,False,False,statistics,1498498105,True,"Suppose that y_{ij} are independent random variables (with mean 1/\theta_{i} for i=1,...g and j=1,..,n_{i}. How do we constuct an exact test for testing \theta_{1}=\theta_{2}...\theta_{g}?

For example in the case g=2 we can construct a test based on the fact that (\theta_{1}\bar{y}_{1})/(\theta_{2}\bar{y})_{2}~F(2n_1,2n_2) so under the null-hypothesis (\bar{y}_{1})/(\bar{y}_{2})~F(2n_1,2n_2) from which we can construct a two-sided test. 

I am aware that the log-likelihood ratio statistic is asymptotically distributed as a chi square random variable with g-1 degrees of freedom under the null-hypothesis but I am looking for an exact test or something which is more accurate than the chi squared test. The exact distribution of the log-likelihood ratio or whether or not it is a one-to-one function of a simple statistic whose distribution can be found readily is not clear (at least when g>2).

**Ideally is there something analogous or similar to Bartlett's test/Levene's test for testing the homogenity of rates?**"
New here ... How many samples to take ... ?,8,6,False,False,False,statistics,1498499849,True,"I'm running a data collection at work... and am new to this (and could use some help!).  
  
I have a piece of machinery that is ""supposed to be good"" for 50K cycles before failure due to a specific type of error.  

I'm trying to determine *the fewest number of samples* I can take to show whether or not this is true (under a number of representative operating conditions) to a certain level of confidence (not necessarily a ""confidence level"", i'm not being rigorous in my terminology because I know I need guidance).    
For example:  

* Action A for time length 1 - 7  
* Action B for time length 1 - 7  
* Action C for time length 1 - 7   

How do I figure out how many samples of each test case to run in order to optimize the number of test cases? (I don't want to have to run a test for 50K cycles ... and I'm not sure we need multiple pieces of machinery either).  

What do I need to read in order to figure this out?  What am I not thinking about?

EDIT:  
Follow up question ... is this the best way to design this experiment?  
The above actions and lengths of time are meant to be representative of a theoretical use case at a customer.  No hard data has been gathered as to *actual* use cases (it's not something that our company keeps for this product) and it's not something I can get time to do since we have a diverse set of customers with broad use cases.  "
How to effectively explain a histogram with density estimates to a layman?,6,1,False,False,False,statistics,1498502098,True,"I was hired by a company to analyse some data, and was asked to present the results to a few people that aren't really used to looking at graphs.

Usually, I'm quite able to explain some harder concepts such as shrinkage estimates, or multivariate regression, when I have to. 

However, when I showed them a histogram with the estimated density, I stumbled quite a lot in my explanation. So, how do you guys do it? :D"
Analyzing number draws,1,2,False,False,False,statistics,1498512673,True,"Hi,

I'm doing a bit of research on my own about biased lottery-style number drawings. I won't mention the specifics, but let's say I have a (small) dataset of 50 past drawings and would like to find out if there is any bias in the numbers - I have reason to suspect there is as it is a special kind of online drawing. 

What kind of methods should I use on the dataset? Obviously the first baby thing would be a histogram of number of draws per number and see if any of the specific numbers' columns stick out. Any more sophisticated methods? Would be good to have a predictive method and compare to next couple of draws. Any good science papers on this?"
Regression on Percentages,2,2,False,False,False,statistics,1498514880,True,[deleted]
Conceptual Statistics Questions,0,1,False,False,False,statistics,1498518518,True,[removed]
"'i before e, except after c' has more exceptions than the rule",14,54,False,False,False,statistics,1498518619,False,
"[Survey- 2 min] Travel Habits (18-35, Male/Female, English Speaking)",0,0,False,False,False,statistics,1498529332,False,
World Statistics!,0,0,False,False,False,statistics,1498532761,True,[removed]
Boxing. Why is the spread so close on the Mayweather Mcgregor fight?,0,0,False,False,False,statistics,1498541776,True,[removed]
statistics problem solver,0,0,False,False,False,statistics,1498559958,False,
Need some guidance here!,13,2,False,False,False,statistics,1498563123,True,"Hi guys

I'm currently developing a research study on hospital infection. For this purpose, I entered the collected data into SPSS. However I'm completely lost and need some guidance regarding what statistical tests should I use to verify my inital hypotheses.

I have variables like ""Years of Experience"" (scale), ""Disinfect equipment"" (nominal, yes or no) or ""Colonized Equipment"" (nominal, yes or no).

If I want to check if there is association between ""disinfecting the material"" (nominal) and ""colonization of the equipment"" (nominal), is it correct to use the chi square test?

Imagining that I want to check if there is an association between years of experience (scale) and the colonization of the equipment (nominal), what is the most correct test to do it?
It is more advantageous not to dichotomize the variable ""colonized equipment"" and to use this variable with the actual values ​​of microorganisms that I found - becoming a scale variable? If yes, which test would you use for the previous example?

Sorry for the big post and thank you for your time"
Homework1.com Offers Best Statistics Homework Help Online Service,0,1,False,False,False,statistics,1498565524,False,
Interpreting Odds ratio from squared term in logistic regressions,1,1,False,False,False,statistics,1498567990,True,[removed]
I believe my professor is committing research fraud,65,73,False,False,False,statistics,1498572862,True,"Dear r/statistics,

A few strange incidents have led me to believe that my university professor (social psychology) is committing large-scale research fraud. However, before I start making accusations that most definitely will damage his persona, I want to add some empirical evidence to my claims.

Does any of you know a way how I could verify the results that he reports in his research papers, without having access to the underlying data?

Thanks in advance! :)"
Quick questions about graphing percents,0,1,False,False,False,statistics,1498573761,True,[deleted]
Python script that generates all Translog production function product variables (x-post from r/econometrics),0,5,False,False,False,statistics,1498575612,True,"Hi /r/statistics!

I've created a Python script that for a given number of production factors (labour, capital etc.), will generate all possible combinations (cross and own products) and then create the Stata command to generate the natural log of all of these products, necessary for a Traslog production function

I found that this was getting a real pain when coding do-files, so I decided to try and automate it. You can find it here:

https://github.com/davidptclark/translog_combinations

Please do let me know if you have any suggestions or comments, I'm always happy to receive feedback!
"
Spatial Smoothing in R,4,3,False,False,False,statistics,1498578098,True,"I've working on a disease and calculating incidence in areas where there are some pretty small denominators, giving me a pretty unstable variance. In GeoDa, you can calculate [smoothed rates](https://geodacenter.github.io/glossary.html#s). I'd like to keep my work all in R but I can't figure out how to do the same outside of GeoDa.

Essentially, given spatial data, I need to ""borrow"" some of the population from nearby counties so that the rates for counties with smaller populations (which may be artificially inflated due to the small denominator) are shrunk toward the global mean. 

It looks fairly complicated but I'm wondering if any of you have done this?"
Difficulty with what stats test to use,2,1,False,False,False,statistics,1498582512,True,[deleted]
Determine rating function for severity measure,2,1,False,False,False,statistics,1498587390,True,"I have data of a medical experiment and want to run a linear regression. More precisely, I have severity degrees of a disease of two independent groups (treatment and control group). The severity degrees are categorical with values {0,1,2,3} (0 meaning no disease, whereas 3 represents a widespread of the disease). In each session, a number of samples is taken from a subject. Next, the samples are rated according to their severity degree. These ratings are averaged over all samples taken and recoded into percentages. Hence, results of two subjects could look like this:

    -----------------------------------------
    | Group     | 0     | 1   | 2   | 3     |
    -----------------------------------------
    | Control   | 0.195 | 0.5 | 0.3 | 0.005 |
    | Treatment | 0.499 | 0.4 | 0.1 | 0.001 |
    -----------------------------------------

For the regression analysis, I want to code the severities into a single score. A first thought would be to sum up severities 1 to 3. That is, the score of the first row of the table would be `score(0.5,0.3,0.005) = 0.5+0.3+0.005 = 0.805`. But this is not a good score function since it disregards the ordinal nature of the severities (e.g. `score(0.005,0.3,0.5)` would yield the same value even though its greatly more severe).

The next idea is to weight the different levels, i.e. `score(s_1,s_2,s_3) = w_1*s_1 + w_2*s_2 + w_3*s_3` (in case of a linear function). Obviously, the weights should satisfy `w1 <= w_2 <= w3`. Is there any good (maybe even common) way to determine such weights?"
Need help understanding the math behind factor analysis. What does the I mean in Cov(F)=I? And how does that make sure that factors are uncorrelated?,3,2,False,False,False,statistics,1498591704,False,
Can unbiased estimators give impossible results?,0,1,False,False,False,statistics,1498591873,True,[removed]
Anyone have a good resource for interpreting HLM7 output?,0,1,False,False,False,statistics,1498595114,True,I'm especially looking for literature or resources on how to interpret the moderating effects of level 1 and level 2 variables on a level 2 variable and outcome variable. Any resources would be appreciated!
Regression,0,1,False,False,False,statistics,1498596411,True,[removed]
Question about trial level analysis using brms (or lme4) in R.,0,1,False,False,False,statistics,1498601457,True,[removed]
Building a Time-Series Model Where Predictor Variables Depend on Each Other,7,10,False,False,False,statistics,1498631472,True,"Hello,

I'm building a model for a project to predict monthly sales of several products based on common attributes. A problem that I'm having problems wrapping my head around is that the outcome of monthly sales data for individual products is dependent on the the sales data for the other products. I.e. The number of hours worked on Product X or spent on marketing is likely somewhat determined by Product Y. To give an example using basketball:

Say I want to predict (very poorly) the points that two players, John Wall and Bradley Beal, score in a given game using 4 variables: 2 game lags for points and minutes played. So the data set looks like:



Game | Player | ActualPts | Lag1Pts | Lag1Min | Lag2Pts | Lag2Min
---|---|----|----|----|----|----
3 | Wall | 25 | 15 | 30 | 25 | 32
3 | Beal | 18 | 30 | 28 | 12 | 26
4| Wall | 45 | 25 | 33 | 15 | 30
4 | Beal | 13 | 18 | 30 | 30 | 28
5 | Wall | 25 | **45** | **35** | 25 | 33
5 | Beal | 25 | **13** | **10** | 18 | 30

The bolded data is an example of what I'm seeing in my project, where John Wall's points are a function of Beal's minutes (and not necessarily Wall's own minutes), because when Beal is off the floor then the offense has to go through Wall. So when Beal plays less, Wall tends to score more with otherwise the same number of minutes. I.e when Product Y gets no budget for marketing, we can expect it to go through Product X thus X sales will increase.

So what I want to learn more about is how to incorporate this affect into my regression/model. In the Bball example, would I add a variable such as ""other player's minutes"" for each lag? This doesn't seem like it would help because I still wouldn't know how much each player plays before the game is played. 

Say I did separate analysis to come up with an ""estimated minutes played"" column for each player before their game. When I would eventually predict on the dataset, would I want to change the ""estimated"" to ""actual"" except for the most current date (we'd need to keep the estimated because the actual hasn't happened yet)? Or keep the whole column as what would have been estimated so the model takes into account |estimated-actual| difference?

I'm midway through *Introduction to Statistical Learning with Applications to R*, and have a suspicion that dealing for dependent predictor variables and other nuances will come up during the latter pattern recognition/machine learning chapters. **However, I would still greatly appreciate it if someone could at least give a high-level and intuitive explanation on how one would account for dependent predictor variables (factors?) to get my head to stop spinning a little bit.** I've worked with linear and linear-transformed regression models for a long time, but never machine learning models.

Thanks!"
How many times should you roll a die to know its probability distribution?,5,22,False,False,False,statistics,1498637388,False,
STATS 4 INDIA : Any good statistics websites for Indian data? (eg. inside),3,5,False,False,False,statistics,1498641137,True,"Hi guys,

I was looking for some website that shared good statistical data, like sales numbers of phones (by brand/model), revenue of different businesses, funding sources of political parties etc. I noticed that while there's lots of data globally, there isn't a good one stop website for Indian data. 

All I get on googling is news figures on newspapers with often very poorly presented data (wall of paragraphs where a graphic would have been so much better), and only data limited to a particular event, not organized searchable stuff.

I saw this awesome site called Statista, but it's behind a huge paywall. I have some examples that I found good below:

1. https://www.statista.com/statistics/269487/top-5-india-smartphone-vendors/

2. http://gs.statcounter.com/

3. http://www.idc.com/promo/smartphone-market-share/vendor

Would you be kind enough to suggest a few? Generic stat sites would be good, otherwise if you have any favorite ones for tech items, finances, etc... those would also add value.

Thanks in advance."
Voter segmentation (political marketing),0,1,False,False,False,statistics,1498659353,True,[removed]
"Correlation analysis on impact of pricing, commission and lead time of quotations",3,3,False,False,False,statistics,1498663550,True,"Hello,

I'm currently working on a project where I am trying to see the correlation of pricing, commission and lead times impacting a conversion of a quotation to a sale,

Items where a quote was converted to a sale is fairly simple and I am able to reference the quoted item to the billed price, lead time and commission assigned,

However I am a bit lost with regards with what to do with the quotes that never become a sale, how do I analyze the impact of pricing, commission and lead times for quotations that never become a sale (assuming that one of those three factors are what broke the camels back for the customer)

Any help with how to flesh this out would be appreciated

"
Highway Safety: National Academies Committee Recommends Using Item Response Theory Models,2,7,False,False,False,statistics,1498663759,False,
What's the difference between a confidence level and 3 sigma? Are they essentially the same thing?,1,2,False,False,False,statistics,1498664861,True,[deleted]
is this the appropriate way to test whether the distribution of random slopes is significantly different from zero?,3,2,False,False,False,statistics,1498692658,True,"I've constructed a mixed effect model in R to test for a fixed effect of condition and fit random slopes to each subject, using the unique task solution for each subject (each subject has a different task solution). It looks like this:

mdl <- lmer(dv ~ condition + (solution|subject), data = E1_data, 
REML = FALSE)

I know that to test whether the fixed effect is significant, I would test against a null model to obtain a likelihood ratio, like this:

mdl.null <- lmer(dv ~ (solution|subject), data = E1_data, 
REML = FALSE)

anova(mdl.null, mdl)
"
How do I assign a probability distribution to all combinations (4500+) of a single variable?,5,7,False,False,False,statistics,1498692767,True,"Hello all. 

I'm trying to simulate the delivery times for a fast food restaurant (i.e. the time it takes the delivery guy to reach the client from the restaurant).

The locations of all clients are put in clusters called ""sectors"". These sectors are like neighborhoods, so all orders that fall in the same sector are assumed to have the same delivery time. Since taking a shortest route problem is out of the question, I have to simulate the time it takes to reach each sector (for which I do have the data for).

The problem is, each restaurant covers 300+ sectors, and then if we take into account that traffic levels vary across the day (say each hour, so from 6:00 AM to 9:00 PM you would have about 15 time intervals), we get 300*15 = 4,500 different combinations. And this is without even taking into account the different days of the week.

So my question is: how can I even begin to assign a probability distribution for each one of these combinations? Is there a way to make it faster?

Thanks in advance."
What is the best correlation test for three multinomial data sets?,0,1,False,False,False,statistics,1498698829,True,[removed]
Factor Analysis - Why would all my factor's variables be negatively loaded?,0,1,False,False,False,statistics,1498714696,True,[removed]
How do I calculate standard deviation in this situation?,7,5,False,False,False,statistics,1498726559,True,"Example Data Set: [Control] {1000, 1500, 2000}, [Experimental] {400, 500, 600}. STDEV-Control = 500, STDEV-Exp = 100. Relative Survival = 500/1500 = 33%

I performed an experiment that measured the cell survival of 2 groups - a control and an experimental. Each group contains 3 measurements, meaning there is a standard deviation for each group. However, I want to calculate the relative cell survival of the experimental versus the control and represent this graphically. So, I take the mean of the experimental (500) and divide it by the mean of the control (1500). I then can plot a bar graph where the control's relative survival =1 and the experimental's relative survival = 0.3333. However, I need to add error bars. What is the new standard deviation associated with these relative survival numbers? Do I need to use standard error? Is this even possible to calculate?

Any help is greatly appreciated! "
How do I perform a Confirmatory Factor Analysis in R?,10,13,False,False,False,statistics,1498739451,True,"**Some background:**

I'm doing a bit of research in the context of my bachelor thesis. 
I'm studying the effects of privacy on the acceptance of location based systems. 

http://imgur.com/a/lWDMl

Here is a schema that describes the relationships I wish to study.

Privacy is hard to define, in my framework I define privacy based on some activities that can form a source of concern. Each of these activities - information collection, processing, spreading, and invasion - is measured using a certain number of items. 

The intention to use location based services is also measured with three separate items.


**What I want to do:**
Is to confirmwhether these variables that measure privacy can be reduced to the attached variable ""privacy concern"". Then I want to see if there is a significant correlation between the intention to use lcoation based services and the confirmed/reduced factor of privacy concern. The aim being to see if privacy concern has a signficant impact on the intention to use Location based systems.

**My questions:**
Is a confirmatory  analysis the right tool to confirm the reduction of those variables to privacy concern? 
What tests are the most important? 
Because i have been drowning in the factor analysis literature.  
What software/packages do you recommend? 
I have been mostly working with Rstudio and tried to get Lavaan working for me but I have had some struggles. 

Would a simple correlation between privacy concern and intention to use LBS suffice to prove my point?

I have been stuck on this problem on a long time and I don't know anyone who's familiar with factor analysis. I don't want you to solve my problems but I need a push in the right direction. 

Thank you for your time."
How can I get into statistics without a Stats degree?,0,1,False,False,False,statistics,1498750595,True,[removed]
Help with Difference-in-Difference-like regression,4,1,False,False,False,statistics,1498760021,True,"For my thesis in Finance, I want to research if mergers cause the merging firms to achieve higher Returns on Assets relative to their industries' Return on Assets. Specifically, I want to find out if this result is stronger for mergers in an oligopolistic industry than for a non-oligopolistic industry.

To do this, I constructed a database with mergers from both oligopolistic industries as well as non-oligopolistic industries, with their Returns on Assets for the five years preceding the merger and five years following the merger. I also have a few control variables (not relevant) as well as:

a) dummies that indicate whether the merger is in an oligopolistic industry

b) dummies that indicate whether the observation (RoA) is pre- or post-merger

The dependent variable of my regression will be `ROA(firm) - ROA(industry)` (outperformance of the company to its industry). How can I now run a regression to find out if oligopolistic mergers cause a higher relative return increase than non-oligopolistic mergers? Can I just create an ""interaction dummy"" that multiplies the ""oligopolistic"" and ""post-merger"" dummy and run some sort of Difference-in-Difference analysis like below and then look at the β3 coefficient?

ROA(firm) - ROA(industry) = α + β1 D(oligopolistic) + β2 D(post-merger) + β3 D(oligopolistic * post_merger) + (control variables)

I hope my question is clear and someone can help me. If anything is unclear please let me know. Thanks in advance!"
"What to do when multiple analyses (n = 25) show that difference in means is not significant, but test mean was higher than control in all cases?",13,9,False,False,False,statistics,1498760937,True,"Hey! I was wondering if there was a way to compare multiple test-control groupings together. 

Essentially, out of 25 test-control groups, the test mean was consistently higher than the control mean (albeit a very small amount), but each time, the p-value was not significant. Is there some kind of way to measure the probability that out of all 25 tests, the test group always had a higher mean? Or would it be correct to assume that had I repeated this test another thousand times, things might balance out?"
differences-in-differences: how many post-test periods needed?,8,1,False,False,False,statistics,1498761252,True,"Hi,
I have a study with 3 pre-intervention time points (i.e. pretest) but only one post-test.  I want to use a differences in differences design to compare whether trajectories of the DV differed to a greater extent than would have been expected with the passage of time (DV is mental health in college students so there is possibly a developmental aspect)

thanks in advance for any help!"
Social Networks Info.,2,1,False,False,False,statistics,1498761468,True,[removed]
Need: Explanation for Statistical Significance,0,1,False,False,False,statistics,1498766290,True,[removed]
What is the best alternative test for F test and t test for two independent sample population?,0,1,False,False,False,statistics,1498771287,True,[removed]
"Is there a way to determine the ""most useful"" (discriminating) variables?",0,1,False,False,False,statistics,1498774614,True,[removed]
computing area under the standard normal curve using R.,5,0,False,False,False,statistics,1498795375,True,[deleted]
Choosing sample sizes for genetic analysis?,2,7,False,False,False,statistics,1498804765,True,[deleted]
Which test do I use?,0,1,False,False,False,statistics,1498836278,True,[removed]
Is there a 0 percentile or a 100 percentile?,3,1,False,False,False,statistics,1498837842,True,"Hi,

I am not a statistician. I am a business intelligence developer and I am helping someone in my company deal with percentiles and building a quartile based report. Basically, I am taking those percentiles and breaking them into four parts. This is simple when there are 100 percentiles, however that is not what I'm dealing with.

The percentiles in the database range from 0-100 (or 101 unique values). This seems impossible to me, but I'm not enough of a statistician to determine if the ranges should ACTUALLY be 0-99 (where I would treat a 100 value like a 99) or 1-100 (where I treat a zero value like a one). Can someone smart here lend me a hand?"
"I analyzed some PGA Tour statistics to see correlations, hope you find this is interesting!",6,24,False,False,False,statistics,1498838933,False,
Looking for book suggestions,6,7,False,False,False,statistics,1498846279,True,"First off I don't really know if I'm allowed to post something like this but I try anyway.

I'm going to uni in a couple months to study statistics. I've got a lot of spare time in the summer so I would like to prepare myself  and get motivated.

I'm genuinely interested in maths&statistics and I'm looking for something to read. I've already gotten the textbook for the course and a book about learning R. What I'm asking for is some more ""general"" or ""interesting"" book in the contrary to the informative, boring course literature. Something to read when I just chill.

I don't really know what I'm looking for so any suggestions are super welcome! (Even boring informative books if they are exceptionally good) :D"
Help: Bayes' Formula Example,0,1,False,False,False,statistics,1498877244,True,[removed]
"In medical terms, what is considered a high prevalence?",0,1,False,False,False,statistics,1498877315,True,[removed]
Statswork- Math it | Synchronized Process | Task Execution and Statistical Data Analysis Services,0,0,False,False,False,statistics,1498884743,False,
21 Must Read Books on Statistics,6,6,False,False,False,statistics,1498885844,False,
25 Online Advertising Stats For Your First Marketing Campaign,0,1,False,False,False,statistics,1498894730,False,
A question about logistic regression and independent variables.,0,1,False,False,False,statistics,1498905327,True,[removed]
Is the wording of this newspaper article statistically correct?,4,6,False,False,False,statistics,1498912268,True,"I'm reading a article on the level of university participation by ethnicity. 

http://www.telegraph.co.uk/education/educationnews/11987142/Ethnic-minorities-more-likely-to-go-to-university-than-white-working-class-British-children.html

A dataset is given on 'Percentage of university participation by ethnicity' and then this statemement is made....

""Overall, Chinese children are 75 per cent more likely to attend university than a white one (32.6 per cent). Separately, a Bangladeshi child is 48.8 per cent more likely to attend while Pakistani children are 44.7 per cent more likely to do so.""

In my mind, because 75% of chinese attend university and 32% of whites attend, this does not mean chinese are 75% more likely than whites to attend. 

Is this like me saying 4% of my sweets are blue, but 6 of my cakes are blue. Therefore cakes are 6% more likely to be blue than sweets?????

Shouldn't the answer be something like, cakes are 2% (i.e. 6-4 = 2) more likely to be blue than sweets, and chinese kids are ~35% more likely to go to uni than white kids.

Thanks"
Methods for converting standardised regression (beta) coefficients into correlation coefficient (r) in a meta-analysis,1,0,False,False,False,statistics,1498914421,True,[deleted]
How can I give these accounts a rating based on these two independent variables?,0,1,False,False,False,statistics,1498923227,True,[removed]
Need help running a 2x2 repeated measures design with lmer in R. trying to prepare for MICE.,0,7,False,False,False,statistics,1498932220,True,"I am working on some field data that has a few problems requiring me to learn some new statistical techniques (and to use R at the same time -- I am a novice).  

For this project, three dependent variables were collected in a 2x2 repeated measures design.  Since there are a few missing data points I will need to eventually run a multiple imputation procedure (e.g., mice).  From what I have read, I will not be able to use the pooled results in a traditional repeated measures ANOVA so I have been trying to figure out the correct way to implement a linear mixed model using R (lme4).

Since I will not be able to run a repeated measures ANOVA, I'd like to use a linear mixed model in an attempt to make similar statistical comparisons.

As a starting point, I've been working with some of my older data from a similar design.  This data is in a 2(condition) x 2(location) design with reaction time as a dependent variable.  To start, I built my dataframe in long format and used .5 and -.5 as an attempt at using deviation coding for the levels of both factors.

the first model I was working with is as follows:

    m1 <- lmer(rt ~ condition*location + (1|subject), olddata)

after performing some more searching, I attempted this second model which, when I ran an ANOVA on the model, produced F ratios (for condition, location, & condition*location) similar to those I found from my previous data

    m2 <- lmer(rt ~ condition*location + (1|subject) + (1|condition:subject) + 
        (1|location:subject), olddata)

Is this second model what I should be using?  or am I incorrect?

I will probably have a few more questions as I muddle my way through this process.  I still need to use this with my imputed data.  I will also need to expand on this to include a few between subjects comparisons with the imputed data (adding a control group and then comparing performance this special population with the control group).  I also will need to work out if this procedure can be used with a Poisson distribution since two of the DVs from the current study are count data.

any help or advice will be greatly appreciated.

edit:  I deleted a few entries from the main data set to attempt a MICE procedure.

I ran:

    imp <- mice(olddatap , m = 50 , maxit = 50 , meth = 'pmm', 
        seed = 245435)

Then I ran:

    done <- with(imp, lmer(rt ~ condition*location + (1|subject) + 
        (1|condition:subject) + (1|location:subject)))

I cannot  seem to run an anova(done) command.  am I doing something small wrong?  or are there significant errors in my thought process?

"
Need help choosing a small sampling size to evaluate a 100 unit production trial. Real World Industry Problem.,5,2,False,False,False,statistics,1498935791,True,"**Goal: I need help choosing a small sampling size to evaluate a 100 unit production trial. Real World Industry Problem.**

A little background: I work for a really small company, and we collaborate with a really big company that is interested in our technology (you might be using one of their devices now...).
 They made 100 units using our technology. Now we need to test the performance of the samples. But, they can't test all 100 samples (it's too expensive).  But they can test a smaller population of samples.The test is either a pass or fail.  We want >90% pass rate at least, but I don't have an exact number.

**What would be a good sampling approach given:**

*100 units total

*20 batches

*5 units per batch

*pass rate needs to be >90% at least.


My first guess would be to simply test one sample from each batch (that would be 20 unit total).  But the big company would prefer to limit the sampling size to smaller that. But they seem open to discussion.

I am simplifying the situation a bit.  If there are any questions, let me know. Any input would be helpful.  Sincere thanks for your time!

Nathan




"
Short Survey on Income and Education. Please take it!,0,1,False,False,False,statistics,1498940769,True,[removed]
[Survey] How easy is it to find Data Assets?,0,0,False,False,False,statistics,1498943893,True,[removed]
"a,b,c grouping, best software",0,1,False,False,False,statistics,1498952847,True,[removed]
Advice for undergrads!,15,20,False,False,False,statistics,1498955301,True,"I hope it's okay that I made another one. The old thread is 5+ years old!
 

EDIT:
added a question to fuel our discussion (thanks /u/m9i3s2le0d7 for the suggestion)"
Finding confidence intervals without standard deviation?,4,8,False,False,False,statistics,1498960219,True,"Not sure if this is even possible but I'm taking a practice quiz with my school and this is the question and answer:

A statistics course has 20 students with an average final grade of 0.700. At 95% confidence, the upper confidence limit for the final grade with 3 decimal places is ~0.901.

I have no idea how the answer is arrived at 0.901. I figure you need the standard deviation to find these things. this is all the information that is given. "
GCSE Statistics,4,3,False,False,False,statistics,1498981685,True,"Hiya, does anybody know how to get the Collins GCSE Statistics Student Book for really cheap/free? As I really need one for my GCSE course. "
Phd without Master degree?,9,6,False,False,False,statistics,1498985034,True,"Is it normal to apply phd programme right after college? Or would it be better to have master degree for applying phd programme? 

If not, what courses must I take? 
I heard the cores are Probability(1,2), Linear Algebra, Analysis(1,2), Stochastic. Anything else? 

Should I consider measure and integration course after analysis 2?"
"If I had a dataset with Y and X1, and was told there may be an unkown latent/confounding variable effect on X1, how do I estimate Y~X1?",3,2,False,False,False,statistics,1499019974,True,[deleted]
Combining different values from different number of variables,6,8,False,False,False,statistics,1499022458,True,"Hi **r/statistics**

I hope you can help me with what seems on the surface to be a very simple problem.

I have data from over 3k respondents where they answer various questions about themselves including whether or not they take a broadband service.

What I want to do is to see what, if any, groups of respondents have the highest rate of saying 'no' to the broadband question.

My temptation is to list all the value combination of variables I care about (such as income, region, age etc.) and see what the propensity to answer 'no' is in each of these combinations.

However, would I be right in saying this is just exactly what multiple regression does? And that I'm doing thing the long way with this approach?

As an addendum, there's another issue. I can get every combination of variable response by using the **expand.grid** function in R but how can I factor in different numbers of variables to be included in this output.

To explain, consider age (young, old) and income (low, medium, high). I can use expand.grid to get me all the combinations of these two variables, but I will also be interested in all those people on low incomes *regardless of age*, so want to include that as a test case. Any easy way to do that in R or do I just need to make use of combn and expand.grid separately?"
Please can I get some help! Bit of an emergency!,5,0,False,False,False,statistics,1499035480,True,[deleted]
"How to find expected stock price, assuming normal distribution, taking into account the probability of a certain event occurring?",5,0,False,False,False,statistics,1499035836,True,"Am I over complicating this?

Would I just find out what a 1 standard deviation move would be and then multiply it by the probability of the event occurring?

"
Pass/fail small sample size statistical analysis,3,8,False,False,False,statistics,1499044897,True,"Hi All,
I need a bit of help regarding a statistical analysis of a test I am going to perform. I am testing an electronics component to determine if it is safe to use at a certain voltage. At the end of the day, I would like to state that with XX% confidence the safe voltage is Y+/-Z.  I will probably have less than 10 components to test. 

What statistics terms/methods should I be reading up on? "
What conclusion can the toy designer make about the proportion of infants who will reach for the rattle? Selected Answer: Correct More than 80% of infants aged between 4 and 6 months will reach for the rattle. Can anyone one explain to me how you get this answer?,4,0,False,False,False,statistics,1499062667,False,
How to construct a regression model with two inter-dependent dependent variables?,20,5,False,False,False,statistics,1499074685,True,"Let's work through a concrete (if somewhat impractical) example:

I'm a medical researcher who has reason to investigate a possible trend in a dataset of tissue samples from the human lung and human brain. We are interested in the number of viral cells of type A found in these tissue samples (measured as a discrete count). 

The dataset could look something like this:

The dataset could look something like this:


    brain_virus_counts lung_virus_counts age brain_tissue_total_cells lung_tissue_total_cells gender ethnicity
    239                         5783                       67    139218   1323494    M    A
    2313                        3528                       72    225815   2328554    F    A
    15                          356                        38    535291   5341823    F    O
    4829                        13458                      81    371234   3351732    F    T

The trend noticed within the data is that there is a greater proportion of `lung_virus_counts`versus `brain_virus_counts`. However, I need to model this in order to quantify this effect. 

How does one model two dependent variables in this fashion? If there was one dependent variable, I would use something like a Poisson GLM model. 

Are there statistical packages (e.g. in R) which allows one to perform this?
"
Need sanity check of apparently wrong answer to basic stats question,0,6,False,False,False,statistics,1499075617,True,[removed]
How to calculate Standard Error when given only a population standard deviation.,0,1,False,False,False,statistics,1499079106,True,[removed]
Grouping conditions for repeated measures ANOVA,2,2,False,False,False,statistics,1499084427,True,"Hi /r/stat,

I have a data set in which I want to compare performance in 5 conditions. As such I run a repeated measures ANOVA (lets ignore linear mixed models for simplicity reasons for now).

While the differences between those 5 conditions are interesting I'm also interesting in whether the averages of conditions ""1+2"" and conditions ""3+4"" differs.

Am I correct in stating that I should simply keep the initial ANOVA with 5 conditions and run a separate paired-test to compare the grouped conditions. I'm pretty sure it would be incorrect to include the 2 groups in the initial ANOVA since that would mean that data from conditions 1,2,3 and 4 is basically used twice in the same model."
"Help with Regression wanted. (Please see picture). There is obviously some kind of linear relation between 0 and 1. Then, there is a break (x>1). How to choose the right function? I work with R. Thank you very much!",28,31,False,False,False,statistics,1499085860,False,
Invrea Scenarios for Construction (Blog post),0,0,False,False,False,statistics,1499087985,False,
Does covariate must be observed variable?,0,1,False,False,False,statistics,1499116281,True,[removed]
Validation of a predictive model with count data?,0,1,False,False,False,statistics,1499130700,True,[deleted]
Factorial within-subjects rANOVA: A priori sample size,7,3,False,False,False,statistics,1499130994,True,"Usually when I'm calculating minimum sample size for factorial designs, GPower is my go-to tool.

Unfortunately, it seems that GPower does not support within-subject repeated measures when there is more than 1 factor.

Specifically, my experiment calls for a 4x4x2x2 within-subjects design. Accordingly, each participant would run 64 trials (4x4x2x2=64), with there being no between-subjects element.

**What is the best way to go about calculating sample size here?**

Many thanks for any advice which can be offered!"
Validating spatial predictions with count data?,0,5,False,False,False,statistics,1499131308,True,"Hello statistics gurus! I am looking to validate a predictive spatial model with a series of location-specific count data.

Specifically, I'm looking to compare a map of predicted human accessibility (on a scale of 0 to X where X is the maximum predicted number of humans to pass through that part of the map). I have count data of the number of people that pass given points over a set sampling time, and I'm not sure if I have the right statistical tool to compare my predictions to the actual data. My current plan is a zero-inflated Poisson regression model with predicted accessibility as a covariate, but some of you with more statistical smarts might have a better tool I'm unfamiliar with.

Thank you for any advice!"
Is it better to double major in CS or Math to get a Stats PhD?,0,1,False,False,False,statistics,1499142766,True,[removed]
Order as covariate,0,1,False,False,False,statistics,1499158595,True,[removed]
How do I convert standardised regression coefficients (beta) above the value of 1 into a correlation coefficient?,14,2,False,False,False,statistics,1499159426,True,"The Peterson and Brown (2005) method of converting beta into r only works for values less than one. 

What method(s) are out there to convert a beta value over one into a correlation? "
Blinding Us to the Obvious? The Effect of Statistical Training on the Evaluation of Evidence,9,25,False,False,False,statistics,1499159701,False,
Broom's tidy function - How to get sample size?,1,10,False,False,False,statistics,1499162523,True,"Hi,

Does anyone know how to get the sample size out of an lm model when using broom's tidy() function, e.g.

    sampledf %>% lm(paste0(""VA ~ snp +"", covars), data=.) %>% tidy(conf.int=T)

Usually, I would extract it separately by looking at the length of the residuals vector, e.g.:

    mod <- lm(paste0(""VA ~ snp +"", covars), data=.)
    length(mod$residuals)

Thanks"
Test the significance of two *differences*?,3,3,False,False,False,statistics,1499180426,True,"I have a number of data points, which result in the array of box plots similar to the below. 

[Boxplots](http://imgur.com/a/gbscw)

For each diagnosis, I tested two different times, P1/P3, in two different conditions, ""all"" and ""subset"". I would like to test whether _the difference between the differences between P1 and P3_ is significant for each metric, for each diagnosis. Is there a method to test the significance of two _differences_?"
Please help me graduate! :(,3,0,False,False,False,statistics,1499183729,True,[removed]
EEG data analysis- psychology,0,1,False,False,False,statistics,1499186062,True,[removed]
"I have to formulate an econometric model in a week (it can be about everything i want), do you guys can give me an idea to start?",2,0,False,False,False,statistics,1499202868,True,
[Regression] How would you go about testing reversion to a benchmark based off how far something is away from its mean?,2,2,False,False,False,statistics,1499208188,True,"I would think a z-score that represents the distance from a benchmark would have some explanatory power on a series reverting.

Essentially I would like to get an idea of how large the z score gets before the series tends to revert back to the benchmark.

Is this possible? Usually the series would divert from the benchmark until the distance is ""unsustainable."""
"How would I go about testing the injury rate for no-huddle vs traditional ""huddle"" offenses in football?",0,1,False,False,False,statistics,1499212297,True,[removed]
How should I run my raffle?,7,7,False,False,False,statistics,1499213372,True,"Not sure if this is the right place to post but I'm sure you guys will have some insightful ideas.

I'm going to be doing a raffle soon and it's stressing me out on how to actually do it. So there will be about 80 prizes and 105 people eligible to win. None of the prizes are significantly better than the rest (all range from $10-$40), but some will obviously be sought after more than others. This group of people have a lot in common (all women, all in a sorority, all ages 20-24).

So here are my problems:

I don't want anyone to win twice.
I don't want someone to be stuck with something they don't like.
I don't want this to take a lot of time, it needs to be a relatively easy solution.

Help me come up with an idea?"
Can someone please explain variance to me?,14,16,False,False,False,statistics,1499214725,True,"In layman's terms please! I still don't get it. I get standard deviation but, outside of variance being standard deviation squared...I'm at a loss. 

Thanks!"
Is this approach to adaptively choosing the window size for a moving average a good idea?,2,5,False,False,False,statistics,1499224445,False,
"Started reading Statistics by Freedman, Pisani as a beginner. Everything was doing great and was easy to understand, until they started to explain histograms. They didn't explain the theory and just started doing exercises. I am missing something?",12,0,False,False,False,statistics,1499225936,True,It was like they completely changed the style of teaching. Maybe I am missing some pages? 
Is job market for statistics majors very competitive? Is a 3.28 GPA good enough?,0,1,False,False,False,statistics,1499234217,True,[removed]
Ideas please! I'm analysing my shopping habits!,0,4,False,False,False,statistics,1499236413,True,"So for the last 6 months I kept (almost) every receipt from all my shopping. It's like 95% food from supermarkets (I'm a passionate home cook) and the remaining is things like personal care, coffee, etc. 

So I'm interested in analysing my shopping habits and spendings. I'd like ideas on what things to look for. So far I'm just categorising my shopping by: Date, item (e.g. canned tuna); categories, not sure yet but maybe food type like vegetables, fruit and protein, personal care. I'm thinking maybe other categories but not sure which; place where I bought it; and cost. 

So, I'd like to ask for ideas on:

- What other categories should I add that would be interesting to look at.
- What kind of analysis would be interesting to try. 

I've been learning R, so I'll try plot it all in ggplot2! "
4 Things To Look Out For When Tackling Your Statistics Homework,0,1,False,False,False,statistics,1499242651,False,
How can I quantify how many classes an algorithm is able to distinguish?,1,2,False,False,False,statistics,1499247908,True,"Hi! In my field there's a lot of hype regarding new analyses methods that churn out a hundred of ""descriptors"" from a single input image. I want to study the ability of each one to actually distinguish the things I do want to distinguish.

I have now 6 different images each of which is different from the other in some aspects. So I aspect that some of my hundred of descriptors should be able to give me different values.  Each image has been acquired many times, each of them ""statistically independent"".

Now... If I had to tell people which descriptor best classifies two things I'd use a cohen's d. If it's 6 you could correctly pick whether a single object is class A or class B. With d = 1 I could make some inference at Group level, with d=0.1 I'd better stop using such classifier.

My task is exclusively to discover which descriptors are Worth using as classifiers in a subsequent experiment and which ones aren't.

What if I have multiple classes? Most likely, one descriptor could find overlapping distributions for images A and B, overlapping distributions for C,D and E, and another separated distribution for image F. 

Is there a way to quantify this, as ""clearly"" as I'd do with a cohen's d for a two class situation?

(I hope my question is not too confused)"
Considering a career- Can anyone help?,9,4,False,False,False,statistics,1499261478,True,"I've been struggling with a focus for my life and career for a long time. I'm coming to think that maybe statistics is maybe the answer, or something similar. 

I've always been very interested in logic, rationality, etc. I've followed Menno Henselments who is a bodybuilder who studied stats and preaches about the Baysian way of thinking, and I've always found all this so appealing and consistent with my own thought patterns. 

Problems: No background education. And i don't even really know where to start. Ideally i'd like to get some idea of what sort of vocation i could potentially get into, in this sort of area, and then i'd probably find it easier to research what kind of education i'd need, and if thats even going to be possible at this point in my life. (I'm 31).

Thanks for any help. 

Edit: Would prefer a career in somewhere somewhat ethical, aka not a bank or advertising "
Difference between statistical analysis vs data mining algorithms,0,1,False,False,False,statistics,1499265716,True,[removed]
GPower - Your thoughts?,5,11,False,False,False,statistics,1499266692,True,"In some of my collaborative work I've started to run across more and more people who use GPower (various versions) to aid them in power analysis and decisions about sample size. I'm looking more into this tool myself but wondered if anyone here has any opinions about this tool.  Is it good? Is it great? Should I recommend it to others?  What are the drawbacks, if any, of GPower?

Thanks in advance for any assistance you can provide!

Link for those unfamiliar with GPower: http://www.gpower.hhu.de/en.html"
Comparing Effects/Models - Interpretation?,2,2,False,False,False,statistics,1499267489,True,"Hello all!

I have a pretty basic question that I just wanted some clarification on:

So, I'm running a regression model with a bunch of independent variables and I have one particular categorical variable with 35 different levels.  I want to be able to assess the effect of each one of the 35 levels on the response variable.  Unfortunately, I have to choose one of these 35 levels as a baseline to compare everything against.  That's not really my goal.  So, instead I ran 35 different models with a new variable that treated my categorical variable as follows: 

1=""specific level of interest""
0=""everything else""

And I used ""everything else"" as my baseline.  So, now I have 35 different effect sizes that I want to compare, but I realized I'm not really comparing them all against the same ""everything else,"" since I'm always changing it out with each new model.  I understand this would be a pretty big problem if the original categorical variable only had like 5 or 6 levels, but I have 35, so I feel like the degree of change in ""everything else"" isn't that much between each model.  

Is that a fair assumption?  Or am I not comparing my numbers in a proper way?  If not, is there a better solution?

Thanks!  "
Cns-staphorst.nlhas global traffic rank #0. Its domain having .nl extension. Its Pagerank is 0/10. It is Not listed on Dmoz.Some threats were reported so it may not considered SAFER to browse.Cns-staphorst.nl has estimated worth of $ 15 and have a daily income of around N/A.,1,0,False,False,False,statistics,1499268057,False,
Master's Level Indepdendent Study Topics,3,3,False,False,False,statistics,1499272174,True,"Hi All, 

I am currently in a position where I have 5 credits hours I need to fill in the Fall semester and 1 credit hour I need to fill in the Winter semester in my Master's program due to having a full time Graduate Assistantship. I have a couple of ideas for some topics but I thought I would ask here if anyone has any suggested topics they wanted to throw out to consider as well. It's a Master's program in Biostatistics more specifically. "
Does anyone know the formula for finding the sample size of an AB Test where alpha is an adjustable variable?,2,3,False,False,False,statistics,1499279673,True,"I've found a few versions of an AB sample size formula online, but the one's I've seen assume an alpha of .05, like in the example below. 

Link is here: https://julienlenestour.com/maths-behind-minimum-sample-size-ab-testing/

I'm looking for the maths behind this type of calculator: http://www.evanmiller.org/ab-testing/sample-size.html#!20;80;5;5;1 

So that I can use the formula in Excel, where alpha is a variable that can be filled by a certain cell. I'm testing a few different types of family wise error adjustments to work out which gives me the most accurate result. 

Any ideas?"
A little help with a game shoot...,0,1,False,False,False,statistics,1499286174,True,[removed]
AB tests probably don't measure what you think they do...,13,25,False,False,False,statistics,1499291463,False,
How difficult is Generalized linear model?,3,1,False,False,False,statistics,1499302888,True,"I'm thinking about taking a university course in Generalized linear models (GLM), but I am wondering maybe it will be too difficult. 

How difficult would you say a course in GLM is, compared to lets say statistical inference? "
How is my professor curving our letter grades?,2,0,False,False,False,statistics,1499303439,True,[deleted]
Determining degrees of freedom for statistical test,0,2,False,False,False,statistics,1499313800,True,"How would you calculate degrees of freedom by hand for a one-way ANOVA if you know the number of participants and the number of treatment groups? 

EX: A researcher conducted a one-way ANOVA with 5 treatment groups and 9 participants

For reference, I am trying to report in this format: d.f. = 7,2"
"New to MCMC, quick question",10,3,False,False,False,statistics,1499329300,True,"Hi all,

So I'm reading up on bayesian modeling and the topic of MCMC has gotten me a bit stuck.

If you don't mind, tell me if I understand this correctly...

So given some data with a known likelihood and prior function, I want to figure out the posterior distribution. In nice textbook examples, this can be computed by hand since those examples are often cases where there are conjugate distributions.
But in the real world, I might want to use a weird prior and the posterior will be weird too, so I'd have to figure out a numerical way of computing this posterior distribution. Using bayes formula, posterior = (prior x likelihood) / normalizing-constant, or i.e. posterior distribution is directly proportional to the prior x likelihood. 

So I can calculate the prior x likelihood, but have no idea what this normalizing constant might be since it's.... < okay insert blank here, I'm not sure what it is (marginal likelihood is ... the probability of the data given all possible configurations of our model parameters??? I'm intuitively and fundamentally confused here>, but that's okay since the prior x likelihood will get me to at least the posterior unnormalized. I can just throw a bunch of darts at this (using some intelligent mcmc algorithm) posterior to figure out the histogram/distribution, and that's the posterior distribution! But wait, now that I have this histogram, I can integrate it to find the total area and this area will determine the normalizing constant (marginal likelihood) since the proper area of a distribution should be equal to 1. 

For any future predictions I want to make, I can't just use the posterior distribution I've calculated with prior x likelihood, I'd have to normalize it using this newly computed value, and integrate over whatever region I want to figure out the probability of a point landing in that space. Tada!


So is that what the point of MCMC is all about? Am I understanding this stuff correctly? Also can someone please help me understand just what the evidence/marginal-likelihood is intuitively? "
How do you guys organise your R scripts?,21,68,False,False,False,statistics,1499334963,True,Im doing my dissertation now and I write like two scripts minimum every day. things are getting out of hand when im trying to find something
What statistical method would you use to compare two different sets of different data?,19,1,False,False,False,statistics,1499340443,True,"Searching the web for the answer to this but all I am discovering comparing means and standard deviation. 
Example
2 tests, both assessing *A* but one measures in *n/20* whereas the other test measures *n* with no limit on score.
What method could be used to compare these values? I feel like an ANOVA method is best. Would I have to convert all values to something like a percentage?"
Main recommendation system algorithms and how they work,0,12,False,False,False,statistics,1499349763,False,
How Online Help with Statistics Homework Can Help a Student,0,1,False,False,False,statistics,1499351505,False,
Teaching master's stats in the fall - any tips or useful materials?,3,2,False,False,False,statistics,1499352723,True,"I'm a quantitative psychology doctoral student, and I'm teaching stats for the first time, and at the master's level. To be honest, there is very little difference between undergrad stats 1 and this class (the most complicated thing we do is multiple regression - I don't think we get into interactions). Does anyone have any useful tips, materials, or suggestions for books/assignments? I have materials from a colleague, but wanted to see what else is out there."
Question about law of large numbers and independent events.,4,0,False,False,False,statistics,1499353157,True,"So lets say I'm gambling with a coin flip, heads I win, tails I lose. My first 50 coin flips are all heads. Now lets say I do another 10 million coin flips. The law of large numbers tells me in the end I theoretically should have 5 million 25 heads and 5 million 25 tails. But independent events tell me I should have 5 million 50 heads and 5 million tails. Which is right?"
What Graduate Programs should I apply to? What would be a good fit?,3,0,False,False,False,statistics,1499358037,True,"**Undergrad Institution: Stony Brook University** (Large State School, has a good reputation for my majors)

**Major(s): Physics, Applied Mathematics and Statistics**

Minor(s): 

**GPA: 3.35 (AMS GPA is 3.6)** bad grades mostly in sophomore year.

Type of Student: International, Male, South Asian

GRE General Test: Will be taking soon. Anticipating > 90% V scores but around 90% Q score.

GRE Subject Test in Mathematics:
How important is it that I take this? I don't really have time to study for another test on top of the GRE and my demanding final year schedule.

**Programs Applying: Masters in Statistics/Masters in Data Science**

Ideally I want to apply for Phd programs but given my crappy undergrad record, I've decided to apply for masters and see if I can get into the Phd programs later on.
 
Research Experience: Essentially none. Did some linguistics research not related to statistics with a pretty famous guy but that was a while ago and no paper came out of it.

Awards/Honors/Recognitions: Deans List, Physics Honor Society. 

Pertinent Activities or Jobs: Irrelevant on-campus job for 3 years and currently doing a data analyst internship at a very small marketing agency.
 
**Letters of Recommendation:** The School is HUGE and all my classes are >100. Its very hard to get to know professors and this is likely the weakest aspect of my application. I will get recommendations from professors I've taken courses with and gotten As but that's about it. I don't even expect them to remember me or know who I am and I just don't see any way around this. I can try to cultivate some relationships till application time in December but I don't really know how to or how successful I will be. 
 
**Course Work:** 

**Math and Stat Courses:** Survey of Probability and Stats (A), Probability Theory (A-), Combinatorics (A-), Data Analysis (A), Deterministic Models (A), Statistical Lab (Basically a course on R programming and SAS), Calculus 1,2,3,4(Taken in Sophomore year so grades are mostly Bs and a C in multivariate), Linear Algebra(A-). Probability and Statistics for Data Science (In Progress)
 
**Physics Courses that may be relevant: **Quantum Mechanics (Some probability and lots of Linear Algebra)(B-), Statistical Mechanics (Lots of probability)(B-)
 
**Computer Science courses:** Object Oriented Programming (A), Data Structures (A-), Analysis of Algorithms (In Progress), Probability and Statistics for Data Science (In Progress), Computational Geometry (combined course by Computer Science and Applied Math department also in progress).

**Online MOOCs:** 4 course specialization in Python by Uni of Michigan (Coursera), 10 course Specialization on Data Science by JHU (Coursera), Machine Learning Course by Andrew Ng (Coursera).

**Where should I apply?**
 Ideally I want to go to a Data Science program but these are limited so any good statistics program. What school match my profile? Where would I have a good change to get in and succeed?

How can I improve my application? Is there anything here I can leverage? How can I build enough rapport with a prof to get a good recommendation letter in 1 semester? Do you have any other advice or suggestions? Any other programs you think I should consider? Any other Specific courses I should take?"
Can I run ttests and ANOVAs on unequal group sizes?,0,1,False,False,False,statistics,1499365097,True,[removed]
Why would anyone use Machine Learning to analyze data ?,7,9,False,False,False,statistics,1499366561,True,"Let's say that a gluten-free bakery has asked to collect data for them, and then analyze the data to see how their gluten-free bakery is doing is comparison to traditional bakeries in the same town. Assuming all the needed data is collected, is there such a thing as trying to analyze data using Machine Learning algorithms ?

The reason I am asking since I heard about this, but I am not sure what the purpose of this would be, don't we just plot the data and construct graphs/visuals to show the analysis that has been done, why would anyone use Machine Learning to analyze data from a survey ?

I see how I would use ML to analyze survey data to check if the machine can recognize the person to was surveyed, but I just don't get the point of ML if all we need to do is analyze data and output graphs/visuals."
Minimum Requirements for a Statistical Study?,2,1,False,False,False,statistics,1499368386,True,I am looking to start a blog about dating and I would like to come up with custom data on my arguments. I would do this through surveys to determine if my hypothesis is true. What is the minimum threshold of participants required to meet industry standard of reliable statistical content?
"R is acting up on me, can someone help me out?",3,2,False,False,False,statistics,1499369218,True,[deleted]
Struggling with Real Analysis,8,4,False,False,False,statistics,1499371606,True,"I was recently taking Modern Analysis I at Columbia and received an F for the course. We used *Principles of Mathematical Analysis* for the course, but I could not understand the material at all and had no idea how to answer any of the exercises. I bought the book *How to Think About Analysis* in order to try to get a better understanding of the material, and while it helped a bit, I still felt completely lost when I went back to reading Rudin.

I had been planning on going for a PhD in Stats or Economics before taking this class, but I'm now reconsidering. From my understanding, these two fields heavily lean on Analysis, but I just can't seem to wrap my mind around it. I also don't have time before I graduate to retake this class so I will be stuck with the grade I received. Is there any course of action that you would recommend to remedy this? Also, are there any resources you know of that could help me to try to learn the material from *Principles of Mathematical Analysis*?"
Design of Experiments Help,0,0,False,False,False,statistics,1499372848,True,"I am trying to set up a design of experiments to screen some initial variables. The information I have is below:

Variable
Type
Range
Notes

I
Factor
A, B, …, N
-
 
CI
Continuous
0 - CIi; i = A,B,…,N
CIi May or may not be equal

R
Factor
AA, BB, …, NN
-
 
CR
Continuous
0 - CRj; j = AA,BB,…,NN
CRj May or may not be equal

D
Factor
AAA, BBB, …, NNN
-
 
CD
Continuous
0 - CDk; k = AAA,BBB,…,NNN
CDk May or may not be equal

T
Continuous
Tmin - Tmax
-
 
Only 1 factor from each of I, R, and D is allowed per experiment.
If factors A, AA, AAA are chosen for an experiment then all CIi = 0 where i != A, and all CRj = 0 where j != AA, and all CDk = 0 where k != AAA.

I was planning a 2 level resolution 4 screening but I don't know how to handle the factors. Any help you can provide I appreciate."
Why do confidence intervals have no role in Bayesian inference?,10,12,False,False,False,statistics,1499381300,True,
Help with Event History Analysis on Stata,1,2,False,False,False,statistics,1499391043,True,[deleted]
Question about probability of getting single for college dorm lottery,1,9,False,False,False,statistics,1499397579,True,"So we have 8 guys choosing between 4 single rooms and 2 double rooms. 4 of us (including me) are allowed to choose between fall or spring semester to have the guaranteed single this year since the other 4 had the choice last year. One of my roommate suggested that we should all choose the same semester since that gives us collectively 4/8 chance to get the single in the other semester. But I think if we choose to all get the single in the same semester, that leaves us individually 1/8 chance of getting single. Therefore if 4 of us split evenly between the semesters, we each would have 1/6 of chance getting singles. I'm wondering who's assumption is correct and what is the best statistical outcome for the group and for me?"
Why is mapping 2D data into a 1D considered bad practice?,3,0,False,False,False,statistics,1499397798,True,"The following is the data that I have:

		Situation	     Class	Date	Bus

		Belch	         1.77	2.23	2.15

		Read	         7.27	2.88	7.17

		Cry	             2.21	3.04	3.08

		Mumble	         3.62	3.12	5.17


The data represents how appropriate are various behaviours in various situations. For example, it is very appropriate to Read in Class, but not on a Data or it is more appropriate to Mumble on a Bus, then on a Data, etc.

Let's say that I need to visually represent the Bus column only:


				   Bus

		Belch		2.15

		Read		7.17

		Cry	  	    3.08

		Mumble	    5.17

I have proposed to visualize it the following way:

		 ------
		| 2.15 |
		| 2.15 |
		 ------
		| 7.17 |
		| 7.17 |
		| 7.17 |
		| 7.17 |
		| 7.17 |
        | 7.17 |
        | 7.17 |
		 ------
		| 3.08 |
		| 3.08 |
		| 3.08 |
		 ------
		| 5.17 |
		| 5.17 |
		| 5.17 |
		| 5.17 |
		| 5.17 |
		 ------

So basically a single bar, IDK if this even exists (is a convention) ... , but my team mates said that this is inappropriate, since we are mapping 2D data onto a 1D graph. Could you help me understand why this is considered a 1D graph, as well as why this would be inappropriate (if this is inappropriate, I am not 100% they are correct).


Thank you."
Looking for major advice,3,1,False,False,False,statistics,1499406961,True,"Maybe ""Looking for major recommendations"" would've been a better title.

Prospective college student here between a rock and a hard place. What undergraduate program would, in your opinion, be of optimal preparation for a career in statistics? Is the demand among employers for experience in programming, software, etc. prevalent enough that computer science may be just as appropriate a major? There isn't much overlap between my university's CS and stats programs other than calculus (the former is offered by the science school, the latter by the business school), so it seems that the only true ""kill two birds with one stone"" option would be a double major, which I doubt I could feasibly manage. Would it perhaps be wisest that I altogether avoid statistics during undergrad and pursue a master's in it?

Also, is there, in any sector, the possibility for one with a CS degree to be just as competent as one with a degree in statistics, or vice versa? Ideally, I'd like to become sufficiently learned to find employment in each field, but I'm unsure if a degree in just one would allow me to do so."
Integrated Statistical Service: A Unique Multi-Disciplinary Statistical Work Flow,0,0,False,False,False,statistics,1499412170,False,
Statistical paradox,4,0,False,False,False,statistics,1499416183,True,"An event with a 30% probability of occurring is 30x more likely to happen than an event with a 1% of occurring. So why isn't a 99% probability considered 30x more likely than a 70% probability (especially as you could say the 70% probability is 30x more likely to NOT occur).

I understand the simple maths involved – it just still seems counter intuitive – 99%>99.999% is a small leap proportionally, but 0.001%>1% is a relatively massive one.

Does this have anything to do with Weber-Fechner law and the idea that we perceive logarithmically?"
Does it make sense to calculate p-value for features in machine learning classification model? • r/MLQuestions,0,3,False,False,False,statistics,1499430022,False,
Why can't you use linear regression for time series data?,22,5,False,False,False,statistics,1499437696,True,"Suppose data is daily sales of a certain items, and the item shows a linear trend (growth). One might consider using a linear regression with the sales as the response and the day (time) as the predictor.

As I understand, one of the assumptions of linear regression is that the residues are not correlated. With time series data, this is often not the case. If there are autocorrelated residues, then linear regression will not be able to ""capture all the trends"" in the data.

What consequence are there if one uses linear regression to model it anyway, and perhaps uses it for prediction. It might still suffice to give a rough result although not optimal?

thanks"
How to accurately propagate error when some measurements have only 1 recorded value?,3,3,False,False,False,statistics,1499441115,True,"Idk how to explain what I mean, so I'll try to make a watered-down example:

Width 1: 22, 24 (Avg 23 +/-4%)
Width 2: 12, 16 (avg 14, +/- 14%)
Width 3: 30 (avg 30, +/-??)

Sum of widths: 67 +/- ??



In this example, how do you account for width 3 only having one measurement taken? Also, would you weight the individual uncertainties to find the total uncertainty? Sorry if the formatting on this post is messed up, I'm on mobile lol

Thanks!"
Best forecasting approach in R?,12,14,False,False,False,statistics,1499443486,True,"I'm using a multiple variable linear regression that does a pretty good job at modeling company balances that I'm working with. This regression, compiled using the ""lm"" function and approximated using the ""predict"" function in R, has around 30 or so explanatory variables which include holiday dummy variables, weekday dummy variables and other company balances that relate to the one being predicted. 

At the moment, it only forecasts one day ahead and this is through using the coefficients of the linear model in vector form and multiplying it with another vector containing a set of the latest explanatory variables. I want to be able to forecast much further out than just one day, but I'm not exactly sure how to go about it. Many online examples use ARIMA models or the Holt Winters method, but I'm unsure that either of these can model the data as accurately as the linear regression has, mainly because of the dummy variables that have been fed as explanatory variables, and therefore I'm not even sure that the forecast results would be as accurate as I like. I could also very well be mistaken. 

So given this, how should I go about forecasting further out than just one day given the model I have? Thanks in advance!"
Need Help with MLE and Model Fitting,0,1,False,False,False,statistics,1499449594,True,[removed]
How do I label the 0s and 1s in an interaction plot in R?,1,1,False,False,False,statistics,1499473057,True,[deleted]
Has anyone used the Gibbons Ross & Shanken test module in Stata or any other statistical languages?,0,3,False,False,False,statistics,1499474566,True,I don't have the time to do it manually using the matrices but can find limited instruction or information on GRS test modules that have been created? Any info is much appreciated. 
IDEA Software,3,2,False,False,False,statistics,1499482001,True,"Hoping this is the right place for this question.

Does anyone use IDEA? If so, what for?"
Are there any texts that formally develop/explain what a 'moment' is?,4,25,False,False,False,statistics,1499491765,True,"Title. I am fascinated by the relative computational ease and useful information found when evaluating a moment, but I've always wondered about its history. How does one, without any prior knowledge of moments, derive it? How did someone come up with E[e^(tx)], and what were the proofs that implied its existence? Who created it? (my guess is Gauss)

Thanks for indulging in my curiousity.   "
Fat tail vs long tail,5,11,False,False,False,statistics,1499504858,True,Are fat tail and long tail referring to the same thing? Are they interchangeable? Or do they refer to two distinct properties of the tail? Google wasn't of much help and I can swear I've read these two terms before but can't seem to differentiate them.
Odd stats question,1,2,False,False,False,statistics,1499517342,True,[deleted]
help with statistics question,0,1,False,False,False,statistics,1499520504,True,[removed]
Statistics,0,1,False,False,False,statistics,1499532836,False,
Reporting regressions (social sciences),0,1,False,False,False,statistics,1499536436,True,[removed]
quick question about standardized predictors and coefficient interpretation,1,3,False,False,False,statistics,1499560081,True,"Hi all, 

I'm trying to interpret a standardized regression coefficient for work and want to make sure I'm thinking about it correctly before I make a fool of myself. 

A researcher's IV is standardized (not their outcome variable). Thus, if I remember correctly, their results can be interpreted such that a 1-standard deviation change in the predictor corresponds with the coefficients change in the outcome variable. Their actual coefficient is .03 (a percentage that goes to 100), meaning an SD change in the IV corresponds with a 3% change in the DV.

I'm trying to extrapolate from this a bit... Is it also accurate to say that a shift from -1 SD to + 1 SD in the predictor corresponds with a .06 in the outcome, i.e., .03 + .03? Or am I mistaken?

Thanks for your time. 
"
How would you interpret this regression?,0,1,False,False,False,statistics,1499568792,True,[deleted]
Are there any good (and free) distribution fitting software?,7,1,False,False,False,statistics,1499570516,True,"Hello /r/statistics, 

I came across this software called EasyFit, which fits a ton of distributions to your data using tests like KS, AD, and so on. Seems like a very useful tool, but a bit expensive for a student like me. So I was wondering, are there any other tools out there that do something similar but for free? Or at least fairly cheap?

Thanks in advance"
Statistics Youtube channels or Facebook pages?,3,4,False,False,False,statistics,1499573466,True,"Does anyone follow any Youtube channels or Facebook pages that talk about statistics? I know Minitab has a surprisingly active Facebook page where they post links to their blog posts, but are there any other good pages/channels for discussing stats?"
What do you look for in a CV/cover letter for an entry level job application?,4,18,False,False,False,statistics,1499580427,True,"Hi! I'm applying to some entry level jobs (public service and private sector) in statistics/data analysis. There are some entry level jobs in my part of the world (Australia), and they do not ask for specific skills/degrees other than an understanding of statistics and either SPSS or R, and the ability to communicate stats to a general public.

My background is in biology, but I used statistics and R for my thesis and undergrad. Nothing advanced, but classical things like t-tests, ANOVA, chi-square, some non-parametric stuff, some multivariate (PCA, correspondence analysis), and pretty graphs in ggplot2. I'm also enrolled in some stats courses and keep learning R.

Unfortunately they're not very specific with the kind of tasks they want the applicant to carry out, they just want some background in stats, knowledge of some stats package, sometimes data management, and mostly to be able to communicate the results. If I were to apply to these entry level jobs, what would help me stand out in my CV and cover letter/application forms? and in possible interviews, what are things worth mentioning?"
24 percent of 24 percent theory,0,0,False,False,False,statistics,1499587839,False,
Everybody lies: How Google search reveals our darkest secrets,8,62,False,False,False,statistics,1499610640,False,
Regression Discontinuity Analysis on data for a research paper. Want to be a co-author?,0,2,False,False,False,statistics,1499620708,True,"Does anyone know how to do a regression discontinuity analysis? I've tried looking it up online but it is way out of my expertise. The draft is almost complete; the missing part is analyzing the data. One colleague recommended using this analysis but he is too busy to help me.

If someone knows how to do this and can help me (or if you want you can analyze the actual data) I can add your name as a coauthor if you'd like. I'm actually going to be presenting this at a research forum on October and would love to get this done soon.

Here is a sample data:
An organization has done an intervention in a rural community and we are trying to show that the number of surgeries have increased because of that intervention.

year 2007, 819 surgeries. year 2008, 1032 surgeries. Year 2009(date of intervention), 1101 surgeries. Year 2010, 1128 surgeries. Year 2011, 1175 surgeries. Year 2012, 1216 surgeries. Year 2013, 1537 surgeries. Year 2014, 1666 surgeries. Year 2015, 2200 surgeries."
I need help on how to differentiate simple and multiple linear regression.,4,0,False,False,False,statistics,1499625113,False,
Probability Distribution Function explained,0,0,False,False,False,statistics,1499632464,False,[deleted]
How to measure qualitative variation,4,1,False,False,False,statistics,1499632500,True,"Hi r/statistics!

I'm working on a hobby project which is a database tool to create cigar reviews. A cigar can have many flavours, which I have grouped into ten categories. These flavours pop up every now and then while smoking (which can take up to an hour or more), so time is an element as well. A good cigar will have many flavours throughout the cigar in alternating succession.

The problem I am trying to solve is finding a measure for the dispersal of these flavour observations, both in time and within each flavour group. I took statistics 101 at uni six times before I passed (after getting tutored, and even then barely) so I would categorize myself as statistics-challenged.

I've Googled and found about a hundred different measures. I briefly considered picking the one with the most exotic name but in the end I only tried the variation ratio [1-(mode/N)]. Problem is: the results look OK (based on my impressions of the cigars) but how would I know? Also, this only takes into account the flavours, but not the time element.

I would be grateful for any insight or pointers you could give me!

Data sample:

Flavour category; time observed

2445; 6-7-2017 19:59:16

2444; 6-7-2017 19:55:16

2444; 6-7-2017 19:54:13

2443; 6-7-2017 19:54:10

2445; 6-7-2017 19:50:50

2448; 6-7-2017 19:50:46

2450; 6-7-2017 19:50:16

2449; 6-7-2017 19:47:54

2443; 6-7-2017 19:46:42

2448; 6-7-2017 19:46:37

2449; 6-7-2017 19:44:34

2443; 6-7-2017 19:43:37

2446; 6-7-2017 19:43:20

2445; 6-7-2017 19:40:26

2448; 6-7-2017 19:40:22

2447; 6-7-2017 19:39:54

2447; 6-7-2017 19:35:54

2444; 6-7-2017 19:35:48

2447; 6-7-2017 19:32:23

2448; 6-7-2017 19:31:13

2444; 6-7-2017 19:29:55

2443; 6-7-2017 19:29:27

2446; 6-7-2017 19:27:42

2448; 6-7-2017 19:26:00

2449; 6-7-2017 19:25:14

2445; 6-7-2017 19:23:53

2446; 6-7-2017 19:21:25

2443; 6-7-2017 19:18:02

2447; 6-7-2017 19:17:43

2450; 6-7-2017 19:16:46

2444; 6-7-2017 19:16:23

2447; 6-7-2017 19:16:10

2443; 6-7-2017 19:12:52

2447; 6-7-2017 19:10:22

2444; 6-7-2017 19:10:21

2445; 6-7-2017 19:10:19

2445; 6-7-2017 19:08:25

2447; 6-7-2017 19:07:55

2447; 6-7-2017 19:06:26

2448; 6-7-2017 19:04:07

2446; 6-7-2017 19:03:15

2446; 6-7-2017 19:00:56

2448; 6-7-2017 19:00:43

2448; 6-7-2017 18:58:21
"
Using the sales of previous two days to predict sales of 3rd day?,2,5,False,False,False,statistics,1499635142,True,"A beverage company is interested in minimizing the days when their stuff are not being sold in stores. 

Two reasons for zero daily sales are:

1. Item is not on store shelf due to poor management, this creates missed sales opportunities.

2. Item is on store shelf, but no one bought it. 

The company is interested in scenario 1, to pin point stores that have bad management. 

The data contains daily sales for 1000 stores, for a whole year, for 10 different products. So for each product, each store, there are 365 days worth of daily sales data. A ""0"" means it was not sold on that day.

The company also sent out people to visit stores daily so we have labels of when a product is on the shelf, and when it is not.

I want to build a logistic regression model to analyze the days when 0 sales occurred for each product/each store, and classify whether it's due to scenario 1, or scenario 2. 

Currently, my predictors are product ID, days of the week (Mon, Tue..), size of the store shelf dedicated to a product, ... 

but since the sales of previous couple of days can have a huge effect on whether something will no longer be on the shelf, I plan to include sales of the previous day, and also the sales of the day before that in my model.

Problem is, the sales of previous few days can be correlated. Will this cause problem in my logistic model?

What recommendations/advice do you have? Any comments would be appreciated. 

Thanks"
Structural equation modelling...basic question,2,1,False,False,False,statistics,1499637666,True,[deleted]
statistics dating,0,0,False,False,False,statistics,1499645351,False,
Multigroup CFA in Mplus,0,1,False,False,False,statistics,1499660351,True,[removed]
Bachelor degree holder seeking advice about potential Master degree in statistics,17,7,False,False,False,statistics,1499666852,True,"I got my Bachelor's a year ago and I applied to about a hundred jobs and got about 2 interviews which were through recruiters and then I didn't end up getting the jobs so I'm doing some job out of my field that doesn't pay anything near what I was told a statistics major would make. It seems like every job wants a MS degree or PhD graduate. The thing is I don't feel like I'd be learning anything new if I pursued my MS in Statistics. The program at my university basically mixed the undergrads with the grads so we all had to take master's classes and do a few culminating projects depending on the concentration we took. I looked into the MS program at a different college that I now live by (SJSU) and the courses are exactly the same MS courses I took, excluding maybe one course on bayesian stats. 

Basically I don't want to go back and redo all the work I've already done just to put on my resume that I have a master's degree, but I feel like without it employer's don't give a crap and will think I don't have this supposed graduate level knowledge. Ideally I'd like to do something in computer science because I enjoyed the programming side of statistics but I won't have the core classes that BS compsci student would have taken like data structures and other aspects of programming so I feel scared to apply for a master's program, but I don't want to get another BS and then be stuck holding two BS degrees. 

Anyone in a similar situation? I'm just looking for some guidance because I feel like I need to do this now before I get stuck in a field I don't enjoy"
Calculation of Speed,0,1,False,False,False,statistics,1499669801,True,[removed]
Majoring in Statistics,15,5,False,False,False,statistics,1499677475,True,"I finished sophomore year at an undergraduate in the US. I haven't taken anything towards statistics or math in any way so I'm basically starting over (I also haven't taken math classes since junior year of high school). I want to relearn math as I'm taking two gap years due to reasons. What should I start with??? In terms of courses such as pre-calculus, etc. I did take math until pre-calc. I was thinking of buying like ""for Dummies"" books and using online resources such as Khan academy, etc. But I want to know which math courses are a good foundation for statistics."
Any good ressource for understanding (more or less advanced) descriptive statistics ?,0,1,False,False,False,statistics,1499680545,True,"Hi, 

Do you guys happen to know a good ressource for learning about some advanced topics in Descriptive Statistics for Data Scientist ? Subjects like how to effectively deal with correlation study among mutltiple heterogenious variables (categorical, numerical ...).

Thanks in advance !"
How can I describe similarity between two curves?,12,2,False,False,False,statistics,1499683885,True,"I have several sets of partnered curves. I want some quantitative method to describe how ""similar"" the two are, so that I can figure out which set has the most similar two curves. 

The two curves have the same x and y axes and units, as well as the same x values. Just different values for y. 

How can I go about this?"
Please help me decide what anslysis to use,6,1,False,False,False,statistics,1499708433,True,"Greetings statisticians - I'd like to crowd source some assistance in deciding what analysis to run.
Here's the situation - I have 2 sets of results; control 1 vs sample 1 + treatment and control 2 vs sample 2 plus treatment where the treatments are identical.
I want to know if treatment results are statistically  significant compared to the control (easy enough) but I also want to know if the difference between treated sample 1 and treated sample 2 are statistically significant and i have no idea what test to run :-( my initial thought is ANOVA but having 2 controls in there will probably skew the results, right? And running more that one analysis is a bit of a no no- so any guidance would be appreciated. 
I don't want anyone doing my work for me, I'll do the tests myself if needs be, i just need help understanding which to run.
Thanks x"
MS in Biostatistics worth it as a MD?,10,11,False,False,False,statistics,1499713129,True,"Hello everyone,

Long story short, I'm a 5th year medical Student (out of 6 years in my country) and I have been interested in statistics and epidemiology since my 2nd year which led me to collaborate with the Biostatistics and Medical Informatics department of my college and having contact with awesome statisticians who led me to learn R/Python and submerge myself in a world that is very interesting. 

I have done quite a bit of reading and learned by experience watching the other statisticians work and I am now ""working"" as statistical consultant for other MDs and being kind of a liason between the doctors and the statisticians because they find it helpful that I know how to speak the language of statistics while also knowing medicine.

I will have quite a bunch of free time during my 6th year and was planning on attending this [MS in Biostatistics](https://fenix.ciencias.ulisboa.pt/degrees/bioestatistica-564500436615179/descricao) in the evening.

I was glimpsing through the curriculum and it doesn't have Calculus or LA as prerequisites and it seems mostly directed to applied statistics and since I think my weakness is mostly in Theoretical and Mathematical Statistics, I'm a bit afraid of investing money in a MS that would end up being too ""cookbook"" approach based.

With that said, I would like some advice on my career. I plan on being a clinical MD that also wants to be a Medical Statistician able to design trials and analyze data. Could I get to that dream by self-learning Calculus/LA and Statistics while continuing to work with my statistician mentors and saving some money or should I go to the MS even if it seems too cookbook based and not very mathematical.

Many thanks everyone and sorry for the long rant, I'm really lost :)"
Statistical significance,0,1,False,False,False,statistics,1499716065,True,[removed]
Does the Margin of Error change when their estimated values are used in a percentage calculation?,2,5,False,False,False,statistics,1499717592,True,"Hello,

I am not a statistics person (I took a very basic intro course three years ago) however I am working with statistical data for an internship, specifically census data from the US Census Bureau.

 I was going to post this on Yahoo Answers to hopefully not seem like a complete idiot but their site, much like their financial future, seems to be failing. If you need clarity I will update/respond to you, I apologize if this doesn't make sense.

 Anyways, for example I am calculating the percent of total people under the poverty line ( estimated population # under the povertyline / estimated total population # of an area) * 100. Each estimate value (population under povertyline and total population) each has a margin of error number. I am wondering if it is possible/needed to alter those numbers to create a margin of error for the percents I am calculating (for example, I have the percent calculations for estimates... do I need to alter the moe?) 

I'm not sure how the moe works (the website says its a 90% confidence interval estimate). I know you can aggregate it by adding the squares of the moe and square rooting it. Idk how it works with dividing/multiplication. Idk if it would make sense/be possible. Thanks!"
Calculating Children's BMI Percentiles,0,2,False,False,False,statistics,1499721747,True,"A professor I am working with asked that I write syntax for SPSS that calculates a child's BMI percentage based on the child's age and sex. To do this, I realize that I need to have reference data. I found source code to do this in SAS, but I am not familiar with SAS syntax and am having a hard time understanding LMS parameters. Here is the link to the source code and information about it: https://www.cdc.gov/nccdphp/dnpao/growthcharts/resources/sas.htm

Another option I have entertained is to somehow use these data tables that provide some information about the BMI percentage per age and weight, but I feel that this option loses a lot of information as we can only say that a child's BMI is in the 25th or 50th percentile rather than an exact number. Here is a link to one of the data tables: https://www.cdc.gov/growthcharts/html_charts/bmiagerev.htm#males

Right now I feel that the best course of action is to convert the SAS code to SPSS. It would really help if someone can explain the LMS parameters, specifically how they work in the code in the ""macros for calculations"" section. However, I am open to other suggestions on how to tackle this problem. 

Thank you! "
Creating a composite of two variables that differ in size,0,1,False,False,False,statistics,1499722196,True,[removed]
New Structural Equation Modeling Subreddit,2,3,False,False,False,statistics,1499728709,True,"If you're really into structural equation modeling like I am, come check out the new structural equation modeling subreddit, /r/semstatistics. I would like to get some resources together as we go along and create a space where people can ask questions and get help with models. Come over and post a cool resource or ask a question. "
How we react to important social issues (Australia),0,1,False,False,False,statistics,1499749433,True,[removed]
Good Stats Jobs with PhD in Computer Science?,7,5,False,False,False,statistics,1499753613,True,"Sounds like a silly question but my passion is for Statistics but I may have a better shot at getting into a better CS PhD with an awesome advisor doing research in machine learning. I know ML and Data Science are only tangentially related but are there good industry jobs in Stats for PhDs in a semi related field?

Also, I'm not asking for advice on pursuing a PhD. I already am sold on doing it. I have better connections and network with this CS program but would actually love to do Statistics with research focus in computational Stats."
The 2017 Updated LinkedIn Stats You Should Know,0,1,False,False,False,statistics,1499765584,False,
Biostatistics career questions,7,4,False,False,False,statistics,1499779204,True,"Hello everyone,

Last year I graduated with a degree in chemical engineering, and throughout undergrad I spent nearly one year doing full time biomedical lab work, both in industry and academia. I've come to understand that I desire a career in biomedical research, but the high failure rate of laboratory work and the monotony of, for instance, cell culture, were not a good fit for my personality. I set my sights for medicine, but my current clinical job has me questioning if patent care justifies the lifestyle sacrifice of a medical education. I envision myself as more scientist than clinician.

From engineering, I understand that I enjoy scientific theory and numbers. I also enjoy a variety of challenges, rather than digging into one topic for years on end. Finally, I value the idea of meeting new people and learning new ideas frequently in the work place, working with large teams, and collaborating throughout the institution. (I dislike working with the same 5 people every day)

Biostatistics sounds like an ideal way to a) avoid the high failure rate and monotony of data collection, b) collaborate extensively throughout the institution, c) continually learn new ideas and meet new teams, d) utilize my strong mathematical and scientific background, and e) work hopefully less than 60 hours per week, especially after training.

Are my ideas grounded in reality? Would biostatistics be a field that would allow me such opportunities? What drawbacks am I undoubtedly overlooking? Are there other careers I should consider? Point C is especially important to me in my career"
Need advice/recommendations on PHD programs for stats.,0,1,False,False,False,statistics,1499790895,True,[removed]
Interrater reliability of multiple raters on multiple measures,0,1,False,False,False,statistics,1499796712,True,[removed]
"At the average location on Earth, there are enough ants within 1.64 miles of you to equal your body weight.",0,1,False,False,False,statistics,1499797995,True,[removed]
Is it okay to run an independent samples T-test with a dichotomous dependent variable?,3,1,False,False,False,statistics,1499798385,True,"I'm going through other journal articles that are running independent samples T-test with a dichotomous dependent variables, but I thought the assumptions of this test was a continuous or interval dep. variable? I'm trying to do this right but it seems published journal articles don't always follow assumptions leaving me confused AF. "
Considering Biostatistics PhD,14,6,False,False,False,statistics,1499804763,True,"Hey all,

I'm currently a rising senior doing a joint major in CS & Math at a non-target university for either major (but ranked around ~60 for overall universities). I'm really interested in Statistics and Biology and would love to understand the two subjects on a much deeper level. As a result, I'm looking into some Biostatistics PhD programs. However, I messed up throughout my first few years of undergrad and ended up with a few Ws on my transcript (in Physics, Chemistry, and Biology), a 3.5 GPA, and a 3.7 in-major GPA. With my background, would I be able to get into a good (maybe top 10-15?) Biostatistics program if I finish strong? Or should I set my sights lower than that? If it helps, I am generally a very good standardized test taker (my scores are usually in the top 1-2%), so my GRE score should hopefully be pretty good. Thanks in advance for your help! I'll list some of my background below.

 

**Courses Completed Already:**

* CS I & II - A

* Programming for Math and Science - A

* Data Structures - A

* Algorithms - B+

* Database Systems - A

* Theory of Computation - A-

* Data Mining - A

* Machine Learning (Graduate course) - A-

* Discrete Math - B+

* Calculus I & II - A

* Multivariable Calculus - B-

* Vector Calculus - A-

* Linear Algebra I - B

* Statistics I (Econ department) - A

 

**Plan to complete:**

* Probability Theory

* Mathematical Statistics

* Numerical Analysis

* Partial Differential Equations

* Linear Algebra II

* Artificial Intelligence (Graduate course)"
Probability Independent Events,0,1,False,False,False,statistics,1499824942,True,[removed]
How do you calculate standard deviation after log-transforming data?,0,1,False,False,False,statistics,1499825779,True,[removed]
Robustness check partial sample analysis,2,5,False,False,False,statistics,1499829213,True,"Hello, I'm interested in doing a robustness test in the spirit of out of sample prediction but with classical multiple regression analysis.

I am thinking about iteratively randomly sampling a portion of my data and estimating the model parameters on the subsamples to see how much the betas vary across samples (and in what portion of samples I get 'significant' results). My hope is to dispel doubts that the results I find in the main analysis are due to just 'p-hacking' through noise.

Does this procedure, or something like it, have a name? Are there packages in conventional software that implement it? Thanks."
I think my professor is asking me to run the wrong test. (T-test help),1,0,False,False,False,statistics,1499833666,True,"Background:

I have the following data.

    Venture Capital Funding (dependent variable in dollars)
    Money_Raised_In_SF (Independent Binary variable)
    Money_Raised_In_NY (Independent Binary variable)
    Money_Raised_In_LA (Independent Binary variable)
    Money_Raised_In_CHI (Independent Binary variable)
    Money_Raised_In_NY (Independent Binary variable)

My Professor wants me to run a "" t-test for difference of two means"" for each city vs SF. Each city has a different number of companies that raised money there. 

What I originally did was run an OLS regression using the dummy variables as my independent variable and the amount as my dependent variable and came out with an equation. 

Is it possible to run a t-test for difference of two means for this data and how would I set it up, or is there another test to validate my data? (apologies if my vocabulary isn't cohesive, I'm just getting back into the swing of things)

"
Years of Statistics crammed into a single Document,9,157,False,False,False,statistics,1499837311,False,
How exactly do you estimate the parameter of a population from a sample statistic?,1,1,False,False,False,statistics,1499838358,True,[deleted]
"Using the reliability equation, what does it mean when t is greater than the mean life value?",6,1,False,False,False,statistics,1499839622,True,"I have a problem where a banker wants to find out how many customers are still with the bank after 9 years. So the **t** value is 9. The **mean life** value, u, is 7.4 years. **Standard deviation** is 0.8 years

**Z-Value**= (x-u)/sigma=2

If we use a normal table we get a P(t) value of 0.9773. Does this mean that after 9 years 97.73% of customers are still with the bank. This cannot be right, it seems like the number should be smaller. "
Help building a multilevel model,0,2,False,False,False,statistics,1499847578,True,"Hi guys,

I have a problem understanding how to build a multi-level model. I totally get the algebra but there seems to be no guide on the order you add things on, or at least I can't find one. I have the following variables:

Maths Score (Out come variable);
SES (socio-economic status, level 1);
Gender (level 1);
School Academic Programme (level 2);
School Type (public or private, level 2);
School (level 2).

I know the first step is to run an intercept only then an intercept variable with the random intercept and compare the model improvement and ICC improvement. But can someone, step-by-step, tell me the order I should add the other variables on. Do I add I.V.s on before introducing the random slope, or the other way round? Do I centre my variables (they do have a meaningful zero, however)."
"For those of you who got their Master's Degree in Germany (or another foreign country), how did you finance that?",12,2,False,False,False,statistics,1499854301,True,"Right now I'm still debating on whether to enter the job market right away at the end of this year with a Bachelor's in Statistics or go for the Master's in Data Science (by saving up by teaching in Korea or something). There's a few schools in Germany that I'm interested in, so I'm wondering how people here financed their living expenses there for two years?"
Best method for determining trends?,0,1,False,False,False,statistics,1499859525,True,"I have a data set of different abundances of animals over the course of a 7 year period and my prof. wants to do a trend analysis on it. He originally suggested an ARIMA model but I'm not sure if this is the best approach to it.

Does anyone have any suggestions on what might be best if we are simply trying to determine whether populations have generally gone up or down over the 7 years?

Thanks!

Edit: In one paper I was reading a researcher who was interested in the same thing used a Spearman's rank order correlation coefficient method, would this be suitable in my case as well?"
Lecture:1 what is Structural Equation Modeling (SEM) using AMOS? | www.statswork.com,0,1,False,False,False,statistics,1499860364,False,
Text classifier algorithms: overview of main approaches with tutorials,0,26,False,False,False,statistics,1499862935,False,
Batch Files in Mplus,0,1,False,False,False,statistics,1499871093,True,[removed]
Propensity Matching (again),8,3,False,False,False,statistics,1499871935,True,"I'm reading this workshop pdf (http://www.npcrc.org/files/NPCRC.Observational-PropensityScoreMethodsWkshop.10-20-14.pdf) and the author suggests *not* using AUC for propensity model evaluation.

I get that the the TRUE goal isn't prediction here, but wouldn't you prefer a model with a .7 AUC over one with .55 given that both reasonably balance the covariates in the matched cohorts?  Or am I thinking about this wrong and it's irrelevant?

It seems to me that your ability to predict enrollment (propensity) is important and AUC makes sense to me as a metric since it's averaging performance across all thresholds (i.e. predictive performance is equally important at a value of .2 as at .6).

Is the author simply saying that maximizing AUC is not your ultimate goal so make sure you evaluate covariate balance?"
Comprehensive Meta Analysis,0,1,False,False,False,statistics,1499872228,True,[removed]
Variance Covariance for ARIMA models,0,3,False,False,False,statistics,1499874566,True,[deleted]
What distribution do you think of when judging attractiveness 1-10?,20,14,False,False,False,statistics,1499882067,True,"Normal, uniform, or something else? I've always thought it should be either normal with sigma of around 1.5, or something like a gamma with k=5 and theta=1. 

Thoughts?"
Question about the discrepancies in data,2,1,False,False,False,statistics,1499882827,True,"Hi all first time posting here so basically, I'm looking to run an OLS, VECM and GARCH model (with the accompanying johansen co integration and other tests necessary to prove data meets requirements for each model) based on daily closing prices of particular future indexes, but I just noticed that the data I have is not even? Like one of the indexes has 519 and the other 2 have 489 but they all run through the same time period. Am I supposed to run my tests as is or try and even it out? 

P.S first time to post here so If I'm posting in the wrong place or not doing it properly just tell me what I need to do to correct it."
What statistical test should I choose?,0,1,False,False,False,statistics,1499889511,True,[removed]
Need help with a Fantasy Football project!,6,1,False,False,False,statistics,1499910963,True,"Hey!

I'm looking to decide for my league what's best: standard scoring, half ppr, or standard ppr. 

For each scoring format, I've gotten data for the top 75 wide receivers and top 75 running backs. The data I got is amount of points scored in each format, and their rank within their position for each scoring format. 

I want a league in which running backs and wide receivers are tradeable. That is, the number 1 ranked WR is scoring a similar amount as the number 1 ranked RB, and same for #2 and #2, and #3 and #3, and so on. 

I also want a league in which there are a lot of good players so that everyone is able to have fun. That is, there amount of points scored within each position is most similar from ranks 1 through 75.

What tests should I run to see which scoring format does this best? 

Thanks!"
NEED HELP IMMEDIATELY HW IS DUE TONIGHT AT MIDNIGHT,15,0,False,False,False,statistics,1499915257,True,[removed]
Another Intro to Stats question,3,1,False,False,False,statistics,1499918125,True,[removed]
Correlation?,0,1,False,False,False,statistics,1499926796,True,[removed]
"How to perform age-adjustment, sex-adjustment, etc. for disease prevalence study?",9,3,False,False,False,statistics,1499935230,True,"Hi /r/statistics! I'm new to this forum, so if I've accidentally stumbled upon the wrong one, I apologize. So...


I'm currently studying the correlation between the prevalence of a particular disease and another variable. Many papers I've read on PubMed have noted that they've ""age-adjusted"" or ""sex-adjusted"" the data. How would a statistician go about doing this? I've been looking all over the Internet for a straightforward ""guide,"" but I still only have a vague idea of what's done: you multiply the prevalence by the increased likelihood of the disease associated with age - or something like that? Do any textbooks cover this pretty well?


Also, for publication, is it good practice to note that age-adjustment and/or sex-adjustment was unable to be performed due to lack of data? For example, the increased likelihood of a particular disease due to age may not have been documented.


Thank you!"
categorical vs continuous,5,0,False,False,False,statistics,1499939973,True,"I was about to ask a collaborator that I prefer continuous data because I prefer parametric tests, this sounded retarded, is it?"
Statistically (nonparametric) compare the performance of two methods,4,1,False,False,False,statistics,1499956896,True,"I need to do some data analysis to measure the effectiveness of two meditation techniques on people's brainwaves. So for the two techniques, there's like before & after EEG data  

What I wanna do is test the differences caused by the two techniques, and see which technique causes a larger difference.   

For now, I'm not too concerned on whether the difference is positive or negative (I lack the knowledge about brainwaves to make a conclusion on which is better)  

What I planned to do is calculate the absolute difference between the reading before and after, and then analyse the difference. It makes sense to me but if there's an error in my thinking, please let me know  

What test would be best to compare the performance for these two methods? Cos I have to use nonparametric techniques, I figured I would just use the Kruskal Wallis ANOVA. Furthermore, I also read something on Passing–Bablok regression and was wondering if this would be suitable as well."
How a low probability affects data distribution?,6,1,False,False,False,statistics,1499957033,True,"I have some data that has a high standard deviation, I know (or am reasonably certain) that this because of the low probability of each observation. Most likely if I were to make more observations eventually the distribution would normalize. However, is there some way I can simply communicate this or show this with an equation or something? Or is there anything I could read that and cite that supports this? 


FYI i'm a civilian , so please go easy.  "
Dixon-Cole model help!,6,2,False,False,False,statistics,1499959887,True,"Hello guys,

I'm trying to model a betting system and i wanted to try out the Dixon-Cole, already implemented the Poisson Distribution without a problem but im having hard times with Dixon.

Does someone have some knowlage about it?

My problem is how to calculate the rho, i read some studies bit it didn't helped.

I'm writing this in python btw.

Thanks a lot!"
Imagine visualizing data in augmented reality...,2,1,False,False,False,statistics,1499965595,False,
An outsider needs help,9,5,False,False,False,statistics,1499967697,True,"Working on a thesis and I'm not sure which statistical test I should use: I took a soil sample and screened for different species of fish bones and tracked 10 species between 2 screen sizes (fine screen vs. 1/4""). I need to know if there was a significant difference in the number of bones found between the two screen sizes.  
  
  
Suggestions?"
Interpreting the results of a linear regression in R,6,3,False,False,False,statistics,1499969732,True,"When you run a linear regression model in R with many different predictors, the output will show that some of the predictors are significant (low p value) while some are not. The significant ones have a star next to it, like ""***"" or ""*"".

Say your model has 10 predictors, but only 4 are significant (have * in R output). If you run another model that contains only the 4 significant predictors, will all of them be significant in the 2nd model? Or is this not necessarily true?

I'm guessing it's not necessarily true, but I cannot explain it. If it is true, then wouldn't it be a good way to do variable selection (as opposed to running a stepwise variable selection that's often slow).

Thanks"
patent data sources and api's?,0,1,False,False,False,statistics,1499970845,True,"what is out there - what is good, what is not?"
Analytics/Machine Learning in Manufacturing Environment.,0,1,False,False,False,statistics,1499971268,True,[removed]
"Bio-statisticians, would you mind talking about your job a little?",41,32,False,False,False,statistics,1499971919,True,"I applied to a Ph.D in Biostats and was accepted earlier this year.  I have an idea of what life after the Ph.D may be like (e.g. working with clinicians, consulting on experimental design, analyzing experimental data) but I would like to hear first hand from some of you.

What is work like?  What are the pros, what are the cons?  What is a day in the life like?  Do you work in a hospital or at a university?  Whatever you think is appropriate."
Making Probability Mathematical | Infinite Series,0,5,False,False,False,statistics,1499972660,False,
Disagree with my coworkers about how to handle an error in a survey conducted...,14,4,False,False,False,statistics,1499974637,True,"At my summer internship, a few interns put together a survey for our project (focusing on millennials age 18-34). The survey was sent out to everyone age 18-34 within our company (had it sent out to an age 18-34 distribution list), and the same survey was sent out to external employees (from different companies).

Problem is, there was an option to select age on the external survey with the following brackets listed:

<18,
18-26,
27-37,
38+
(I know, this doesn't make much sense given that we only wanted age 18-34, but I was not in charge of the survey).

Anyway, once I got the results and saw the age brackets, I told everyone that the results were not valid and could not be used for our presentation. Yes, we can remove the <18 and 38+ responses, but we would also need to remove the 27-37 category, because we can not decipher who in that category was 35,36 and 37 (did not fall under 18-34). 

This would mean we only included those 18-26 in our external data collection, which is not the definition we are using for our entire project, and it is not the same demographic studied in our internal survey.

Everyone told me that I'm being irrational/anal, and that they will simply remove all of these responses, have a few more people take the survey, and change the option to 18-34 instead of 18-26.

This BLEW my mind! This will lose the integrity of the data, and at this point, I believe it is not useable. We are presenting these findings to senior leadership and I do not want to be present data that has been skewed/manipulated in such a way! Am I wrong? What do I do? No one will listen to me..."
Masters in Statistics Chances,0,1,False,False,False,statistics,1499977322,True,[removed]
Relative Importance for Categorical Variables,0,2,False,False,False,statistics,1499977595,True,"We perform a ""key driver analysis"" to identify improvement priorities to management based on our data. We essentially rank standardized beta coefficients produced from a linear regression. This is under the assumption that the larger the standardized coefficient the more impact on the outcome if you make a change to it.

After thinking through this method I have a few questions. For the most part it is fairly straight forward, but I am unsure how to incorporate categorical variables into the rank. 

* Do I throw all the betas for each levels of a categorical variable into the ranking? 
* Or should I take only the level with the largest magnitude and treat it as the largest possible ""impact"" of this variable. 
* Also, what do I do with the missing implied reference class?
"
ELI5: General Additive Model.,2,4,False,False,False,statistics,1499979819,True,"So this is probably impossible, but I'm trying to wrap my head around the concept of the GAM, and the explanations I'm running into are... well, it's not working. It's been years since college calc 3 and stats, and getting back into it will require some boot-strapping.

If you can't ELI5, can we ELI an undergrad?"
When to use Bonferroni's multiple comparisons test,1,2,False,False,False,statistics,1499991344,True,"Claim: that the average sugar content (grams/serving) differs for the three types of breakfast foods (cereal, pop tarts, and granola or breakfast bars)

http://imgur.com/a/UH3KS

heres a picture of my spss tables.

do i need to do the bonferroni?"
GEE vs Mixed Models in R,8,2,False,False,False,statistics,1499995038,True,"I found this [great post](https://www.reddit.com/r/statistics/comments/16k9z6/can_anyone_help_me_understand_when_to_use/) on Generalized Estimating Equations (GEE) vs Generalized Linear Mixed models (GLMM) from 4 years ago. I was wondering if anyone could expound on this and maybe give an example in R. Lets say I wanted to model count data that could be from the Poisson family in a longitudinal analysis study. What would this look like in R? What packages do you use?


EDIT:  [lme4: Mixed-effects modeling with R](http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf). This is exactly what I was looking for. Thanks for the help."
Question about regression for predicting GPA from course grades,3,2,False,False,False,statistics,1499996041,True,[deleted]
Chances of getting in to top statistics graduate programs (i.e. global top 20)?,2,1,False,False,False,statistics,1500012175,True,"TL;DR - What are my chances of getting into a top statistics graduate program (either masters or doctorate) with excellent marks but little research experience?

I am a student at a Canadian university ranked 151-200 in mathematics and statistics (on QS at least).  My major is statistics.  My GPA is a 4.30 out of a possible 4.33, and I have A+ grades in every math and stats course I have ever taken, obviously these include: calc I-III, intro algebra (theoretical version), ODEs, intro complex analysis, real analysis I and II, regression, time series, probability theory, multivariate stats.  If I maintain my current GPA I am likely going to graduate inside the top three people in my graduating class (top GPA in grads this summer was 4.29).

I graduate next year, but only have one semester of research experience and no publications.  I am curious, for the sake of my ambitions and time + application money's sake, how likely am I to get into a top school such as Stanford/Harvard/Princeton, etc. for statistics or mathematics?  I imagine mathematics (or CS) requires more of a research background than the stats does (this is coming from the advisor at my university and my research supervisor for the one semester I have).

I appreciate any advice you guys can give.  Thanks!"
Question About Methodology To Determine Sample Size,4,0,False,False,False,statistics,1500020136,True,"Hello.

I wanted to ask a question for anybody working in data science about the methodology used to calculating required sample size. For example in one of its reports, Gallup International made a [survey about atheism worldwide](https://docs.google.com/file/d/0B9FfgKGkLgPrZHVESmVnUUxSNTg/edit?pli=1). 

However, In my opinion, the used sample size is so small; For a country like China with population of 1.3B, they are using a sample size of 500 respondents.

What I want mainly to discuss about is the used methodology to determine sample size, it's a [very famous formula](https://www.qualtrics.com/blog/determining-sample-size/). There are a lot of [online calculators](http://www.surveysystem.com/sscalc.htm) which are using the same one. And I want to quote:

> The mathematics of probability prove that the size of the population is irrelevant unless the size of the sample exceeds a few percent of the total population you are examining. This means that a sample of 500 people is equally useful in examining the opinions of a state of 15,000,000 as it would a city of 100,000.

Can someone explain more about this ""mathematics probability"" or the research paper which led to such conclusion? For me, It doesn't make any sense to ask 500 people about their opinions in a country of 1.3B, and then later come out with observations about the whole country just from that small sample size. (50% of Chinese people don't use x product...). I would like to understand why the large population doesn't matter."
biostatistics in environmental/conservation fields? Is there such a thing?,9,10,False,False,False,statistics,1500048261,True,"I often see posts about biostatistics, but everyone seems to always be working in fields like pharmaceuticals, clinical stuff, cellular stuff, etc. But I never see anyone comment about environmental sciences, ecology, conservation etc. Are there any biostatisticians here that work on those environmental fields? Do you exist? :P Is there any future in this, specially in Australia/NZ? If you do exist what is your background and what is your work like? "
No linear algebra in undergrad?,20,1,False,False,False,statistics,1500048701,True,"I'm looking at the admissions requirements for a few biostatistics PhD programs, and it seems like linear algebra is a pretty universal requirement. My undergraduate major, chemical engineering, did not require linear algebra. Should I apply anyway, or is there any way to remedy this problem?"
What happens to a KR-20 score of a test if all students score well.,3,2,False,False,False,statistics,1500054596,True,"Hey all, 
I am a teacher using examsoft which reports a KR-20 for each assessment. The latest test had an average of 85% with a stand dev of 5.4. The KR-20 escapes me but it was between 0.1 and 0.2 and most discrimination indexes were rather decent (.2-.8 unless the question was easy and everyone got it right). 
 
**My question**: as a group of students clumps towards the top, does the KR-20 automatically go down? 

In general, students that had previously demonstrated mastery did well on this test and the more questionable students were predictably lower scoring, so what gives? The assessment seemed good to me.

There were only 28 questions - is that a factor?

thanks in advance - p.s. ELI5 please.  "
Analysis of Deviance GAM,7,8,False,False,False,statistics,1500060023,True,"So I'm using the gam package in R to compare nested models, and specifically anova.gam. This question is specific to this package as well as about likelihood ratio tests in general. 

So it appears that the anova gam function just uses the sum of squared residuals as deviance and then just does a chi squared test on the difference. However, if the family is gaussian, shouldn't there be a variance parameter in there? Wouldn't this be the difference between deviance and scaled deviance?

Thanks!

Edit: Did more research, the anova function does display what I assume to be the correct p value (<2.2e-16); however, it doesn't display the chi2 value it's using... I am using gam::gam "
Common population parameter of interest - meaning in a meta-analysis?,2,2,False,False,False,statistics,1500062579,True,"Hi All,

A statistician friend recently helped me critique a meta-analysis which combined recovery rates for an illness from various studies, but which did so using studies of very different durations and with very heterogeneous samples and methodologies. The I-2 stat in this meta-analysis was 99.8%.

My friend said the meta-analysis' combined estimate was meaningless because ""there is no combined estimate if there is no possible common population parameter of interest"". 

I wondered if others here could explain what the sentence above in quotes means - especially the ""common population parameter of interest"". What is this, and why is it required for a meta-analysis to be a meaningful summary of multiple studies? If you could help me understand this I'd be grateful.

The meta-analysis in question is here, and my friend was critiquing the combined estimate of figure 2 (page 1301) - https://www.yellowbrickprogram.com/ArticlePDF/Jaaskelainen-2013-A-systematic-review-Bull.pdf"
Best data linking software? Other stat software recommendations?,11,8,False,False,False,statistics,1500069104,True,"I moved into a position that uses Microsoft Access to link two datasets using personal identifiers. It's pretty wonky and we have so many data elements that I've reached (and have exceeded) a seemingly arbitrary column limit of 250 elements. I need something better. What are your suggestions? I've downloaded CDC's Link Plus but have yet to find time to fiddle with it. 

Also- any statistical software you might recommend I purchase (I have some funds I need to use before my grant year ends). I currently use SPSS but I want to purchase SAS. I'm thinking of minitab. Just for reference I conduct biostatistical analyses.

Thanks!"
Decision Tree without any correlation between attributes,7,3,False,False,False,statistics,1500072618,True,"Hello guys,

I've got no idea about statistics and just wanted to ask a short question.
Is there any relationship between correlation and a decision tree?
E.g. I've got a data set with multiple attributes and one class. But none of these attributes are correlating according to Excel. Would a decision tree be meaningful in any way?

Sorry for this very dumb question, but I'm looking for help :("
Americans raped by dolphins each year: 14 Americans killed by foreign terrorists each year: 6,1,2,False,False,False,statistics,1500078957,True,[deleted]
Degrees of freedom for cross-level HLM,1,1,False,False,False,statistics,1500121385,True,"Hi everyone. I'm using[ Preacher's tool](http://www.quantpsy.org/interact/hlm2.htm) to probe 2 ways interactions in HLM. I have fixed intercepts. However, I'm getting stuck on Case #3 for df(int) and df(slope). I know df(int) is for the intercept, but for which slope is df(slope) referring to? I'm assuming it's for the interaction, however df for the interaction is the same as the df for the intercept (and all Level 1 predictors, the only thing that's different is the df for Level 2. So I'm confused why there are 2 separate boxes for it. Is there a situation where df(int) could be different from the df of the interaction for a 2 way cross level interaction?

Here is some output btw. age, salary, and erg haz are level 1, gni is level 2, and then there's the erghazXgni interaction. df is in bold (sorry, I don't know how to format a table on reddit).

Fixed effects: SYMPTOMS ~ GC.AGE + GC.SALARY + GC.ERGHAZ + GR.GNI 

                Value  Std.Error    DF  t-value p-value

(Intercept)  4.000822 0.04158244 **13072** 96.21423  0.0000

GC.AGE       0.175420 0.01248241 **13072** 14.05338  0.0000

GC.SALARY   -0.100831 0.01479287 **13072** -6.81618  0.0000

GC.ERGHAZ    0.447793 0.01626886 **13072** 27.52455  0.0000

GR.GNI      -0.057218 0.04121573    **33** -1.38825  0.1744

GC.ERGHAZ:GR.GNI  0.022988 0.01622783 **13072**  1.41660  0.1566
"
Linear Algebra Requirement,3,5,False,False,False,statistics,1500137860,True,"I graduated with an engineering degree, but managed to not take linear algebra.  Turns out, this is a common requirement for biostatistics graduate programs!  I've found the following course, which would allow me to finish the course well ahead of application deadlines and is relatively cheap:

http://und.edu/academics/extended-learning/online-distance/courses/math207/

Would this satisfy the requirement, even though it is only 2 credit hours?  Or should I opt for a 4 credit, in-person community college class, even though the final exam would be after the December 1st application deadlines?"
Cronbach's alpha calculation question,4,1,False,False,False,statistics,1500138409,True,"Hi guys, hope you are all doing fine. 
I have an exam on monday and need to know how to calculate Cronbach's alpha. 
I know that you have to use the spearman brown formula when you shorten your scale. I can't quite figure out what to do when the scale gets longer. 
Here is the example question I struggle with: 

Imagine Cronbach's alpha for research A was 0.7. Which value for Cronbach's alpha can we expect when we extend the scale with 10 parallel items (the final scale has 15 items). 
The expected value will be:
a) 0.44
b) 0.82
c) 0.88
d) 0.97

Does anybody know the formula I have to use in this case and how to calculate this ? 

Thanks for helping!!"
"Quick R tutorial for someone who eats, sleeps and drinks MATLAB? Mostly need R for the graphs/plotting",0,1,False,False,False,statistics,1500138873,True,[deleted]
Rolling Regression in R,2,2,False,False,False,statistics,1500139117,True,"So I'm trying to figure out how the ""roll"" library works in R. I think I have a pretty good idea, but the weighting values make no sense to me. Here's an example:


    library(roll)

    roll.test.1 <- as.matrix(data.frame(rnorm(100),rnorm(100),rnorm(100),rnorm(100)))


    roll.test <- roll_lm(as.matrix(roll.test.1[ ,(2:ncol(roll.test.1))]),as.matrix(roll.test.1[ ,1]), width = 30, weights = 0.001^(29:0))


    roll.test$r.squared

    R-squared
      [1,]        NA
    ...
     [28,]        NA
     [29,]        NA
     [30,] 1.0000000
     [31,] 0.9999945
     [32,] 1.0000000
     [33,] 1.0000000
     [34,] 1.0000000
     [35,] 1.0000000
     [36,] 1.0000000
    ....
     [91,] 0.9999998
     [92,] 1.0000000
     [93,] 1.0000000
     [94,] 0.9999753
     [95,] 1.0000000
     [96,] 1.0000000
     [97,] 1.0000000
     [98,] 1.0000000
     [99,] 1.0000000
    [100,] 1.0000000

These are random normal variables being used to predict random normal variables, how and why is R-squared nearly 100%? The example used in ?roll_lm uses an exponential decay model (0.9^(29:0) in this case). But the smaller I make the coefficient, the better the fit becomes. What am I missing? I would like to use weights for a project I'm currently working on, but the way they're generating results in roll_lm() makes no sense to me.

At first I thought this was because weights didn't sum to 1, but using a different method with the rnorm(100) data, where the most recent timestep has a weight of .9 and the other 29 are .1/29, I get a similar result for R-squared values. Any idea how the ""weights = "" piece works?"
"Do these sets of data qualify as being ""categorical data""?",0,1,False,False,False,statistics,1500148097,True,[removed]
Question,2,0,False,False,False,statistics,1500149629,True,[removed]
Lottery numbers,4,0,False,False,False,statistics,1500195931,True,"I am playing lottery every week. Statistically, is it better to play same combination of numbers every week or to change combination every week ?"
The most useful math courses you took?,29,39,False,False,False,statistics,1500199910,True,"Not counting the obvious ones like Calc I-III, pre-college math, or linear algebra"
Stats question - Need to compare 2 sets of qualitative data with some 0 values to make sure the cohorts are equivalent.,2,2,False,False,False,statistics,1500246235,True,"Hi all, ran into a problem while working on the analysis for a research paper. I need to compare 2 variables, Race, and Ethnicity, between 2 cohorts to make sure they were equally represented. At first I thought I would do a chi-square but found out I can not do that with any 0 values. A fishers test only does 2x2. Is there any other way to compare these two groups?

For example, Race is separated as such: 

Race: 

Group1: Unknown 5. White 29. (total 34)
 
Group2: Black 2, ""other"" 1, Unknown 8, White 46 (total 57) 

tl;dr how do I compare these 2 groups to make sure the 2 cohorts in a research study are equal (or not) "
"if I have a pearson correlation coefficient, how do I find statistical significance?",10,1,False,False,False,statistics,1500258083,True,"okay, so I am a maths/stats idiot (but I am essentially acing my politics degree where maths/stats is normally not normally required as a skill :3) and I am needing to use some statistical analysis techniques for a dissertation.

**(skip this part if you want, it's just what I'm confused about beforehand - you can skip right to my actual question in the next bold bit)** 

I understand that p = probability of a null hypothesis being false.

I understand that statistical significance will generally be 0.05 away from the mean of a two tailed set of results (so this is a standard deviation away)

for a null hypothesis in correlation analysis, apparently you just need a figure of 0...?

DO = probability of null hypothesis being true (so basically what to be expected)

DA = probability of null hypothesis being rejected (something not expected at a statistically significant level)


there's something called Z (confidence interval...? I can't remember) but I do not recall what this is important for from the top of my head. I am thinking these are very likely going to be relevant, so I am essentially saying ""don't worry, I look these things up - feel free to refer to ""p"" and ""DO/DA"", etc

anyway, here's the important part (in bold):

-

**if I have a data table right up in excel where I have A1-10 and B-10 and I have, let's say, calculated a pearson correlation coefficient of 0.5.**

**what I now what to do is find out how statistically significant this number is. and here's the thing: it seems very hard to actually understand this semi-manually; I see that the formula for this is very complex, but I need to at least know how to input my figures into some generator/calculator so that it will tell me the statistical significance value (i.e., again, from what I gather, if I get a number larger than 0.05, it is statistically significant; anything lower than this one level of standard deviation will be more necessary for more important tests).**

-

also: I'm confused about whether this has anything to do with a t-test. I've seen this kind of stat test before for data in politics and it seems to generate statistical significance too...?

also: when I thought I found an online calculator for this statistical significance, it was asking me for how many degrees of freedom. and, yeah, I don't know how many degrees I am dealing with. because I sort of don't get what this means, and I could make a few suggestions as to what it could be asking, but I won't embarrass myself with probably wrong answers

**so I need to know how to get a value of statistical signifiance given: (i) my A1-10 and B1-10 values, and (ii) my pearson correlation coefficient (i.e 0.5). is there a simple way of getting this value?** 

I will be so happy if somebody could help me because honestly this is making me feel like such a moron - I've watched explanations on youtube a lot and it seems like I end up watching wrong videos where they're explaining something long and complex that I never needed to know to begin with. thank you! "
[Stats Noob] What can I do with this data?,4,1,False,False,False,statistics,1500272476,True,"So I'm beginning project in a course that is unrelated to statistics, but requires me to flex some of my statistics muscles (Only ever took 200-Level stats, not my strong point). This particular portion is sort of me going above-and-beyond, but the more I'm delving into this topic the more interesting I find it and I just want to be able to get the most out of this data.

So I have in Column A, a list of movies that are in a particular genre released within the past 20 years. And then in the remaining columns, I have dozens of columns of metadata related to each movie. For example, some columns would be:

*Worldwide Box Office Returns

*Production Budget

*Advertising Budget

*Critical Reception to Film

*% Box Office dropoff from Week 1-2

*# of Theaters Released 

*Highest Grossing Stars in Film

*Release Date

*Per Capita disposable income @ time of release (and other high level industry/economic indicators)

*Worldwide Box Office of movies released around time of release

*Google Search for Trailers prior to release (1 year, 6mo, 3mo, etc)

*etc

So that's great, I have a wonderful dataset here that is way more than I need for this project, but what can I do with this? Obviously I'd like to find which variables most correlate with Box Office success, which is interesting enough, but with all the data I have I must be able to do something more. 

Thanks in advance for the help!"
Statistics Work Flow Process | Statswork,3,0,False,False,False,statistics,1500286208,False,
Suggestions on Applied Stats Master's in Europe?,5,5,False,False,False,statistics,1500293363,True,"Hi, can you suggest good universities in Europe to apply for a Master's in Applied Stats? I am not considering very competitive ones like Oxford, Cambridge or even Imperial College of London.

For the time being my end goal would be to work in the data science industry. I'm skeptic towards data science specific masters' though and that's why I'm thinking about applied stats.

Thanks"
Missing value imputation on a non-normal distribution - EM vs Regression? Or else?,0,1,False,False,False,statistics,1500300853,True,[removed]
Bonferroni correction on statistics generated from the same normal distribution,9,1,False,False,False,statistics,1500301493,True,"I have 4 treatment groups from an experiment with polymers. The 4 groups were analyzed using Gel Exclusion chromatography to identify molar mass. You can assume the distribution curve generated is normal. From this, the peak value, the number average average molecular weigh (Mn=sum(Mi*ni)/sum(Ni)), mass average molecular weight (Mw=sum(Mi^2*ni)/sum(Ni*Mi)), and the z-average molecular weight (Mz=sum(Mi^3*ni)/sum(Ni*Mi^2)) were all reported. I am doing a independent t-test for each molecular weight compared to the same molecular weight in other groups. 

My question is for the bonforinnin correction where pi= alpha/# of null hypothesis should I use 0.05/6 (for the 6 comparisons between groups for a given molar mass), or should is uses pi=0.05/20 (6 group comparisons * 4 molecular weights =20)."
All Models Are Wrong,0,1,False,False,False,statistics,1500307360,False,
M.S. Applied Statistics: Potential Salary Expectations?,18,19,False,False,False,statistics,1500308621,True,"
Here is the issue, I am trying to estimate my potential salary after grad school. After graduation, I would have an M.S in Applied Statistics and 1-2 years of work experience. Ideally, I would like to make at least $65,000, however, the lowest salary I would consider is $55,000. So, my question to you: Is this a reasonable salary to consider in New Jersey/NYC? I don’t want to lowball myself, but also, I don’t want to alienate employers. What salary is reasonable to ask for with a master’s degree in Applied Statistics and 1-2 years of experience in New Jersey/NYC? I see that some statistics salaries in NJ/NYC are really high (suspected outliers) and I would like someone else’s perspective on this. Thank you.
"
Bayesian Adaptive Methods For Marketing,15,21,False,False,False,statistics,1500319035,True,"Due to dose toxicity, Bayesian Adaptive Methods are important in phase 1 clinical trials as they allow the experiment to change while in the trial. This maximizes inference while minimizing toxicity to the patient. I am curious if these methods have been applied in a business setting. If a business is trying to price an item it seems that bayesian adaptive methods would be a great fit for this problem. Prices that are too low may lead to poor returns for the business while prices that are too high could deter customers from returning. Is there any further reading on this topic, or any negative consequences to using this approach to product pricing? What are the most common current methods for product pricing? "
Need help with Correlation,14,5,False,False,False,statistics,1500323834,True,"I'm working on my graduate research, but I'm not 100% confident in my recollection of Research Methods, and my professor just refers me to articles that haven't helped.

My Project: Researching contributing factors of property abandonment in my town. 

My Data: I have a list of abandoned properties, which ones have been sold in foreclosure, and which ones were sold as multi-property listings (i.e. to developers), and when each were last sold.

My Goal: I want to find any correlations between foreclosure and abandonment, or multi-prop sales and abandonment. Also, I want to know if more recently sold properties have a higher likelihood of being abandoned (by looking at the data, it sure seems like most have been sold in the last 5 years)

My Method: Here's where I'm unsure. Should I code the yes/no of foreclosure and multi-prop sales as 0/1, then plug them into [http://d2r5da613aq50s.cloudfront.net/wp-content/uploads/359971.image0.png](this) equation to find correlation? Like I said, its been a few years since research methods, and I just need to work this out! And for the dates, should I average those out, and find the standard deviation just like a discrete set of numbers? 

Any help or pointers would be appreciated!"
How can I model arrival of entities in my simulations?,7,3,False,False,False,statistics,1500325464,True,"Let's say that I have a simulation of a line at a grocery store check out, I need some way to simulate the fact that a new customer does not queue up to the line every 5 seconds, I have also heard that using `random()` functions is bad, since they follow Uniform Distribution.

How I can make the arrival of customers as random as possible ?

I have read a lot about something called [Poison Distribution](http://www.heppenstall.ca/academics/doc/246/2_Poisson_Distribution.pdf), but still cannot make sense of it, I have also stumbled upon something called [Poison Process](https://stats.stackexchange.com/questions/87988/what-is-the-difference-between-a-distribution-and-a-process-poisson).

I understand the concept of [Poison Distribution](https://www.youtube.com/watch?v=Fk02TW6reiA), but still have trouble understanding how I can apply it to generate random time intervals in my simulation.

What is the most common way of simulating arrival of entities in a simulation ?
Also, if anyone knows of a built in function in Java that helps to simulate random arrival times."
"Chi-Square Test: What does it mean if ""19% of data is missing""?",19,2,False,False,False,statistics,1500332973,True,"I am doing some statistical analysis on some survey data, and when i run the Chi Square test in SAS, it gives me chi-square value, but at the very end it has this message:
WARNING: 19% of the data are missing.

What does this exactly mean and how does this affect how I interpret the results of my Chi-Square test? I know this is quite vague and I can give more details as needed.

In this case, I am measuring the statistical significance in people's responses to a question (e.g. how does you feel about topic X, answer with option 1, 2, or 3) by the categorical variable of religion. "
How to determine what groups of ordinal scale rating observations were done incorrectly and remove/adjust them?,0,1,False,False,False,statistics,1500337691,True,[deleted]
Alternative to multiple one-sample T-tests for determining if multiple means are different from zero?,6,5,False,False,False,statistics,1500360996,True,"I am trying to work out if a change in protein sequence (that is the amino acid composition of that protein) makes it melt at a higher temperature. To do this I test melting temperatures of proteins with different protein sequences and then work out a value for the change in melting temperature by subtracting one melting temperature from the other. This allows me to work out what change in melting temperature is caused by a change in sequence. 

I have generated 12 data sets, each consisting of 5 values for the change in melting temperature caused by a particular change in protein sequence. I would like to work out which of these changes results in a statistically significant change, i.e is the change different from zero.

The way I would normally do this is 12 one-sample T-tests, comparing each data set in turn to zero but I am worried about the increasing chance of making a type I error if I do this. Is there an alternative test I could do to compare each data set to zero but keep the chance of a Type I error minimised? For instance is it valid to do a One Way ANOVA in which I add a data set with mean of zero to serve as comparison?"
what book(s) do you recommend to learn probability theory and statistics?,23,53,False,False,False,statistics,1500388351,True,
Is there a way in SPSS to account for a changing covariate?,2,4,False,False,False,statistics,1500390872,True,"I’m doing a study of two groups, one that attends consult A and one that attends consult B; both groups were measured at two points. I’m trying to find whether attendance to either consult directly correlates to changes on the independent variable (expressed as a numerical scale), accounting for covariates like gender and changes in weight.  
[I found this tutorial](http://www.statsmakemecry.com/smmctheblog/how-to-conduct-a-repeated-measures-mancova-in-spss.html) that helped me greatly, particularly on how to interpret the output, but I don’t know how to take into account for changes in weight.  
Is there a way in SPSS to account for a changing covariate? Or should I convert the weight from two variables to a single ± number (as in, + gained X kilograms – lost X kilograms)?  
  
*English is not my primary language, I’m sorry for any mistakes"
Some questions related to linear regression,5,3,False,False,False,statistics,1500394580,True,"Hey guys,

I am reading through *Introduction to Statistical Learning* and I have a few questions related to linear regression. I was hoping some of you guys can answer them.

1) Why does a pattern when plotting residuals indicate non-linearity? I'm interested in the why. Furthemore, how can I notice certain patterns? Is straight line not a pattern?

2) I got this output in R:

    Call:
    lm(formula = Sales ~ Price + Urban + US, data = Carseats)

    Residuals:
        Min      1Q  Median      3Q     Max 
    -6.9206 -1.6220 -0.0564  1.5786  7.0581 

    Coefficients:
                      Estimate      Std. Error t value    Pr(>|t|)    
    (Intercept) 13.043469   0.651012  20.036  < 2e-16 ***
    Price       -0.054459   0.005242 -10.389  < 2e-16 ***
    UrbanYes    -0.021916   0.271650  -0.081    0.936    
    USYes        1.200573   0.259042   4.635 4.86e-06 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 2.472 on 396 degrees of freedom
    Multiple R-squared:  0.2393,    Adjusted R-squared:  0.2335 
    F-statistic: 41.52 on 3 and 396 DF,  p-value: < 2.2e-16

I know that lower p-values mean that there that variable is more statistically significant. But, what is the t-value? What does it mean? Let's say I had an absence of p-value; how would I calculate statistical significance of a variable then?

3) This one is similar to question 2). I got this output from R:

    Call:
    lm(formula = y ~ x1 + x2)

    Residuals:
        Min      1Q  Median      3Q     Max 
    -2.8311 -0.7273 -0.0537  0.6338  2.3359 

    Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
    (Intercept)   2.1305     0.2319   9.188 7.61e-15 ***
    x1            1.4396     0.7212   1.996   0.0487 *  
    x2            1.0097     1.1337   0.891   0.3754    
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 1.056 on 97 degrees of freedom
    Multiple R-squared:  0.2088,    Adjusted R-squared:  0.1925 
    F-statistic:  12.8 on 2 and 97 DF,  p-value: 1.164e-05

I am asked to comment on rejecting the null hypothesis that beta 1 (the coefficient that is multiplied with `x1`) is 0 and also that I comment on rejecting the null hypothesis that beta 2 (the coefficient that is multiplied with `x2`) is 0. I think that when I know the answer to 2), I will also know the answer to 3), but feel free to add anything on this question if you need to.

Thanks in advance!"
Sampling distribution playground,0,1,False,False,False,statistics,1500404668,False,
Question About Wording from Cox Regression,0,1,False,False,False,statistics,1500405270,True,"Hi all!

I'm having trouble understanding the wording in the memo I got back from my institution's statistics department regarding my research project. I was wondering if you guys could help me out!

Univariate Cox regression was utilized to analyze potential factors affecting the effectiveness of a treatment. Explain this sentence for me: People with diagnoses are 0.583 times less likely to have nausea than people with no diagnosis. Does this mean that people with diagnosis are 1.72 times (1/0.518) more likely to experience nausea than people with no diagnosis?

Thanks for the help!!"
Interactive Mind Map to learn Statistics,1,12,False,False,False,statistics,1500408428,False,
Want to transition - career help please (HR Analytics),0,1,False,False,False,statistics,1500410297,True,[removed]
need advice in choosing a statistical test for a multi-sample qualitative data set.,2,2,False,False,False,statistics,1500411146,True,[deleted]
Calculating Proportional Change,2,2,False,False,False,statistics,1500419020,True,"My PI left me a series of instructions and I'm kind of stuck on calculating proportional change. I'm working on modeling home range size in animals, given in km^2, as a function of age. So a 2 year old male would have a home range size of 35 km^2.

We sampled a population of male cats over five years. So cat 1 would have home range values over that five year time span. We know the age of the cat and the home range size of the cat at that age. We calculated home ranges for 10 cats, every three months, over those five years. Now, due to accidents, sickness, whatever, we have limited data for some of those cats.

So basically, how do I calculate the proprotional change in home range size over time as a function of their age. Here's an example set of data. Cat 1 has home range size data for 2 years. We arbitrarily broke it up into 3 month intervals.

Age / Home Range Size

2 / 11

2.25 / 22

2.5 / 10

2.75 / 25

3 / 15

3.25 / 10

3.5 / 18

4 / 22


From the proportional change information, I'll do an arcsine transformation and a mixed effect model. Any help is appreciated! Sorry if this is quite elementary and simple. My background is in ecology and not statistics."
What's the use of Measure a Theory?,8,5,False,False,False,statistics,1500423584,True,"I mean in practical terms, what's the use of Measure Theory? I understand it is useful to develop rigorously the theory behind any field.  But does anyone ever actually use Measure Theory to aid in, say computation? Theory for theory's sake?"
Master's vs. Doctorate in Statistics?,0,1,False,False,False,statistics,1500425735,True,[removed]
Different scale data analysis,2,2,False,False,False,statistics,1500443506,True,"I have two independent variables that collect response in likert scale 1-5 (strongly disagree- strongly agree). Meanwhile I have two dependent variables also but they are measured in true false (1/0)..
How can I analyze the data (regression/correlation or any other) and draw conclusion??? (I am using spss)"
Need something to help fill the gaps,2,1,False,False,False,statistics,1500467803,True,"So I'm taking a summer Elementary Stats class, and I'm barely able to keep up. I need to find something that can help me out get a better understanding, any suggestions would be awesome."
Finding SE of Slope from SE of Estimate,4,3,False,False,False,statistics,1500468939,True,"I'm having issues finding the SE of the Slope to use for a T-test in a monte carlo simulation to test whether my coefficient (beta) is significantly different from 1. The problem is my intercept is fixed for 0.

I know the formula is   

SE of Estimate / SSX = SE of Slope

This seems to hold as long as there is an intercept. 

Using LINEST function in Excel I can confirm that SE of Estimate / SSX = SE of Slope is true when there is an intercept, but as soon as I don't have an intercept I can no longer replicate the results from LINEST.

I have linked an imagine from Excel to clarify if it isn't clear

http://imgur.com/a/ovSC9

What is the issue?

"
Measuring tempory increases in a time series data set,6,5,False,False,False,statistics,1500473452,True,"I have a times series data set and plotting a line graph makes it look like there is a 'bump' in the data set. Is it possible to find out if that increase is significant? 

The data is for the average goals per game for top division football in England per year. I have data for other leagues in Europe and South America to compare it against but I want to find out how I could test for significant differences in the data. 

Any pointers would be amazing. "
Help choosing Designed Experiment in Digital Marketing,0,1,False,False,False,statistics,1500474028,True,[removed]
Exclude level-2 groups in multilevel binary logistic regression,0,1,False,False,False,statistics,1500481586,True,[removed]
Need a breakdown of what statistics techniques I would need to solve these hackathon questions,5,4,False,False,False,statistics,1500483532,True,"Hi there. So let me preface my questions by saying I am not looking for answers to these questions. I would like the solve them on my own or programmatically. These pertain towards the hackathon that the NBA is hosting in NYC for university students.

**Basketball Analytics Questions:**

After the Golden State Warriors acquired former MVP Kevin Durant in 2016, some NBA fans speculated that the Warriors would not lose consecutive games at any point of the season.*

a) If you wanted to determine the probability that this prediction would be true (i.e., that the Warriors would never lose consecutive games at any point during an 82-game season), what is one approach (or a few approaches) you may use to solve the problem? What answer do you get? Exact answers are of course welcome, but approaches that lead to approximations (and those approximations) are fine, too (please specify the precision of your estimate). Assume the Warriors have an 80% chance of winning each and every game.

b) So, would you have agreed with that hypothesis?

c) Finally, at least what % of a team's games would a team need to be expected to win (assuming that win probability stays constant from game to game) for there to be a greater than 50% chance that the team never suffers consecutive losses at any point in the season?

**Bonus Question**:When are teams eliminated from playoff contention? Using the results for the 2016-17 regular season as a test case for your quantitative solution, please generate the date that each team was eliminated from playoff contention. We are purely looking for a date when a team was eliminated from playoff consideration, not any specific seed. 

**Business Analytics Questions:**

A term in an NBA team's lease agreement states that it must pay a penalty fee every time it fails to reach 17,000 attendees in three consecutive home games. The onerous fee has caused the team to request assistance in assessing its risk. The team estimates the probability it will reach 17,000 attendees in a home game is 75%, and each game has an equal likelihood of eclipsing the 17,000 threshold.*
a) Given the 75% likelihood of hitting that number each game, what are the odds that the team will avoid a three-game streak with attendance under 17,000 for the entire season? Please build a simple simulator to determine your answer. (Note that there are 41 regular season home games; assume each game is independent.)

b) The team has the option of pursuing an aggressive comp ticket strategy that can increase its odds of hitting the 17,000-attendee mark in every game. Suppose it wants to have a 90% chance of avoiding paying any penalty fees throughout the entire season. Assume the probability, p, of eclipsing 17,000 attendees is the same for every home game - and each game is independent. What probability p should it target in order to achieve a 90% chance to avoid paying any fees?

**TL:DR**: So it's been a while since I've covered stats or data intensive computing problems. But I've learned regression techniques, clustering techniques, and classification techniques in these courses. What types of techniques or other stats techniques would you and should you apply to each of these problems and why?


"
Weighted values from correlation coefficients,3,6,False,False,False,statistics,1500489022,True,"I've been pressed to find a similar existing example of what I'm trying to do, but maybe i'm not phrasing it incorrectly.

Essentially, I want to do a dynamic pricing model. Create a formula where i take the correlation coefficients of various variables related to pricing, and then when calculating a certain price, increment the price using a combination of the other variable value and its correlation coefficient to price.

Essentially creating some sort of weight value from the correlation coefficient that affects the calculations(the weights would get updated as the correlation coefficients get updated, as new datasets come in)

So if you have the variables : price, distance, timerange1(binary),timerange2(binary)

price = initial price + distance*correlation + timerange(0 or 1)*corr +timerange2(0 or 1)*corr

The operators in there are placeholders just to get the idea across, I was wondering if there is an established way of doing something like this, or the closest thing I could base this off of?

Thank you."
[Stats Noob] How to analyze this neuroscience data?,1,1,False,False,False,statistics,1500496773,True,"Hello friends!

Thanks in advance for any assistance you guys might be able to provide! I need a little help with what statistical tests to run for my datasets. I’m measuring changes the dynamics of dendritic spines in animals, and my experiments are trying to detect a difference between two types of animals in response to a specific manipulation (whisker trimming).

So, I have two sets of animals which are each then divided into two conditions -  whisker trimming and a sham trimming for control. Additionally, each animal has measurements from a baseline control period of 5 days, followed by the trimming, then measurements for another 5 days.

So, in effect we have 8 different data categories - for each of two different types of animals, for whisker trimming/vs sham, and for baseline vs after trimming.

The actual data set I’m getting are images of neural dendrites and their spines (i.e., like a tree branch and its leaves), once per day for 5 days in each condition. From this, I’m currently measuring:

Spine density - for a given branch, spines per unit length - I’m currently averaging this for each individual branch over the 5 days of imaging

Spine elimination - the % of spines that disappear from a branch per day, again averaged over the 5 days

Spine formation - same as above but for appearance

Survival fraction - for a given branch, what fraction of spines that were present on day 1 are present on day 2, 3, 4, and 5

Spine lifetime - for this the individual units are spines, not branches; this is just a simple measure of how many days a given spine is present out of the 5 days of imaging

Sorry if I wasn’t very clear; please let me know if I can make any clarifications! What statistical test(s) do I use to compare this data across conditions, and are there any more interesting ways to look at this data?

Thanks so much!"
Probability and Statistics with Calculus,11,1,False,False,False,statistics,1500500426,True,"Hey /r/statistics! I'm a college student going into my third year and I'm freaking out a little bit. My Statistics class has a notoriously hard professor in addition to involving Calculus. I was really good in Calc I with differentiation, limits, and basic indefinite and definite integrals but had a really bad time in Calc II with advanced integration and integrating volume. I wanted to ask anybody who had experience with a class like this what level and intensity of Calculus I'm about to face and it there's anything particular you would recommend reviewing. Thanks so much in advance!"
[stats noob] Statistical test for comparing scores,0,1,False,False,False,statistics,1500503477,True,[removed]
How to find test statistic (x^2) in spss or any online calculator. Given an actual and expected group.,1,0,False,False,False,statistics,1500506212,False,[deleted]
calculating playoff odds for fantasy football,1,2,False,False,False,statistics,1500506233,True,[deleted]
Formating data for survival analysis with delayed/late entry?,4,2,False,False,False,statistics,1500524917,True,"Can someone recommend some references that describe in specific and practical terms what the data set should look like? I have read Singer and Willett and Rabe-Hesketh and Skrondel (sp?), but I am still not clear on it. 

Based on my current understanding, in the case of discrete time, we add people who have been at risk since before the start of the study so that only begin to contribute data after X number of observations for which they have been at risk. 

For example, if we take a random same and conducted interviews every year for three years and want to do survival analysis for time till some event. Say risk begins when the sample is age 17, but some members were already 19 at first data collection. In a person period data set, a person who was 17 at first observation would contribute data for observations at age 17, 18, and 19, while someone who is 19 at the start would contribute data to three periods corresponding to age 19, 20, 21. 

Is this correct? "
Question about the TI-83 Plus Calculator,0,5,False,False,False,statistics,1500525282,True,[removed]
Please help me solve this,0,1,False,False,False,statistics,1500526167,True,[removed]
Exclude level-2 groups in multilevel binary logistic regression,0,1,False,False,False,statistics,1500537586,True,[removed]
Correlated Variables in (Two-Step-)Cluster Analysis,0,1,False,False,False,statistics,1500545127,True,[removed]
"Hello, I am taking a class next semester called Psych 161 and need some help explaining what's the difference between stats and this class.",0,1,False,False,False,statistics,1500549154,True,[removed]
Economicshelpdesk.com Offers Statistics Assignment Help for Better Result Bigger Score,0,1,False,False,False,statistics,1500550266,False,
[Stats Noob] How to analyse these data (carpal tunnel)?,5,2,False,False,False,statistics,1500550439,True,"1) Assessing the suitability and accuracy of diagnostic questionnaire (the Kamath-Stothard) in diagnosing CTS in comparison with NCS 

Background: We will be studying the effectiveness of a certain questionnaire in diagnosing carpal tunnel syndrome, with the results from NCS (nerve conduction study) being the reference point. Basically, if nerve conduction study says yes and the questionnaire says yes to the diagnosis, the questionnaire is said to be valid for that particular case. 

2) Assessing the correlation of CTS severity as measured by NCS (via the Canterbury NCS Severity Scale) to CTS symptom severity as measured by the Boston Carpal Tunnel Syndrome Questionnaire (BCTQ) 

Background: NCS measures the effectiveness of nerve conduction. The BCTQ measures the severity of symptoms exhibited in carpal tunnel.

What kind of statistical analyses do we use in these two cases? Help! 

Thank you!

Edit: our sample size is around 250 cases"
Data Structures Related to Machine Learning Algorithms,3,10,False,False,False,statistics,1500559857,False,
Is a Masters in Statistics possible?,17,2,False,False,False,statistics,1500560573,True,"I was an economics major (3.8 GPA) and have only taken up to Calculus I. I'm interested in pursuing a Masters in Stats or Applied Stats so that I can eventually pursue a career in analytics in the future. If I take Calculus II-III and additional math courses such as Linear Algebra at a local community college, do I have any chance at a Masters program? "
Statistical analysis of control vs treatment. Can I analyze in total trials or animals?,0,1,False,False,False,statistics,1500569336,True,"Hello,

I have ran an experiment. 
In the same group of mice (n=6) i have ran a control injection, control injection, control injection, and drug injection, drug injection, drug injection.

originally I planned to average the 3 control and 3 drug days and then group the data among mice.
But, someone in my lab mentioned that I could list it all as trials, so when i run the stats instead of having an n=6, i could run it as a n=18. The average of an average is still an average so really all that would be changing is the STDEV / Error of the means  (and all that jazz). My question is, is this something thats allowed or is this an statistical calculation error?

Also, instead of averages I was planning on summing the results of the three days. But the question still remains, can i analyze them a  18 trials  or n=6 mice?

Hope that makes sense.

Also, can someone also explain it in both ELI5 and statistical jargon so I can help defend the choices in my next thesis committee meeting. Really appreciate the help. My supervisor is statistically challenged as well haha! I really want to do the statistics as accurate as possible and had no where else to turn. Thank you in advance.

Thank you,
a statically challenged student"
Gage r&R vs Nested Anova,1,1,False,False,False,statistics,1500574558,True,"Is gage R&R just a subtype of a Nested Anova?  And if so, why even run a gage R&R when a nested Anova gives you more info, flexibility, and allows more levels of nesting?"
How to do T Tests on Aggregate Data?,7,1,False,False,False,statistics,1500578287,True,"I have an aggregate data set, with the total N of each test and total number of successes per test. I do not have access to the data at the individual level, nor do I have SD. Would it be possible to perform a hypothesis test with only this information, and if so how?"
"I like to count things. I recently spent 3 days at Disneyland (Anaheim, California) and counted all the pro sports apparel I saw. Here are my findings.",19,45,False,False,False,statistics,1500584046,True,"Methodology   
I spent Sunday, Monday, Tuesday (July 16-18, 2017) at Disneyland. Beginning with my check-in to the Disneyland Hotel at 6pm on July 16 until midnight July 19 I counted every instance of pro sports apparel I saw. During this time, I was (mostly) actively searching the crowd for instances. This includes the Disneyland Hotel, Downtown Disney, Disneyland, and Disney California Adventure. If one person had multiple instances of memorabilia (hat and shirt for example) I only counted that once. If I saw the same person within a short timeframe, I only counted that person once, but it's possible a person got counted multiple times after an hour or so. Generally, I remembered the numbers in my head for about an hour or so, then jotted the totals down on my cell phone workpad. Mistakes are possible, but I have a pretty good memory, so the final numbers are pretty robust. I did not count any college sports, even USC which some would argue qualifies as a pro sports team.   

Findings   
Any teams not listed indicates that I witnessed zero instances of their apparel.   

NBA   
Warriors - 192   
Cavaliers - 3   
Lakers - 2   
Hawks - 1   

NHL   
Kings - 2   
Blackhawks - 2   
Rangers - 1   
Bruins - 1   
Canadiens - 1   
Blues - 1   
Ducks - 1   

MLB   
Dodgers - 51   
Angels - 6   
Rangers - 4   
Royals - 4   
Giants - 4   
Blue Jays - 4   
Nationals - 2   
Yankees - 2   
White Sox - 1 (it was a Billy Koch jersey; not too many of those out there)   
Red Sox - 1   
Astros - 1 (me)   
Diamondbacks - 1   
Cubs - 1   
Tigers - I'm not sure. The stylized D for Detroit looks a lot like the Disney D. I didn't see any that were definitely Tigers, so the ones that were borderline I just assumed were for Disney.   

NFL   
Steelers - 6   
Cowboys - 4   
Chargers - 3   
Packers - 1   
Texans - 1 (me)   
49ers - 1   
Colts - 1 (FTC)   

MLS   
Real Salt Lake - 1   
Portland Timbers - 1   
L.A. Galaxy - 1   

Other   
FC Barcelona - 1   
Tottenham Hotspur - 1"
Why isn't everything normally distributed?,9,3,False,False,False,statistics,1500594809,False,
Purposely AB testing for inconclusive results?,6,1,False,False,False,statistics,1500598142,True,"My agency is tasked with rolling out a new site design, but they want to make sure the results to their core KPIs are neutral.

Is this as simple as testing for inconclusive results with a pre-determined sample size? Is there a different kind of analysis our team should be doing?"
Resources for research?,0,1,False,False,False,statistics,1500604541,True,"I'm applying to a call center forecasting position. Apparently they're in the midst of their modeling, and they have new data they want help utilizing to inform their new model. I'm familiar with call center forecasting, and the types of information that informed the forecast, but I was using the information provided by the forecasts to inform my decisions rather than creating forecasts. I think it's something I can learn to do, but it sounds like they want some one to hit the ground running with modeling and regression analytics. Suggestions for research so I could go in to this more informed than I am?"
ROC Curve - Help?,0,1,False,False,False,statistics,1500605109,True,[deleted]
Looking for information on what to do with data collected from a cell line experiment,1,1,False,False,False,statistics,1500606523,True,"I am a new to working with cell line experiments. A lab person that I know ran a study comprised of a set of identical experiments run on several different days. Each experiment involves the treatment of cells from the exact same cell line. I am very confused about how cell line data is analyzed, since to my (limited) knowledge, they are essentially one biological sample that has been expanded into a larger pool of cells. My confusion involve issues which include, is this an n of 1?..., does it make any sense to run an analysis only to conclude with a result *within* a particular cell line, are the various sources of variation biological or technical, etc. 

Any advice would be greatly appreciated"
"Which statistical test should I use when comparing data (paired, raw count data) from 2003 to data from 2006?????",0,1,False,False,False,statistics,1500641423,True,[removed]
"Anyone has to share any ideas on how to present multidimensional ""clustered"" data?",14,8,False,False,False,statistics,1500641725,True,"I am doing this calibration experiment where I try to find the relation between the parameters of 2 different models. So for different values of the first models 5 parameters I would like to see if different groups cluster to certain values of the latter model's parameters.


EG. see if for a high variance low displacement I end up calibrating to parameters close to the ones I end up with for low variance high displacement.


Any ideas on how to present such data graphically?"
[Stats Moron] Help with Two Way ANOVA please!,0,1,False,False,False,statistics,1500644553,True,[removed]
How to manage subgroup averages in Minitab for a large data set?,0,1,False,False,False,statistics,1500644882,True,"I have a large data set that is not normally distributed, nor does it appear to be transformable.  I have been advised to place the data into subgroups and use those subgroup averages to create normally distributed data.  


Can Minitab provide multiple subgroup averages, or am I just going to need to separate the data out and calculate the subgroup averages myself?  (The data set has about 20,000 points.)


Thank you."
Econometrics Empirical Study Advice,0,1,False,False,False,statistics,1500648262,True,[removed]
"Conference Recommendations for someone working on clinical trials, Patient Reported Outcomes or Psychometrics",5,2,False,False,False,statistics,1500654439,True,"I work in pharma where I specialize in patient reported outcomes/quality of life (highly related to psychometric) and clinical trials. I'm trained as an Epidemiologist (but working as a Biostatistican), thus my training in statistics is more applied (i.e. I haven't taken calculus for example). So I am unsure conferences such like the one by the American Statistical Association, might be useful for me as it may be too high level math for me. However, I am in charge of developing analyses for projects and try to think out of the box.

I was talking to my boss (who is based in Europe, but I am based in the US) about conferences and it might be useful for me to identify possible conferences that is useful for me to attend to network and grow scientifically.  

Does anyone have any recommendations on what conferences would be worthwhile for me to attend? I am starting to generate a list, as of right now, I have the two main pharma conferences (ISOQOL and ISPOR) on my list.. but looking for others to consider."
Opinions on combining multiple two waves of panel data into a single wave?,5,3,False,False,False,statistics,1500655678,True,"I have five waves of prospective panel data. The first two are six months apart and refer to the previous interview as the reference period. The latter three do the same, but are a year apart. 

I can think of all sorts of reasons related to measurement validity and reliability as to why averaging/combining the first two waves to create a single wave with comparable reference period might be a bad idea, but is there really a problem with it?

Does anyone have opinions on the matter?"
Introducing /r/arXiv_Plus,0,3,False,False,False,statistics,1500665738,True,"/r/arXiv_Plus is a new subreddit dedicated to helping scientists and science enthusiasts find papers that might interest them on arXiv.

A resource like this has become necessary due to the massive influx of papers that arXiv receives each year (over 100,000), making it difficult for readers to find which papers best suite their interests. By introducing Reddit's voting system, users can effectively curate arXiv, improving everyone's experience with it.

/r/arXiv_Plus encourages readers to submit any papers they find that they consider to be well written and of note within their respective field. When making a submission, there is a full suite of post flairs available to allow users to more easily find papers that may interest them. In addition to that, there is a set of six buttons on the sidebar, allowing for users to filter all posts by physics, mathematics, quantitative biology, quantitative finance, computer science, and statistics (so if I only want to see papers on statistics, I would click the ""statistics"" button).

Only links that direct to arXiv.org are permitted (and no URL shorteners).

Users are required to give a brief explanation of the paper they have posted in the comments shortly after posting. If this explanation is found to be insufficient, you may be asked to improve it.

I have taken the liberty of preparing an example post on the subreddit already.

The subreddit includes a monthly sticky post for aiding users in finding papers that they recall reading, but cannot find. Papers from all sources are permitted, but arXiv links are most encouraged.

There is an automoderator customized to assure a safe and professional environment for everyone, but moderator positions are open to anyone who believes that they have something to contribute to /r/arXiv_Plus as a moderator (\*cough* CSS wizards).

Thank you, and happy reading! I hope to see you around on /r/arXiv_Plus!

---

Before anyone asks, I did get permission to make this post from the wonderful moderators of /r/Statistics.

---

If you have any suggestions on how to improve /r/arXiv_Plus, feel free to [message the moderators.](https://www.reddit.com/message/compose?to=%2Fr%2FarXiv_Plus)"
Determining which variables correlate to ranking (PCA?),7,3,False,False,False,statistics,1500667762,True,"Hi everyone, 

I'm an engineer who took basic stats and linear algebra but this is a little over my head.

Here is the problem:

Say I have a list of criteria whose values can be 0 or 1 and a list of outcomes that are ranked. For example, ten people ran a race and their outcomes are their ranks 1-10. For each of these ten people I have responses to a yes or no question (did you run competitively at the college level, did you eat breakfast this morning, have you been to Brazil before), some of which I expect to correlate with the outcome and some of which I don't.

I want to determine which of the question(s) best predict how the racers finished.

My intuition says this is a dimension reduction problem that might involve PCA. But I don't know how to incorporate ranking into PCA - it seems like PCA would answer the question, ""which questions are most related to each other?"" Does the ranking simply become another dimension?

I am probably way off base, so any prod in the right direction would be greatly appreciated!

"
ratio of estimates,1,3,False,False,False,statistics,1500670179,True,"I am ultimately trying to estimate x_1/x_2. I have models a_1, a_2 and b_1, b_2. If we consider x_1, then model a performs best by a lot in mse. The same thing can be said for x_2. Yet, the mse for the ratio estimate is slightly better with model b. 

Has anyone encountered this before? Some resources or a place to look would be great."
What is the Probability for Grandchildren to Be Born G-B-G-B-G-B-G-B-G-B-B? The last one was a Boy.,21,0,False,False,False,statistics,1500685809,True,[removed]
"(probably a very stupid and easy question:) multiple linear regression (SPSS). I have an independent and dependent variable, each with sub-groups. why am I only getting one group as my model?",9,4,False,False,False,statistics,1500688495,True,"to make this all the more easy, I will paste a picture of what I am looking at in SPSS:

http://imgur.com/a/1an1b

-

I am trying to do a multiple linear regression test to find a correlation and subsequent statistical significance between 2 variables, and both variables have sub-variables (if that's the term?). basically there are different answers to each question. I am wanting to know if one particular kind of answer within each of these correlates well and with statistical significance

but my table at the end is only showing the independent variable without regards to the different sub-answers (as this is a questionnaire). why is this? I want to know whether distinct variables go well together but it seems to be lumping them all in. what am I doing wrong? I want it to go down the list with each sub-group at the end, but it won't

I'd be amazingly grateful if somebody could help me with this - I am a total statistics novice :/ and this is pretty important (for a dissertation)

I have looked this kind of problem up and I'm told that it has to do with dummy variables - do I need them? "
How to start getting into election forecasting?,3,2,False,False,False,statistics,1500693166,True,"I am a third year mathematics students and I am really interested in election forecasting. Are there any books, techniques or papers that I should read to get a start? 

Thanks for any suggestions :)"
Market Research Statistics,14,12,False,False,False,statistics,1500694749,True,"Hi all,

I majored in psychology (BS - 2 stats classes) and have worked in a couple clinical research labs and a market research firm (very shoddy - only stayed ~5 months) since graduation.

I really enjoy market research and am hoping to increase my statistical knowledge to improve my chances of getting some interviews for larger research firms (plan to apply in the next few months, once I'm comfortable I can talk intelligently about the subject matter).

What level of statistics is necessary for these jobs? A masters in stats seems overkill and my background likely isn't suitable (I understand the theory behind hypothesis testing and such, but I've only taken up to calculus I, so haven't gotten too heavily into the mathematical side).

I've seen ads mentioning both SPSS and R. I am proficient enough in SPSS to code and do basic data analysis. I have very rudimentary knowledge of R. Should I spend time advancing my skills in this area?

Thanks!

"
Is there a graph out there that shows the percentage of virgins by age?,0,1,False,False,False,statistics,1500714072,True,[deleted]
Max. concurrent users peak,0,1,False,False,False,statistics,1500714463,True,[removed]
13 CTA Stats to Quantify Its POWER for BRANDS,0,1,False,False,False,statistics,1500716137,False,
Need help with power analysis with G Power to determine sample size needed for study,0,1,False,False,False,statistics,1500724762,True,[removed]
Experimentation Design Analysis,0,1,False,False,False,statistics,1500724961,True,[removed]
Statistics and Data Science resources for Rubyists,0,4,False,False,False,statistics,1500726602,False,
Need help with Power analysis to calculate needed sample size,0,1,False,False,False,statistics,1500729346,True,[removed]
Empirical Bayes - Cherry picking?,11,9,False,False,False,statistics,1500734695,True,"So I just finished reading a great book [Introduction to Empirical Bayes](https://gumroad.com/l/empirical-bayes#). I thought that the book was great, but building priors from the data felt wrong. I was trained that you come up with an analysis plan then you collect data then you test the hypothesis you previously determined in your analysis plan. When you do an analysis on data that has already collected this puts you into post-selective inference where you have to be much more stringent on what you call ""significant"" [see here](http://www.pnas.org/content/112/25/7629.short). I think that machine learning has something analogous which is called ""cherry picking"" which means picking predictors before setting up test and training sets([Introduction to Statistical Learning](https://www.youtube.com/watch?v=S06JpVoNaA0&feature=youtu.be)).

Given what I have previously learned it seems to me that empirical Bayes is based on a weak foundation. Do people use it just in settings where data was generated passively. If so this may be justifiable, but it does not seem correct to use it when doing rigorous experimental design, yet I know that Brad Efron does use empirical Bayes specifically for Biostatistics, generally a very NHST field.

My questions are: 

(1)How is empirical Bayes valid? 

(2)In what situations is it used? 

(3)In what situations should you avoid using the empirical Bayes approach and why? 

(4)Are people using it in fields other than Biostatistics and if so in what situations are they using it?

"
New Nature Human Behavior paper: 72 of us make the case to redefine statistical significance from .05 to .005,41,73,False,False,False,statistics,1500747173,False,
In need of some participants for a 5-minute survey,0,1,False,False,False,statistics,1500753646,True,[removed]
Lecture notes from a workshop on survey writing,0,3,False,False,False,statistics,1500756297,False,
Accurate Prediction of Electoral Outcomes,0,2,False,False,False,statistics,1500761802,False,
ELI5: Difference between standard deviation and standard error,14,39,False,False,False,statistics,1500765209,True,Could someone explain this with possibly an example? Thanks!
"An Open Science Project on Statistics: Doing the power analysis, equivalence test, NHST and computing the Bayes Factor to compare the IMDB Ratings of a few most recent movies by the legendary directors Satyajit Ray and Akira Kurosawa (in R)",0,3,False,False,False,statistics,1500766222,False,
What can you do with a bachelors in Statistics?,0,1,False,False,False,statistics,1500767389,True,[removed]
Mediation: nonsignificant mediator in c' output,0,1,False,False,False,statistics,1500768178,True,"My question concerns how to interpret a standard Baron & Kenny (1986) mediation output. My results are essentially the following:

* Pathway c (DV ~ IV): significant
* Pathway a (M ~ IV): significant
* Pathway b (DV ~ M): significant
* Pathway c' (DV ~ IV + M): nonsignificant

However, when looking at the output for pathway c' containing both the mediator and IV as predictors, my mediator is now nonsignificant despite being significant in the output for pathway b.

Does this matter for mediation? Or, so long as pathway b is significant, then it does not matter if the mediator is now nonsignificant in the output for pathway c'?"
"As a Masters student, what kind of part time jobs should I be looking for?",5,4,False,False,False,statistics,1500773190,True,"As the title says, I want to take out as little loan as possible while going to school. My Bachelors degree is in mathematics and I used to work part time as a tutor.

Right now I'm looking for a research assistant position at my new school. But if nothing lands, are there anything else I can look for? I know there are always restaurant jobs and cashiering at a grocery store. But I want a little more since I've got my bachelors degree now. 

edit: ps my Masters is in Biostat, if that makes a difference. "
how would I make a computed variable out of this mathematical formula?,3,1,False,False,False,statistics,1500775359,True,"I am new to SPSS and I have a variable that I need to compute. However, it uses square roots, squares and brackets.

my formula is:

the square root of: (half of:) (variableA - variableB) ^2

yes, I'm probably using too many brackets - I'm too used to using a calculator whereby it cared to this extent about them

I don't know how to input these into SPSS when computing a new variable - I thought I knew how to do by using the term ""sqrt(_)"" to do the square root, but that didn't work - or maybe it didn't work because my brackets are pretty liberal.

if anybody could help me here I'm be really pleased"
How to use a generalised linear mixed model to make predictions?,9,11,False,False,False,statistics,1500819266,True,"I've run a GLMM and been presented with a table of values. I know it should be easy to arrange these values into an equation which will allow me to calculate estimates of my target variable (D). What's the correct way of formatting this output into the equation? Something to do with exponentiating the coefficient?

I'm not 100% sure that I should have used a 2-way model for my fixed effects, can someone shed light on that, too?

http://imgur.com/a/aTkyo

Many thanks."
Distance learning- Statistics,13,13,False,False,False,statistics,1500834133,True,"Hi, 

I'm keen to retrain in statistics and struggling to figure out what my distance learning options are. 

All i can seem to find is the OU, which is very expensive and looking like it might be on the way out. Everywhere else that does stats distance learning seems to be post grad stuff. 

Can anyone help?"
"Engineers, why did you switch to statistics?",15,22,False,False,False,statistics,1500834138,True,"Hello r/statistics!

I have a bachelor of science in chemical engineering, and until recently I've been interested in medical school.  My interests changed when I started a clinical job, and I realized that I may not value patient care enough to justify the personal and monetary sacrifices involved in becoming a physician.  Biostatistics has captured my attention because it seems to be a way for me to become involved in a broad scope of biomedical research, and it resonates with my lifelong interest in computer programming.  While I plan to continue my clinical work to see how my perspectives on medicine evolve, I'm trying to gain a thorough understanding of biostatistics to determine if the career is right for me.  Today, I'm specifically interested in learning about the field from the perspective of former engineers.  Why did you make the switch to biostatistics?  Why didn't you pursue graduate education in engineering instead?  Do you have any words of wisdom for someone considering the career field, pitfalls to avoid, etc?

Thanks!"
[Career Advice] How to become a science writer/journalist with a stats background?,0,1,False,False,False,statistics,1500849838,True,[deleted]
How to get into science writing/journalism with just a BS in stats?,5,10,False,False,False,statistics,1500850601,True,"Last year I graduated with my BS in mathematics/statistics, and have been working as a data analyst at a pharma company since.

Recently I've begun to consider pursuing a career in science writing/journalism, but I've noticed that most of those reporters either have a deep background in journalism and an interest in science, or a graduate-level education in a field of science, and an interest in writing.

While I'd like to focus on health/epidemiology topics, as it's what I'm most passionate about (though, I'd be happy with any discipline in science), I don't really have any professional or formal educational experience in that field.

Is there a feasible route to breaking into that field for someone with a background in stats?

My stats education has given me a fantastic toolkit for understanding and assessing scientific papers from many fields (and I do it a lot for fun), but I'm not sure if that training qualifies as sufficient expertise for that sort of job.

Any insight or advice would be greatly appreciated, thank you!"
Biostats PhD,3,2,False,False,False,statistics,1500854771,True,"Good evening r/statistics,

I'm a recent (within the last two years) graduate with a degree in mathematics and am getting myself prepared for graduate studies.   To give a little background info, since I was a kid, I wanted to be a part of the medical field in some capacity. In undergraduate, I started off as a biology major doing pre-med, but as time went on I realized I enjoyed the rigor of mathematics and its applications in biology. So I transitioned from a biology to a mathematics major, where I focused on computational mathematics and mathematical biology for my upper level elective requirements. I also was fortunate enough to be selected to work on two separate research projects that spanned over a two year period, which focused on biomechanics and computational neuroscience/dynamical systems, respectively. This research made me realize that my true passion was to work in the medical/health care field as some type of mathematician.

Now with that being said, I applied to mathematics graduate programs at the end of my final year of undergraduate, and was accepted to one without funding (I did not take this offer). I am still doing some work and am set to submit a paper on my computational neuroscience project to a journal before applying this fall, but I am leaning towards applying to mathematics, computational neuroscience, and biostats programs. 

The reason I include biostats programs is because I've become much more interested in stats in the past two years, as I've worked in local government on police data. I've also noticed just how involved biostatisticians are and continue to be in biomedical and health care research, along within pharmaceuticals, which is awesome! As somebody who wants job security, to work in a group with medical/health care professionals, and to not necessarily work in academia, along with the possible research projects I could work on in my PhD (i.e. clinical trials, risk prediction, survival analysis, statistical computing, etc.), biostats programs look very attractive.

However, I only took one calculus-based stats course in undergrad and since I've been out of school for awhile, I am beginning to feel overwhelmed. So for those here that may have similar backgrounds, can you tell me what I'd expect from graduate school in biostats and how much catch up I'd have to play to succeed? As a final note, I've picked up some intro stats books and am planning on auditing real analysis in the fall as a refresher while studying for my GREs and working on my paper. I think this will be helpful in terms of my prep.

Thank you all so very much.

"
Best statistics book?,2,4,False,False,False,statistics,1500859659,True,"I realize this is very general, but does anyone know a well rounded stats book that covers a variety of methods/concepts. 

I'm looking for something that helps understand why we run certain tests, where they come from, how to interpret etc, but without having to dig through my individual stats textbooks. My textbooks can be little number heavy and circumvent the main ideas at times. A go to book for general stats questions would be great."
Weighted Average,0,1,False,False,False,statistics,1500864601,True,[removed]
Bachelor of statistics?,1,1,False,False,False,statistics,1500868524,True,[removed]
Any statistics master here can help with statistics problem??,0,0,False,False,False,statistics,1500874355,False,[deleted]
Great resources for statistics?,11,19,False,False,False,statistics,1500885737,True,"Hello guys, I'm looking for books, MOOC's, courses from intermediate to advanced stats, bayesian and robust stats, etc.

Does anyone have great resources I could read and study to improve in those areas?

Thanks a lot!"
How to find correlations using frequency data (Spearman's Rank)?,0,1,False,False,False,statistics,1500892511,True,[removed]
Sales forecast including price impact in excel,1,2,False,False,False,statistics,1500896927,True,"This is all in excel.

I have sales data for the past 3 years and I have been successfully using triple exponential smoothing (Holt winters) to forecast sales for a company.

However due to the company now focusing on price changes and promotions in the future, I will want to add the price factor along with it. I will most likely have price info for the last 3 years too and pretty much will need to forecast sales given a particular price.

Is there a way I can add this parameter to triple exponential smoothing? Or is there a way to combine another model with it? Or do you guys suggest just starting with something else altogether?

Please provide some guidance as I have researched a bit but couldn't come up with any actionable advice.

Thank you."
Interview Question: How much data would you get for regression?,8,13,False,False,False,statistics,1500910002,True,"During a previous interview, I was asked to do a case study on a classification problem. I chose logistic regression, and then I was asked how many data I would get. I mentioned that depends on the cost of data acquisition and the ""variance"" that we want for the model. It was accepted but I'm not sure I answered it right.

By ""variance"", I meant the bias-variance trade off in a model. For example, if you use more training data (n is large), the ""variance"" of the model will be smaller in that if you used a different training data set (equal n), your model wouldn't be that different. For instance, if I decides to  use a model with 3 predictors, if I have 1000 training points, the variance of my model will be a lot smaller than if I had 5 training points. In another word, should I use a different 1000 training points, I wouldn't expect my model to change that much. But in the case of 5 training points, if I had gotten a different 5 training points, my model might be drastically different.

However, I'm not sure how to ""quantify"" this variance. Is there an equation for it?

I know for linear regression, there is an equation that relates the standard error of the OLS coefficients to the number of data points n (right?). Is this related to the variance of the model that I'm talking about? But if you have 3 predictors, and each coefficients have their own standard error, how do you relate that to the variance of the overall model?

Even though I used linear regression in my example, I guess works similarly for logistic regression?

Thanks"
Modifying 5 year cancer survival outcomes for 2 year study,0,2,False,False,False,statistics,1500910225,True,"(I also posted in r/math)

Hey guys,

I'm currently working on a cancer study. We want to validate the truth of predictive survival outcomes. The survival predictions are likelihood death %, 5-year predictions based on age, sex, tumor size, cormorbities etc. I'm using the val.prob function in R to compare probabilities to outcomes. Our hypothesis based on previous data is that this prediction is not accurate. We want to directly show that by using actual survival probabilities from 87 patients. 

The problem is that we don't have 5 year data, we have 2 year data. I've thought of two ways to go about this problem and wanted to know what you guys think.

The first is a simple way, by turning the 5 year probabilities into single year probabilities. The question I have here how to do this. If for example, there is a 50% chance of dying in 5 years, what's the probability of dying in a single year? (Assuming equal probabilities)

The other is using the KM survival curves. As they approximate the true survival outcomes, is it possible to use this? If so, how?

Thanks for any ideas and help. Sorry if this is the wrong sub."
Conditional probability,0,1,False,False,False,statistics,1500910282,True,[removed]
FREE Book suggestions:Introduction to Statistics,0,1,False,False,False,statistics,1500920456,True,[removed]
Permutation Test vs. T Test,3,7,False,False,False,statistics,1500920806,True,"Is there ever a time when a T Test would be preferable to a permutation test, all other things being equal? Permutation seems like the superior option in most cases. "
How to cluster the data for matrix? How to avoid having empty entries in matrix?,0,1,False,False,False,statistics,1500929022,True,[removed]
"Any idea how employable a ""Computational Science"" degree would be?",0,1,False,False,False,statistics,1500931460,True,[removed]
How to find the equilibrium of three variables?,5,6,False,False,False,statistics,1500938280,True,"I'd like to graph and find the equilibrium of a data set with three variables (see table). I'd like to achieve essentially a 3D version of a  typical supply and demand curve, in that the equilibrium is identified. 

Has anyone previously achieved this? Is there a best practice for doing this? Any help would be greatly appreciated! 

Cheers,


Name | Var1 | Var2 | Var3 |
---|---|----|----|----
Name 1 | 1900 | 0.68 | 3.25 |
Name 2 | 1000 | 0.72 | 2.6 |
Name 3 | 390 | 0.60 | 1.74 |

Edit: I'm not necessarily looking for this to be made, just to be pointed in the right direction"
Manually calculating z scores,0,1,False,False,False,statistics,1500944738,True,[removed]
Include the number and biomass in same bar graph,2,1,False,False,False,statistics,1500949410,True,"Hello guys.

I have a data set which has 

    Treatment   Weight   Number
    W1             32          3
    W2             43          5
    W3             40          3
    W4             44          7

How can I approach this data set. I want to show the number and biomass in same bar graph. The X axis will be the treatment i guess. I am using R studio

Thanks"
How to combine standard deviations of two sample groups of different size?,1,3,False,False,False,statistics,1500971944,True,"Hey everyone, sorry if the question is common but I couldn't find anything specific in the subreddit (or anywhere, for that matter).

I have a set of 24 data points that I have to combine to a set of 48 data points to produce an average ± standard deviation. However, I don't have the individual values of the second group, only its final average and standard deviation. In short, I have two means and two standard deviations that I need to combine into one.

I have been scouring the internet but I only seem to find people contradicting each other. The only useful thing I have found is a formula that seems to work, but only when the sample size is the same, which is not my case. This formula gives the combined variance of the groups as the sum of variances for the two groups divided by two, plus the difference of means divided by two and squared, [found here](https://stats.stackexchange.com/a/276035).

Is there any decent way to take into account the different sample size and solve this problem?

EDIT: also, the only post that I found on here that seemed to talk about that points to a Wikipedia section that is not there anymore, as it was linked to OP about 2 years ago.

EDIT 2: I may have found it while waiting for an answer. [This formula](https://stats.stackexchange.com/a/56000), the last one specifically, seems to successfully produce the same STDEV from the two individual ones as the one created by combining the individual data, both with identical and different sample sizes. I'll leave it here if anyone needs it or if anyone wants to confirm it."
A simple (probably) SPSS question,2,1,False,False,False,statistics,1500976269,True,"I'm quite new to SPSS and need help with multiple response sets. I have a few data sets based on questionnaire answers. 
The issue I'm having is one question can (but not always) have >1 answer and as a result the data has more than 1 column per these questions (e.g. Q1 a , Q1 b, Q1 c etc.
Ive defined these as 'multiple response sets' but cannot see where to utilise these for further analysis e.g. to select cases with a certain pattern of answers or to get demographics based on these MR sets and not the individual parts.
Can anyone help out or point me to a good resource which would show me how to do these next steps?"
Before and after guidance from Statswork for analysis and interpretation,0,0,False,False,False,statistics,1500976728,False,
"Percentage accumulation data over time between two treatments, is ANOVA appropriate?",4,5,False,False,False,statistics,1500988524,True,"I'm working with percentage accumulation data (% of fish arriving at a camera, increasing to 100%) between two treatments (Camera drops with/without presence of a second species) over ten 3 minute timebins. 

Would a Univariate ANOVA in SPSS looking at the significance of interaction between treatment and time (Timebin*Presence) be appropriate for testing difference in the accumulation rates? 

[Simple graph showing the two data series](http://imgur.com/a/yg720)"
How much statistics do I need to know to do machine learning?,1,0,False,False,False,statistics,1500992643,True,[removed]
What is the difference between fixed regressors and conditional regressors in linear regression?,0,2,False,False,False,statistics,1500997893,True,"To my understanding, by conditioning on regressors they become non-stochastic which I thought meant fixed. But in some of the older statistics literature I have seen distinctions between the fixed and conditional but no clear explanation of what is the difference."
Need help generating random numbers within groups in SPSS,1,3,False,False,False,statistics,1500998501,True,"So I have a large dataset with over 100 organizations, each of which has an average of, say, 70 respondents, but ranging from 1 to 282 respondents per organization - so a large range.

I want to randomly assign respondents (rows) to one of two sub samples. I want to then use one subsample for my X variable and a separate subsample for my Y variable (analysis is at the organizational level). However, I want to be sure that each organization has approximately half of its cases in each subsample (I'll probably exclude organizations with very low n's). 

Does anyone know of a way I can randomly assign numbers to each row within organizations?

Please feel free to ask clarifying questions if something I've said doesn't make sense!"
Merging cross-sectional data,6,1,False,False,False,statistics,1501001154,True,[deleted]
Some super simple questions on grade inflation project (building normal distributions),2,8,False,False,False,statistics,1501003000,True,"I'm doing writing a project on some for statistical explanations behind my school's grade inflation / grade inflation in general.

Some questions on this topic:

1. I've made normal distributions for the GPA of each year with their different means, it's fair to assume they'll be normally distributed because of the Central Limit Theorem right? 

2. My school did not provide data on a std deviation. I'm currently estimating .35 because this cumulatively has 99.63% of the distribution once I reach 4.0 for the year I'm currently testing. Where something like .5 only cumulatively has 96.99%. I know this is kind of a wonky way to estimate a variable. I know it has to be between .3 and .6 because anything else seems huge or too small in a 4.0 scale. If there's a mathematical way to estimate std. deviation just given mean and sample size (not the individual sampleS' data) let me know.

3. One hypothesis for the grade inflation is that the incoming classes are becoming more competitive since there are more applicants and a lower percentage of acceptance. Essentially what I want to do is convert former year's distribution of grades into 2016 acceptance numbers, so if in 1990 acceptance was 40% and 2016 it's 30%.** I want to cut off the bottom 25% of the 1990 distribution and calculate a new mean for the remaining 75%.** How can I do this? I understand how to find z-scores and the place where there is 75% remaining, just not how to calculate the mean from the left over remaining part of the distribution. If there are resources to learn how to do this in excel or if someone can explain it to me I'd appreciate it. After getting the new means for each year in terms of 2016 acceptance I'm going to see how much effect that has on the disparity of the GPA inflation. I also have the issue where a few years are a lower % of acceptance than 2016. For these years I want to inflate the bottom the % of difference (probably 31/30, or 3.3% of my normal distribution) and find the new mean, I have no idea how to do this or the best technique for this approach.

^ I also know there's a lot of assumptions in this, because kids don't purely get into school based on GPA and I don't know if the increase of the kids applying's expected GPAs at school would be normally distributed, top heavy, or bottom heavy. It's just one theory.

Clearly I don't know too much about Statistics, but I'm trying to learn. If you have software recommendations that'll simplify my questions (currently I'm using excel) I'd be happy to hear them. Thanks"
Cross validation with linear models -- how to minimize overfitting,7,1,False,False,False,statistics,1501004181,True,"I'm fitting a large series of linear models, and am trying to find the 'best' model that minimizes error on a validation set (and hopefully a subsequent test set).  I'm using the leave one out cross validation trick (e.g., https://robjhyndman.com/hyndsight/crossvalidation/) and am choosing the model with the minimal CV stat.  As expected I get my best possible model with respect to the validation set (i.e., all the data really since we're dealing with LOOCV), and a slightly worse prediction accuracy for the test set.  Makes sense.  But I want to see if I can do a little better than that. 

I'm wondering if anyone knows if it's theoretically possible to select another model that performs slightly worse on the validation set and slightly better on the test set by picking maybe the model that doesn't actually have the minimal CV but perhaps sits at about the 5th percentile.  I realize this is getting into a little bit of a tricky situation since I'm kind of using the test set to figure out how to choose a model, but this is actually just for a hobby of mine and not for a critical application, so I'm OK with playing around a bit.  "
Have a distorter variable in my results. What does that mean?,0,1,False,False,False,statistics,1501013664,True,"I ran a regression model with two predictor variables (x1 and x2).  Then, ran an interaction model with the same variables and found that one variable (x1) reversed the relationship of the other, (x2) making it a distorter variable (x1).  

Regression model

* y = x1 + (-x2) + error

Interaction model

* y = x1 + x2 + (x1 * x2) + error

What does that mean about x1?  Why does the distortion occur?

Is it a poor measurement? 

Was there not enough variability?

Something else?

I'm trying to understand the meaning behind it and all I can find on Google is the definition of a distorter variable, but no further elaboration why it occurs.

Thanks in advanced!"
What is a good voting strategy for repeated classification attempts?,9,7,False,False,False,statistics,1501019302,True,"Assume that a measurement is taken from the same sample and is then fed into a probabilistic classification algorithm. There's is some error associated with each measurement, so the classification is somewhat prone to mistakes. The output of the classification algorithm is the vector of probabilities of the measurement to belong to one of the categories. Now, let's repeat the measurement from the same sample multiple times.

At this point I have two options:

Average all individual measurements and classify the result (it works but not as great as I want it to).
Alternatively, classify each individual measurement, and then vote on the result.
The problem is complicated by the fact, that sometimes it is genuinely not clear whether the sample should belong to one class or the other and may very well be on the border between the two. An ideal voter would be able to capture that nuance and report something like: class A with 60% confidence and class B 40% confidence.

Another complication is that the number of measurements should be ideally minimized.

What would be a good voting strategy to achieve that? A simple majority vote seems like it capable of doing the job, but it takes a lot of measurements to converge on any sort of solution, especially when there is no class that clearly dominates over all others."
Stats Canada raids millions of homes after learning nation's families hiding .5 children (humour),0,0,False,False,False,statistics,1501020959,False,[deleted]
teaching statistics,2,1,False,False,False,statistics,1501023517,True,[deleted]
Am I using partial correlations correctly?,5,8,False,False,False,statistics,1501039760,True,"Hi! For my stats class I'm studying the correlation between two continuous variables: age and working memory capacity. My prediction is that the relationship between age and WMC is mediated by your health (ex. vision) and attention span. To measure health and attention my participants took a self-report survey. Their scores are also continuous.

When doing the correlational analysis I found that there was no significant correlation between age and WMC (I'm using SPSS btw). But when I ran a partial correlation between age and WMC while controlling for health and attention, I got a significant correlation. Is this the correct way to use partial correlations? Specifically, can I use a partial correlation test even though my variables were not correlated in the beginning? Thanks! :)"
What are the odds?,4,1,False,False,False,statistics,1501039841,True,"I've tried looking for an online calculator and was hoping someone could help me. I'm trying to work out the odds of something happening, basically over 5 days someone did something 7 times. Each time they had a 3% chance for a bad outcome, the bad outcome happened 3 times from the 7 taken. What is the statistical % of this happening/odds of it happening and how would you work it out? "
"How common is it (statistically) to have 74 downvotes but no comments. Might depend on the sub, but has to be statistically insignificant, a few standard deviations. I ask because I wonder about industry-manipulation. Healthcare in this case.",15,0,False,False,False,statistics,1501056924,False,
how do I interpret this regression output?,11,1,False,False,False,statistics,1501073720,True,"http://imgur.com/a/yyVGF

^ here is what I am looking at

my variables are (independent:) voters for parties 4, 5 and 7 in 2011 (dummy), and (dependent:) non-turnout in 2014 (dummy).

I am wanting to conclude a statistic from the results I get

i.e. ""voters for parties 4, 5 and 7 in 2011 were less likely to vote in 2014 by _ %""

how do I come to a conclusion that fills in the blank ^ from my image (pasted)?"
Consulting as a statistician?,0,1,False,False,False,statistics,1501082485,True,[removed]
Fixed vs Random factors in ANOVA,9,5,False,False,False,statistics,1501083227,True,"I've run some experiments involving dropping a 12 month old ATD from bed-height, investigating femur loads. I'm looking at differences in a few variables (compression, bending, torsion...) between landing on two surfaces (n=12 falls for each surface). Complicating things is that despite being careful to replicate starting position and using an actuator to push in the same location with the same force, I had two different fall dynamics - one in which the head hits first, one in which the leg swings out a bit and hits first. A t-test on fall dynamic shows significant differences between my two fall dynamics.

My first instinct was to include Fall Dynamic as a fixed factor in a two-way ANOVA along with Surface. My colleague, however, suggested including Fall Dynamic as a random factor.

Neither of us are exactly an expert in statistics, though we have a pretty decent working knowledge that's largely self taught from books by Andy Field (Discovering Statistics Using SPSS) and Portney and Watkins (Foundation of Clinical Research). Neither book covers using random factors in ANOVA. Doing some Googling, I'm not sure this quite fits either one. Fixed factors seem to be controlled things that I'm seeking to investigate. Random factors seem to be random sampling from among a larger population (e.g. investigating effect of medical complaint on ER wait time, sampling from 5 of the 50 hospitals in your network - Hospital would be a random factor).

Using Fall Dynamic as a fixed vs random factor has a big impact on the result of the test so I want to make sure I get it right. Can anyone offer any advice on what to do or what to read to make my decision?

Thanks so much for any help!
"
What does a normal distribution plot show?,7,10,False,False,False,statistics,1501087170,True,"Hi, i'm doing data analysis for a research project and i'm just learning how to use Minitab. I've been looking at different ways to represent data and I saw a probability plot, and was wondering what it actually measures/if its helpful. I just don't understand what the y-axis is measuring/representing, you can change it to 'probability', 'percent' or 'score'. 

Here's my data: 
https://ibb.co/d4OHek 

I was just looking for a type of graph that would represent the differences in my two sample measurements. 
"
Monte Carlo Simulations and Machine Learning for Forecasting,4,4,False,False,False,statistics,1501088117,True,"Suppose I am trying to forecast system failures per month for the next 3 years. I strictly just want to produce a forecast of system failures. Would it be better to run Monte Carlo simulations or a machine learning algorithm (eg/ neural nets)? My objective is to provide an as accurate as possible forecast of when system failures will occur.

If I were to run a Monte Carlo simulation, I would be able to factor in varying future operational events that could impact the system failure forecast. Whereas, machine learning will take historical data and predict the future.

I'm still learning about Monte Carlos and Machine Learning so maybe I am comparing apples and oranges? Let me know!"
"Enumeration method, minimum number known alive",1,1,False,False,False,statistics,1501096442,True,I am doing a research project on abundance of small mammals. I have live trapping data from 4 consecutive nights. How would I apply the min number known alive counting method (including recaptures) to determine relative abundance of small mammals in the area?
When should I use Welch's T-Test?,6,1,False,False,False,statistics,1501111920,True,"Suppose I run an experiment and my results show that my two groups (test and control) have different variance.  How different is different enough to use Welch's T-Test instead of Student's T-Test?

I was thinking I could form bootstrapped confidence intervals for the variance, and if they were non-overlapping use Welch's.  Does that sound superfluous?  "
"Expected value vs. Expected utility, which is better for decision making?",5,0,False,False,False,statistics,1501112700,True,"The expected value/EMV for Job B is higher than Job A by a huge margin, yet the expected utility for Job A is higher than Job B, but only by a small margin. Both EV and EU use the same inputs as far as dollar amounts and statistical probability of the jobs working out. 

I didn't have access to turnover numbers to come up with an actual probability of each jobs likelihood of working out, so I just went by my own  assessment based on the things I learned and how well I meshed with each hiring manager.

Although Job B has a higher EV, Job A is a better choice going by EU, but why is EU a better measure?

For example, given the choice of taking 1 billion dollars for sure, or 6 billion dollars if you roll a 6 on a single roll of a die. 

In this scenario the EV is the same for 1 or 6 billion, but the EU is MUCH higher for taking the sure 1 billion dollars vs. the .16666% chance of winning 6 billion.

So am I thinking of this right? EU seems to be a better tool for decision making than EV?"
Multiple imputation question: how to use multiple imputed data that has been combined with Rubin's Rules in a regression.,0,1,False,False,False,statistics,1501113193,True,[removed]
Tail bounds under sparse correlation,0,8,False,False,False,statistics,1501124651,False,
Books on Tensor Methods/Analysis in Machine Learning?,0,1,False,False,False,statistics,1501136253,True,[removed]
Question about sampling,0,1,False,False,False,statistics,1501141761,True,"Let's say I want to sample from a high-dimensional distribution. Is it possible use a linear projection matrix to a lower-dimensional distribution, sample in the lower-dimensional space, and then transform the samples back into the original high-dimensional distribution? 

For example, let's say I want to sample from a distribution in 4-space, such as P(x) = e^(-eta * x) where  eta = [2, 3, 4, 2] and x = [x1, x2, x3, x4]^T. Let A be a 3x4 transformation matrix, and sample z = [z1, z2, z3]^T from the distribution Q(z) = e^(-eta * A^T * z). Then using our samples of z, can we somehow get generated samples of x from the original problem? This is just an example, but I was thinking more of in general."
Appropriate statistical test for comparing means,14,4,False,False,False,statistics,1501147845,True,"As a medical student conducting research, I'm faced with interpreting data I have gathered from patient blood results. I'm statistically retarded and thus am asking for a small advice from those who are more adept. 

I have 48 patients.
They all have a number given to them between 1 and 4. (9 different categories in total because you can have 1A, 1B etc.)
This number corresponds to their respective cancer grade.
The higher the number, the ""worse"" the cancer is.

Given this; I would like to compare the mean over all survival of these patients for each of the cancer stages.

I went ahead and conducted a one-way ANOVA using ""overall survival"" as dependent list and ""Cancer stage"" as factor. p=0.1
Here's a link to the test (http://imgur.com/a/ZQuXn)

Now comes my question. I'm assuming the data comes out as not significant mainly because apart from the 2 highest stages of cancer patients did relatively equally.

That being said, I know there's also another way to compare means which is called ""Kruskal-Wallis""  I'm not sure if I was supposed to use that test in this case.
I tried to google this but I became more confused as to what I was supposed to do. Any help is appreciated."
Economicshelpdesk.com Offers Best Statistics Assignment Help Service,0,1,False,False,False,statistics,1501159868,False,
allowed to drop items out of questionnaire after scale and factor analysis,0,1,False,False,False,statistics,1501162915,True,"Hey Reddit,    


I am working on a dataset with an independent variable with 3 levels (three races), 12 covariates (of which we will be dropping probably around 10) and then there's the dependent variables.    


We have attitude scores (10 items) towards all three races, so 30 dependent variables in total. I checked chronbachs alpha for all attitudes per race (so three checks) and found that I can consider all as three scores: pos vs neg attitude towards race one, pos vs neg towards race 2 and same for race 3.    


However, our power is still very low since we are dealing with about 60 participants in total. That's why I was hoping to reduce the dependent variable to two scores. Pos vs neg against own race and pos vs neg against other races. The second variable does not give me problems, I get a very high chronbachs alpha by taking those 20 items together. However, the attitude to self item does.
    

My chronbachs alpha for those ten items is 0.675. There is one item in the questionaire, if I drop it my alpha raises to .73. I did factor analysis and found that my first factor (ev 3.750) had high loadings with all items except that one item.    


My question: Is it ok to drop that item if I am just interested in a pos vs. neg attitude towards people of their own race vs people of another race."
Entering a quantitative methods-based master's program. What additional skills will enhance my employability?,6,0,False,False,False,statistics,1501163894,True,"I had [previously](https://www.reddit.com/r/statistics/comments/6emnuy/entering_a_quantitative_methodsbased_masters/?ref=share&ref_source=link) posted on this sub about a master's program that I will be entering soon. I am looking to get into research and statistics in some capacity, although I am not quite sure what exactly I want to do. My program focuses on statistics as it applies to educational research. While I would certainly love to work in educational research, I would like to have other options in the event that those types of jobs are not available. 

My question is, what things can I do on my own to enhance my employability? 

You can click the link above to see what classes I will be taking in this program. One thing that I know would be useful is to become familiar with programming. Most non-educational research jobs I have seen require you to know SQL, and unfortunately my program does not touch on this at all. Is it possible for me (someone with no knowledge of programming at all) to self-teach this?

Please let me know what you think. Any and all suggestions/feedback welcome. "
Word/Phonetic Similarity Scoring?,0,1,False,False,False,statistics,1501165040,True,[removed]
What's the actual risk of [the pill] compared to the published risk? The published risk has more than half of all women getting pregnant after 10 years of use.,0,1,False,False,False,statistics,1501166400,False,[deleted]
"What does a +, #, * on a bar graph mean?",3,0,False,False,False,statistics,1501168367,True,"When you're trying to show significance, you put an asterisk above at particular bar correct? What does it mean with these bars have a + or # above them?"
Weight variables before or after transformations?,0,0,False,False,False,statistics,1501173297,True,"Hi everyone, I have what I hope is a simple question but I'm coming up dry when I search online.  I'm running an HLM analysis and some of my variables need to be transformed to meet assumptions.  Because my data were  nonrandomly sampled, I also need to weight my cases.  Question is, do I apply the weights before I run my transformations or after??  I ran some initial descriptives with and without weights and they don't change much but there is a change.  Please help if you can!

EDIT: title should be ""weight CASES before or after variable transformations"" sorry!"
How do I compare groups based on Hazard Ratio and Confidence Interval?,3,3,False,False,False,statistics,1501179131,True,"So, I have data on different groups, their Hazard Ratios and 95% CrI against the same comparator.
Which test can be used to compare the groups and check if they are statistically significant between themselves?
(I have attached the data below)

http://i.imgur.com/PrhFa5Z.png"
[Statistics Question] Trying to figure out the probably in a cellphone game.,3,0,False,False,False,statistics,1501184705,True,"Hello! I am trying to figure out the odds of getting a reward from a cell phone game and I just wanted to make sure my math is right here. I'm way too removed from college to get this right on my own.

So the game is set up where you have units and there is an equal change of getting all unit in a particular banner. There are 54 possibilities at and equal chance thus the probability of getting any individual unit is 1/54 or 1.85%. 

The game also has a ""Pull 11"" option where you get 11 units for the price of 10. Each of these rolls are not dependent on each other (I can confirm to getting 2 of the same thing on one of these pulls). So is the probably of pulling an individual unit from these type of pull akin to 1/54 + 1/54 ... 1/54 which would be 20.3% OR would you use a binomial calculation where x = 1, n = 11,and P = .0185 for 16.9%?

At the moment I am assuming the 16.9% is correct.

...

That is the first part. THEN the game also has a system where on your third ""Pull 11"" pull you can a 1.5x bonus on one of 6 featured units and on your fifth ""Pull 11"" pull you get a 2x bonus. 

I THINK i have the odds correct for the 1.5x bonus the odds are either 2.65% or 1.77% based on if the unit is a featured unit or not. And the 2x ends up being 3.39% and 1.69%. I think those are correct. 
... 

My last bit is is trying to figure out the odds of getting exactly one of the featured units if you did 5 ""Pull 11"". I have 99.42% which seems ... high.

Assuming the ""11 Pull"" would use the binomial I get        
1: x = 1, n = 11,and P = .0185 = .1690       
2: x = 1, n = 11,and P = .0185 = .1690    
3: x = 1, n = 11,and P = .0265 = .2231     
4: x = 1, n = 11,and P = .0185 = .1690    
5: x = 1, n = 11,and P = .0339 = .2641     
... which would get .9942... which isn't right... right?    "
Strange results from log regressions,0,1,False,False,False,statistics,1501185666,True,[removed]
Question about what type of statistical test I should use?,7,1,False,False,False,statistics,1501192110,True,"Hey, I have a single dataset of % cell viabilities and I need to compare them to a control viability. Which statistical test should I use? If I had more than one data set I would use a one-way ANOVA, but since it's just one I'm not sure what to use."
Graduate Programs U.S.,6,1,False,False,False,statistics,1501194766,True,"What are the best masters program for Statistics? 

I'm currently getting a Bachelors in Economics and I want to be a data scientist if some sort. "
Anyone have Experience with Statistics Certificates?,9,12,False,False,False,statistics,1501195606,True,"Hello, hopefully this post is okay in this subreddit...
I'm interested in learning advanced statistical topics due to my interest in Data Science, so I'm looking into certificate programs with a potential to do a Masters. Does anyone know of any programs that are good or have experience in any or if certificates are even worth it?

The list I have currently is:

* Penn Stated Applied Stats

* Colorado State Regression Models

* Texas A&M Statistics

* OSU Data Analytics

* Univ of Idaho Statistics

* Univ of Kansas Applied Statistics

* Rochester Institute of Tech Applied Statistics

Thanks for your help."
Statistics homework question,1,0,False,False,False,statistics,1501200742,True,[removed]
What probability to use for calculations when you don't know the exact probability?,1,1,False,False,False,statistics,1501210251,True,"For expected value, expected utility, etc. Say you don't know the probability that job A will work out vs. job B, or whether you will get a bonus at either job. 

What probability do you use? .5?"
Could someone tell me what outward facing brackets mean?,7,1,False,False,False,statistics,1501219003,True,"At the bottom of this slide:

http://imgur.com/a/B93Lf

Like I get it's saying that since the probability that X is some exact value is 0, ""greater or equal to"" is the same as ""greater than"" and same for ""less than"". But what are the outward brackets meaning? Those make no sense to me. 
"
Some questions regarding classification,2,4,False,False,False,statistics,1501232652,True,"Hello guys,

I'm reading [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) and I have a few questions on classification. Methods for classification that are covered are logistic regression, linear discriminant analysis, quadratic discriminant analysis and K nearest neighbours. I'll italic my questions, so if someone can answer them, that'd be great.

1) I'm reffering to page 170 in the book, tasks 6 and 7. 

For task 6a), I get a negative value if I plug in the values, that is: `-6 + 0.05 * 40 + 3.5 = -0.5`. *Does this mean the student hasn't got a chance to pass the statistics class?*

For task 6b), I get 60 hours. I treat the number of hours as a variable and my question must yield the result 0.5 (because that's 50%). *Is this correct?* (Side note: these two questions might be more suited at r/homeworkhelp, but I figured putting them here because my other questions are more ""theoretical"").

I did task 7 by first computing the density function, f(x), then putting that into the Bayes classifier. *Can someone tell me why do we even need the Bayes classifier?* Couldn't we just compute f(x) and the observation belongs to the class with the largest f(x) value?

2) Page 168, task 2

I have trouble manipulating the sum sign in my proofs. Since it's used often in statistics, I'd like to learn what I can do with it (i.e. can I rule something out in front of the sum sign). *Any resources on sum sign arithmetic?*

3) Page 168, task 3

Do different covariance matrix correspond to different standard deviations? In the ""hint"" I am given it says that I shouldn't assume that standard deviations are equal across all classes... *Can someone explain the reasoning behind this?*

4) Page 168, task 4

In task 4a), I am asked to compute the average fraction of observations used for predictions. Since we use the observations 10% to the left/right of X, I know that we will use the full range between 0.1 and 0.9. However, between 0.0 to 0.1 and 0.9 to 1.0 the range will ""go out of scope"". For example, for 0.95, we will use the range of [0.85, 1.0], thus ""not filling"" our 10% on the right side. I want to easily and elegantly calculate these edge cases (from 0.0 to 0.1 and 0.9 to 1.0). *Is there any way to do this elegantly?*

Thanks in advance!"
Is it possible to fill in the 'breaks' in ggplot2 in R?,4,0,False,False,False,statistics,1501249188,True,"I have time series' for the SST of several 'pixels' of the pacific ocean. I have managed to get a map showing the pixels I have time series' for (by longitude and latitude) and now I am wondering if there is a way to add colour to the ones that are the ocean and those that are landlocked (as obviously there won't be a time series for SST on land). Once I write my analysis I would also like to colour each pixel by what the time series is classified as so if there is a way to do this then that'd be great too. If it helps, I have a 501x2 matrix set up with the lat/lon values I have pixels for so they can be referenced if needed. 

The code I have to generate the map is: 

mp1 <- fortify(map(fill=TRUE, plot=FALSE))
mp2 <- mp1
mp2$long <- mp2$long + 360
mp2$group <- mp2$group + max(mp2$group) + 1
mp <- rbind(mp1, mp2)
ggplot(aes(x = long, y = lat, group = group), data = mp) + 
  geom_path()  + 
  scale_x_continuous(name=""Longitude"",minor_breaks=seq(170,244,2), limits = c(170, 244)) + 
  scale_y_continuous(name= ""Latitude"",minor_breaks=seq(30,62,2),limits = c(30, 62)) 

Thanks"
How to calculate probability that a sports team will win?,2,0,False,False,False,statistics,1501250867,True,[deleted]
Suitable minors for stats degree,7,1,False,False,False,statistics,1501251588,True,"I'm going to study stats in university soon, and would just like to ask what are some suitable minors for my statistics degree? I was thinking of doing a psychology minor at first, but upon careful consideration came to the conclusion that this would just be more of an interest and would not value-add to my degree.

Anyway, back to my question: What are some good minors to complement a statistics degree?"
Win rate analysis for a personal project : literature recommendation,2,2,False,False,False,statistics,1501251685,False,
Confidence interval for maximum value?,0,1,False,False,False,statistics,1501252282,True,[removed]
Calculate probability of a list of probabilities?,4,0,False,False,False,statistics,1501257140,True,"For some reason I can't figure this out, or maybe I'm making it more complicated than it actually is.

I have a list of grad schools that I'll be applying to. Each one is assigned a probability that I am accepted, based on my numbers and how they compare to the school's admissions data. 

How can I calculate the probability that I get into at least ONE of the list of schools, based on the set of individual probabilities?
"
The % of positive responses to my survey have increased 1.3% this year. How do I find out if this is significant?,0,1,False,False,False,statistics,1501258472,True,[removed]
Dissertation Help - Violation of Assumption,2,3,False,False,False,statistics,1501258531,True,"Hey everyone! I am currently doing my masters dissertation and with my supervisor out of the country and time running out.. I'm hopeful that someone on here may be able to offer SO guidance.

So the current situation: I have a study that has a lower sample size n=79

There is one independent variable, and two outcome variables. Within the independent variable I am looking into three different population groups. This is all fine and has been cleared however my issue lies in unequal sample sizes. 

Group 1: 41
Group 2: 30
Group 3: 8

I have been advised that the route I need to go is a one-way Anova in order to determine if there is variance in the mean scores on my two outcome variables. As well as to see how this differs between the three groups. 

The issue appears now that I have run the One- way ANOVA. I ran a test of homogeneity of variance (the levenes statistic) and found that I have violated this assumption. I can reasonably determine that this issue is derived from my unequal group sizes, however I now have no idea how to go about fixing this so that I can continue on with my statistical analysis. 

TL;DR - Working on masters dissertation and supervisor is not available to help me out! Violated the assumption of homogeneity of variances in my one-way ANOVA. Need help figuring out the next steps to fix this issue and continue my statistical analysis. 
"
"What is ""Unreliability Index, U""?",0,2,False,False,False,statistics,1501261791,True,"Hi,

I am using the [val.prob](https://www.rdocumentation.org/packages/rms/versions/5.1-1/topics/val.prob) function in R to compare predicted to actual outcomes.

I have a question about where to find information about this ""Unreliability Index"". I can't seem to find any study that explains what it is. I have found a few studies that use it but I do not know where it came from.

If anybody could either help me understand or link me to some resources that would be great. "
Big names in statistics want to shake up much-maligned P value,64,55,False,False,False,statistics,1501262537,False,
Senior Business Intelligence Analyst looking for some advice....,0,1,False,False,False,statistics,1501262741,True,[removed]
Can I get into a Masters in Statistics with a Bachelor's in Math?,9,1,False,False,False,statistics,1501272303,True,[deleted]
"A simple mean maths question but never got taught how to do it, could anyone here help? :)",3,2,False,False,False,statistics,1501273500,True,"There are 17 students in a statistics class. The mean IQ of these students is exactly 135.00. When a new student joins the class, the mean IQ of the class now becomes 134.00. What is the IQ of the new student who joined the class?"
Do multinomial logistic regressions ever require a follow-up?,1,1,False,False,False,statistics,1501276181,True,"If one finds effects on several of the dependent variables, is that enough, or is it good practice to perform some sort of follow up analysis on those DVs in isolation?"
May I have your help for few statista.com reports ?,0,1,False,False,False,statistics,1501276286,True,[removed]
Death at 78.74 Years?,4,0,False,False,False,statistics,1501278999,True,[deleted]
Where is a good place or website to find academic papers written by statisticians?,4,3,False,False,False,statistics,1501295474,True,"I'm interested to see what the real work is that goes into an academic paper but don't know where to start.

Also does anyone know of major ones published within the past couple years?"
How to calculate social media user engagement,0,1,False,False,False,statistics,1501295685,True,"I’m developing an app in which one of the functions are measuring user engagement. For example, with Instagram I had the idea of taking the last 100 uploads then dividing the average likes to comments.. then dividing that by the number of followers.

The goal is to get a number which the users engagement matters more than a number of followers they have, then using that number to rank all users of my app.

Would that be a good way of ranking engagement?"
Is there an all-encompassing statistics release announcements website like this for the USA?,0,1,False,False,False,statistics,1501301865,False,
what can I conclude from these betas and this quadratic regression visual?,6,1,False,False,False,statistics,1501310494,True,"http://imgur.com/a/xwQrv

context: I'm shit at this.

I've been told that to find the turning point, you need to do ""B = (-B1 / 2(B^2 ) but what does this mean for my results in the image? I get an answer that is -0.00005375 from them but what on earth does this mean? I want to state how X variable relates to the Y variable but I have no idea how to use these values. how can I say ""-0.00005375 is where the turning point occurs""? this surely isn't the case here? I want to say ""at a certain point for X variable, there is a change in the association with Y""

also:

what does this formula mean?

B0 + B1B + B2B^2

B0 works out the effect of B?"
Getting a degree in statistics while working,8,8,False,False,False,statistics,1501321897,True,"Hey statistics redditers! 

A year ago I tumbled into a data scientist job and I enjoy working in this field a lot. I have always been interesed in statistics so I am thinking of switching from self-study to getting the actual master. So, I am now in the process of figuring out which university to go to, how many courses I would take, etc. Since I don't have the means to quit working for two years, I want to do a limited amount of courses which I would self-study mostly.

If you have gotten a master in statistics, and in particular got one while working, would you care to give me some advice?

What I am interested to know is:
How do-able was it to get a degree while working?
How long did it take you to complete the full master program? How many study-points/courses did you get each semester?
Do you think a master degree an advantage in the data-science job market?

Which university did you go to and would you recommend it? (I live in Belgium so any univeristy in the EU would be a viable option for me.)
Was your university friendly towards working students?

Many thanks!"
Coding Test for Job Interview,3,0,False,False,False,statistics,1501331571,True,"If you were a hiring manager, and you were bringing on a new statistician to whom you gave a SAS coding test (data set with some instructions on what to do) - what would you be looking for from them? Obviously yes, to complete the assignment on time. But.. what extra things would be impressive?"
Question about Probabilities and significance,1,2,False,False,False,statistics,1501343476,True,"First, I'm not asking for homework help or for anything like that.

I'm using data from a poll, and I want to know if the difference I've detected is significant or just due to random chance. Basically the poll asked reddit users if they preferred Dark Mode, Light Mode, or didn't care about which they used. 

Ignoring all of the (many) biases in the sampling for now (I'll return to those later when I'm assessing the data), I received 82 votes for Dark mode, 68 for Light mode, and 106 for No preference, with 256 total responses. 

All I want to know is if that difference between Dark Mode and Light Mode (82 vs. 68) is significant. I don't know what test I'd use, or if there is one. I was thinking about a Chi Squared Goodness of Fit test, comparing the sampled data to another set where the numbers are both 75 for Dark and Light mode and then 106 for no preference, but I'm not sure if that would work. 

If anyone could help me, I'd really appreciate it. And if I'm posting on the wrong sub, just let me know where to post and I'll quickly delete this post. "
Buzzfeed's coverage of p < 0.005: These People Are Trying To Fix A Huge Problem In Science,21,60,False,False,False,statistics,1501343590,False,
Determining Cat Chirality,7,33,False,False,False,statistics,1501344328,False,
What About Reddit Statistics?,0,0,False,False,False,statistics,1501344974,False,
Not sure how this subreddit works...,0,1,False,False,False,statistics,1501345931,True,[deleted]
"I'm conducting a quick survey for students in STEM (Science Tech Eng Math) disciplines about your education and the data science job market. I'm collecting this data to help my department determine if it should fund big data tech workshops, but I can gladly share the results back on this thread!",3,7,False,False,False,statistics,1501352930,False,
What is a good way to learn about Bayesian Analysis?,10,28,False,False,False,statistics,1501358369,True,"With all the pushback on the utility of p value, I have wanted to learn more about Bayesian Analysis for awhile. However, I do not know where to start.

Does anyone know of any MOOCs, webpages, books, etc that are good for learning about Bayesian Analysis?"
Class width and Class Limits question,2,2,False,False,False,statistics,1501358879,True,"Hi guys, this isn't homework but I am at a loss here to explain why the same formula doesn't give me the same result. Any insight would be appreciated!

My question: 
A data set with whole numbers has a low value of 10 and a high value of 120. Find the class width and the class limits for a frequency table with 5 classes.

My procedure:
To find the class width, I subtract the smallest data value from the largest data value and divide this by the total number of classes.

Thus, 120-10 = 110 / 5 = 22.

Now in order to ensure that all the classes taken together cover the data, I need to increase the result to the next whole number which gives me 23. 

http://imgur.com/a/H8iDk

Thus, my class width is 23.

So now,  the lowest data value (10) will be my lower class limit of the first class. Because the class width is 23, I must add 23 to the lowest class limit in the first class to find the lowest class width in the second class:

10 - 32
33 - 55
56 - 78
79 - 101
102 - 124

My problem:

I cannot understand why my upper class limit in the 5th class is higher than 120 which is the highest data value in the set. 
I must be doing something wrong that I am not seeing and is going over my head. I've done these exact same steps in a data set where the lowest value is 20 and the highest is 82 with 7 classes and got the correct result:

82-20= 62 / 7 = 8.8 < 9 

20-28
29-37
38-46
47-55
56-64
65-73
74-82

"
GRAMBLING STATE UNIVERSITY,1,1,False,False,False,statistics,1501360507,False,
My school doesnt offer a statistics major but there is a stats minor and some econometrics classes for econ majors. What should I do?,5,4,False,False,False,statistics,1501362565,True,Should I double math/econ major? Or should I do a major in math and minor in stats? Or should I major in econ with a stats minor and a math minor?
"What are some uses and applications of complex numbers in statistics, if any?",2,1,False,False,False,statistics,1501377614,True,[deleted]
Logistic Regression,0,7,False,False,False,statistics,1501380708,False,
"Finishing MS, still feel behind. Learn Python, SQL, or machine learning?",9,14,False,False,False,statistics,1501385283,True,"Two questions here actually. I'm graduating in a couple months with a stats MS. Right now I'm only comfortable with R thanks to my thesis/internship. 

Looking at data science jobs I appear underqualified the majority of the time since my only work experience is my internship/thesis, which was more in the realm of biostats. I usually see ""required experience"" listing programming languages I've never used.

So what should I focus on learning the most? I also didn't take a Bayesian or optimization course, should I focus on learning those for now? Or learn C++, Python, Tableu, SQL, or learning some ML methods? (the only ML class I took was neural nets). 

Second question - do you think it would be a poor choice to take a position like a business intelligence analyst where you don't really use high level statistics or programming languages? For example this one seems to just use Tableu:

https://seeker.worksourcewa.com/jobview/GetJob.aspx?JobID=185362225

Edit: Thank you all very much, SQL will be first on my list!"
"if I am wanting to regress one aspect each for two nominal variables, one dependent and one independent, do I have to code them both 0 and the other ones 1 in both cases for SPSS?",20,6,False,False,False,statistics,1501425257,True,"so I had issues with this problem in the past but I think I know what could be an issue at the moment

if I have a nominal variable with 3 choices (dependent) and I have an independent nominal variable with 10~ choices, if I want to test the effect of one (or more) option for the independent variable on one option on the dependent variable, do I put in the opposites of the relevant dependent as =1 (else=0) and the opposites of the independent as =1? or do I do the opposite of the dependent as =1(else=0) and the independent as 0? which ones need to be which value? I'm really confused. 

my specific case: I have turnout in elections: 1 is that they turned out to vote and 2 and 3 are ""didn't turn out"" and ""couldn't make it"" (same thing, basically). these are my dependent variable. my independent variables are which party they supported in the last election, and the number values are matched with the parties at the election going down the list. I want to find out how turnout this  time was influenced by which party or parties that they supported last time. which ones do I code as 1 and which ones do I code as 0? if I have certain parties I want to see the effect on turnout for, are they to be coded as 0 and turnout as 0 too? "
Semester project,0,1,False,False,False,statistics,1501428069,True,"This is my first time on this sub Reddit, but I was wandering if anyone has any ideas for my semester project. The basis is that we have to use at least one section of tools that we've learned this summer. I take college intro to statistics at a community college in Louisiana if that happens to help. "
What is your preferred clustering algorithm for customer segments?,2,7,False,False,False,statistics,1501430724,True,"I have a customer survey dataset with mostly categorical and interval style questions and we want to develop customer segments based on this data. 

I've read regular k-means is not great for mixed data and others suggest using a algorithm using the  gowers distance. Others suggest Latent Class Analysis.

Thoughts?"
Estimating size of user base from non uniformly distributed samples?,13,7,False,False,False,statistics,1501435213,True,"Hi all,
I have a transactional dataset for a web-based service, including metadata for messages sent on the service between users, for the month of June. Meta data include the sender and receiver's user ids.
User ids seem to be just running numbers, starting from 1 (so, for example, the user with id 1001 created her/his account directly after the user with id 1000, and so on).

I was reminded of the [German tank problem](https://en.wikipedia.org/wiki/German_tank_problem) and was wondering if I could use the user ids to estimate how many users ever registered to the service (or, at least, by the end of June). 


Here is where I have my doubts: 

+ Users aren't uniformly distributed. Since each message sent between two users creates a new row in the data, user ids for users that chat more will be more prevalent. (Is it ok to just 'normalize' the user ids first? that is, make sure each user id will not appear more than once per day/all period?).

+ Time and population bias: Not all users actually appear in the dataset (if users don't use messaging, I will not have them in the dataset), so in a way, any sampling I will do will be from a subset of the population. In addition it's reasonable to expect that newly registered users will be more likely to appear in the data (they just signed up and want to check things out) than older users (users that signed up 5 years ago may have stopped using the service 4 years ago), which might also introduce a bias I'm not sure how to deal with.


To cut a long story short, what methods would you suggest to estimate the total size of the user base? What do you think about my concerns?

I should clarify I'm asking this out of curiosity. I have no business gain from knowing the number of users said service has. Thanks all!"
Trying to build a weighting/scoring system between correlation and percentile,0,0,False,False,False,statistics,1501435860,True,"I'm currently looking at trends for population in cities. For example, what the impact of factors like income have on future growth. What I really want is to get the relative importance of each factor, i.e. a weighting on the input, and then a score for each city.

The program that consumes different macro factors and the correlation (r) between those factors on future population. I want to return a weighting for each factor, where I divide r for one factor by the sum the r of all factors. The score for each factor by city is going to be the weighting of the factor multiplied by the percentile the city lies in for it. For example, if income has an r of 0.6, and the sum of all r is 5, the weighting is 0.6/5 (0.12). If New York's income is in the 99th percentile, the score for New York/Income is 0.12 * 99.

However, is it a good idea to use the percentile? For normally distributed data I usually use the z-score to determine scores. If percentile doesn't work, is there something similar to z-score for non-normal data?"
Multicollinearity in mediation,3,4,False,False,False,statistics,1501441122,True,"I'm running a mediation with multiple IVs and multiple mediators predicting one DV: [model](http://i.imgur.com/P162kFc.jpg).  My mediators are moderately correlated with one another (~.3 to .6), but a factor analysis of the measure identified them as separate subscales, so I have theoretical basis for including them all as separate variables.  My IVs measure different types of psychological symptoms (depression, anxiety, and OCD), and they're also significantly correlated with one another (.46 to .54).

Is it safe for me to run these models as planned, or do I need to run everything independently?  For reference, I'm using the SPSS MEDIATE macro by Andrew Hayes."
Gambling Problem Reference or Answer?,1,1,False,False,False,statistics,1501441595,True,"Hello!

I'm trying to find out my course of action for a video game that's based around gambling. The idea is to find the path that leads to the highest probability of gains ($). Essentially, I'm hoping that I can find a way to make money in the game with the lowest amount of risk, or in different words, which method can I use for the highest probability of success.

The game works like this;

You pick a value, ranging 2 through 4

If you choose 2 you pay $2 and you roll for a 52% chance at $4

If you choose 3 you pay $3 and you roll for a 52% chance win for $4, and if you win that, you can choose to roll for a subsequent 35% roll for $12 (or abstain and take the $5), however, if you lose the second roll you lose your winnings from the first.

If you choose 4 you pay $4 and you roll for a 52% chance win for $4, and if you win that, you can choose to roll for a subsequent 35% roll for $12 (or abstain and take the $6), however if you lose the second roll you lose you winnings from the first. OR you can choose to roll for another 52% roll for another $4, instead of rolling a 30% for $16, regardless of if you win or lose the first roll. 

I'm having the hardest time putting my calculations into context and making sense of what I am attempting to do. I just seem to get lost in the sauce. Which choice of 2-4, and subsequent paths, leads to the highest probability of success vs reward? What I am hoping to find is a statistical path that leads to the highest probability of constant profit. What can I do here to stack the odds in my favor as much as possible? Which path do I take?"
"PLEASE HELP! on Stats final in SPSS- ANOVA, Pearson's, Chi Squares",0,1,False,False,False,statistics,1501444221,True,[removed]
Is a PCA appropriate in this case?,1,0,False,False,False,statistics,1501445834,True,[deleted]
Figuring out the average price per m2,8,0,False,False,False,statistics,1501455241,True,"So I'm looking at houses at the moment and they don't show an average price per m2 for the area, and I'm too lazy to average the hundreds of houses out by hand, but I remember from statistics class that I can somehow find a rough average, assuming the data is distributed evenly.

Can someone refresh my memory?"
What Time of Day is Trump Most Likely to Tweet?,17,72,False,False,False,statistics,1501456029,False,
Predicting third party error selection based on historical selection,5,5,False,False,False,statistics,1501456508,True,"I have basic statistics knowledge, but went up to Calculus III until I decided to go into law, so it's been a while.

**The Problem:**
How do I predict which records are likely to be errors based upon previous errors?
I have about 10,000 records. Of those records, 78 were identified by a third party to be in error. I have a bigger dataset that is about 3 times as big. Their error analysis is not perfect, but they run the record through their own checks to identify and they don't necessarily check each record, so they are likely to miss errors. To be clear, I'm not trying to predict how many errors are in a population, but the probability that any given record is likely or not likely to be identified as an error by the third party.

**Data**

There is one dependent variable, i.e. whether the record is selected or not selected to be an error.

There are 6 independent variables:

1. Date Recorded, 
2. $ Billed, 
3. $ Paid, 
4. % of Billed to Paid, 
5. Group, 
6. Date Selected

**Assumptions**

1. The higher the % of Billed to Paid, the more likely it will be selected to be an error (this is based upon basic statistical tests, like mean, median, quartiles), 
2. If a record is a selected from a certain group AND has a high % of Billed to Paid, it is more likely that it will also be selected, 
3. The older the date recorded, the less likely it will be selected
"
"Extremely small effect size, but ""significant"" p-value in t-test. Why?",4,4,False,False,False,statistics,1501458375,True,"I did a t-test on a group of data with 110 data points against 0, i.e. whether the group of data points is significantly different from 0 or not. (I just implemented this in Matlab with ttest(data)). The p-value is quite significant (p=.0005). *But*, the effect size is very very small - mean = 0.01, SD = 0.045, SE = 0.004. Also, these data points are regression coefficients, which doesn't really have a meaningful unit, so I'm not sure if .01 is big or small.

Can I believe this result? What does this mean? Does it makes sense to report this result, and how should I correctly interpret it?"
About the mean parameter in a gamma distributed sample,1,1,False,False,False,statistics,1501500080,True,[deleted]
Common statistical errors in online A/B testing - explained + how to avoid them,1,21,False,False,False,statistics,1501503164,False,
Help with standard deviation calculation - unequal sample sizes/known population size.,0,1,False,False,False,statistics,1501514341,True,[removed]
Analyzing the difference between two groups (previous interview question),4,8,False,False,False,statistics,1501516809,True,"In a preview interview, I had a case study where I had to classify whether someone will get a disease or not. The probability of getting the disease is 1%.

I mentioned I will use a logistic regression, and I would use many variables such as demographics info, medical history, lifestyle factors, etc. I mentioned it doesn't hurt to start with many variables since we can do variable selection later.

I was asked how many data sample I would need. I said it depends on the cost of data acquisition and the acceptable variance of the model. But later I realized there is a ""rule of 10"" rule of thumb for logistic regression where the number of samples should be (number of predictors) * 10 / (the probability of the rare case).

Anyway, the follow up question is that because the data samples are taken from those that can to the hospital for a checkup, can the results be applied toward the general population? I said no because people that go for check ups tend to be health conscious and therefore would not be an accurate representation of the general population.

But I was asked how to compensate for this and I drew a blank. And how would you quantify the ""difference"" between those that came for checkups and the general population?

Can you advise on anything I answered poorly or wrong so I can improve?

I answered as long as our model contains all the variables that may be different across the two groups, and if those who came for checkups have a wide scatter of values for those predictors (even if the mean might be different than the general population), then the logistic model would still be valid for the general population.

thanks"
Using coefficient of variation to study distribution of comment vote counts of a submission on reddit.,0,0,False,False,False,statistics,1501522843,True,"I'm trying to study the distribution of comment ""points"" (upvotes-downvotes) on posts of various subreddits, and how the distribution varies with the subreddit of choice. For example, a ""funny"" or non serious subreddit should have high variation - the comment with highest number of votes will vary to a great extent from the mean, than a ""serious"" subreddit, where the distribution will be more uniform (less variation from the mean).

I thought of using the coefficient of variation as a metric to explain this behaviour, and the results seem to make sense, the posts on ""serious"" subreddits have lower CV than those on ""non serious"" subreddits. But, it seems that CV should not be used as a metric for data which are not from ""ratio scale"", and since comment points can have negative values, they do not come from a ratio scale.

Is there a way to get around this problem? Somehow transform the data (comment vote counts) to a ratio scale? Or use some other metric of variation to continue this analysis, which is also consistent theoretically? Does the fact that the mean of most of these distributions is not close to zero (is a high positive value) help the case of using CV?"
Testing for autocorrelation of residuals in panel logistic model,0,1,False,False,False,statistics,1501529384,True,"Hi,

I am working with an unbalanced panel data set, with a binary dependent variable.  The data set is quasi-survival, as each statistical unit can contribute unlimited successes (coded as 0), but can only contribute one failure (coded as 1), at which point it drops out of the data.  I've clustered standard errors where they need to be clustered, which should correct for autocorrelation anyway, but bear with me.

I want to know a few things regarding autocorrelation of the residuals in logistic panel data:

1) Does such an analysis even make sense?  Is autocorrelation of residuals in a quasi-survival logit model nonsensical?

2) Can I apply the Wooldridge test for panel residual autocorrelation to a logit model?  I would think not, but no harm in checking.  If not, is there an equivalent test I could run for nonlinear models?

Thank you so much."
Could someone give me a non-technical explanation of Hamiltonian Monte Carlo sampling?,8,5,False,False,False,statistics,1501533334,True,Edit: My current understanding is that this is a random walk procedure that generates a posterior distribution by using a combination of the prior and likelihood distributions to accept/reject subsequent steps. It is different from the Metropolis-Hastings algorithm because it does *something* to sample *certain* parts of space more so. 
"For those of you who have done a full time MS in Stats, how was it for you?",0,1,False,False,False,statistics,1501541866,True,[removed]
"Tried to check my code with a normal distribution, opened a hole in the space time continuum.",8,89,False,False,False,statistics,1501543757,False,
Questions on Kalman filtering/smoothing,1,1,False,False,False,statistics,1501559756,True,"I have several large (> 10-20 Gb ~77million point) temperature time series files that I'd like to clean up. I have parsed the data into 2 column vectors. (Timestamps and temperatures)...The data span a range of 2 years, have variable sampling frequencies, and many many sensor ""drop outs"". The ""drop outs"" consist of time intervals where no data is collected for periods of seconds to days and also with intervals where the sensor logged ~100 points at the same time. 

I have looked into ""extended"" Kalman filtering/smoothing for nonlinear noisy data and am convinced that the method will work nicely if I can implement it. I work mainly with MATLAB but am familiar with Python and R. 

My question concerns the process model and the matrices F and H . I don't know how to build them given my scenario. For many examples the process model is a polynomial describing the ""process""...for example for tracking a falling object the process model would be y=x+x't+1/2x''t^2. For now I don't need to worry much about the control input matrix(B). 

I can post code and data if needed. 

What am I missing? 
"
Is there a formula for combining two correlation coefficients into a single correlation?,0,1,False,False,False,statistics,1501582409,True,[deleted]
Data visualization can have a great effect on statistical presentations,0,1,False,False,False,statistics,1501582699,False,
Matching Test and Control Subjects with repetition (x/post with /r/askstatistics),0,1,False,False,False,statistics,1501589123,True,[removed]
Sexual harassment in universities report - explanation of confidence interval,0,1,False,False,False,statistics,1501589835,True,[removed]
Machine Learning Translation and the Google Translate Algorithm,0,18,False,False,False,statistics,1501592311,False,
Multivariable Cox regression and/or Kaplan-meier in SPSS ~ probably easy question,1,1,False,False,False,statistics,1501600673,True,"Hi everyone, I am a stats noob trying to self teach for a research project.

Lets say I do a Kaplan-meier survival curve or Cox regression and find some factor, say drug A to be associated with longer survival or decreased Hazard ratio on Cox. At this point I would like to control for things like age, tumor stage, other comorbidities (i.e. diabetes) to see if the effect remains. How do I go about doing this for either a Cox regression or Kaplan-meier in SPSS 24?
Additionally, would it be sufficient to compare characteristics on the side for the groups using drug A vs. drug B? So for example just doing independent t-tests for continuous variables (i.e. age) and chi-squared for categorical (i.e. diabetes or not); then if I found no significant differences between groups I wouldn't need to do multivariable cox or kaplan-meier since it would be the same as univariate, right?

Sorry if this is a novice question but I need to teach all of this to myself. Also, if I am saying something that just doesn't make since please point it out as it will help me learn. Thank you"
How to find most intercorrelated set of items from a larger sample,2,1,False,False,False,statistics,1501607366,True,"I've got about 700 respondents to a set of 31 questions (all scored dichotomously - 1 = right, 0 - wrong). The internal consistency reliability (degree to which items intercorrelate) is too low. So I want to choose a subset of items with better intercorrelations. Ideally, I'd like to use at least 20 of the 31 items.

Is there a name for the procedure I'd use? Is there a formal algorithm for this?

Thanks."
Any problems with including random slopes for control variables?,0,1,False,False,False,statistics,1501611722,True,"Suppose I have an experiment in which participants each saw 100 words; these words could be one of two types (call them Type A or B). I also have several participant variables (e.g., vocabulary size) and item variables (e.g., length). I am interested in whether Type A vs. Type B words lead to more successes. I also include these participant and item variables in the model as fixed effects, in order to control for them.

My question is: Am I correct to also include random item slopes for these participant variables, and random subject slopes for these item variables? As an example, using R notation:

    Accuracy ~ WordType + Vocabulary + Length + (1 + Length + WordType|Subject) + (1 + Vocabulary|Item)

In my mind, including these random slopes will make the fixed effects more accurate, and thus they will serve as better control variables.

Is there anything wrong with this?"
when should you use quadratic regression and when should you use cubic regression?,16,2,False,False,False,statistics,1501612545,True,
"in SPSS, how do I perform a bivariate logistic regression with weightings?",0,1,False,False,False,statistics,1501613163,True,"if I have a certain response to a question as my dependent variable and I have another response as my dependent but I want to make it so that my sample is representative of the population, what do I do to make this the case? the weighting for my demographics aren't to do with these responses - the demographics are just in the background, I assume. how do I weight responses by their size relative to the population? "
Measuring customer attrition after a branch closure,2,1,False,False,False,statistics,1501614194,True,"Let's say that there were 1,000 customers who had primarily transacted at a branch prior to the branch's announced closure. Among this cohort, 40 customers closed their accounts with the bank the month following closure, and another 20 customers closed their accounts with the bank two months following closure. If I want to measure the rate of attrition post-closure for the second month, should the denominator be 1000 or 960 (1000 minus the 40 who left in the prior month)?"
"What level of education have you attained, what do you do for work, and do you like it?",106,52,False,False,False,statistics,1501622024,True,
Box Cox Transformation-Finding Lambda,1,2,False,False,False,statistics,1501629074,True,"I'm practicing my Python skills and creating a bunch of programs that essentially mimic the scipy.stats package. If I'm doing a Box-Cox transformation, what algorithm or steps would I need to take in order to find the lambda necessary to best approximate a normal distribution? I've used the Box-Cox transformation before in university-level projects (using R, not Python), but since I'm only an undergrad taking the lower-level stats courses, I didn't learn how to derive the Box-Cox transformation. All I know is that lambda will be the maximized log-likelihood value of something.

What steps should I take in order to find lambda? Or do you think this task may be beyond my understanding?"
Best metrics to determine goodness of fit?,4,1,False,False,False,statistics,1501635312,True,"I'm writing a Matlab program to fit a large amount of experimental data. My goal is to have the program choose from a wide range of initial parameter values (all I specify is the upper and lower bounds in parameter space), and converge of the values that give the best fit. In order to do this, the program needs to loop through various parameter choices, and make a decision on what works best by minimizing/maximizing some statistical information. I'm currently using the fitnlm function, which calculates various stats about the fit. 


My question is, what metric(s) are best to use in determining the absolute goodness of fit for a single model? (I know that I can use something like BIC, AIC to compare multiple models, I want to know if a single model is a good fit). Maximizing the F-statistic seems useful, but there's also chi^2, SSE, RMSE, etc. I could also minimize/maximize some product or quotient of these values. Any suggestions or resources on how to approach this problem? Thanks."
Strategies for finding neighboring solutions for simulated annealing,11,3,False,False,False,statistics,1501645799,True,"I'm working on an optimization problem and attempting to use simulated annealing as a heuristic. My goal is to optimize placement of k objects given some cost function. Solutions take the form of a set of k ordered pairs representing points in an M*N grid. I'm not sure how to best find a neighboring solution given a current solution. I've considered shifting each point by 1 or 0 units in a random direction. What might be a good approach to finding a neighboring solution given a current set of points?

Since I'm also trying to learn more about SA, what makes a good neighbor-finding algorithm and how close to the current solution should the neighbor be? Also, if randomness is involved, why is choosing a ""neighbor"" better than generating a random solution?

EDIT: The solution space consists of all combinations of k distinct, integer-valued points in the plane, each bounded by [0, M-1] in the horizontal direction and [0, N-1] in the vertical direction (there are MxN choose k solutions). Given M = 4, N = 5, k = 3, one possible solution would be [(0, 0), (3, 4), (2, 2)].

(To simplify this, it's possible to map each ordered pair to an integer in the range [0, MxN-1], simplifying the solution space to all combinations of k distinct integers in the set {0, 1, ..., MxN-1}.

EDIT2: I'm using CV(RMSE) as my objective function. I've implemented my above proposal of shifting points randomly by 1 unit, but it yields much higher values than a complete search of the solution space.
"
Power analysis for one-sided regression?,12,1,False,False,False,statistics,1501648867,True,"I want to run a simple linear regression: *y = ax + b + epsilon.* I want to test whether the value of *a* is significantly greater than 0. I know as a fact that *a* cannot be negative given the nature of the problem.

My goal is to run a power analysis to determine the required sample size to get *alpha* = 0.005, *power* = 0.95, *effect size* = *d* (*d* can be determined later). This would be simple if I were computing a two-sided hypothesis test for *a*. However, seeing as *a* cannot be negative, the problem is more difficult."
Which methods/subfields are most used in clinical trials/personalized medicine?,2,3,False,False,False,statistics,1501657805,True,"I want to learn more about what statisticians do in the personalized medicine world.

What are the statistical tools and methods widely used or needed to be developed to advance personalized medicine?

I would imagine survival analysis but that's all I can think of

Also not sure what to flair this as"
I need help with this AP Statistics Problem. Its problem number five and I don't understand how to do it,0,1,False,False,False,statistics,1501667717,False,[deleted]
Question about how you properly calculate some odds for fantasy football,5,4,False,False,False,statistics,1501673175,True,"I am wanting to take a betting odds approach to some of my decision making info fantasy football. 

In theory,  if a player gets 10 points for scoring 1 goal,  and there odds to score 1 goal is 50%, and there odds to score two goals is 10%, how many predicted points should they have?


Is it 10 points "" 50% + 10 points * 10% = 6 points?"
Do you know any websites where I can find free datasets?,9,10,False,False,False,statistics,1501674573,True,"I'm looking for websites where I can find and download free datasets. For example HR data, Marketing data etc. I'd like to use them with a data analyser to test it. I need free to use datasets.
Could you recommend some kind of websites? Thank you in advance."
combining multiple columns into one in R,6,1,False,False,False,statistics,1501674947,True,"I have a dataframe of a survey with multiple people over many years that looks as follows.

first column (canton) is where people live

second column (year) is the year they were participating in the survey

i merged this with the macro variable voting-regime over time (regime1985-1995, regime1995-2000, regime2000-2005) in the different cantons

canton | year | regime1985-1995 | regime1995-2000 | regime2000-2005
---|---|----|----|----
ZH | 1986 | 1 | 2 | 2
AG | 1999 | 3 | 3 | 3
ZH | 1995 | 1 | 2 | 2
SZ | 2002 | 2 | 2 | 1


Is there a way to create a new variable that consist of the regime for each person in the year they were surveyed? how could I do that? So it would look like that:

edit: regimecombined should be the value selected from the 3 possible timeframes (regime85-95, regime95-00, regime00-05) according to the year the person was surveyed. e.g. person in row 1 was surveyed in 1986 and therefore gets the value 1 from column 3 (regime85-95)


canton | year | regime1985-1995 | regime1995-2000 | regime2000-2005 | **REGIMECOMBINED**
---|---|----|----|----|----
ZH | 1986 | 1 | 2 | 2 | 1
AG | 1999 | 3 | 3 | 3| 3
ZH | 1995 | 1 | 2 | 2| 2
SZ | 2002 | 2 | 2 | 1| 1

thanks so much!"
Guidance on when to use RS2 (Rao-Scott) chi square adjustment over RS3 (Satterhwaite)?,0,1,False,False,False,statistics,1501675392,True,[removed]
Statistics Question,2,1,False,False,False,statistics,1501678603,True,[removed]
Paired statistics for %GC of DNA sequences,0,1,False,False,False,statistics,1501679980,True,[removed]
"What's considered a ""good"" or ""bad"" MAD/Mean ratio when forecasting?",0,2,False,False,False,statistics,1501680610,True,"I'm running a lot of exponential smoothing forecasts automatically on a lot of data sets and I was planning on running a query where it gives me all the MAD/Mean ratio's of all the trends from highest to lowest. This way I can go to the data sets with high MAD/Mean ratios to see why the forecast fit line isn't too ""good"". Is there a rule of thumb where if the MAD/Mean ratio is above x, then the forecast is probably not a great projection."
Multiple hypothesis correction with lots of p_val == 0 tests,3,2,False,False,False,statistics,1501683583,True,[deleted]
Help working with Excel 2013,4,1,False,False,False,statistics,1501687081,True,[deleted]
"Any recommendations on how to get ancient, PDF image scans into a spreadsheet?",11,1,False,False,False,statistics,1501689850,True,"I've got about 200 pages of data tables from old statistical reports (some going back to 1900) that I need to get into a spreadsheet.  The PDFs were scanned as images from original reports, so I can't just select the data.  I'm debating an OCR software, but I've had limited success.  Has any one else successfully tackled a similar problem, or am I doomed to keying each value in?

Thanks!"
"Another hole in the space time continuum, im sorry guys I keep changing reality",0,1,False,False,False,statistics,1501691889,False,[deleted]
Introduction to Statistics: A Free Course,2,54,False,False,False,statistics,1501699655,False,
Urgent: Statistics help,0,1,False,False,False,statistics,1501699689,True,[removed]
How can I find the standard deviation by which a certain sample deviates from the norm.,2,1,False,False,False,statistics,1501726827,True,"I don't know a lot about statistics, can someone help me here?

There is a genetic defect that affects about 1 in 1000 people. After testing a group of 350 people with special characteristics, 9 people with the defect were found. How can I find out by how many standard deviations this special group deviates from the ""1 in 1000"" average"
Quick question on a geometric mean problem....,5,5,False,False,False,statistics,1501735029,True,"Q: ""what is the geometric mean return for the time period presented?""

(the data is in excel next to the year it happened)

data is:

10.21%
20.10%
-11.12%
8.46%
16.04%
2.97%
-9.10%
10.75%
1.28%
0.69%

I try using GEOMEAN on excel but it won't calculate because there are negative numbers. Any help is appreciated.
"
Mediating and Moderating Variables,5,1,False,False,False,statistics,1501752714,True,"Would it be fair to say that moderating variables are continuous variables whereas mediating variables are dichotomous variables, both of which influence C from initial cause A?

Because I can't seem to wrap my head around their difference.  It seems to me that any variable that can more or less vary in intensity would be a moderating one whereas a variable that can either be present or absent is a mediating one.  

Examples of each would be helpful.  

Thanks."
Top Five Critical Factors to be Considered While doing Statistics for your Doctorate Thesis,0,0,False,False,False,statistics,1501762761,False,
T test: How does it work?,8,0,False,False,False,statistics,1501779785,True,"I know a T test can determine variability between data sets, and when I plug my data sets into the excel function ""t-Test: Paired Two Sample for Means"" IT gives me P(T<=t) less than 0.05 so my data sets are independent, I just don't know how it does it.

Any advice on the mathematics behind Excel's function: ""t-Test: Paired Two Sample for Means"" would be greatly appreciated."
"In a medical experiment, if my clinical group is n=64 and my control group is n=20, do I necessarily need to use non-parametric tests (due to the difference in sample sizes)? Or am I otherwise limited in which tests I can use?",19,10,False,False,False,statistics,1501781101,True,"I am wondering if I can use straight up independent t-tests, and anovas, and other standard tests. Do I need to make any sort of compensation for the fact that my control group is so small (blame the data collectors for that!)"
Dealing with Skewed Data and expected value,2,2,False,False,False,statistics,1501781471,True,So i'm analyzing response times to emails and noticed that my data is highly skewed by several emails with extremely long response times. I found that there is a 50% chance that a response will come in under 10 minutes but the total average response time is almost 2 hours due to these outliers. What can I say about the expected response time? 
Quick question on binomial distribution,6,1,False,False,False,statistics,1501783979,True,"The student body of a large university consists of 60% female students. A random sample of 8 students is selected. What is the probability that among the students in the sample at least 6 are male?


the answer is .0499 but I dont know how to get that. I did BINOM.DIST(6,8,.4,0) in excel and it came back with .041288.

thanks!"
Exploratory factor analysis with a predictor variable?,0,1,False,False,False,statistics,1501786070,True,"Experimental Design: participant takes a drug, and fills out a survey at various time points. The survey is designed to measure affect, by asking the participant to respond to assorted emotion adjectives on a seven-point scale. 

The responses can probably be described by two factors: energy, and pleasantness. I would like to know how the drug affects these variables over time.

 

So far, I have performed exploratory factor analysis on the survey items, and plotted the factor scores against time since taking the drug. Then, I performed a nonlinear regression to estimate the absorption and elimination rates.

 



I feel like this isn't the ideal way to analyze the data, because the drug could influences multiple latent variables, making them correlated.

How should I handle this? I don't have any experience with structural equation modelling, but I'll learn if I have to. It seems like I'd want a model where time affects drug plasma concentration (nonlinearly), which affects two unobserved variables (energy and pleasantness), both of which affect the response to each survey item.  

Am I on the right track? How bad would it be to just do a factor analysis, and use the scores in a nonlinear regression?

Thanks."
Simpson's Paradox and Statistical Urban Legends: Gender Bias at Berkeley,0,13,False,False,False,statistics,1501787340,False,
Why does my linear model systematically over/under-predict outcomes depending on how far above or below the expected value the prediction is?,3,2,False,False,False,statistics,1501787933,True,"Sorry for the ridiculous title.  I'm trying to predict outcomes of a players performance in a game in terms of an aggregate performance score (which are pretty close to normally distributed for a given player).  Each player has a relatively long history of playing under a variety of different conditions, and I fit a linear model to each player's data using some subset of available features (with the selected features chosen based on leave one out cross validation).  Now, I check my model against a test set to see how it performs.  The outcome isn't terrible, but I feel like it could be better.  In playing around with this I found that if I plot the z-score of the predicted value (based on the historical mean and std of a players scores) against the difference between the prediction and the test point, I get a highly significant correlation (R^2 = ~0.62).  What the relationship says is that if I predict an outcome that is 1 standard deviation above a players mean, then that outcome is likely about 7 points above what is actually going to happen, and vice versa for 1 standard deviation below.  The model is most accurate when it predicts someone is going to have an average game.  It's a fairly marked linear relationship. 

So, I'm a little perplexed by this.  As an academic scientist I've never come across anything like this in my research (btw: this is for a hobby of mine and not work).  I'm trying to diagnose what might be causing this, but I'm curious if there are any tell-tale signs that the experts here might pick up on.  "
Fitting a regression with different intercepts but identical slope.,2,2,False,False,False,statistics,1501788046,True,"I'm reading An Introduction to Generalized Linear Models by Dobson and Barnett.

In the introductory chapter, the authors describe a problem whereby they would like to test a hypothesis that males and females have similar growth rate during their gestational period.

Here is some of the data.

       age weight gender
       <dbl>  <dbl>  <chr>
     1    40   3126 Female
     2    39   2817 Female
     3    38   2795   Male
     4    35   2925   Male
     5    42   3210 Female
     6    38   2991 Female
     7    38   2975   Male
     8    36   2412 Female
     9    39   2875 Female
    10    40   3317 Female

They authors want to fit something like

Y_j = a_j + beta age_ij 

As you can see, the regressions have different intercepts but the same slope.  I'm hoping to recreate some of the results, but can't seem to fit this model in R.  Can someone point to a way to achieve this?"
Test to compare the correlation of one variable to the standard deviation of another?,0,1,False,False,False,statistics,1501789056,True,[removed]
"Statistics Done Wrong - ""a guide to the most popular statistical errors and slip-ups committed by scientists every day""",12,212,False,False,False,statistics,1501790948,False,
"Success-Failure Testing, Establishing a baseline",0,1,False,False,False,statistics,1501791172,True,"First, apologies if the title and/or question are poorly worded. It's not due to laziness, just ignorance. 

I am used to evaluating a system based on certain factors: confidence level (90%), allowable number of failures (r), and a performance target (usually 99--99.98%).

I can come up with a sample size required to measure the performance of a system with those parameters (I have a nifty Excel spreadsheet that does it for me).

What I'm stumped on--and this is where my ignorance will really show--is how do I go about establishing what you might call baseline performance? That is, without a target performance-- simply to evaluate the likelihood of failure (or success) of a system? 

To put it another way, I want to figure out, using a statistically significant sample, how my system is performing. Is it successful 99% of the time? 95%? 91%? 

Maybe yet another way to put it, if my allowable number of failures exceeds the one I used to calculate the sample size, can I use that and the sample size to figure out actual performance?

Thanks again for your patience, and any help you can offer. Of course feel free to ask clarifying questions as needed."
Trimmed Mean question,0,1,False,False,False,statistics,1501800295,True,[deleted]
I am a Bayesian trying to be constructive about hypothesis testing,1,1,False,False,False,statistics,1501800628,True,"I am writing a [blog post](https://www.statlect.com/glossary/null-hypothesis) about classical testing of null hypotheses. Despite being Bayesian, I want to be as constructive as possible. In the last section of the post I give practical advice on how to formulate and conduct hypothesis testing. The three main pieces of advice are: 1) remember to write the null hypothesis in such a way that rejection of the null is the interesting result; 2) always compute and discuss the power of your test; 3) do robustness checks in order to make sure that model mis-specification is not biasing your test.
What other pieces of advice do you think it is essential to give?"
"What gpa would a person need to get into a say, a non top 10 statistic grad school?",0,1,False,False,False,statistics,1501807210,True,[deleted]
How should I go about getting a position in Statistics with only a BS in it? How tough is the job market?,0,1,False,False,False,statistics,1501809749,True,[removed]
Help with a pretty easy stats problem,5,0,False,False,False,statistics,1501812205,True,"Here for the third time in 24 hours, sorry in advanced. Here's the question:

The proprietor of a boutique in New York wanted to determine the average age of his customers. A random sample of 25 customers revealed an average age of 28 years with a standard deviation of 10 years. Determine a 95% confidence interval estimate for the average age of all his customers. Assume the population of customer ages is normally distributed.

Za/2 for 95% confidence = 1.96. st dev divided by sq root of sample size. so 10 divided by sqrt 25 (which is 5) equals two. 2 times 1.96 is 3.92.

 age range should be 24.08 to 31.92. professor's answer says 23.872 to 32.128. anyone see what i'm doing wrong?"
What are the numbers?,0,1,False,False,False,statistics,1501814076,True,[removed]
Communicating significance to non-statisticians,0,3,False,False,False,statistics,1501823822,False,
OLS Constant Term,5,1,False,False,False,statistics,1501824598,True,"In OLS regressions is the constant term mandatory? I'm noticing an R-squared of .91 without it and .22 with it for some model testing I'm doing.  
I thought it isn't mandatory and just means an intercept.  My data is like X=[[.3,.4],[.34,.3]]; Y=[.35,.32]"
SPSS Assignment Help Services,0,1,False,False,False,statistics,1501832059,False,
"Which ""lighter"" books on statistics like "" How to lie with statistics"" and Nate Silvers book would you recommend for a past time read?",7,17,False,False,False,statistics,1501839473,True,
"Standard Error of the Mean, and Gross Confusion!!",1,1,False,False,False,statistics,1501846887,True,[removed]
"if you have one dummy (=1) in a simple logistic regression, do we do B(exp) of the constant + B(exp) of our dummy variable to find the odds increase of that variable?",6,3,False,False,False,statistics,1501851693,True,"I'm kind of confused about this. I've been told this is what you need to do but now I'm asking myself ""is that actually right though?"""
Books for Medical Statistics,4,1,False,False,False,statistics,1501856446,True,"So far, I know of Oxford Handbook of Medical Statistics and Essential Medical Statistics (Kirkwood).

Anyone read both of them and would strongly recommend buying one over the other?

Thanks!"
Advice regarding my Msc Mathematics,8,3,False,False,False,statistics,1501857861,True,"Hi - I've been offered a place to study a Msc in Mathematics at St Andrews in the UK. My Bsc is in Physics and not Maths. I choose Maths because I want to go into data science in physics, as I'm aiming to do a Physics PhD after the Maths Msc.

So basically I want to learn as much statistics as possible during my Msc, but I've been told after enquiring about modules that because I'm a physics graduate I will not be eligible for the statistics Msc optional modules, only applied mathematics modules.

Studying these statistics modules is important to me and I don't see the point in accepting the offer without doing them - but yes I do realise I have huge gaps in my knowledge that would make the courses hard. However I am extremely dedicated and hard-working and believe I can succeed in the courses, but I don't know how to convince the module coordinators this.

These are the important modules for me to study -
Advanced Data Analysis
Advanced Analytical Techniques
Statistical Modelling
Advanced Statistical Inference
Advanced Bayesian Inference
Applied Multivariate Analysis

I believe Bayesian and Statistical Inference are the most prerequisite heavy modules.

Does anyone have some advice?
I did reply back to try and convince them I can do it and really want to but I'm not sure if it'll be successful. If they still say no should I email the module lecturers and explain the same thing to them? I don't want to go behind anyone's back which is the problem.

Edit: Also I'd appreciate any reccomended texts I can go over to prepare myself, and a list of topics I'd need know well"
When is data normalization necessary?,2,1,False,False,False,statistics,1501874770,True,"Here's my situation: I have 10 geographical regions which answered a survey. Some regions have answered more than others. If I want to compare these regions, do I need to do any normalization beforehand?

For instance, they each answered 10 questions which either strongly disagree all the way up to strongly agree. I'm trying to see which region disagrees the most with the survey questions. Wouldn't this be unfair since some regions simply have more people answering them than others? "
Some questions on linear model selection and regularization,0,2,False,False,False,statistics,1501876518,True,"Hello guys,

I'm reading through the book [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf). I have some questions regarding Chapter 6 of the book, titled ""Linear model selection and regularization"". There are a lot of questions here and I re-read the relevant sections in the book in order to answer them myself. I did answer some of them, but those below are the ones left unanswered. I am hoping someone can answer them.

1) Why do backward stepwise selection and forward stepwise selection have more than 1 + p(p+1)/2 models?  (page 209) It doesn't make sense to me; if p = 5, then forward stepwise selection would start with 5 models (it starts from the null model, then tests which of the predictors is ""the best""), then after it added that predictor, it searches among the remaining 4. That would be 5+4+3+2+1 = 15, or as in the formula 1 + 5(5+1)/2 = 16. How can it be more than that?

2) Page 217. - Why is the standardization formula given the way it is? How come that when we standardize all the predictors, the standard deviation is equal to 1?

3) Page 221. - It says ""When p = 2, then (6.8) indicates that the lasso coefficient estimates have the smallest RSS out of all points that lie within the diamond defined by |β1| + |β2| ≤ s"". How do we know this?

4) Page 224 (A Simple Special Case for Ridge Regression and the Lasso) - What does our assumption that n = p have to do with simplyfing our case? And why do we derive the formula we derive in 6.11 (top of page 225)?

5) Page 224 - Here a ""data matrix X"" is mentioned; do we place data in matrices for easier manipulation? In this particular case, what does a diagional matrix give us?

6) Page 241 - 242. - How do I interpret these boxplots? I don't know what those three boxplots mean.

7) Page 216 - How come that the formula for l2 norm is the square root of the sum of the squares of all the coefficients?

8) What is l1 norm?

9) Page 255. - Why don't we first perform cross-validation, instead of making a model with a grid of lambdas (lamdas ranging from 10^-2 to 10^10), then performing cross-validation to see the best value of lamdba?

10) Page 260. - Here are my answers for task 3 and 4. If someone could check them, I'd appreciate it.

Task 3:

a) Training RSS - Steadily decrease.

b) Test RSS - Decrease initially, and then eventually start increasing in a U shape.

c) Variance - Steadily increase.

d) Squared bias - Steadily decrease.

e) Irreducible error - Remain constant.

Task 4:

a) Training RSS - Steadily decrease.

b) Test RSS - Decrease initially, and then eventually start increasing in a U shape.

c) Variance - Steadily decrease.

d) Squared bias - Steadily increase.

e) Irreducible error - Remain constant.

Thanks in advance!"
Analysing a large cross-sectional dataset.,0,1,False,False,False,statistics,1501879268,True,"Hi there - I’ll get to the point straight away.
I have a very large dataset of 300,000 participants. The data are from 10 years of NHIS surveys.

I don’t have much experience with samples this large, and I am only really trained in SPSS.

My main issues are:
How does one even begin to deal with outliers when there are thousands? (Spare me the debate about what constitutes an outlier if possible!).

How do I account for missing data, while still using the complex samples module in SPSS? (which doesn’t allow you to analyse multiply imputed data). Oh and, if one response in the survey is dependent on another I.e. ‘do you smoke?’ (Y/N) and then if Y - ‘how many do you smoke?’. Theoretically, would it make sense to impute data for the second response, even though I know why the response rate is relatively very small?

Are there any methods I could use to negate the need to worry about homogeneity of variance and the like?
The data are pretty disgusting. 

We are talking about 50% missing data on key variables, with very skewed and kurtosisisisisised distributions.

Ps. If anyone wants to collaborate on this project ala formal statistical advising let me know! When it’s eventually analysed and written up, I have some good journals in mind.

Thanks everyone."
What gpa would you recommend to get into statistics grad school?,9,1,False,False,False,statistics,1501879451,True,
There’s a debate raging in science about what should count as “significant”,71,65,False,False,False,statistics,1501883100,False,
Graduation in Statistics,2,0,False,False,False,statistics,1501888586,True,"Hey, everyone.

I would like to know the pros and the cons of a graduation in statistics."
A little confused about Linear Mixed Models,1,10,False,False,False,statistics,1501890967,True,"Maybe someone can help me work through this:

We measured the depended variable over a 9 day period (repeated measures) under 4 different conditions, along with several other potential predictors that aren't central to the research question. Things like gender, BMI, Age, etc that we'd like to control for.

Does it make sense to designate *day* and *condition* as fixed factors, and the others as random factors? My understanding of linear mixed models is very limited, but I was essentially told in my undergraduate stats class that random factors are ones you don't care about, but may be sources of variability. 

Is that remotely correct?

"
Has anyone here done Lean Six Sigma that could possibly help me over the next two weeks out of the kindness of their heart?,1,1,False,False,False,statistics,1501896012,True,"Hi - I'm doing a lean six sigma training for work and my grad program. I don't come from a stats background or engineering background so I'm having a bit of a hard time doing this. 

I just need someone who I can lean on for explanations and assistance over the next two weeks or so. Not all the time (I hope lol). Anyway, please let me know."
How is the Statistics job market for people with Bachelor's Degrees only?,0,1,False,False,False,statistics,1501898019,True,[removed]
Is there such thing as taking the percent change of the percent change?,3,0,False,False,False,statistics,1501898775,True,
"Continuous Random Variable, Probability Density function, Cumulative Density Function",0,1,False,False,False,statistics,1501935814,True,[removed]
Jensen's inequality,0,1,False,False,False,statistics,1501938293,True,[removed]
what if I am averaging some regression results but one of them is not statistically significant (0.54)? do I exclude this one?,2,1,False,False,False,statistics,1501958493,True,
www.eurohttp.com website worth,0,0,False,False,False,statistics,1501960897,False,
How best to analyze this data? have limited data points.,0,1,False,False,False,statistics,1501968680,True,[deleted]
About Variance operation and its properties,0,8,False,False,False,statistics,1501969568,False,
Probability and Excel Help,4,0,False,False,False,statistics,1501978811,True,"Hello,

I'm having a hard time figuring out how to use Excel formulas to figure out the following. I know the formulas to use it pen and paper, however, the class requires that we use Excel and show it's formulas. Any and all help would be appreciative. 

                A          B         AB        O
Rh+         .34       .09       .04      .38
Rh-          .06       .02       .01       .06

a. What is the probability a person will have type O blood?
b. What is the probability a person will be Rh-?
c. What is the probability a married couple will both be Rh-?
d. What is the probability a married couple will both have type AB blood?
e. What is the probability a person will be Rh- given she or he has type O blood?
f. What is the probability a person will have type B blood given he or she is Rh+?"
Why is biostatistics its own discipline when statistics has so many different applications in other fields?,31,47,False,False,False,statistics,1501991521,True,"It seems like biostatistics is the only subfield / application of statistics that is often its own discipline or department at universities. Why is that? You can apply statistics to a host of different disciplines like economics, psychology, other social sciences, computer science, etc. So why is biostatistics different? Why is biology designated separately from other applications of statistics in so many universities?"
how do you solve this?,1,0,False,False,False,statistics,1502027618,True,"Hi reddit I am in a statistics class and have been struggling to solve this problem. Any ideas how to solve this? Thanks.
For a set of data points, the equations of two lines are suggested. Determine which line fits the set of data points better, according to the least-squares criterion.

Line A: y = 1 + 0.9x
Line B: y = 0.8 + 1.1x

		



"
ECUCI Malaysia Official - Online Casino Games In ONE MOBILE APP SCR888 3WIN8 Club Suncity Lucky,0,1,False,False,False,statistics,1502067193,False,
need some literature or study material for survival analysis,0,1,False,False,False,statistics,1502108766,True,[removed]
Causal mediation analysis product method,0,3,False,False,False,statistics,1502110167,True,Are there any benefits in using causal mediation analysis over the traditional mediation analysis in cases where there is no interaction between the exposure and mediator? 
"Help? I know transformations in general and that the answer is I(Z) (0,1), but not getting it.",1,4,False,False,False,statistics,1502117764,False,[deleted]
Can every independent variable be binary (for qualitative data) in linear regression?,4,1,False,False,False,statistics,1502120765,True,"I have a dependent variable that can only assume a whole number value of 0-10. I have 10 questions that are qualitative that I would like to run regression on to determine a function for my DV. Can I do this with all binary independent variables?

For instance, one question is answered with:

A) not at all
B) weak
C) moderate
D) strong

I assume I will need a total of 3 binary independent variables for this.

X1 = {1: if not at all, 0: otherwise}

...

X3 = {1: if moderate, 0: otherwise}"
Testing for discrimination in college admissions,4,3,False,False,False,statistics,1502121830,False,
Ordinal Data and Nominal Data,0,0,False,False,False,statistics,1502122660,False,
"How U.S. government statistics work, explained by the country’s Chief Statistician",3,39,False,False,False,statistics,1502125138,False,
What Statistical Tools to Use for Research,0,1,False,False,False,statistics,1502127493,True,[removed]
Help with Gage R&R calculations.,0,1,False,False,False,statistics,1502130631,True,[deleted]
Can you go into biostatisics with a masters in statistics?,6,0,False,False,False,statistics,1502131799,True,"Or is it difficult to get into? I currently have a BS in Biology and would prefer to go general statistics for the masters so that there's more fields I can consider going into. I don't want to be ""stuck"" by getting a biostatisics degree if it turns out I don't like it and would rather apply statistics in a different field"
"Looking for help with Markov Chains, DnD related.",12,5,False,False,False,statistics,1502134128,True,"**If you don't know how Dungeons and Dragons 5e Death Saving Throws work, read the section below before proceeding.**

------------------------------
In DnD you get to make death saving throws if you drop to 0 HP. This is state 00. For clarity, I'll refer to this states values as ""XY"".

Every turn you then roll a twenty sided die without modifiers. If the value is between 2 and 9, you increase Y by 1. This represents a saving throw failure. A Y value of 3 or greater means you die. 

If the value is between 10 and 19, you increase X by 1. This is a saving throw success. An X value of 3 means you are now Stable. 

If the value is 1, you increase Y by 2. This is a critical failure. Once again, a Y value of 3 or greater is Dead.

If the value is 20, you critically succeed. DnD is weird though and it does not count as a success. You instead are conscious and at 1HP. There is no X or Y value attributed to this, so we will just call it 1HP.

In order to die, you need to have your Y value be equal to or greater than 3, without reaching a Stable or 1HP state. Your X and Y values also cannot be reduced, only added to. They are only removed when you reach Stable, 1HP or Dead.

-----------------------------

**THE PROBLEM**

What I am trying to solve is a stupid homebrew class that changes the death saving rules to requiring **9 failures in order to die**. Yes I already know its OP and its not being allowed, but we joked about how damn near impossible it would be to die, and we started to wonder about the actual numbers involved.

Anyways, now it changes to needing a Y value greater than or equal to 9 to die, while still requiring that we don't reach a Stable or 1HP condition.

The only way I know how to do it is list all the paths, even duplicates, multiply the probability from step to step and then multiply by 100 to get a percentage chance for each path, and then add the percentages together for the total probability of getting any one of those paths that lead to death. There are however, so many different paths to reach that total, and I really don't want to sit down and brute force each individual route and its probability. 

I haven't taken a stats or analysis class in around 5 years, so this is the best I can do for now. I've taken up to Calculus 2 so far as well, but its still been a while since that class (I'm a Chemistry Major).

[Here is my excel sheet with the chain](https://docs.google.com/spreadsheets/d/1VU9DkrGin8Z7pwl1t5ggGJVAk6T6C_50PPULFp8BMGA/edit?usp=sharing). I'm lazy so I didn't include the zeros since there are so many 0 values as you can't remove successes or failures as you go to another step. Page 1 is the 9 failure scenario, page 2 is the 3 failure scenario. 

The end goal is I want the total probability of going from state 00 to Dead in the 9 Failure Markov Chain. Any and all help would be appreciated. "
"I'm not sure what MOE to use in my calculation for the sample size I would need for a study of 700,000 people.",0,1,False,False,False,statistics,1502139541,True,[removed]
Google Trends - weekly data for >5y periods,0,1,False,False,False,statistics,1502142026,True,[removed]
Is anom the best approach?,4,1,False,False,False,statistics,1502145932,True,"Hi all, I took basic stats in college but didn't learn it terribly well. I would like to start learning more, however. My question at the moment is: is there a day/time of the week that is better to post to on facebook, with ""better"" being defined as how many impressions a post gets. If I have several dozen data points (posts and their impressions) for each time slot (hours in a day) and would like to figure which time slot has ""more"" impressions than others, would anom be the best approach? I am a little overwhelmed in regard to ensuring the assumptions are not being broken and learning about transformations - I am just wondering if I am on the right track before I ""waste"" too much time.

Thanks!"
"Interview at a capital management firm, no idea what to expect.",1,1,False,False,False,statistics,1502146092,True,"Nearing the end of my MS stats program. I've never taken a finance-related class and my undergrad was biology. Applied to a jr. research analyst (which is a level or two below a quantitative research analyst I assume) and have an interview this week.

How would I prepare for this? I notice they ""require"" knowledge of a couple concepts that I never took a class for. My plan is just to review material for the courses I did take, since I forget quite a few things after not seeing it for 6 - 12 months and I figure they don't like that excuse. Do I need to expect coding questions?
"
How to analyze data set with n=2?,14,1,False,False,False,statistics,1502157727,True,[deleted]
Light-hearted stats fun || In the Judeo-Frequentist tradition...(X-post R/sassyStatistician) [OC],0,0,False,False,False,statistics,1502160125,False,[deleted]
Stuck with an idea,4,3,False,False,False,statistics,1502163632,True,"I am trying to see if the following has some sensible solution. Suppose that you get a list with the heights of the students in the class (n=1,000), which you know are normally distributed, but you don't know what the mean is. Further suppose that your list only includes kids that are shorter than 1.4 meters (n=200) or higher than 1.7 meters (n=300).

Can you approximate the true mean based on these data? How?"
Need help on the Statistical breakdown.,0,1,False,False,False,statistics,1502177690,False,
Back-testing Probability of Default,0,1,False,False,False,statistics,1502179088,True,[removed]
Ratios involving zero?,16,4,False,False,False,statistics,1502189346,True,"Hi all, I have a data set where I need to get ratios, but a lot of the data points are zero. Since I need to get a numeric value, it is possible to approximate 0 to 0.1^-20? Or what is the most accurate way to derive a numeric value for ratios when the denominator is zero?"
We propose to change the default P-value threshold for statistical significance for claims of new discoveries from 0.05 to 0.005 - signed by 72 statisticians,45,96,False,False,False,statistics,1502191649,False,
Fisher's Exact Test,5,1,False,False,False,statistics,1502198671,True,"I've been looking at the Chi squared Independence test, and Fisher's Exact test and it made me wonder.  If you ignore the difficulty of calculation, and you had the proper assumptions to do either test, which one is better? Are they the same? "
Introduction to Imitation Learning,0,7,False,False,False,statistics,1502199151,False,
is a quantitative methods degree similar to a applied statistics degree?,4,2,False,False,False,statistics,1502205843,True,"the university of western australia offers a quantitative methods major

since I have never heard of another uni offering a quantitative methods major, what would it most likely compare too? I am guessing based on the units that it is an applied statistics degree?

this is the unit course outline http://handbooks.uwa.edu.au/majors/majordetails?code=MJD-QTMTD

i would really appreciate if someone could have a look at the handbook and comment on whether they think it is worth doing. 

thanks, also please tell me if you think there's a better place to post this question aswell
"
"For linear regression, why should we assume the predictor and response variables are normally distributed?",5,2,False,False,False,statistics,1502208007,True,"I get that for simple linear regression, say y=b0+b1*x + error,

the errors should be normally distributed, have no autocorrelation, constant variance. I also understand that there should be no collinearity, and that the relationship between x and y should be linear.

Those are the standard assumptions in an intro to regressions.

However, as I'm learning about power transforms such as the Box Cox transformation, it's said that the Box Cox attempts to transform either the predictor x or response y to normally distributed if they are skewed in the first place. 

But other sources say it's not important for x and y to be normally distributed. 

So I'm thinking, if both x and y are uniformly distributed, and then there is a roughly linear relationship between them, wouldn't linear regression also work? You can still have the error be normally distributed. 

Can you comment or advise on the normality assumption? I feel the point of the Box Cox or other power transforms is to deal with non-constant variances of the error terms, and that's a more important goal and making the response normally distributed..."
How can one compare Betteley's Addition law for expectations with P(A ∪ B) = P(A) + P(B) − P(A ∩ B),0,1,False,False,False,statistics,1502208588,True,[removed]
Question on longitudinal analysis,0,1,False,False,False,statistics,1502214908,True,"I wish to know if longitudinal analysis is possible in the following situation, and if so, how are the data best arranged:

Say I have various measures of individuals called measures a, b, c from several countries. (a, b, c could be measures on living conditions, satisfaction with health care etc... and they are a mix of integer and nominal. One of these is the d.v.)

I collect a, b, c in waves across several years but from different samples each time but from the same countries.

I also have country level (ratio) data from each country, say health care spending compared to GDP from each wave.

What is the best way to analyse these types of data? Can a longitudinal analysis be done on this?

I hope this question makes sense!

Thanks"
Latest Updates from the Bureau of Labor Statistics (bls.gov),0,1,False,False,False,statistics,1502221304,True,"#Compensation in private industry and state and local government, 3-month percent change, seasonally adjusted



| Month     | Private industry | Government | 
|:-----------|------------------:|------------:| 
| Mar 2006  | 0.6%             | 0.7%       | 
| June 2006 | 0.8%             | 1.0%       | 
| Sept 2006 | 0.9%             | 1.3%       | 
| Dec 2006  | 0.8%             | 1.1%       | 
| Mar 2007  | 0.6%             | 1.2%       | 
| June 2007 | 0.9%             | 1.0%       | 
| Sept 2007 | 0.8%             | 0.9%       | 
| Dec 2007  | 0.9%             | 1.0%       | 
| Mar 2008  | 0.7%             | 0.6%       | 
| June 2008 | 0.7%             | 0.7%       | 
| Sept 2008 | 0.6%             | 1.0%       | 
| Dec 2008  | 0.5%             | 0.6%       | 
| Mar 2009  | 0.2%             | 0.7%       | 
| June 2009 | 0.2%             | 0.7%       | 
| Sept 2009 | 0.4%             | 0.3%       | 
| Dec 2009  | 0.5%             | 0.5%       | 
| Mar 2010  | 0.6%             | 0.4%       | 
| June 2010 | 0.5%             | 0.5%       | 
| Sept 2010 | 0.4%             | 0.3%       | 
| Dec 2010  | 0.4%             | 0.5%       | 
| Mar 2011  | 0.6%             | 0.5%       | 
| June 2011 | 0.8%             | 0.3%       | 
| Sept 2011 | 0.4%             | 0.3%       | 
| Dec 2011  | 0.4%             | 0.3%       | 
| Mar 2012  | 0.5%             | 0.5%       | 
| June 2012 | 0.5%             | 0.5%       | 
| Sept 2012 | 0.4%             | 0.5%       | 
| Dec 2012  | 0.3%             | 0.3%       | 
| Mar 2013  | 0.6%             | 0.5%       | 
| June 2013 | 0.5%             | 0.4%       | 
| Sept 2013 | 0.4%             | 0.3%       | 
| Dec 2013  | 0.5%             | 0.7%       | 
| Mar 2014  | 0.3%             | 0.5%       | 
| June 2014 | 0.8%             | 0.5%       | 
| Sept 2014 | 0.7%             | 0.5%       | 
| Dec 2014  | 0.5%             | 0.6%       | 
| Mar 2015  | 0.7%             | 0.6%       | 
| June 2015 | 0.0%             | 0.6%       | 
| Sept 2015 | 0.6%             | 0.6%       | 
| Dec 2015  | 0.5%             | 0.7%       | 
| Mar 2016  | 0.6%             | 0.5%       | 
| June 2016 | 0.6%             | 0.5%       | 
| Sept 2016 | 0.5%             | 0.8%       | 
| Dec 2016  | 0.5%             | 0.6%       | 
| Mar 2017  | 0.8%             | 0.6%       | 
| June 2017 | 0.5%             | 0.5%       | 



#^[*Source*](https://www.bls.gov/charts/employment-cost-index/compensation-in-private-industry-and-state-and-local-government-3-month-percent-change.htm#) 



-----------------------------------------------------



#Selected unemployment indicators, seasonally adjusted



**The first three dated collums from the left indicate unemployed persons, the following collums from the left indicate unemployement rates**. 



^Characteristic/Age ^Diagnostic | ^July ^2016 | ^June ^2017 | ^July ^2017 | ^July ^2016 | ^Mar. ^2017 | ^Apr. ^2017 | ^May ^2017 | ^June ^2017 | ^July ^2017 | 
|:-------------------------------|-------:|-------:|-------:|------:|------:|------:|------:|------:|------:| 
| Total, 16 years and over      | 7,749 | 6,977 | 6,981 | 4.9  | 4.5  | 4.4  | 4.3  | 4.4  | 4.3  |   
| 16 to 19 years                | 920   | 801   | 775   | 15.6 | 13.7 | 14.7 | 14.3 | 13.3 | 13.2 |   
| 16 to 17 years                | 326   | 312   | 332   | 15.3 | 17.4 | 16.8 | 13.1 | 13.9 | 15.5 |   
| 18 to 19 years                | 590   | 498   | 428   | 15.7 | 11.2 | 12.5 | 14.6 | 13.2 | 11.6 |  
| 20 years and over             | 6,829 | 6,175 | 6,205 | 4.5  | 4.1  | 4.0  | 3.9  | 4.0  | 4.0  |   
| 20 to 24 years                | 1,347 | 1,130 | 1,133 | 8.9  | 7.3  | 7.3  | 6.7  | 7.5  | 7.4  |   
| 25 years and over             | 5,472 | 5,078 | 5,078 | 4.0  | 3.8  | 3.6  | 3.6  | 3.7  | 3.6  |   
| 25 to 54 years                | 4,140 | 3,902 | 3,914 | 4.1  | 3.9  | 3.8  | 3.8  | 3.8  | 3.8  |   
| 25 to 34 years                | 1,789 | 1,575 | 1,652 | 5.0  | 4.5  | 4.4  | 4.9  | 4.4  | 4.6  |   
| 35 to 44 years                | 1,151 | 1,113 | 1,184 | 3.5  | 3.9  | 3.6  | 3.3  | 3.4  | 3.6  |   
| 45 to 54 years                | 1,200 | 1,214 | 1,078 | 3.5  | 3.2  | 3.4  | 3.2  | 3.6  | 3.2  |   
| 55 years and over             | 1,324 | 1,160 | 1,162 | 3.7  | 3.4  | 3.2  | 3.1  | 3.2  | 3.2  |   
| Men, 16 years and over        | 4,274 | 3,702 | 3,715 | 5.0  | 4.6  | 4.4  | 4.2  | 4.4  | 4.4  |   
| 16 to 19 years                | 499   | 434   | 437   | 16.6 | 14.8 | 16.3 | 15.7 | 14.4 | 15.2 |   
| 16 to 17 years                | 154   | 151   | 174   | 15.1 | 17.7 | 15.9 | 13.7 | 14.4 | 17.9 |   
| 18 to 19 years                | 342   | 292   | 254   | 17.3 | 12.8 | 15.3 | 16.4 | 14.8 | 13.4 |   
| 20 years and over             | 3,775 | 3,268 | 3,278 | 4.6  | 4.3  | 4.0  | 3.8  | 4.0  | 4.0  |   
| 20 to 24 years                | 791   | 666   | 642   | 10.0 | 8.5  | 8.4  | 7.9  | 8.4  | 8.0  |   
| 25 years and over             | 2,980 | 2,607 | 2,634 | 4.0  | 3.8  | 3.5  | 3.4  | 3.5  | 3.6  |   
| 25 to 54 years                | 2,239 | 2,024 | 2,068 | 4.1  | 3.9  | 3.6  | 3.6  | 3.7  | 3.8  |   
| 25 to 34 years                | 961   | 868   | 871   | 5.0  | 4.8  | 4.4  | 4.6  | 4.5  | 4.5  |  
| 35 to 44 years                | 618   | 560   | 624   | 3.5  | 3.8  | 3.4  | 3.0  | 3.2  | 3.5  |   
| 45 to 54 years                | 660   | 596   | 573   | 3.7  | 3.1  | 2.8  | 3.0  | 3.4  | 3.2  |   
| 55 years and over             | 741   | 583   | 566   | 3.8  | 3.4  | 3.2  | 2.9  | 3.0  | 2.9  |   
| Women, 16 years and over      | 3,475 | 3,274 | 3,265 | 4.7  | 4.3  | 4.4  | 4.3  | 4.4  | 4.3  |   
| 16 to 19 years                | 421   | 367   | 338   | 14.6 | 12.6 | 13.1 | 12.8 | 12.2 | 11.3 |   
| 16 to 17 years                | 172   | 162   | 158   | 15.5 | 17.0 | 17.6 | 12.6 | 13.5 | 13.5 |   
| 18 to 19 years                | 248   | 206   | 174   | 13.9 | 9.6  | 9.5  | 12.6 | 11.3 | 9.7  |   
| 20 years and over             | 3,054 | 2,907 | 2,927 | 4.3  | 4.0  | 4.1  | 4.0  | 4.0  | 4.0  |   
| 20 to 24 years                | 557   | 464   | 491   | 7.6  | 6.0  | 6.1  | 5.3  | 6.4  | 6.7  |   
| 25 years and over             | 2,492 | 2,471 | 2,444 | 3.9  | 3.8  | 3.8  | 3.9  | 3.8  | 3.7  |   
| 25 to 54 years                | 1,901 | 1,878 | 1,846 | 4.0  | 3.9  | 4.1  | 4.1  | 3.9  | 3.8  |   
| 25 to 34 years                | 829   | 707   | 781   | 5.0  | 4.2  | 4.4  | 5.1  | 4.2  | 4.6  |   
| 35 to 44 years                | 533   | 553   | 560   | 3.5  | 4.1  | 3.9  | 3.7  | 3.6  | 3.7  |   
| 45 to 54 years                | 540   | 618   | 505   | 3.4  | 3.4  | 4.0  | 3.5  | 3.9  | 3.2  |   
| 55 years and over             | 564   | 581   | 578   | 3.4  | 3.3  | 3.2  | 3.4  | 3.4  | 3.4  |  
| Married men, spouse present   | 1,232 | 1,052 | 1,110 | 2.6  | 2.6  | 2.4  | 2.3  | 2.2  | 2.4  |   
| Married women, spouse present | 1,101 | 1,030 | 1,060 | 3.0  | 2.8  | 2.8  | 2.5  | 2.8  | 2.9  |   
| Women who maintain families   | 765   | 703   | 703   | 7.2  | 5.5  | 6.0  | 6.8  | 6.9  | 6.8  |   
| Full-time workers             | 6,333 | 5,775 | 5,717 | 4.9  | 4.4  | 4.3  | 4.2  | 4.4  | 4.3  |  
| Part-time workers             | 1,439 | 1,194 | 1,291 | 5.0  | 4.9  | 5.0  | 4.8  | 4.2  | 4.5  |  




* Refers to persons in opposite-sex couples only.


* Data are not seasonally adjusted. Refers to female householders residing with one or more family members, but not an opposite-sex spouse.


* Full-time workers are unemployed persons who have expressed a desire to work full time (35 hours or more per week) or are on layoff from full-time jobs.


* Part-time workers are unemployed persons who have expressed a desire to work part time (less than 35 hours per week) or are on layoff from part-time jobs. 


#^[*Source*](https://www.bls.gov/news.release/empsit.t10.htm) 


--------------------------------------------------------------




#Average weekly hours and overtime of all employees on private nonfarm payrolls by industry sector, seasonally adjusted



**Bold** indicates Overtime hours. 



 
Industry | ^July ^2016 | ^May ^2017 | ^June ^2017 | ^July ^2017 | 
|:--------------------------------------|------:|------:|------:|------:| 
| Total private                        | 34.4 | 34.4 | 34.5 | 34.5 | 
| Goods-producing                      | 40.3 | 40.4 | 40.4 | 40.5 | 
| Mining and logging                   | 43.3 | 45.2 | 45.0 | 45.2 | 
| Construction                         | 39.2 | 39.2 | 39.2 | 39.2 | 
| Manufacturing                        | 40.8 | 40.7 | 40.9 | 40.9 | 
| Durable goods                        | 41.3 | 41.3 | 41.4 | 41.4 | 
| Nondurable goods                     | 39.9 | 39.9 | 40.1 | 40.2 | 
| Private service-providing            | 33.3 | 33.3 | 33.3 | 33.3 | 
| Trade, transportation, and utilities | 34.4 | 34.4 | 34.4 | 34.4 | 
| Wholesale trade                      | 38.9 | 39.0 | 39.0 | 39.0 | 
| Retail trade                         | 31.1 | 31.0 | 31.0 | 31.0 | 
| Transportation and warehousing       | 38.8 | 38.8 | 39.0 | 38.9 | 
| Utilities                            | 42.5 | 42.0 | 42.3 | 42.1 | 
| Information                          | 36.1 | 36.3 | 36.3 | 36.3 | 
| Financial activities                 | 37.6 | 37.4 | 37.6 | 37.5 | 
| Professional and business services   | 36.1 | 36.1 | 36.1 | 36.2 | 
| Education and health services        | 32.9 | 32.9 | 32.9 | 32.9 | 
| Leisure and hospitality              | 26.1 | 26.1 | 26.1 | 26.1 | 
| Other services                       | 32.0 | 31.8 | 31.9 | 31.8 | 
| **Manufacturing**                    | 3.3  | 3.3  | 3.3  | 3.3  | 
| **Durable goods**                    | 3.3  | 3.3  | 3.3  | 3.3  | 
| **Nondurable goods**                 | 3.3  | 3.3  | 3.4  | 3.4  | 





#^[*Source*](https://www.bls.gov/news.release/empsit.t10.htm)




#[You can compare this with the Average weekly hours and overtime of production and nonsupervisory employees on private nonfarm payrolls by industry sector, seasonally adjusted](https://www.bls.gov/news.release/empsit.t23.htm). 




--------------------------------------------------------------------



#[That's all, folks](https://i.imgur.com/UvxOjaV.gif)! 




**Sincerely**, 




-u/_Constructed_ 







"
"Why is mode consider a measure of ""central"" tendency. Wouldn't it be the most extreme?",16,3,False,False,False,statistics,1502227870,True,"Mean and median are easy to think of as being ""central"" or in the middle, but what about mode? I guess in a normal distribution the mode is the middle, but data are rarely a perfect normal distribution."
Is there a way to identify drivers of an outcome with this data structure?,1,5,False,False,False,statistics,1502228166,True,"Been asked in my current position to identify the drivers of absenteeism for staff at my organization.

The data i have to do this is records of each absence, with each row representing a single absence with the following variables:

-absence date
-absence reason
-employee Id
-employee department
-employee age
-employee gender

I seperated the dataset into 2 datasets, 1 with employee data, 1 row per employee and then 1 dataset with data for each absence so i can create a dashboard.

However, i dont know how i would approach identifying the drivers of absenteeism.

Currently i am providing descriptive statistics and then recommending they put together a dataset with all employees with their characteristics and then a variable showing the total number of absences they had over the past year, regardless of whether they had an absense or not (where current one only has employees with absences). From that i believe i could develope a regression model identifying the variables that are significantly associated with absenteeism and a subsequent relative importance analysis to identify the most important variables.

Is there any type of inferencial statistics or other analysis i can do to identify what drives absenteeism?"
How can I plot to see the trends of two completely unrelated data sets?,4,2,False,False,False,statistics,1502231990,True,"I took statistics classes 8 years ago so I don't remember most of what I've learnt. However, I have two completely different data sets. 

1. Price of milk in the local super market in a zip code 
2. Housing prices in a particular zip code

I want to see the trend of the two over a period of 10 years. (Using numbers) Price of the milk is typically between $0.50 to $5 tops. The housing prices can go up to millions. What's the best way to visual all this data on the same chart? 

Reason I am not just straight up plotting as is, is because the milk prices just lay low close to 0 and I cannot clearly see the trends. I want to visualize both of them on the same chart and not have to compare two different charts. I was considering adding a constant such as 100,000 to all the milk prices to somewhat make it look equal on the graph"
Casino with 50% odds...REALLY!?,0,1,False,False,False,statistics,1502233242,True,[removed]
"Use of Knightian Uncertainty in statistics, or an equivalent term?",1,1,False,False,False,statistics,1502240049,True,"I'm working with general statistics, and am looking for something that gets across the same idea of Knightian Uncertainty ('risk in economics that cannot be quantified/calculated'), but not explicitly relating to risk or economics. I've seen Knightian Uncertainty used on occasion in a more general sense, but I'm not sure if that's acceptable enough to pass off normally.

Is it fair to use the term to refer to general statistics? Or is there some other term I can use that means the same sort of thing in a more general sense?"
"There's something very basic with neural nets and statistics that I am not understanding...""curse of dimensionality""",0,1,False,False,False,statistics,1502249190,True,[removed]
Need advice on a SPSS program,2,1,False,False,False,statistics,1502254050,True,[deleted]
Stat to compare two Reference Ranges,4,7,False,False,False,statistics,1502269867,True,"Hi fellow statisticians !
I'm looking for a way to compare 2 references ranges calculated from a sample (n=56) using 2,5 and 97,5 percentiles.

I used a U-test to compare means but I need to compare the whole range, and be able to state ""The ranges are significantly different/even"", not just the mean. 
I though about comparing (U-test) limits (upper and lower) or even the median but I really don't know if it is relevant.

does any-one know a workflow for this kind of purpose ?

Thanks !"
"How does the U.S. statistics system work, explained by the country’s Chief Statistician",3,8,False,False,False,statistics,1502276302,False,
"Is there another totally different meaning of ""p-value""?",2,5,False,False,False,statistics,1502287143,True,"I'm reading up on the development of an assessment test, and they keep referring to a measure of difficulty as the ""p-value"". This is pretty confusing to me. To be clear, it doesn't seem that they are doing what would be pretty stupid and judging how difficult a question is by how low the p-value is when comparing performance to a null hypothesis of chance expectation, but are simply using it as a proportion of testers that got it correct. I.e. 58% of testers got this right, so it's difficulty is indicated by a ""p-value"" of 0.58.

The rest of the write up demonstrates reasonably high-level knowledge of statistics, so it would be weird if this was just a nomenclature mistake. But I've never seen this before and can't find it anywhere else. "
Baruch College MS in Statistics,3,14,False,False,False,statistics,1502288165,True,Does anybody have any experience about the MS in Statistics program at Baruch that they could share or if it's even worth applying to? I'm looking at several different Master Programs in Stats in New York right now and I wanted to get a pulse of this particular College.
"Interested in relearning statistics, looking for useful resources.",1,1,False,False,False,statistics,1502293776,True,"Hello all. I took intro to stats previously, but the professor was a first-time teacher from another country, (different ideas of what is appropriate etc., etc. on top of department pushing him = ) the workload was way too high and I didn't learn the concepts as well as I should have. I would like to relearn stats. I, of course, have my textbook........somewhere. I will try to find that. In the meantime, I am wondering if you fine people might be able to help me out. Are there any resources that you all would recommend for accomplishing the following:
1) covering the material covered in intro stats once again
2) expanding from that base (I have also learned various other statistical methods in other math and science courses in college, but I want to bring everything together and learn it well)

I find statistics fascinating and now want to take the time to go back and learn it well. This area is a glaring weak when it comes to high-level mathematics. I genuinely wish to rectify this.

(online courses are also welcome, by the way)"
What is the purpose of using a spline?,5,4,False,False,False,statistics,1502301404,True,"I'm helping on a project and I'm having trouble understanding a spline my supervisor is presenting. In one graph we looked at hazard ratios across 9 groups of continuous variable. In another graph we look at a restricted cubic spline that shows hazard ratios across the same variable, but I guess it allows the continuous variable to not have to be divided into 9 groups?

I'm confused because it seems like we're presenting something needlessly complicated. When the variable is divided into 9 groups we can see an increasing trend in HRs as the group number increases. What information is the spline adding? We only have 4 knots, so it seems like it's less informative than the simpler graph that has 9 groups. "
[R-related] How do you know from a summary(model) that your model doesn't fit your data?,5,1,False,False,False,statistics,1502308223,True,"[This blog here](http://data.princeton.edu/wws509/r/overdispersion.html) goes through examples of over-dispersion. I'm a biologist so my stats knowledge could use some polishing; I don't quite follow their conclusion in the first example (A Poisson Model) where they write

> mp <- glm(art~fem+mar+kid5+phd+ment, family=poisson, data=ab)

> summary(mp)

They show the output and conclude ""We can see that the model obviously doesn't fit the data"".

Which part about that summary() output overtly indicates that the model doesn't fit the data?"
"Statistically speaking, which is the right way to approach a survey?",2,1,False,False,False,statistics,1502311759,True,"I don't want anyone getting into the actual argument itself, but I'm second guessing myself because both of them are quite insistent.

So pretty much there's two users arguing about how to answer a survey, one is saying that when answering a survey, you should be emphatic and consider how other people in general would answer, the second guy is arguing that you should only use yourself as a reference when answering a survey.

Right now I agree with the second guy.

Statistically speaking, which is the correct method? Should you be emphatic with your answer or just answer what you think?

https://www.reddit.com/r/Android/comments/6s5dpj/live_pictures_of_galaxy_note_8_leak/dle9vz5/?context=10000"
Can someone with Gage R&R experience help me understand what end values I should be producing to verify the repeatability of tools?,1,1,False,False,False,statistics,1502316734,True,"I want to run 10 parts, 3 times each, 1 operator(this is due to the operator being automated).



DoF = 20, IJ(K-1) where K is trials and I and J are the number of operators and parts. 



Using the (subgroup average-overall average)^2 divided by 20 (the DoF), I can get my s^2 value. Then I can solve for the S value. 


I don't understand why I care about this value. I also don't know what else to do from here. I am trying to follow online instructions but cannot very easily. Can anyone offer insight or help?


"
The math of chance / streaks. [Bernoulli process?[,0,1,False,False,False,statistics,1502318600,True,[removed]
"The importance of Bayes statistics, explained with predictive policing",0,3,False,False,False,statistics,1502319222,False,
Is there an R Package that can append squared semi-partial correlations to linear models?,4,1,False,False,False,statistics,1502319754,True,
What do I need to learn to be able to answer these line classification problems?,1,1,False,False,False,statistics,1502324184,True,[removed]
What aspect of statistics did it take an embarrassingly long time to click for you?,36,31,False,False,False,statistics,1502326595,True,"For me it was the difference between testing, estimation and prediction. I was halfway through my 3rd year of grad school before I figured this one out."
"Laptop recommendation, requiring JMP",0,0,False,False,False,statistics,1502337327,True,"Hello, I'm starting graduate school soon and in need of a new laptop. My research would require using JMP. What would you recommend?"
Mutual Information Statistical Significance,0,1,False,False,False,statistics,1502343840,True,"I'm working with Corpus Linguistics using software called Sketch Engine to measure correlation between words withing a text with [MI^3 scores](https://www.sketchengine.co.uk/documentation/statistics-used-in-sketch-engine/). 

I tried at /r/linguistics and did even get a single comment.  

Maybe you can help me out here.  I'm not good at statistics, so this question may seem ignorant.  How can I calculate p-values from the MI score?  If I'm measuring correlation, are p-values even necessary.  The best possible solution is if somebody knows the minimum threshold for statistical significance if MI, rather the me fumbling through the mathematics for it all. "
UMBC MS in statistics.,0,0,False,False,False,statistics,1502359491,True,Does anyone know anything about this program?
"Statistical representation of musical artist's longevity, quick question",0,1,False,False,False,statistics,1502363922,True,"I am attempting to come to some sort of statistical conclusion as to which rapper has the most impressive longevity. Thus far I have my data set up with the following columns:

- Album No.
- Average critical review score
- Album sales (US)
- Chart position (Billboard 200)

I have then been looking at the correlation between the album number and the other 3 columns to determine if the more albums they put out, the less relevant or successful they are.

This doesn't account for how many years they have been rapping though. Jay-Z released 12 albums in a 17 year period, but LL Cool J released 12 albums in a 23 year period. If all else was the same (sales, critical reviews, chart position), how would I use excel or another tool to draw a concrete conclusion about who has the most longevity?

I am defining longevity as popularity over either a period of time or as it relates to how many albums they have released. 

[Here](http://imgur.com/a/SD649) is a portion of my data set.

Thank you! "
A simple description to understand Bayes Theorem,0,12,False,False,False,statistics,1502364103,False,
Latent Growth Curve Modeling,0,1,False,False,False,statistics,1502373689,True,"I am currently planning on examining a piecewise/spline growth model with some self-report data. I have a basic understanding of LGCM, but I would like to read more on how to properly identify the ""knot"" in the data and also how to compare models that do not have similar parameter. 

My hopes are to compare the spline growth model to another model (non-spline) measuring a similar construct, but with a different questionnaire. Therefore, the parameters will be different (one will have 2 slopes for the spline, while the other will only have one). 

Any help would be greatly appreciated!"
Suggestions on building a simple website for a local statistics nonprofit?,0,1,False,False,False,statistics,1502375119,True,[removed]
Thought I’d share a meme that one of my lecturer’s created a few years ago.,14,97,False,False,False,statistics,1502377049,False,
Any CMU students who can ask Shalizi nicely to start updating his blog again?,2,3,False,False,False,statistics,1502377357,True,"A little off topic and the title is in jest, but I really really miss the Shalizi blog posts.  I think he got tenure and probably took sabbatical, but I hope he'll get back to posting again (and not just about statistics).  Anyways, if anyone has the inside scoop, please let him know that his Internet fan base misses him a lot."
Treating subgrouped data as though it were individual/iid,3,3,False,False,False,statistics,1502388252,True,"In statistical process control there is a concept of ""rational subgrouping"", where samples/data is grouped in such a way as to identify changes in mean/variance *between* a subgroup, versus *within* a subgroup.

It is also done to help alleviate issues of non-normality since the means of the subgroups should be normally distributed (depending on N) thanks to central limit theorem.

An example of a rational subgroup in a production facility would be a n=5 samples taken from a single shift, for every shift.

Anyways, I'm working with some data that has clear subgrouping in how it's generated, 5 samples taken on one day by an operator once a month.  But the data is being treated as individual data, meaning they assume it is IID (which it clearly isn't), and the rest of the statistics are being calculated following that logic.

I know this is wrong, and my gut tells me that this will lead to incorrect calculations of sigma, but I'm not certain.   Does anyone have any thoughts on this?"
How is type II error (beta) used in hypothesis testing?,10,3,False,False,False,statistics,1502389130,True,"When I carry out the following hypothesis testing,

H0: population mean = 5

H1: population mean != 5

I have to select alpha (probability of type 1 error) to find the critical value which I can use to reject or fail to reject H0.

How is the probability of type 2 error used? 

Please explain with an example that uses both types of error."
Question about epi stats,0,1,False,False,False,statistics,1502396442,True,[removed]
Ways to keep my stats skills sharp?,3,3,False,False,False,statistics,1502399749,True,"I already have a B.S. in Statistics but the job I currently do involves no statistics.  Planning on entering the job market soon and I'm gonna be looking for jobs in the Stats field.

How can I keep my skills sharp until then? Mainly looking for R programming practice as well as trying not to forget everything I learned about theory.  Are there any books/classes/websites I could use to keep up?"
Question about a calibration curve.,2,2,False,False,False,statistics,1502408967,True,"If I'm making a calibration curve with 3 sets of standards, would I average the individual values from the 3 sets of standards or would I plot each standard on its own and run a curve through that?

For example if I was running a calibration curve for 10, 25, 100 ppm of a sample and I ran 3 sets of a 10 ppm standard, 3 sets of a 25 ppm standard, and 3 sets of a 100 ppm standard, would I plot all 9 points and run a curve through it or would I average each set of 3 standards out and plot a curve through those points?  And statistically why is one better/different than the other?

Thanks!"
Major for a career as a statistican?,5,1,False,False,False,statistics,1502413649,True,"Hello everyone, I'm new to this redit and I need some advice. I am very intrigued by the idea of being a statistican, and I have read that it usually takes a Masters in Statistics. I was wondering if a Bachelors in Finance or Econ and then a Masters in Statistics could prepare me for this career. I have also considered getting a minor in Applied Statistics. I would rather get a business undergrad than applied statistics in order to increase the amount of careers i can pursue in the future. Thank you for any input you can give!"
"Can someone, in plain English, explain to me how to interpret the idea of covariance?",4,1,False,False,False,statistics,1502416810,True,Looking for intuition. 
Is it fair to say that most social programmes don’t work? A statistical analysis.,2,12,False,False,False,statistics,1502420517,False,
MC model for Return on Equity Model for Loans,0,3,False,False,False,statistics,1502423640,True,"I'm working on a project that involves coming up with a model that predicts ROE on a loan.  We want then use this model to predict the ROE on all our current loans and then establish a hurdle by taking a weighted average on the predicted ROE for each loan. ROE = net interest income/ capitalized costs. This model is to combine 3 components:

1) Deposit and Loan information
     This is things like how much in deposits the customer will keep with us, how much is he going to borrow, his current DSCR ratio, credit score, type of loan like is it ag, commercial, etc, for what industry as each industry have different average interest rates, what kind of collateral is it? (Buses and tractors hold less value vs real estate collateral), how much in deposit fees will we make, etc.

2) Operational Costs
      This is the amount of time and costs it takes to put together the loan. Smaller loans take less then larger loans etc.

3) Product Information
       What kinds of products will the customer have. Will he have
automatic loan payments vs manually paying it via check, online banking, etc. Basically how sticky the relationship is with the place.
 
This to me sounds like Monte Carlo simulation would be a start but combining all these factors into this seems complicated. Any ideas of how to go about this would be great. Thanks!"
"Probably a stupid question, but I'd appreciate an answer",0,1,False,False,False,statistics,1502431834,True,[removed]
Minitab software extortion?,9,4,False,False,False,statistics,1502451532,True,"I just got this email (below) from kivuto as I had paid for and downloaded Minitab a few weeks ago - is this extortion? It seems like a new low in squeezing money out of customers? Is this legit or have I been conned?
Can anyone recommend a better alternative to Minitab? 

*Your order includes Kivuto's Basic Access Guarantee, which will expire on blahmonth blahdate, 2017.

After this date, your download and/or key(s) will be archived and as a result this information will no longer be available under 'Your Account' on the WebStore. In the event that you misplace your key or need to re-download your software as a later date, it is recommended that you back-up this information to avoid losing access to your product order information.

To back-up your order, Kivuto has provided some options below.

Recommended Back-up Options for You

    1.    Extended Access Guarantee

To ensure that your download and/or key(s) remain accessible to you, you can extend your coverage to 24 months with the Extended Access Guarantee for $4.95. With this service, Kivuto will back up your download and/or key(s) on their servers, allowing you to access this information at any time under the ""Your Account"" section of the WebStore.

The Extended Access Guarantee service does not extend the duration of your license. If you purchased a time-limited license (i.e.: six or twelve month rental) your license will still expire in that time frame.

To purchase the Extended Access Guarantee for the item(s) in your order, sign in to your account by clicking on the link below.  A one-time fee of $4.95 will guarantee access to all items in your order.

http://estore.onthehub.com/blahblahblahblah=Acblahblah_campaign=Access-Guarantee

    2.    Personal Back-up Copy

If you do not want to take advantage of Kivuto's optional back-up service, you will need to provide your own back-up in the event you need to install it again in the future. Ensure that your downloaded software is backed-up on a DVD or external hard drive and you write down your product key.

Retrieving Your Information after the Access Expires

If you need to re-download the software you purchased and/or view your product key after your Basic or Extended access expires, you can still recover this information by paying a retrieval fee of $11.95.

Order Number: blahblah
Order Details:
    Minitab 17 (Multilanguage) (06-Month Rental)*"
What are the statistical effects of running a one-way within subjects ANOVA with three levels where one of the levels has less trials than the others?,4,3,False,False,False,statistics,1502474529,True,"I might have phrased the question inaccurately, but please bear with me. 

Suppose I was looking for a difference in three levels of a within subjects variable. 
Those three levels happen to be a person's accuracy response to identifying Happy, Sad, and Neutral faces.
Each one is simply their accuracy score on each level of the variable. 
On each level, there were 12 trials that a person had for each emotion to determine their accuracy. 
For this example participant, their accuracy for Happy was 8 , Sad was 2, and 6 for Neutral. 
However, due to a flaw in the experimental design, there were only 10 valid trials for the neutral condition. 
Knowing this, you convert each of the accuracy measures to percents, so that they all can be better measures relative to each other, 
by doing 8/12 , 2/12 , and 6/10.

Is there anything inherently wrong in this procedure to address this mistake in design? Are their consequences to the results?"
How do I go from beginner to advanced statistics,16,28,False,False,False,statistics,1502478299,True,"My experience in statistics is severely lacking as I have never taken a formal statistics class in math. I did take a ""statistics for pharmaceutical design"" course but the professor was towards the end of his career/health and stopped teaching in the middle of the semester.

I am currently trying to read Statistics for Experimenters by Box based on amazon reviews but I don't understand the logic or the details of the text (I guess my mind doesn't work the same way as many of the reviewers). Are there any recommendations in reading to go from beginner to advanced? I am a MATLAB user and would like to implement statistical analysis to my experiments as a chemist."
What would I put in a table reporting a regression model built using LASSO?,8,3,False,False,False,statistics,1502480549,True,"I'm used to regression models having tables including: beta weights and S.E.s, squared semi-partial correlations and an overall R2 value for the model.

When I build I model using LASSO in R I only get coefficients for the predictors. Is it possible to produce the equivalent of the other things I mentioned? If not, are there other things to report?"
Confused: Setting contrasts in a model vs. multiple comparisons,2,2,False,False,False,statistics,1502481186,True,"We're trying to analyze data that has one within subject variable, one between subject variable, and a dependent variable. My colleague and I came to entirely different conclusions on how to approach the model, and neither of us know enough about statistics to determine the best way forward.

**My idea:** Model the two predictors (and the interaction between the two), then test the specific hypotheses we have with a multiple comparison procedure. This is what I remember being taught to do if planned comparisons weren't made. 

**My colleague's idea:** Model the predictors, interactions, and set contrasts to test our hypotheses as variables in the model. This is something that my colleague's textbook suggested. 

Is either one of these approaches more valid than the other? The second one seems bizarre to me, but I don't know enough to criticize it."
Quick Question: Vocabulary,0,6,False,False,False,statistics,1502481446,True,[deleted]
Weak math background looking to get into Stats,11,2,False,False,False,statistics,1502495649,True,"Hey guys, as someone coming from a non mathematical undergrad background, I was hoping you could offer some friendly advice about getting into an MS in Stats program. I've been looking over past threads on here and have discovered lots of valuable information, but I thought I would post my own situation to add more to the conversation. Basically, I graduated a year ago with a BA in international relations, however my last quarter I discovered that I really enjoyed and excelled in mathematics, and had a great interest in Statistics in particular. My math background was severely lacking so I'm currently taking math courses at my local community college. I should be finished with calc 3 by next summer, and I plan to take the linear algebra and differential equations courses they offer within that timeframe as well. My question is will that be enough for admission or are there other classes that I should absolutely have on my transcript before I even consider applying? Thanks in advance.  "
Opinions on UF Biostatistics M.S. Online Program,3,8,False,False,False,statistics,1502513368,True,"http://biostat.ufl.edu/education/msonline/

I'm in the process of researching Biostatistics graduate programs and ran across this in my search.

My goal is to pursue a PhD, however I'm curious to see how anyone here feel about the idea of online M.S. coursework in Biostats. "
Economicshelpdesk.com Offers Best Statistics Homework Help Service,0,1,False,False,False,statistics,1502522214,False,
How exactly do you obtain the pmf from the cdf for discrete random variables?,0,1,False,False,False,statistics,1502524262,True,[deleted]
14 Experiential Marketing Stats to Change Your Life Forever,0,1,False,False,False,statistics,1502524485,False,
UK's Custom Dissertation Writing Service : Coursework Writing Service,0,1,False,False,False,statistics,1502535247,True,[removed]
Possible newbie question - desperately need some guidance.,7,0,False,False,False,statistics,1502543455,True,"I'm currently doing my dissertation on the analysis of the effect of motivation and job satisfaction on employee performance. I have created a questionnaire with 24 questions (4 of them demographics and the rest on 5 point likert scale) 

I am having so much trouble trying to show a correlation between these factors - I've even doctored a lot of the results but pearson's and spearmans show abysmal levels of correlation.

Here's a snapshot of my data and shitty correlation - https://imgur.com/a/3PnLJ

I would really really appreciate some help. I just need to know a way to get a proper correlation between my figures to help my hypotheses.

Thanks in advance!

Edit - Apologies but when I mean doctored, I altered my results significantly to show a correlation to see if it will work but it still doesn't! I have raw data that I have gathered which I will be using but I need to know how which method I should employ.."
Bayes Rule and Expectations,5,3,False,False,False,statistics,1502550302,True,"Hi guys,
I study Economics but we did not have that much statistics in our program (I'm from East Europe, lol)

I have a little problem in understanding the Bayes Rule. I watched a few vids and read in a few books, but I can not find the solution or I am just not intelligent enough :/

Here is my problem:
There are two Persons. 
Person 1 has to choose between Investing with much money or not so much money and the investment is good with probability (1+p)/2 and wrong with (1-p)/2.
p is uniform prior between [0,p'], this is commonly known by both persons.
the expected value is E=(2+p')/4p' at least I think so.

Person 2 tries to make an expectation of the information quality p that Person 1 had.
The text says as follows:
p"" (her impression on p) for choosing Investment with less money and Investment being good with: p''=[p'(3+2p')]/[3(2+p')]

p""(2) Investing with less money and Investment being bad is
p""(2)=[p'(3-2p')]/[3(2-p')]

To understand the excercise I need to understand how these p"" are calculated. I guess it is the Rule of Bayes, but I cannot transfer it on my problem. 
I guess it is not that hard, so I hope some of you guys could help me.
"
"""Enough to be dangerous"" intro to regression without strong math/stats background?",18,19,False,False,False,statistics,1502554576,True,"I have a friend who's good in Excel but otherwise has no strong math/stats background (certainly no linear algebra). He asked me for a recommendation for starting out with regression without coursework, and I was dumbfounded that I couldn't give him one -- I've only ever been involved in learning and teaching stats/econometrics with the requisite theory.

So, does anyone have a recommendation for software (preferably free, but Python/R would be too steep of a learning curve) and a book or online course to help him get started? 

I have an intuitive sense that someone could learn regression and experiment with it first... once they see the power of it, it might be motivation to pursue future more rigorous work. If anyone has experience teaching or self-teaching with this approach, I'd be interested to hear about your experience."
[Interview help] Data Scientist interview task that has very few variables... I'm unsure how to approach it in a way that incorporates any type of sophisticated modeling. Any ideas or help?,9,0,False,False,False,statistics,1502564438,True,"As in the title, I've been sent a data set to perform a task but the data only contains 6 variables which I'm not sure how to necessarily tackle in a sophisticated way, due to the lack of information. FYI, I know how to code in R and have been teaching myself Python.


**[Background to the data]**    
The company performed a test (taking place in weeks 53, 54, 55) comparing marketing campaign results delivered via two different media (TV and VOD). So ideally, I will make use of a method that will allow me to test which is most effective. I know that $150,000 was spent on both TV/VOD.
  

**[The data]**  
The data provided covers 64 weeks and is for 3 different UK markets (Control, VOD, TV) with each market having 2 corresponding variables (Traffic and Revenue).    
  
  
**[Task at hand]**  
Describe and execute an analysis plan that enables me to make recommendations on marketing budget allocation. 

  
*so...* other than general descriptive statistical methods, visualisation and a YoY comparison of performance. I'm not sure what opportunity I have to implement some modelling."
Help with Bayesian coding: How to compute Inverse-Gamma density?,0,1,False,False,False,statistics,1502565831,True,[removed]
Learn about standardizing the data.,0,1,False,False,False,statistics,1502580911,False,
Two questions: Is sampling without replacement random and does sample size influence expected value?,5,3,False,False,False,statistics,1502586874,True,"For a sample to be considered random, each item in the population has to have an equal chance of being chosen. However, when you sample without replacement, the probability of any one item being sampled changes as the sampling frame decreases. In this case, is sampling without replacement random or does the viability of a random sample depend more on other factors? 

My second question relates to sample size. In my workplace (work requires no formal use of statistical techniques other than reporting mean values), there is a survey that we ask clients to answer (not really random but it's the only option). One question asks clients to rate their health on a scale of 1-5. So far there have been 500+ responses. I insisted that we no longer need this question on the survey since the average from the survey will most likely remain unchanged. Excluding the effect of sample size on power (since we do not report variance or provide CIs), was I correct in saying that there is essentially a ""saturation point"" with survey data collection?

I would greatly appreciate any answers to these questions!"
Need help - Debunking some bad psychology with statistics,8,27,False,False,False,statistics,1502601197,True,"I want to solve a statistics problem, but the mathematics is a bit beyond my training. I have tried to find the topic that would help me figure it out (and I learned some very interesting mathematics along the way), but I haven’t found a satisfactory solution for me. I’m hoping that you can help:

I want to evaluate the claims of advocates for a relatively new personality inventory. The university where I teach is considering using this inventory for our incoming students.

Spoiler alert (without saying what it is):

a.	The rationale for the test is ridiculous. Because I don’t want to expose myself to any legal trouble, you’re going to have to trust me on this one.

b.	It fails every test of a scientific evaluation. At the very least, it’s neither testable nor falsifiable.

c.	It cannot be validated.

d.	It gives the subject a “result”, but there is no way to confirm that it has any evidential value.

e.	Even if it had evidential value, the subjects are 17-to-18-year-old university students, who will be very different people by the time we graduate them, or we wouldn’t be doing our jobs as professors.

f.	It has no predictive value, at least that anyone dares to claim.

g.	These are the best things that I can say about it.

Okay, now that you know that I’m completely unbiased and objective…

One of the characteristics of flim-flam artists, snake oil salesmen, and other false prophets of scientific truth is that they make over-the-top, fantastic, can’t-possibly-be-right claims about the things that they want us to accept.

That, and they want our money. Lots of it, as a matter of record.

These guys are clearly trying for the latter. I’m trying to expose the former as false, with a little more rigor than name-calling.

The inventory consists of a bit more than 400 pairs of statements about personality, of which a randomly selected subset of approximately 175 pairs is presented to the subject. The subject is asked to indicate the level of agreement with one or the other on an integer scale of 1 to 5, where “1” is strongest agreement with the first statement, and “5” is strongest agreement with the second.

After the subject makes their choices, a proprietary algorithm generates a result, ranking the subject on 34 personality dimensions, such as “Caring”, “Empathetic”, “Quixotic”, etc. If you pay the standard, nominal fee for taking the test, you get a report of the 5 dimensions where the subject scored the highest, complete with detailed, horoscope-like descriptions of these five personality dimensions. For an additional fee, the subject can get the report in the same format, but showing the top ten dimensions. And, wouldn’t you know it, for even more money, the subject can get the complete list of all 34 dimensions, listed from “most like you”, to “least like you”.

I was asked to sit through a seminar, presented by representatives of the organization that created this tool. In it, the claim is made that retesting of this inventory is discouraged, mostly because their “studies” have shown that, while there is some variability in the top five personality dimensions, the second top five is, on average, contained in the first top ten, with a hit rate of 98%.

I should mention here, that I did some research into how the test is administered, and I can report that if you take the inventory a second time, the subset of about 175 pairs is reselected from the larger set by “resampling with replacement”. In other words, it’s all but a certainty that there will be items from the second test that appeared in the first.

At any rate, when I heard this 98% figure, I nearly jumped out of my chair, to expose this lie. And because I’m asking for a mathematics favor from you, let me tell you a little something about psychology:

If that 98% figure was real, and reproducible across ANY demographic and/or time, it would be the biggest sensation in personality assessment. Ever. It would be hard to invent the superlatives that would be necessary to describe it. Private-sector organizations would pay insane amounts of money to license it. It would revolutionize all kinds of disciplines such as vocational assessment and project management, which are languishing under the intellectual burden of low reproducibility. Even the Minnesota Multiphasic Personality Inventory (many decades old, and arguably, the gold standard in this area) measures about a dozen personality dimensions, with a test-retest reproducibility of about 0.50 to about 0.80. I’ve seen test-retest reliability for the Myers-Briggs Type Indicator as high as about 0.80, but that’s all.

This claim was one of many that are outrageous, and I seriously considered betting real money on this. But then I considered that I didn’t know the actual odds of my success. So I held my tongue until there was a break in the presentation, made my argument to students who also figured this out by the social psychology argument, and tried to calculate the odds by myself. I couldn’t do it with any confidence (pun intended).

Here are my questions:

1.	What is the likelihood that, in a sample of “N” subjects who take this inventory exam twice, that “X” items in the top five of the 34 dimensions measured by the second test (“X” is an integer, such that 5 >= X >= 0) are found among the top ten dimensions of the first test, if the reproducibility of the inventory is some value “Y” (1 >= Y >= 0)?
2.	Am I on the right track to suppose that this is a Bayesian problem, with the claimed hit rate of 98% being the prior? As a recovering frequentist, if it is, can you advise me as to the technique for testing this prior?
3.	If there is a straightforward statistical test of the 98%-hit-rate claim, is there a way to calculate the adjustment in the payout odds if I reduce the hit rate requirement? In other words, if I was going to wager even money on a test of the 98% claim, what reduced hit rate can I offer, to justify, say, a Z:1 payout in my favor (Z > 1), given the conditions of the inventory as described above?
4.	If I am totally off-base here, what is the right way to think about this problem, and test the data from repeated measures of this inventory tool?

On a side note, I’d like to mention that, in the course of trying to study enough probability to solve this on my own, I came across the topic of rencontres numbers, which I must confess, I still don’t understand as well as I should, but it’s so darn interesting, and it has the most interesting application to concepts that are closely related to the one that I’m struggling with now.

But I digress.

Can anyone help me with the probability mathematics of this inventory problem, or at least identify the topics that I should be learning, so that I can figure this out?

Many thanks in advance for your time and consideration.
"
I don't quite get this simple triple integral for a joint pdf,0,1,False,False,False,statistics,1502601896,True,[removed]
Any suggestions for research articles using Bayesian statistics?,3,9,False,False,False,statistics,1502604105,True,"Preferably in medicine or social sciences. 

I have read a few articles and think I generally understand the concepts, but need to see it in action, used to test hypothesis in a published format. Probably trying to stay away from neural networks and machine learning for now. Any suggestions?"
Calculating consistency,3,1,False,False,False,statistics,1502615840,True,"Currently I'm facing a problem as an analyst for my e-sports (gaming) team. Within the game there's a certain rating which is calculated which determines a players performance within the game. This value is usually somewhere between 0 and 5, where a value of 1 and higher means you've had a positive contribution to the game. 

Basically I want to calculate how consistent my players are. This means that I want to get a rate which determines how often and by how much they are deviating (both positive and negative combined) from their average over a predetermined time span (usually monthy). 

I would like to use this value to determine if changing habits would increase consistency (I'm assuming that our team is more consistent during weekdays amd I'd like to confirm that statistically).

If you guys know what type of value I could use for this it would be greatly appreciated. "
Recommend a stats book?,5,25,False,False,False,statistics,1502626813,True,"I'm doing an engineering masters in machine learning. I'm finding some of the derivations in pattern recognition and machine learning - bishop, quite hard. For example I've been looking at [derivations for conditional multivariate gaussian](https://stats.stackexchange.com/questions/30588/deriving-the-conditional-distributions-of-a-multivariate-normal-distribution) and i would have never worked out the top rated answer in a million years, there are similar derivations to this in the book. Id also like to learn about stuff like kullback-leibler divergence, entropy, cross entropy. Can anyone recommend some books?"
What is the name of this recursive problem?,0,1,False,False,False,statistics,1502629772,True,[deleted]
Mediation analysis help!,2,2,False,False,False,statistics,1502636418,True,"Hi folks, apologies if I'm asking this in the wrong place.

So I'm trying to mediation analysis using the PROCESS macro on a small dataset (n=25).

When I ran a correlation matrix my mediator (SC) and dependent variable (A) were significantly correlated (r=-.46, p<.05) but when I ran the mediation analysis it looks like there is no effect (b=.15, p=.59). Does this make sense? Perhaps I am interpreting the output incorrectly..This is my first time using mediation analysis. Any help much appreciated!
"
"[Question] Comparing usage by gender of Bing, Google, and Yahoo. From this data, can we make the claim that: ""Women use Bing/Yahoo more than Google""? Can someone explain the comparison to population in another way, I'm a bit confused by the concept.",2,1,False,False,False,statistics,1502666532,False,
Rookie regression question/tips (recoding variable),0,1,False,False,False,statistics,1502702446,True,[removed]
How do visualize Mann-Whitney results?,6,3,False,False,False,statistics,1502703473,True,"My dataset is in this way: I've got 40 people who underwent two activities (Call it activity A and activity B). After each activity, we recorded their scores. 


I wanna see if the scores after each activity differ and if so, which one is better. I used the Mann Whitney U Test to do this since my data isn't normally distributed.


From my results, I can see that activity A is better (scores are higher) but I need to visualize this in a proper way


I was thinking of just doing a box plot to compare the scores for A & B, or I would plot two 'overlapping' histograms (one for A scores and one for B scores) to compare them. (Obviously the one that shifts more to the right would be better.


I also saw something on using a ROC curve to visualize it cos the area under the curve is related to the Mann Whitney U statistic but I'm having trouble trying to understand how to do this in SPSS
"
Mediation Analysis Using INDIRECT in SPSS,3,3,False,False,False,statistics,1502705591,True,"Hi all! I'm struggling to analyze some outputs from the Hayes' Indirect add-on in SPSS. Specifically, neither A or B paths is significant, and the bootstrapping results also suggest no significance. However, the c path is slightly more significant than c', and the model summary is also significant. WHAT DOES THIS ALL MEAN? Google is useless for answering this query, so thought I'd see if anyone here might have an idea. Thanks very, very much in advance! "
"Descriptive Statistics Key Terms, Explained",0,4,False,False,False,statistics,1502726037,False,
Propensity Scores: A Primer,2,8,False,False,False,statistics,1502726625,False,
Calculate statistical significance between data for simple experiment,10,3,False,False,False,statistics,1502741254,True,"I have two data sets. Set 1 passed 100 times and failed 7 times. Set 2 passed 100 times and failed 11 times. 

Is the difference statistically significant?  What's the equation so that in the future (with different results) I can figure this out for myself?

Explain like: My stats knowledge is not great, algebra is very good, excel is very good (if there's an easy way to do it in excel, that'd be awesome)

Thanks!!"
Are modern mainstream musicians canceling more concerts than mainstream musicians 40 years ago?,2,0,False,False,False,statistics,1502754078,True,[deleted]
How to create custom Sankey diagrams using R,1,11,False,False,False,statistics,1502760681,False,
"Is there a high demand in the public sector for people with Master's degrees in Statistics? If so, in what kind of job positions?",9,8,False,False,False,statistics,1502778235,True," Are there more public sector jobs in big cities, especially data analysis/stats? Which type of stats skills are most highly coveted in the public sector?"
New job that involves a lot of stats. Advice?,6,6,False,False,False,statistics,1502787941,True,"My background is in intelligence analysis and I have literally zero statistical training. My old job had dedicated data scientists who could run numbers for us while we did the research and collation work. 

I just got a new job which has no data scientists, despite being an almost completely statistics based analysis unit. Our day to day tasking involves pulling out huge chunks of traffic data and analyzing it. This basically amounts to pivot tables in excel, where we turn on filters until we find a ""trend."" These ""trends"" are often incredibly minor increases or decreases in the numbers, where a few months or years of consecutive increases is labelled a ""trend"". We then produce a report, explaining how things like ""40-55 year old female drivers are getting involved in more collisions on rural roads (+2.1% from last year)."" Sometimes these results come from minuscule pools of data. 

I know basically nothing about mathematics or statistics. But I am an experienced analyst. As far as I'm concerned, this is the epitome of data mining. The overall trend might be that collisions are decreasing. And that rural roads are getting safer. But we go through the dataset and pick the *only* sub-group that is getting worse (or better, occasionally) and report about it like it's a major issue. Subsequently, resources and money is spent chasing these random statistical fluctuations. We have zero criteria for what amount of increase or decrease is reportable. We have no criteria for what kind of dataset is too small for a meaningful result to be drawn. 

It seems like nobody I have talked to even understand the concept of p-hacking. I mean, realistically not even **I** understand it in anything more than the principle. I don't think any of them understand that data sets have elements of statistical fluctuations, or that they incorporate chance. I don't think anybody understands what a bell curve is. My section is run by a lawyer. 

Does this meet your criteria for data-mining? Am I wrong about this being a bad practice? Is there anything I can do to help change the culture of my section? My first few attempts to talk about sample size with my lawyer boss were failures. So either I need to get better at explaining or learn enough about this stuff to get this stuff fixed. In this case, is there any really basic statistics podcasts or youtube videos I can get into that might help with my job? "
Tips on staying focused and motivated studying part time statistics whilst working full time?,2,2,False,False,False,statistics,1502788960,True,"I recently began a graduate certificate in applied statistics online. I enjoy statistics in general, and I enjoyed doing statistics in my biology undergrad. But I'm struggling to stay in focus with study content as I'm working full time. To those who've studied online courses like this one, what tips do you have to stay focused and motivated? 

I enjoy doing statistics in problems I'm working on, but studying statistics isn't exactly the most exciting thing ever! Reading a stats textbook and doing stats exercises isn't exactly as exciting as reading about coral reef conservation :P Help! "
Understanding probability density functions,4,2,False,False,False,statistics,1502794383,True,So I'm an economics undergrad studying a statistics module for a year now but I just haven't been able to get my head around pdf's all this time despite spending a considerable length of time on them. All definitions given online just don't stick in my head. Can someone explain in the *simplest* of terms what they are and how they work? Thanks in advance :)
Comparison between one element and a group,4,2,False,False,False,statistics,1502796281,True,"Hi. I have a population a, b, c, d and e. If I want to compare some metric of element {a} against the same metric of the whole group, to benchmark it and provide a relative position. Should I remove the data of element {a} from the group? Should I compare {a} vs {a, b, c, d, e} or {a} vs {b, c, d, e}?"
Support Vector Machines tutorial from examples,1,19,False,False,False,statistics,1502802605,False,
How to predict the future cash flows of a communal family,3,1,False,False,False,statistics,1502802901,True,"Dear Redditors,


I have an interesting problem I’d like to solve and I’d hope that you guys could help me out with it.

 

Let’s assume I live in a communal family that uses 1 bank account for everyone, and I am the designated treasurer. It’s my job to hold enough cash in the bank account so that everyone can pay their bills. I also don’t want to keep too much money in the checking account because of the opportunity costs (I could have invested in on the stock market instead).

 

The problem is that there are many variables that I can’t predict; various non-fixed income streams and a various amount of outflows. Some of the outflows are rather stable (e.g. groceries, electricity), but some are volatile (e.g. purchase of a new car, emergencies).

 

Is there any statistical approach that I can use to better predict the future cash flows so that I know what the optimal amount of cash is that I should keep as a buffer?

 "
Understanding Story Proof of this Binomial coefficient equation,4,6,False,False,False,statistics,1502808732,True,"I'm learning the foundations of probability by going through the Stats 110 course form Harvard. One of the first practice questions is this: 

http://imgur.com/2befz6z


The answer given is: 

http://imgur.com/XHcg2er

I don't really understand the left side. He says there there is (n, k-1) ways to have a group size k to have the president. Its k-1 because they've already chosen the president. I get that part. But then he chooses another group of size k which doesn't include the president. This part I don't understand, I'm really sure how its relevant at all. Surely if he wanted to choose a group of 3 which didn't have the president, you would sample out of a group of n-1 because you've taken the president away? 

"
Career opportunities,8,1,False,False,False,statistics,1502810225,True,"Hi guys, long time follower of this sub, currently finishing a bioinformatics and computational biology masters with my thesis focusing on using machine learning to create new medical diagnostics. I have taken modules in web dev, sql, Python, data  mining,  statistics and a few in genetic sequencing. I'd really like to go down a more statistics / data mining route when I'm finished but all job postings I see are looking for years of experience. Just wondering if you guys could give me a realistic opinion on weather I'll be able to get a job easily when I'm done or should I hedge my bets and take a pHD which was offered to me?"
"How to interpret a case where after adding a random intercept, predictors are no longer significant?",1,6,False,False,False,statistics,1502817068,True,"I find it more straightforward to interpret a case where adding a random *slope* results in a predictor no longer being significant, as what we thought was a genuine effect slope is explainable through participant (say) variability in the slope. But I'm having more trouble interpreting a case where adding a random *intercept* does the same?"
GSCV for degrees of freedom for a natural spline,1,1,False,False,False,statistics,1502819094,True,"I'm running through ISLR to bone up on some R coding.  In particular, I am trying to fit a natural spline to some data.  I've done this successfully, and would now like to do a gridsearchCV for the best degrees of freedom.  

Here is some code I have written
    
    library(ISLR)
    library(magrittr)
    library(caret)
    library(splines)
    attach(Wage)
    library(boot)
    
    age.grid = seq(range(age)[1],range(age)[2]) #Set up points to be predicted
    
    lm.fit = glm(wage~ns(age,df = 6), data = Wage) #fit model
    
    
    pred = predict(fit, newdata = list(age = age.grid), interval = 'confidence') %>% as.data.frame() #predict new
    
    
    #Plotting
    plot(age,wage,col = 'gray')
    lines(age.grid,pred$fit, lwd = 2)
    lines(age.grid,pred$upr, lwd = 2,lty = 'dashed')
    lines(age.grid,pred$lwr, lwd = 2,lty = 'dashed')
    
    detach(Wage)
    
    
 Can someone recommend a good function or library to do the Gridsearch CV with?  I know caret has some functions for CV; will caret work with natural splines?"
Correlations between explanatory variables versus correlations between estimated coefficients.,4,11,False,False,False,statistics,1502820628,True,"I do a lot of work with linear models (in this case, kriging, but that isn't important here), but a recent discussion demonstrated that I don't really get the practical difference between calculating correlations between explanatory variables (ie, the raw sampled data) and correlations between the estimated coefficients of the linear model.

I was always taught to investigate correlations between the explanatory variables prior to fitting a model.  Maybe scatterplots, maybe 1v1 regressions, but you were supposed to do this up-front.  Once you had established that the explanatory variables were not highly correlated with each other, then you fit the model.  Because the explanatory variables are uncorrelated, the coefficients can be interpreted unconditionally, and you never even look at the correlation matrix between the estimated coefficients.  

However, I'm now hearing that it is very important to also look at the covariance matrix of the estimated coefficients.  I guess my question is just why you need to do this.  If you've already established that the explanatory variables are mutually uncorrelated, what useful information could you ascertain from the covariance matrix of the estimated coefficients (aside from the variances along the diagonal, which are obviously important)?

In my case, we're estimating coefficients (and simultaneously kriging parameters) with maximum likelihood, if that makes a difference."
"Python Alternative to Stata's ""vce(cluster)""",1,0,False,False,False,statistics,1502820864,True,"Do any of you know if there is a way to replicate this functionality in python?



> vce(cluster clustvar) specifies that the standard errors allow for intragroup correlation, relaxing the usual requirement that the observations be independent. That is to say, the observations are independent across groups (clusters) but not necessarily within groups. clustvar specifies to which group each observation belongs, for example, vce(cluster personid) in data with repeated observations on individuals. vce(cluster clustvar) affects the standard errors and variance– covariance matrix of the estimators but not the estimated coefficients; see [U] 20.21 Obtaining robust variance estimates.

Found here: http://www.stata.com/manuals13/xtvce_options.pdf"
James Damore Google Memo and Media Criticisms,1,0,False,False,False,statistics,1502827358,True,"Hello all, I have been reading some of the media criticisms of the Memo released by Google employee James Damore and common criticism levied by several outlets is that he is using statistics which he claims are representative of the population at large and applying them to a subset of the population (namely men and women in tech) to support the hypothesis that differences in men and women in tech stem, in part, from biological factors. I am not very well informed in the field of statistics, but judging by Damore's educational background, he seems like a pretty smart guy who I would assume understands statistics. My question is, is that assumption wrong? Is Damore either intentionally or unintentionally misusing statistics to support his claims, or otherwise engaging in some sort of intellectual dishonesty? Regardless of the politics surrounding the issues, I just want to get to mathematical validity of his claims. Specifically, if I were to choose a random sampling of individuals from a specific industry (tech or otherwise) and conduct the studies he cites, should I expect to get the same result, at least up to finding the mean differences he outlines in his memo. I am sure that sampling IQ for instance among software engineers would yield a higher mean IQ than the population, but would there still be a proportionally similar gap in mean IQ between male and female software engineers?"
How useful is a class in Multivariate Analysis in industry?,0,1,False,False,False,statistics,1502835583,True,[removed]
need high N to run SEM for my Masters thesis survey,0,1,False,False,False,statistics,1502840486,True,[removed]
Does binom.test use a correction?,4,3,False,False,False,statistics,1502841733,True,[deleted]
Nice Generalization of the K-NN Clustering Algorithm - Also Useful for Data Reduction,0,1,False,False,False,statistics,1502843697,False,
Question about exploratory data analysis,5,2,False,False,False,statistics,1502853990,True,"So I am doing a physics PhD and have been given a data set and asked to determine if some of the variables might be related in some way. Each data point has a number of different observations associated with it and we are particularly interested to know if there is some dependence on one of the variables, let's say X. However, we strongly suspect that the dependence will be different for different values of another of the variables, call it Y.

So, following my advisor's suggestion, I have split the data into several samples which are separated into regular intervals of Y. Now for each of these subsamples, I have investigated the dependence of the remaining variables of interest (I'll call these Z) on the variable X. I did this with a simple linear model fit and estimate of the correlation. I figured this would tell me if there were any significant trends in the Z variables that could be explained by changes in X for each subsample.

My advisor was unhappy with this and wants me instead to make bins in X for each sample and find the mean Z for each bin in X, along with the error. They claim this is because the linear fits could be affected by outliers in Z or by the values for extremes in X. However, to me it seems that any trend in the binned version of the analysis would be subject to similar issues. (There is generally a lot of variance in the Z values so I think it is unlikely that a relation any more complex than linear could be determined.) In addition, my advisor also wants me to remove the local mean for the Z values in each sample, substracting the mean Z for a given bin in Y. This is so that any trend in Z which we see along X will not actually be due to variation depending on Y.

This sort of makes sense to me but I get the feeling that this is not the correct way to go about doing something like this. I was wondering if anyone could prove to me why it should be done this way and not another (I have trouble understanding when my advisor makes statistical arguments) or whether this should actually be done a different way. Of course a full generative model that can explain all the variations between each variable involved would make more sense to me but maybe this is okay for just exploratory analysis. I'm just worried that if it is the wrong method, we would miss something or see something that isn't there."
Determining likelihood of a given measurement - Help Please,0,1,False,False,False,statistics,1502859194,True,[removed]
Is anyone with a statistics degree (masters+) having trouble finding work?,12,19,False,False,False,statistics,1502861030,True,"Sorry if this is too personal, I could do an anonymous poll if it is.

Just wondering if anyone is actually struggling to find work given that they have a masters or higher and are proficient coders"
How To Deal With The Shortage Of Knowledge Using Statistics Tutor Help,0,1,False,False,False,statistics,1502863624,False,
"How would I describe a correlation relationship, as i can't say causal and don't really want to say coincidence.",8,1,False,False,False,statistics,1502868709,True,"I determined a correlation coefficient to be 0.62, which is moderately strong between nomophobia and the length of mobile phone usage. So how would I say it is a causal relationship, without actually saying it?

Or what would I say to interpret or describe a correlation coefficient of 0.62?

Thanks in advance!"
"Finweavers, Inc. | Next Generation Quantamental Trading Platform",0,1,False,False,False,statistics,1502878253,False,
Correlation help,5,2,False,False,False,statistics,1502880457,True,"I'm trying to correlate two likert-scale type data sets over SPSS but the correlation values are very contradicting. As you can see here http://imgur.com/a/Cx2Dn the data shows an obviously negative correlation but pearson's r and Spearman's rho is positive!

Any idea whats happening?

"
Simulating Selection Bias for Efron's Biased Coin,0,2,False,False,False,statistics,1502882444,True,"Currently, I am working on different Biased Coin Designs and while numerically the results for the selection bias are clear, I struggle to simulate them.

Atkinson describes the expected bias B_n as an estimation from n_sim simulations as B_n = (number of correct guesses of allocation to patient n − number of incorrect guesses)/n_sim.

And here the struggle begins. I simulate the guesses for an allocation. The guessing strategy is always allocation to the underrepresented method. If the methods are balanced, always allocate to method A.
Then I calculate how often the guess and the allocation with Efron's biased coin match. At this point I should have all the information to plot the selection bias. But the values for my B_n aren't even near to what they should look like. Numerically, the bias should oscilliate between 1/3 and 1/6. My results are far from that. You can see a extract of my resluts in the table.

n|guess|Efron|Bias                      
:--|:--|:--|:--                                
1|A|A|0.0333
2|B|A|0
3|B|B|0.0333
..|..|..|..
12|B|B|0.0666
13|A|A|0.1

Do you have any ideas on what I am doing wrong? Thanks!

**TL;DR**: How do you simulate the selection bias for Efron's Biased coin?"
What Math classes should someone have taken before the apply for a Stats Masters program?,1,2,False,False,False,statistics,1502885863,True,[deleted]
I got a job! I'm terrified! Tips please!,16,22,False,False,False,statistics,1502892320,True,"So I just got offered my first full time job ever, as an entry level data analyst. My background is in biology, but my skillset is enough for the job they offered me. Anyways, I'm terrified! What are your tips for someone starting their first job as a data analyst? What are good things to aim for and things to avoid?"
Proper use of coefficient of variance?,2,2,False,False,False,statistics,1502896534,True,"I am currently in the process of writing my dissertation and have a question about some of my data analysis.  I am trying to determine if defoliation (removal of leaves) on later planted soybeans causes any more yield loss than on earlier planted soybeans.  

I have 6 planting dates each spaced approximately 2 weeks apart.  At each planting date I have a non-defoliated plot and a defoliated plot.  When analyzing actual yield loss there is no difference across the planting dates.  However, when I analyze the yield loss as a percent of the non-defoliated plot in each planting date there is a significant increase in the percent of yield loss in later planting dates.  This is due to lower yields in later planting dates which is driving the percent yield loss up.

The percent yield loss is important when talking to farmers who will use this data due to wide variations in fields (soybean variety, soil type, irrigation, row spacing, yield potential, etc.) across Mississippi, Arkansas, Louisiana, and Tennessee. 

I was trying to use coefficient of variance as a way to determine which model would be better.  In my first 3 planting dates actual yield loss has a better CV and in my last three planting dates the percent yield has the better CV.  The overall averages of both are essentially the same.  Besides that I'm not even sure I can compare the two since one is absolute and one is relative.  

Is there another test I should use in this situation?  Or am I going about this all wrong? Any thoughts will be greatly appreciated!"
A step by step guide to summarize data in R,7,62,False,False,False,statistics,1502905704,False,
Course recommendation to build intuition also for strong fundamentals of statistics,0,1,False,False,False,statistics,1502907397,True,[removed]
A 43% Chance That Statistics Never Lie,3,5,False,False,False,statistics,1502910548,False,
Avarage sale of song by views on youtube,0,1,False,False,False,statistics,1502911959,True,[removed]
"""And the larger and more significant the claimed difference, the bigger is the group size required to supply the supporting evidence.""",1,0,False,False,False,statistics,1502914255,False,
Can anyone here help me with a few questions?,4,0,False,False,False,statistics,1502914897,True,http://imgur.com/a/qdiCy. Here's one of them. I truly appreciate the help. Thankyou
Really need help with this question.,2,0,False,False,False,statistics,1502915528,True,[deleted]
Help needed to solve a logical puzzle in the design of a biology experiment,0,1,False,False,False,statistics,1502916372,True,[removed]
An individual event has a 13% chance of success. How many of these individual events would I need in order to guarantee success?,14,0,False,False,False,statistics,1502920861,True,"I'm having trouble wrapping my head around this scenario. Could a fellow redditor help me?

An individual event has a 13% chance of success. How many of these individual events would I need in order to guarantee success?

"
Question about analyzing a smartphone game.,0,1,False,False,False,statistics,1502921102,True,"Posted this on another sub 2 days ago, but didn't get any replies. 

I play Disney Emoji Blitz, and there are two different goals/rewards a player can earn: Coins and points. Depending on the emoji you select, the coins to point ratio differs significantly. Some are great for points but lousy for coins and vice versa. Some are fairly evenly balanced.

I'd like to create confidence intervals to estimate the ratio of coins to points for different emojis to share with fellow players. This is simple enough on the surface, but there are some game mechanics that might prevent this from applying outside my game.

1) The game selects 4 other emojis (all with widely varying point totals) to be on the board with the player selected emoji. Clearing an emoji earns you that point value, and nearly 1000 emojis will be cleared in a single, average game. The selection of the other 4 does not affect coin totals as far as I can tell. There will be widely varying point totals both between players and within their games and even different emojis in their collections than I have. 

On top of that, each emoji grows stronger thru a different game mechanic which drastically improves its point value and its coin production. A level 5 emoji may produce 3 or 4 times more coins than its level 1 version.

2) There is a % bonus applied to the points gathered from all emojis during a special mode that can activate anywhere from 1 to 5 times per game (for most players), but certain emojis and great players can routinely get 10 or more bonus modes in a game. Both point and coin totals increase above their normal rate during blitz, the name for the special mode.

Since the game-selected emojis are essentially random, I was just going to use all of the games that end up in my random sample to determine the interval and not worry about what the individual point totals from each game are.

I know I'd have to report my results as ""Level 3 Mickey has a ratio of [CI]"", so people are aware of the level difference issue. I just wonder how useful this might be to someone else with possibly widely varying point totals for their emojis in their game. And the number of blitzes a player can get also makes me think the result informs me but isn't so informative to others.

Just from playing I know who to use for high coins or high points, so I don't want to waste any time jotting down sample data and crunching numbers that probably won't make me say, ""I should use _______ instead of ______."" But if it's not useless info to share, then I see it as worthwhile to pursue especially for new players.

Is there just too much variation between players with different emoji collections, different levels within each collection, different skill levels for activating the blitz, and different point totals within each collection for this to be helpful for anyone but me?

Thanx in advance for your thoughts."
"Harvard's Summer Program for Biostatistics is off limits to ""privileged"" white people.",13,0,False,False,False,statistics,1502922605,True,"Harvard has a summer program for biostatistics with these requirements for admission:

Be one or more of the following:

‣ A member of a group that is underrepresented in graduate education (African American, Hispanic/Latino, American Indian/Alaskan Natives, Pacific Islander or Multiracial/Biracial)

‣ A first-generation college student (neither parent nor legal guardian has a bachelor’s degree)

‣ A low-income student as defined by the U.S. Department of Education

‣ A disabled student according to the definition of the Americans with Disabilities Act of 1990 and Section 504 of the Rehabilitation Act of 1973

If you are a white person who is not a low income student, disabled, or first generation student, you are now allowed to attend, regardless of your merits or desires.

How is this acceptable? That is extreme racism. That is extreme prejudice. The state of contemporary universities are so insanely fucking backwards. Why is there no protest over this and why is Harvard not taking flak for this?

https://www.hsph.harvard.edu/biostatistics/diversity/summer-program/eligibility-application/
"
Is there a significant difference in career opportunities in obtaining a MS in Statistics vs a MS in Applied/Computational Math?,3,5,False,False,False,statistics,1502937852,True,"So it seems that most mathy jobs for non-PhD's are data science, finance, or actuary. Is there actually any difference in terms of access to jobs available for MS Stats when compared to jobs available for MS in Applied Maths? I ask because I'm debating in applying to one of these, and want to get a sense of which degree will take me farthest."
What is the most useful math course for statistics I can take ?,28,3,False,False,False,statistics,1502954250,True,"Except for calc1-3, differential equations, linear algebra and scientific computing. Don't say real analysis btw, too difficult and doesn't go the semester I have an open spot for a math course.

What do you guys think of Biostatistics btw? I could take that instead of the one math course, but idk. I might be very uninformed on what Biostatistics is, but it doesn't sound interesting to me, It feels like it's something biology students take who are leaning a bit towards statistics? Looking at salaries too, regular master in statistics beats out biostatistics, and I sure as hell don't wanna take any biology courses, or any of that sort..."
Why do people hate SAS?,28,25,False,False,False,statistics,1502962457,True,I'm just very curious about this. In my uni the main software we're taught is SAS. We got like a basic R course but that was it with R unless your thesis requires you learning R specifically.
Correlation help?,0,1,False,False,False,statistics,1502962715,True,[deleted]
[R] Vector Autoregression for Time Series Analysis,1,20,False,False,False,statistics,1502962928,False,
"For a linear model, if modeling A ~ B + B^2, why do I still need B, if I already 'have' b^2 in there?",10,0,False,False,False,statistics,1502965257,True,"Apologies if I'm using wrong notation. It's based on this line of R code:

&gt; roller.lm &lt;- lm(depression ~ weight + I(weight^2), data=roller)

I'm personally not entirely certain why it woudn't be valid to just remove weight, since weight^2 is already in there. "
Correlation help?,4,0,False,False,False,statistics,1502965653,True,"When you get hot (heat), you sweat. Heat causes sweat to evaporate. So is this ""A causes B, and, A causes C"""
Two questions about significance testing.,5,1,False,False,False,statistics,1502972547,True,"1. Can I use p-values to test for significance in data (lets say between two variables) without null hypotheses?

2. What statistical analyses can I use to determine significance between variables without hypotheses? Just using it to test for significant associations between 2 variables.

I am a noob at statistics, so let me know if I have said anything wrong above.

Thanks."
Generative Adversarial Networks (GANs): engine and applications,0,11,False,False,False,statistics,1502975924,False,
Is the bias-variance trade off dependent on the number of samples?,8,6,False,False,False,statistics,1502982316,True,"We know in modeling, there is a bias/variance trade off. The expected test error can be written as variance plus bias squared of the trained model.

As you increase the degree of freedom (flexibility) of the model, you might get a lower bias but also higher variance. ""Variance"" here means the variance of the model output for a unseen test data I assume. 

So for a fixed sample size n, there is a trade off between variance and bias. But if you increase n, wouldn't both variance and bias go down? So in another word, this ""trade off"" only occurs at a fixed n.

For example, in linear regression, as n increases, the variance of the model goes down as the standard errors of the coefficients decreases. But does the bias drop too?"
Fact checking/extrapolating from peer-review research guidance,1,2,False,False,False,statistics,1502988840,True,"Can anyone tell me what the limitations of using statistical software to plug in data from nutritional studies to remove specific confounds? Assuming of course you don't have the raw foundational data but simply the numbers they provide in the tables? 

For example, I would like to determine hazard ratios for items the researchers did not parse (like smoking, bmi, etc.). Or would that be impossible without having the specific individual measurements? My hunch is that you would need those numbers...but what type of generalizations could you assume from just their incidence numbers?

"
How should I approach categorical predictors with high cardinality?,10,2,False,False,False,statistics,1503004959,True,"I've got 8 predictors in my data set and approximately 3500 levels between them.  Some are Stats/Cities, others are model types for cars.

I've tried MCA but with 24K rows and approx 3K columns, it takes a long time and doesn't reduce the dimension enough to be useful.  

Any suggestions about how I could approach this?"
Mixed matched sampling case-control,0,6,False,False,False,statistics,1503010234,True,"The dataset I'm working with is comprised of data from a case-control study from several locations, however, the ratio of # of cases to # of controls varies by location from no controls at all to 1:9 (case:control). Are there any existing studies which have dealt with similar data or published methods that could be used for power simulations? Thanks!"
"Trying to figure out how many people will be attending my wedding based of probability of each invitee attending, but don't know stats.",7,1,False,False,False,statistics,1503016709,True,"First, I apologize if this isn't the place to post, but I'm in a bind as I need to decide my venue tomorrow and it depends on how many people will come.

I have a spreadsheet with each row representing a guest. I have a field that says ""attendance probability"" that ranges from 1-4. 1 is very unlikely to attend, 4 is will attend. I want to sum these probabilities up and apply it the total number of guests invited to get a new total, but don't know how.

Here is an example of the table:



Guest | Attendance probability
-----|----------------------
Guest 1 | 4
Guest 2 | 3
Guest 3 | 4
Guest 4 | 1
Guest 5 | 2


I promise this is not homework! Thank you very much in advance!"
Questions about small dataset and Lasso regression,2,1,False,False,False,statistics,1503017188,True,"I have a dataset consisting of around 50 rows of data with 10 columns of numerical values. I ran `cross_val_score` in Python for Linear Regression and got some negative scores, which I think is a major problem

Is it because I have too many columns for such a small datasize? 

I then ran Lasso Regression. When I increase `alpha` from 50 to 100, I notice the coefficients for most of the columns stay roughly the same. For example, one coefficient changes from 116 to 122, another from 28990 to 29170, etc. However, one of the coefficients changes from -1219388 to 0. When I then increase `alpha` from 100 to 300, most of the coefficients barely change, except one of the coefficients changes from -1880000 to 0. Is this reasonable?"
"Just google ""Despite limited statistical power""",5,36,False,False,False,statistics,1503017203,False,
Expectation for Poisson - what is the math doing??,0,1,False,False,False,statistics,1503019715,True,[removed]
Sufficient statistics,5,3,False,False,False,statistics,1503023130,True,"Hi so currently I'm doing a statistics course using wackerlys mathematical statistics with applications and I'm having some trouble understanding sufficient statistics, are there any good resources or videos I could use to understand it better? "
The detailed saga of a student questioning a statistics professor about a quiz question,11,1,False,False,False,statistics,1503025346,True,"First, the quiz question: A group of young mothers completed a parenting attitudes scale and their scores are normally distributed. Marla's z-score was 1 and Julie's z-score was -1. If you were to convert their z-scores to a raw score out of 100 points, how many points higher would Marla's score be than Julie's?

Professor’s correct answer: 68

Professor,
I just finished the quiz. I really didn't understand Question 15. I understand that to calculate the raw score from z-score, I would multiply the z-score by the standard deviation, and then add the mean. Even if I assume the mean is 50, I don't think I'm able to calculate the raw score unless I also have the standard deviation. If you could explain the question to me, I would really appreciate it!
Thanks,    
schlechtesEis

__Hi schlechtesEis,        Remember that the standard deviation of 100 is always the same. One standard deviation is going to be 34% plus and minus. So if one score is +1 and one score is -1, then the difference in scores will be 68 points.    Does that make sense?    Professor__

Professor,    
I'm still having a lot of trouble understanding this. I understand that 68% of the scores will be between the z scores of -1 to 1, but I don't understand how that would translate into a point value difference of 68 in terms of raw score. 

I'm also having trouble understanding how the standard deviation is always the same for 100. Looking at page 71 in the textbook, I see two potential distributions that could be possible for the problem, with two different standard deviations. If the scores are more concentrated around the mean, wouldn't that make the standard deviation smaller, changing the answer to the problem?    
 
Thanks,    
schlechtesEis

__Hi schlechtesEis,        I think you are over thinking this. Remember that it's 68%...any percentage of 100 is the same as the number. So 68% of 100 is 68 points. If the sample was only 50, then 68% would only be 34 points. Does that make sense?      
Professor__

Professor,
I'm really sorry that I'm not getting it. If their scores are 68 points apart, then the value of one standard deviation is 34. Assuming a mean of 50, that would mean that a z-score of 2 would be a raw score of 118, and a z-score of 1.5 would be a raw score of 101. According to table 1 (pg 441 in textbook), this means that 6.68% of the testers scored above 101, and 2.28% scored above a 118, which is impossible. I think it would be helpful for me to meet with you.        

schlechtesEis

Professor,    
Thank you so much for meeting with me earlier. I have kept thinking about the problem, and I've tried to write out the logic behind my understanding of the answer. 

 
From what I can understand, the scores are 68 points apart based on the logic that 68% of the data is between z=-1 and z=1, and 68% of 100 is 68.


 
By the same logic that we use to arrive at the 68 answer, we would also conclude that two people with z=-2 and z=2 would have a raw score difference of 95. Similarly, with this logic, the raw scores for z=3 and z=-3 should have a 99.5 point difference.


 
But, if we hold the standard of deviation as 34, which it would be if the scores were 68 points apart at z=1 and z=-1, then the following values would be true:    
        	z=2, raw score=118    
        	z=-2, raw score=-18 (difference of 136 between z=2 and z=-2)    
        	z=3, raw score=152    
        	z=-3, raw score=-52 (difference of 204 between z=3 and z=-3)    
 
That is why it seems like holding the raw score difference to be 68 just because 68% of the data is between z=1 and z=-1 seems impossible. The logic is unable to continue to the 95% and 99% levels.


 
 
 
Additionally, it seems possible that a test with scores from 0 to 100 could result in many different “normal” distributions. The only thing that seems to change is the variance. But the different possible curves seem to show that the answer to this problem would have to depend on the standard deviation. With a standard deviation of 34, the scores would be 68 points apart. But with an SD of 10, the scores at z=1 and z=-1 would only differ by 20 raw points.


 
I’ve tried to write out exactly what I’m thinking as I try and process this, so please correct any thinking errors to get me on the right path.


 

**Professor: So, here’s a couple of screenshots that might help your student, and a link: 
http://www.zscorecalculator.com/ 
In the -1 z score, the percentage falls at around 15.87%, so 15.87/100 points on the test. This percentile is represented by everything in the red shaded area.  
In the +1 z score, the percentage falls at around the 84.13%, so 84/100 points on the test. This percentile is represented by everything in the red shaded area.  
When you subtract the two percentiles, or scores on the test out of 100, you get 84.13-15.87, or 68.26, or around 68 points difference.    
Let me know if this makes more sense!**



schlechtesEis: Thanks for the response. This clears up exactly where our difference in understanding exists (as always, correct me if I’m wrong)    
 
If the answer is 68, the premise is that the percentile rank equals the percent scored. (Ex. A raw score of 80/100 or 80% will be at the 80th percentile, raw score of 30/100 or 30% will be at the 30th percentile)    
 
While I agree that the two scores are 68 percentile points apart, the premise that percentile equals percent does not seem like it can be correct in the vast majority of normal distributions. Percentile refers to the percentage of respondents who scored at or below a given score level, so it is relative to the performance of the other respondents. Percentage/raw score is absolute for each individual.    
 
Can you explain why the percentile has to equal the percentage correct in the answer? I think that is the bottom line that I’m questioning.    
 
Thanks!    
schlechtesEis



**Hi schlechtesEis,    
I just wanted to touch base to make sure we had cleared up any confusion regarding that quiz question. I just spoke with another professor and we think that your confusion might be related to the definition of a normal distribution. If the bell curve is taller or flatter, they are bell curves, but not normally distributed. The normal distribution is a very special bell curve where the standard deviation is always the same and the shape of the curve is always the same.    
Does that help?    
Professor**



**TL;DR: Tried to question a professor’s logic, did not succeed.**


Edit: Formatting"
Random testing and the amount of required data [x-post from r/math],5,1,False,False,False,statistics,1503045446,True,"Hi all.

I've set up a camera (at work) which records our product and sends photos to a folder ""bad"" if the product serial number is wrong, and to a folder ""good"" if the serial number is correct. 

I'm currently testing the system and it seems fine. However, I'd like to randomly check the good photos in order to determine that the good ones are really good. The problem is, I have two days of photos (24hx2) and around 6500 photos per day (so 13000 in total).
Counting the bad ones was easy, 50 photos in total.


How many of the 6500 daily photos should I check manually and do I just randomly do it or is there something that should give me the best ""randomness"". Is there a rule for something like that? I'm really confident that the system works (~95%); I could just randomly open photos and confirm but I was just wondering is there a statistical/mathematical rule in how I should do it.

Cheers!

"
Help turning three percentage variables into one variable for glm?,10,1,False,False,False,statistics,1503051693,True,"So I have collected data about the relative composition of fish in a shoal, as a percentage (e.g. 85% surgeonfish, 10% parrotfish, 5% goatfish) giving me three separate variables. I'm trying to see if I can somehow have the same information but in a single variable for my analyisis. 

I am a stats beginner, so what I'm asking for could be completely impossible, but I hope you guys can help."
Master's in Statistics : how employable?,43,15,False,False,False,statistics,1503062491,True,"I'm preparing for admission to a local statistics (computation emphasis) master's program. I want to know how employable this degree would be upon graduation. And does reputation of the university matter much?  Since it's an in-state school, tuition will just be about $8k a year, so debt won't be huge, but what kinds of jobs am I likely to get with this degree? And does this field intensely favor PhDs to the point where it would be quite difficult to find meaningful work with just a master's? 

Thanks so much "
I don't know whether I am wrong or right.,0,1,False,False,False,statistics,1503063535,True,[removed]
Mixed models SPSS: fixed and random effects,18,13,False,False,False,statistics,1503063708,True,"When to add variables to either fixed and random effects to the model in mixed models? I am left quite confused after trying to figure it out. My data is longitudinal and the file is in a long format. The variable in question is steps per day. I want to think that it is a random effect because the participants had no goals or limits, and every praticipant has a uniqe score. On the other hand, when looking into the litterature it recommends adding it to the model as a fixed effect.

Do anyone have any comments?"
Is it possible to fit a reasonable curve to this dataset?,5,0,False,False,False,statistics,1503072204,True,[deleted]
Difference-in-Differences with dichotomous DV?,1,7,False,False,False,statistics,1503074317,True,"The data are in a panel, and the model is two-way fixed effects (time, unit), so it's essentially a diff-in-diff (though there are more than 2 periods). The IVs and covariates are all dichotomous as well.

What's the standard practice here? Is this somewhere you can get away with doing a linear probability model?"
Modern Survey Design Book,4,13,False,False,False,statistics,1503081248,True,"Hi, I am looking for a good modern survey design book. If there is one that uses R that is even better, but I would be happy just knowing what is out there."
Need some Graduate school advice.. I have the choices of taking one 4 credits course or two 3 credits courses.,8,0,False,False,False,statistics,1503085676,True,"This is going to be my first semester at graduate school for Biostat. There's a beginner level stat course (4 credits) that I can waive but I will eventually have to make up for the credits. The problem is that there aren't any 1 credit courses, only 3 credits. So I will end up having to take two 3 credits courses. By doing this I will be paying about $450 extra. 

I have a B.S. in Math stat focused. So I would love to take more advanced classes than the beginner level. But I don't know if the extra cost is worth it. And also the beginner level will most likely be a free A on my transcript. 

So what would you guys do in my situation?

p.s. I dont know if it matters, but I want to get a Ph.D. eventually. "
Methods for estimating relationship between HS courses and college outcomes?,4,1,False,False,False,statistics,1503088704,True,"I'm on a team developing a tool for middle and high school students to help them choose the courses they'll take in high school. One idea we have is to integrate ""nudges"" into the tool by highlighting courses that increase the likelihood that students will experience certain outcomes (go to college, complete a bachelor's degree, etc.). 

I have access to a database that contains individual-level data for all students in my state. The database includes things like students' demographic characteristics, test scores, schools attended, and every course they completed in high school. We can also link K12 records with college data to see the colleges students enrolled in, their declared major, and the degrees they earned. This allows us to analyze the relationship between HS courses and postsecondary outcomes, controlling for other relevant factors. 

Are there any statistical methods that could be used to assess these relationships, apart from a huge regression model with dummy variables for each of the 1200+ HS courses students can take? "
What are the odds that a person from Australia visiting Europe is within a mile from 3 separate terror attacks in 3 different countries all in the span of 3 months?,4,0,False,False,False,statistics,1503096179,True,The question is based on this [article](https://www.thesun.co.uk/news/4272384/aussie-woman-26-who-escaped-barcelona-terror-was-caught-up-in-london-bridge-notre-dame-attacks/)
Lasso and PCA giving strange results on this data,3,4,False,False,False,statistics,1503098415,True,[deleted]
Building Upper and Lower Limits in SQL,0,1,False,False,False,statistics,1503103881,True,[removed]
"Knoema says Germany is kidnapping capital, yet every other source says it's Mexico? Is Knoema reliable?",7,10,False,False,False,statistics,1503108152,False,
How to use confusion matrix?,0,0,False,False,False,statistics,1503115069,True,Provide some examples of how to evaluate a confusion matrix or performance of a model?
Computing S-value from R-squared for SVM?,0,3,False,False,False,statistics,1503118731,True,"I split a dataset consisting of 3 columns for `X` and 1 column for `y`. 

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25)

I then a support vector regression model to generate predictions

    clf = svm.SVR(kernel='linear')
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

I then computed the R-squared value

    r2_score(y_pred, y_test)

How do I then compute the S-value? Is it just `stdev(y_pred)*sqrt(1-R^2)`"
Critique an article,0,1,False,False,False,statistics,1503126350,True,[removed]
Are data scientists considered market research analysts and/or operations research analysts?,0,1,False,False,False,statistics,1503149263,True,[removed]
Large number experiments in R with full code.,0,23,False,False,False,statistics,1503171673,False,[deleted]
"What to report on CI, power, sample size?",5,2,False,False,False,statistics,1503177194,True,"Hello friends ! I've done a study with correlations, multiple regression analysis, and mediation analysis. 

My study had overall weak to medium correlations and betas around .2-.3. and CI intervals more often than not included 0. 

My sample size was n=100 and should have been +300. What should i say more than that they were to few? That the Margin of error ended up being 10.11%, and what else?

I haven't reported any CI in the results, should i? Should i boostrap them to 5000? Or Can i just report in the discussion section that effects were weak to medium but not generalizable with confidence as beta intervals overlapped more often than not with 0 ?

Thank you, trully appreciate any guidance!"
How to figure out if my data is MAR or NMAR in SPSS?,12,5,False,False,False,statistics,1503188008,True,"Hi!

I'm trying to sort this out and struggling quite a bit. 

Of 101 participants, I have two cases that are each missing one questionnaire answer which is making my Little's MCAR significant.

I'm trying to sort out if the data is MAR or NMAR using SPSS's missing value analysis. But instead of getting a t-test/p value, I'm getting this: http://imgur.com/a/0oQy7

What am I missing? I feel like it should be obvious. 

Thanks!"
Minor Recommendation: Math vs CS,4,2,False,False,False,statistics,1503207125,True,"I am pursuing to major in Statistics, but I can choose minor either math or cs. Minoring in math will do more on analysis(1,2, and/or measure theory). Minoring in CS will cover 6 courses including algorithm based on Java, computational statistics, AI, etc. Appreciate if you guys can share any opinion regarding this."
"Penn State MS Applied Stats, anyone done it?",2,2,False,False,False,statistics,1503220155,True,"I'm seriously interested in this program. My motivation isn't to use it to become a professional statistician, but rather to deepen my knowledge of statistics. I'm working and studying in the field of machine learning, and slowly crawling my way toward a doctorate. However, it's mostly an engineering and CS focus, and I'm personally very interested in research methodologies. I guess all of that is to say that I'm not interested in financial value so much as I am in the educational value.


Or is there another part-time/online program that anyone would recommend? Including international ones. It would require a tremendous personal sacrifice at this point to attend on-campus classes though, which rules out my top choice :("
"If a study finds a small result, but the result is not statistically significant, can I make proclamations based on that result?",0,0,False,False,False,statistics,1503220786,True,[deleted]
"r/statistics, for those of you interested in learning data analysis with Python, I highly recommend you check out the Data Analysis series by Sentdex.",18,163,False,False,False,statistics,1503221922,False,
Trying to understand the logic of this author on interpreting an Analysis of Deviance Table in R,5,1,False,False,False,statistics,1503249569,True,"I'm reading through [this tutorial](https://cran.r-project.org/web/packages/aster/vignettes/tutor.pdf) of an R package and trying to follow it. My background is in biology, and while I am steadily learning fundamental stats, it isn't my strongest skill at the moment.  If you read from the beginning of this document, you'll quickly get the sense that this author is a very passionate writer. Unfortunately that means some of his explanations are lacking a bit.

Go to section 2.2 (page 6)

He is fitting a model that includes population as a variable. The output shows that population is not significant. Go to ""Interpretation"". He says that you should basically ignore significant asterisks. 

In 2.3 model comparison, he does an anova with a model that excludes pop. The resulting output shows that they are significantly different from each other. 

His explanation for understanding this result isn't the best, and it's why I'm here. He says:

&gt;So much for making inferences from a bunch of signif. stars! Not one of the signif. stars for the pop dummy variables in the printout for aout1 was anywhere near as significant as the likelihood ratio test p-value here. See what we mean?

No man. I don't see what you mean. I thought the deviance table is just comparing two models against each other. Why is he comparing 0.03554 to the pop p values in aout1? Can someone please explain it more directly?"
Statistics without stories are meaningless. Stories without statistics are conjecture.,0,1,False,False,False,statistics,1503250046,True,[deleted]
"Knowing the numbers on the dies, how can I find out which player has a higher chance of winning?",5,1,False,False,False,statistics,1503252895,True,"I was doing a [Kattis problem](https://open.kattis.com/problems/dicegame) and I can't seem to figure out why one of the answers I found online works.

-------------------------------------------------

Example :

Player 1: 1 4 1 4

Player 2: 1 6 1 6

Player 1 has two dice, which have numbers from 1 to 4 (4 sides)

Player 2 has two dice, which have numbers from 1 to 6 (6 sides)

In this case Player 2 wins.

-------------------------------------------------

Example:

Player 1: 2 5 2 7

Player 2: 1 5 2 5

Player 1 has two dice, a die, which has 4 sides and number 2 to 5, and a die, which has 6 sides and number 2 to 7.

Player 2 has two dice, a die, which has 4 sides and number 1 to 5, and a die, which has 4 sides and number 2 to 5.

In this case Player 1 wins.

-------------------------------------------------

The solution I found online was to add all the number, so...

Player 1: 1+4+1+4 = 10 

Player 2: 1+6+1+6 = 14 - Winner, because 14 &gt; 10

Player 1: 2+5+2+7 = 16 - Winner, because 16 &gt; 13

Player 2: 1+5+2+5 = 13

I do NOT understand why this work, please help !!"
"In the most laymen way possible, how do I know if my statistical model should be considered conditional or unconditional?",1,4,False,False,False,statistics,1503263903,True,"Let's say my data is set up as such:

DV

* seedv - seed count of a fruit (zero-inflated poisson)

IV

* pop - denotes the population of plants (factor)

* plant_trt - whether the plant was sprayed with an insecticide or not (factor)

* trt - whether a flower was allowed to be naturally pollinated by a bee, or whether it was fully saturated with pollen (factor)

*  fruit set - whether or not a flower matured into a fruit (Bernoulli)

My model in R is set up as:

mod &lt;- glm(seedv~pop+plant_trt+trt, family=poisson, data=dat)

I am experimenting with a new R package called Aster which allows different probability distributions to be in the same model. I need to know, however, if my model is conditional or unconditional. I thought it was simply conditional, being that it's a linear regression, but in [this document outlying the Aster package](https://cran.r-project.org/web/packages/aster/vignettes/tutor.pdf), at the top of page 25, it seems to be more complicated. The author basically says you should know best what your model is.

Any idea what my model is?"
SPSS tutorials,4,1,False,False,False,statistics,1503280224,True,"I've never used this program before, and I'm taking a class in Educational Statistics starting up this fall.  I have the [book](https://www.amazon.co.jp/gp/product/1138024570/ref=oh_aui_detailpage_o06_s00?ie=UTF8&amp;psc=1) A Guide to Doing Statistics in Second Language Research Using SPSS and R, but I am looking for an on-line how to, kind of like all the Excel walkthroughs I see.

Anyone have any suggestions?  Thanks!"
Questionnaire for a successful career in clinical research &amp; analytics field,0,0,False,False,False,statistics,1503301136,False,
Help with multiple regression problem,0,1,False,False,False,statistics,1503309328,True,[removed]
"What are the odds of me guessing a 3 (no suit called), a 9 (no suit called) then a Jack (suit called - Spades) in exact order of a 52 card deck?",12,0,False,False,False,statistics,1503309644,False,
"Biostatisticians, where did you find your first experience?",15,17,False,False,False,statistics,1503314408,True,"I've recently begun studying a biostatistics degree, and I'm really enjoying it so far. I feel like I'm being challenged and learning quite a bit.

One thing I feel like I'm missing is some actual experience in the field and I'm not sure where to begin looking. 

While searching around I've seen CRO, pharmaceutical, and university research mentioned as different types of organisations that hire biostatisticians. I'm not sure what type of organisation I should be approaching, or as a student without relevant experience how I should make an approach.

People who have made it in the field, can I ask if you have any advice for someone starting out? Where would you look? What approach would you take? Are there any resources for finding placements that you could recommend?

I'm based in Australia if that's relevant. Any advice would be very welcome!

Thanks in advance."
How to Find the Mean?,0,1,False,False,False,statistics,1503318859,False,
Statistics Assignment Help Service,0,1,False,False,False,statistics,1503320074,False,
Is Z-score a valid measure for a highly skewed distribution?,8,5,False,False,False,statistics,1503325465,True,"I understand z score is a measure of how many standard deviations a statistic is from the mean. But I've only seen this in use for normal distributions.

Lately, I'm dealing with a highly skewed distribution. For example, daily sales of a niche item at a store where on most days, the sales will be 0. I want to model how well a day sold by calculating the z-score as a measure. But I'm not sure it's valid in this case since the sales are not normally distributed. Do I need to transform the data (via log or power transform) to a normal distribution first?

Also, the usually ""68 95 99 rule"" only applies to normal distributions right? As in, 68% of data are within 1 sd of the mean...etc

Back to my sales question, would using quantiles be a better measure? Just want to have a measure of ""how well a day sold relative to other days.""

Thanks"
Can a Standardized Mean Difference be expressed as a percentage?,2,1,False,False,False,statistics,1503325686,True,"For instance, if the SMD between the mean of X between group A and B is 0.62 SDs, is it accurate to say that ""variable X in group A was 62% increased relative to group B""?"
False Negative Rate &amp; Type II Error,0,1,False,False,False,statistics,1503331116,True,"In the [wikapedia article for False Positive Rate](https://en.wikipedia.org/wiki/False_positive_rate), it states that the Type I Error rate and FPR differ from one another because Type I Error rate has the premise of being **a-prioi** in regards to the significance level and FPR deals with only **post-prior**.

I can't find an article dedicated specifically to False Negative Rate, but can I assume that the difference between a a-prior and post-prior are the same for Type II Error and FNR?"
Computing S-value from R-squared for SVM?,0,0,False,False,False,statistics,1503332301,True,"I split a dataset consisting of 3 columns for `X` and 1 column for `y`. 

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25)

I then a support vector regression model to generate predictions

    clf = svm.SVR(kernel='linear')
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

I then computed the R-squared value

    r2_score(y_pred, y_test)

How do I then compute the S-value? Is it just `stdev(y_pred)*sqrt(1-R^2)`"
How do you hand calculate Pearson's r effect size from an ANOVA output?,2,1,False,False,False,statistics,1503334769,True,"I'm attempting to hand calculate a Pearson's r effect size of a single group contrast from an ANOVA output. There are a number of formulas and guides online, all of which seem to require the individual values from the dataset, which I do not have available (only the data output).

My output has four tables: descriptive statistics, ANOVA table, contrast coefficients and contrast tests.

Is it possible to calculate Pearson's r from any of the values offered in these tables i.e. sum of squares? If so how could I do it?

Thanks for your help"
Ordinal Logistic Regression with Explanatory variables as Ordinal,2,1,False,False,False,statistics,1503346333,True,"Hello,

I'm attempting to do an Ordinal Logistic Regression on Customer Likelihood to Recommend, which is commonly known as Net Promoter Score (NPS).  I am using raw survey data from about 500,000 surveys, and doing the analysis in R.

The survey is composed of 11 questions, the 1st being the ""Likelihood to Recommend"" on a Scale of 1 -10.  The other 10 questions ask the customer to rate their experience on a scale of 0 - 10 on attributes like Price, Product availability, Associate friendliness, checkout time, etc.

For the ""Likelihood to Recommend"", I've assigned each individual survey result into the 3 typical buckets of NPS:  Scores of 0 - 6 are ""detractors"", 7-8 are ""Passives"", and 9-10 are ""Promoters"".

Since the other 10 questions are also Ordinal in nature, what would be the appropriate method to an Ordinal Logistic Regression with those as the explanatory variables?  I'd like to show how these different Attributes affect the overall Likelihood to Recommend. Should I assign these to a grouping like the NPS Bucket of Detractors/Passives/Promoters as well?  My initial analysis was to keep them as continuous, but the nature of this scale is Ordinal and limited to 0-10, and I'm not sure if that is correct or not.

I've done the regression a few times, and the Odds-Ratios appear to be accurate, however, I don't know if I'm meeting the Parallel Slopes Assumption.

Thanks"
Masters of Statistics Programs in Canada,10,7,False,False,False,statistics,1503350581,True,"I'm currently applying to Masters of Statistics programs in Canada. I graduated with a BMath/BBA double degree, majoring in Stats/Finance, my marks for my last two years are decent (mid 80s) and I have a resume of mainly accounting/finance jobs. 

My issue is that my letters of reference are going to be weak as I didn't really form any relationships with professors. I do have two that have agreed to write one for me, though.

Can anyone comment on my chances of getting accepted and what schools I should be focusing on?"
Encoding ordinal variables,1,2,False,False,False,statistics,1503361751,True,[deleted]
If Y is dependent on two variables,1,1,False,False,False,statistics,1503366158,True,[deleted]
"My final is in 4 days. I am completely clueless and lost because my teacher barely explained anything. I am not good with statistics. If anyone can, please help.",12,0,False,False,False,statistics,1503373465,False,
How much of my exam should I have remarked?,11,0,False,False,False,statistics,1503398144,True,"I received 124/200 marks overall. The boundary for a C grade is 105/200 marks and the boundary for a B grade is 129/200.
Therefore I am 20 marks above a D grade and 5 below a B grade.

The breakdown of marks was as follows:

Paper 1 - 36/40
Paper 2 - 43/60
Paper 3 - 19/40
Paper 4 - 26/60

I cannot have paper 1 remarked.

Which of the other papers should I have re-marked?

EDIT: oops I meant ""re-mark"" not ""remark""!
EDIT 2: Americans may know ""marking"" as ""grading""."
"How do I recreate Signorino and Ritter's (1999) S-score in stata, using COW alliance data?",1,0,False,False,False,statistics,1503404434,True,"I am aware I need to actually generate the measure, however I have trouble coding it. Same goes for vis Cohens κ and Scott’s π. There are measures of alliance portfolio similarity, the last two are chance-corrected indices.
Any help will be greatly appreciated
"
Ensemble Learning to Improve Machine Learning Results,3,13,False,False,False,statistics,1503406630,False,
Best way to show biggest gainers and losers?,2,1,False,False,False,statistics,1503409944,True,"I have a dataset where we have values for year 2010 and 2011. We need to show the biggest % gainers and losers but we have some zero values for 2010 which lead to not being able to calculate percent gain! how can we show that these are a significant increase? should we change our approach?

example data below

http://imgur.com/a/EvDcH"
Stats: Turn 5 factors into 1,0,1,False,False,False,statistics,1503422530,True,[removed]
How do I make a single variable from 5?,0,1,False,False,False,statistics,1503424188,True,[removed]
Conflicting linear regression assumptions -- need clarification,6,3,False,False,False,statistics,1503425456,True,"Reviewing assumptions for linear regression, I'm getting conflicting info on whether there needs to be a strict linear (i.e., straight-line) relationship between IV and DV. I've always learned there needs to be a strict linear relationship, but I've discovered [this resource](http://www.theanalysisfactor.com/regression-modelshow-do-you-know-you-need-a-polynomial/) and [this resource](http://blog.minitab.com/blog/adventures-in-statistics-2/what-is-the-difference-between-linear-and-nonlinear-equations-in-regression-analysis) online saying only the beta coefficients of the linear equation need to be linear to satisfy the linearity assumption. 

In other words, the predictors themselves can be quadratic, cubic, etc., but as long as the beta coefficients themselves are linear (i.e. single degree), this would mean the regression line can be curved but still satisfy the linear assumption (?!)

Is this to say that the regression line itself does not necessarily need to be a straight line to satisfy the linearity assumption? The overwhelming majority of resources I find online state there does need to be a strictly straight line relationship between IVs and the DV. I'm confused and would love some clarification. Thank you!
"
"If I have participants filling out a 5 point likert scale (0-5), is it possible for the standard deviation to be as high as 8?",12,1,False,False,False,statistics,1503434132,True,
"What are the odds or probability of this scenario happening or better yet, how would I set up this formula?",4,1,False,False,False,statistics,1503434163,True,"My stats knowledge has been extremely fuzzy, haven't taken stats in over 3 years and am already out of school. 


So my scenario is the following: 


There are X number of games, I am predicting that each game my team will have a Y% of winning the game. What is the likely hood of my team not having a losing streak of more than 1 throughout the X number of games. 


X = Fixed Amount


Y = Fixed Percentage and same throughout every game"
Ive calculated this 10x and I'm still getting the wrong answer... WTF???,3,0,False,False,False,statistics,1503437620,False,
Statistics Question Likelihood,0,1,False,False,False,statistics,1503439479,True,[removed]
"I'm looking for a good reference on the problems with stepwise regression, and the reason that LASSO is a preferred alternative.",13,28,False,False,False,statistics,1503441913,True,"Most of what I've learned about these issues have come from StackExchange, which makes it difficult when it comes time to write up a results section! If anyone could point me towards the typical references on these topics, it would be much appreciated."
self-learning SAS,0,1,False,False,False,statistics,1503445947,True,[removed]
Alternatives to k=2 when using AIC?,2,1,False,False,False,statistics,1503448618,True,"From what I know, the default penalty per additional parameter in a model, when calculating AIC, is 2. One alternative I've come across is to use log(number of observations) aka BIC.

It seems that you can make a model selection approach using AIC more conservative/liberal by tweaking this penalty value, so I'm wondering if there are thoughts/rules of thumb out there on the ""best"" penalty to use?"
Computing S-value from R-squared or RMSE?,3,1,False,False,False,statistics,1503449841,True,"For a given model, for example Linear Regression, I want to compute the S-Value

I know how to get the R-squared value and RMSE using the predicted values for `y` and the actual `y`

How do I then compute the S-value? Is it just `stdev(y_pred)*sqrt(1-R^2)`? Or is it the same as the RMSE?
"
Programs to use to calculate a sample size.,6,2,False,False,False,statistics,1503485163,True,"I am doing a cross-sectional study about the prevalence of depression in type 2 diabetic patients. 

Total diabetic type 2 population is around 50,000
From previous studies done, prevalence of depression is around 30%.
Confidence interval needed: 95%

I would like to ask if there is a good online (no download) calculator I can use to calculate sample size.  Any help would be appreciated!!"
Can someone help solve this,2,0,False,False,False,statistics,1503490297,True,"https://reddit-uploaded-media.s3-accelerate.amazonaws.com/images%2Ft2_16jx90g%2F6gpowaix9hhz
So I have finals soon and I think this should be a piece of cake to solve for some of you, so basically I need to solve the arithmetic average, mode and median. I know how to do the arithmetic average, but I have a problem with the median, I did the cumulative frequency, and I need to choose the L1 for my median formula, english isn't my first language and it's pretty difficult for me to translate these terms, so 
tl;dr can someone solve my median"
Please help: Wilcoxon test and ANOVA (interaction),0,1,False,False,False,statistics,1503502321,True,[removed]
Trying to determine if survey was tainted,2,1,False,False,False,statistics,1503506977,True,[deleted]
Best resource (book/video lectures) to self study statistics (computer engineering student here). Any suggestion?,6,21,False,False,False,statistics,1503513223,True,"Hi everyone,
I am a computer engineering student interested in learnin more about data science. My university does not cover statistics though, so I need a good resource to self study it. I am comfortable with calculus 1, 2 linear albegra and geometry. Thanks."
Is there a relationship between the absolute value of a regression coefficient and its t statistic?,3,1,False,False,False,statistics,1503516679,True,"I am wondering if steeper slopes are more likely to be found significant?

My guess is yes. The numerator in a t-statistic for a regression slope is how much it improves the model vs. the mean (assuming it is the only predictor). If the observed values are far away from the mean (i.e., requiring a steeper slope) then it seems the numerator of the t-statistic will be higher.

Or, are they completely independent?

Edit: Bonus question that I'm just adding here instead of posting separately: How is it possible to calculate the standard error in the estimate of a regression coefficient when there isn't a distribution of values?"
Continuous to Categorical Variable,4,1,False,False,False,statistics,1503524181,True,"Suppose I am performing a regression and I have an independent continuous variable that I want to express instead as a rank 1..n, e.g. 'x1 &gt;= 30' = 1, '30 &gt; x1 &gt;= 20' = 2, ..., 'x1 = 0' = n.  How am I supposed to decide how to bin the data points into these n ranks?  

Might it be subjective, or should I be aiming for an underlying distribution between the ranks, or something else?

Thanks!"
Building a classifier on accelerometer data?,0,1,False,False,False,statistics,1503524264,True,"I am currently working on a project where, given k people, we wish to distinguish their driving activity.  We have data from six drivers on six different cell phones of the same make and model over the same course.  I am using [this paper](http://www.autosec.org/pubs/fingerprint.pdf) as a guide, but they use OBD port data instead of noisier cell phone data.  Features include what this paper describes: Aggregating about 36 summary statistics over sliding, overlapping windows for every raw feature, which for me consists of x, y, and z components of linear acceleration, gravity, and gyroscope.  These summary statistics are typical things like mean and median, PAA time series components, and powers of Fourier transforms.  The raw data is about 1.5 million data points that spans 20 minutes and is sampled at 200 Hz.

I am very skeptical of the evaluation criteria used in this paper.  They performed 10-fold crossvalidation without a separate test set, which is concerning not just because the same data used to tune parameters was used to evaluate the model, but because it does not take into account the temporal nature of the data.  Even over sliding windows, it could lead to a ""fill-in-the-blank"" style evaluation where it's filling in segments using information for similar, nearby segments and won't generalize to new data.

My evaluation criteria is currently a 70/30 train/test split, where the first chronological 70% of the points are aggregated separately from the last 30% of the points, which makes sense if our goal is to generalize this model to new paths.  The issue is that with six drivers, I am getting 40% accuracy (class distribution is almost entirely symmetric) at the most on all preliminary models (Random Forest, Naive Bayes, Elastic Net, Decision Tree).  I'm reasonably certain that this isn't due to overfitting because of the underlying regularization in elastic net, and the bagging in random forest.

My initial suspicion was that poor preprocessing (I didn't run any kind of filter) was leading to the low accuracy, but the model does fairly well telling only some people apart.  However I am starting to suspect that maybe my test set is flawed, since maybe it includes points that by virtue of being ""unexciting"" will not allow for distinguishing drivers.

Are there any glaring blind spots in what I'm describing?  Should I focus more on preprocessing to try to boost accuracy?  If so, how should I go about doing so?  Is my evaluation criteria flawed?  Normally I'd ask someone in my office, but working with sensor data is all brand new to us.

For reference, [here's a segment of about 10 seconds of accelerometer  data](http://i.imgur.com/fpiiIkU.png).  Vertical axis is m/s and horizontal axis is observation number.  There's no way that some of these values are real because no one is going to change acceleration that much in like 50 milliseconds."
"Good Resources for Learning More About Stats for ""Mathematically Mature""?",3,3,False,False,False,statistics,1503541885,True,"Hi guys, I haven't found anything that addresses this in this sub.

I have a masters in applied mathematics: lots of linear algebra, ODEs, analysis, and some probability. I have some experience doing prediction with linear regression, logistic regression, and ""machine learning"" classifiers like random forest, svm, etc., but these are more about prediction and evaluating models empirically which is a lot easier than doing inference.

What might be better resources for me to learn more about using fundamental models to do inference? I'd also ideally learn some about modeling time series, statistical tests, and design of experiments. 

 I appreciate any help!"
Machine Learning Model Ensembling with Fast Iterations,0,11,False,False,False,statistics,1503556111,True,"* [ML Model Ensembling with Fast Iterations - Blog Article](https://blog.dataversioncontrol.com/ml-model-ensembling-with-fast-iterations-91e8cad6a9b5)
* [Materials of a case study to build a DVC-based ML pipeline for an R project with ensemble prediction - GitHub](https://github.com/gvyshnya/DVC_R_Ensemble)
* [ML Pipeline](https://github.com/gvyshnya/DVC_R_Ensemble/blob/master/logical_workflow.png)
* [Kaggle Competition](https://inclass.kaggle.com/c/pred-411-2016-04-u3-wine/)

**Overview**

R scripts that constitute the elements of the machine learning pipeline for the ML experiments to tackle the supervised-learning regression problem to predict wine sales.

R scripts were originally developed by George Vyshnya back in 2016 as the above-mentioned completion was active. The scripts were then slightly modified in 2017 to demonstrate the potential of building repeatable and reusable production-ready ML pipelines for R-based applications, using the prominent Data Version Control (DVC) application."
Industry Data and Forecasts by Sector and Country,0,3,False,False,False,statistics,1503571508,False,
"Statistic question, need help!",2,0,False,False,False,statistics,1503588653,True,[removed]
Is my fantasy football commissioner cheating us on draft picks?,23,17,False,False,False,statistics,1503592463,True,"Hello all,

I wanted to get your guys opinion on the draft order in my fantasy football league.  Each year our commissioner does a supposedly random draw of the draft order.  I compiled the draft order and found that in the past 5 years (length of time league has existed) he has placed top 4 or lower.

He has averaged a draft placement of 2.6 over the last 5 years.  I treated the average draft placement as a normal distribution and found that there is a ~0.007% chance that he ends up with that number...  Since a lower draft pick means better options, I am wondering if he has been pulling one over on us by fixing the draft order.  What are your guys thoughts?  I am no whiz with statistics so was hoping for some help.

Thanks


*Edit: Removed link for security purposes.  There are 12 teams over a span of 5 years.  His draft placements in each respective year are 1, 1, 4, 4, 3."
Why does stepwise model selection lead to overfitting?,10,17,False,False,False,statistics,1503597925,True,"I think I finally understand how penalized regression deal with overfitting (i.e., why reducing the combined weight of coefficients in a model deals with overfitting). But I've realized that I don't entirely understand what it is about stepwise regression that leads to overfitting. I know that one reason is that there is no partitioning of the data into training and test sets. But there must be other reasons as well?"
Microsoft Professional Program : Data Science,0,11,False,False,False,statistics,1503598904,True,"Has anyone had experience with this program? If so, what did you like and dislike? How has it helped you?"
Job vs PhD stats,0,0,False,False,False,statistics,1503606687,True,[removed]
Job vs phd,0,1,False,False,False,statistics,1503615174,True,[removed]
Statistics Project for an interview,3,2,False,False,False,statistics,1503615804,True,I've been getting some interviews for internships lately and they often ask to talk about a statistics project I have done recently. What is a good answer to this and what kind of project can impress someone who interviews me. Thanks for help
Considering a PhD in Statistics/Biostatistics,8,5,False,False,False,statistics,1503620123,True,"Background:
BS in Nutrition 
Registered Dietitian
In Progress: MPH in Biostats and Epidemiology

I graduate this spring, but don't feel like my MPH has given me a very good background in data analysis. I can work with SAS and R and have aced my courses so far, but am a little concerned they have been so easy considering I've never taken Calc 1. I don't want to be stuck in a research assistant type position, or solely programming SAS, making $30-50k. I'm interested in Pharma, NIH work/research, or even government positions in analysis at the FDA or CDC.  Or just something interesting, possibly public health or nutrition related, where I can contribute on the research/development team.

Question:
Because my BS is not math-based, it seems I would have to take two or three calc courses plus a few other advanced math courses to even apply for a PhD in statistics. Is this the same for Biostatistics PhD's? Given my background and career goals, should I be looking into other PhD's or just spend a year doing the pre reqs? Alternatively, are there better career opportunities for an MPH in Biostats than I am perceiving? 

Thanks for any advice!"
Simulations for Power analysis,4,3,False,False,False,statistics,1503623855,True,"Power analysis is a pain. Right now I use PASS. Does anyone know of a guide that presents a uniform way to perform power analysis. I am guessing that this would take the form of simulations, but anything would be useful. "
Overlapping area between machine learning and statistics.,0,1,False,False,False,statistics,1503633833,True,[removed]
"What conclusion can we draw from mean, mode, and median of a data set?",5,1,False,False,False,statistics,1503638198,True,"I know that mean, mode, and median are all measures of central tendency, and fall under descriptive statistic.

What I am not sure about is what I can say about the data while looking at a graph of means, modes, or medians.

The mean makes the most sense to draw conclusion from, for example, I am given daily sale rates of a company over a couple of years. I can find the mean for each month of each year, plot bars and conclude that on AVERAGE the sales in the summer months are higher...

However, if I find the mode for each month or the median, what do they tell me? The mode would tell me the most frequently occurring number of sales. And the median would again tell me the average.

I am kinda answering my own question based on what I know and understand. Could you guys give me more insight and information on what the mean, mode, and median can tell us about the data?

PS. I am not looking for definitions of mean, mode, and median.


Thank you."
How to Create an Online Choice Simulator,0,1,False,False,False,statistics,1503642600,False,
Model fit criteria when predicting a fractional outcome,0,1,False,False,False,statistics,1503654303,True,"Hi r/statistics,

I am working on a project where I am comparing different modeling approaches to modeling a fractional outcome. I’ve started out using Stata. I am trying out the different models on a freely available data set found here: http://www.stata-press.com/data/r14/401k. The fractional outcome in this dataset (prate) is bounded between 0 and 1 with around 33% being exactly 1.  

While there may be good substantive reasons for choosing one model over another, I am interested in how to empirically compare one model’s fit to another. Specifically, I would like to compare these different model specifications: (1) Linear regression, (2) fractional logistic regression, (3) one-inflated beta regression and (4) linear regression with a transformed y (e.g. square root). 

So far, I’ve been relying on cross-validation, but I am having problems deciding which criterion to use – and they don’t agree on which model to prefer. I’ve started using rmse, but searching some of the literature on the topic, I’ve also tried: Spearman Rank Correlation between predicted and actual y, mean absolute error, R-squared and information criteria (AIC – without using cross-validation).

Which criteria would be the way to go and why? 
"
Looking for advice on analysing User experience survey that used a 7 point likert scale,7,3,False,False,False,statistics,1503659569,True,"Hi everyone,
I have conducted a user experience survey and gathered 39 items worth  of data on three games. Each item is a statement with a 7 point likert scale (from strongly disagree(1) to strongly agree(7)).

Now with these 39 items are groupings, i.e. the first 6 items are observing the enjoyment within the user experience survey. The next 6 items look at control, so on so forth.

So, I have worked out that the data is not going to be normally distributed. I have been told that I need to use a method other than descriptive statistics to observe the impact and if the results are significant. But I feel a within-subject analysis of overall user experience is a better course of analysing the results. Comparing the overall likert scale mean from the user experience survey?


Apologies for the long explanation. If anyone has any thoughts or input it will be greatly appreciated. 

Thank you
"
New Design of Experiments R Package 'skpr' on CRAN: Generate Optimal Experimental Designs and Evaluate Power,6,58,False,False,False,statistics,1503665552,False,
Help: Model to forecast store sale for next 2 months,3,2,False,False,False,statistics,1503673072,True,"I am building a model that should forecast dollar sales of 5 products for 100 regional stores for the next three months.

I have 4 years of monthly sales data for these stores. My target is balance or (change in balance) and my independent variables are:

income tier (binned 0-50k, 50-100k...)
region / state
competitor prices
competitor sales
historical rate of new customers walk-ins
historical rate of one time customers vs repeats

Before going into building a model (regression), I'm trying to identify segments that I would want to model separately. For example, building a model for low income tier in specific regions separate of model for high income in other regions. I am wondering if there is a way to classify the combination of these segments that I should model separately.

For example, can I use xgboost classification process to combines and split these segments that would allow me to build a more accurate model? Or how can I justify combining regions that have similar sales patterns."
HELP: Newbie doesn't understand which model to use and is totally clueless.,3,1,False,False,False,statistics,1503677735,True,"Hello! I am trying to determine which model would be best to use to solve the following problem.  I am looking at the occurrence of patient to patient violence in a psychiatric hospital.  I have the time of day that these events occurred for the past 3 months (about 30 events total) and I need to do a data analysis to determine if there is trend for what time these incidences occurred or a period of time during which these fights are occurring more frequently, so that potentially additional staff members can be added to that time period.  However, I have no idea which model would be best because I do not understand much of what I am reading.  My assumption is that patient violence = dependent variable and time of day = independent variable. (?) Would a regression model be best? If so, would that be considered linear regression?  Or would this be a t-test? Or something else?  Please have mercy on my statistically challenged soul.  I am trying to help a friend in need.  

"
Law degree but love mathematics/statistics,5,5,False,False,False,statistics,1503677887,True,"Recently graduated in the U.K. With a law degree, but I've decided it's not for me and I now want to change my career path to something with numbers (although I'm not quite sure what). 

I did mathematics in sixth form (college for all Americans out there) before uni, but decided to do something different. Whilst studying my law degree I taught myself first year university level mathematics purely because I loved the subject. I'm now at that stage where i want to do a masters in statistics, but I'm not sure what I can do to get into a masters course when my degree doesn't involve a mathematical discipline. 

I wondered if there is anyone out there who has been in a similar situation, or if anyone can provide me with any advice whatsoever? I'd also like to know if anyone can give me ideas for a career (other than data science which I've already looked at and decided it's not for me).  

Many thanks. Any help would be greatly appreciated "
Classification problem where each class has the same covariate,1,0,False,False,False,statistics,1503682343,True,"I was trying to make a model predicting the winner of a horse race and came across an interesting problem: suppose you have seven horses in a race and one variable for each horse, speed last race. To make a simple multinomial logistic regression model predicting the winner, we would typically model Y=intercept + 7 sets of Beta * X; we then get seven different betas, even though each covariate represents one horses speed the last race. Does anyone have a solution to do this better? Should I just simply set each of the 7 Betas to their average? How would I also extend this to different models? Thanks"
HELP: Which stats should I use to determine only two possible outcomes?,9,0,False,False,False,statistics,1503682842,True,"Essentially I have two groups of mice, stick them in a tube 2 at a time on either end, and watch which mouse comes out first thus losing the match. I know that the number of wins should be reported as a percentage of the total number of matches. Given that, how I would compare the two groups? Should I consider each group separately against a test value of 50 and check significance that way or... (it's odd because a loss is 0 and 1 is a win) a binomial test?(never done this before, but it seems appropriate)...yeah, any help would be appreciated. "
Small vs Large breasts: The Survey,0,1,False,False,False,statistics,1503689510,True,[deleted]
Small vs Large Breasts: The Survey,3,1,False,False,False,statistics,1503690113,True,[removed]
Impossible Statistics,4,0,False,False,False,statistics,1503692513,True,[removed]
Where can I find reliable statistics on the proportion of water usage (personal use vs industry vs agriculture) in Europe?,1,1,False,False,False,statistics,1503697903,True,"I'm looking for statistics of water usage in Europe to check how domestic water usage compares to industry's and agricultural's usage in Europe.

At the very least, a simple pie chart with the percentages (and the source) would do.

This is just for personal curiosity. Thanks!"
IBM designed a platform that's aimed at making life easier for data scientists,1,0,False,False,False,statistics,1503698967,False,
Converting number of hospital beds into per population - need help,6,1,False,False,False,statistics,1503708944,True,"I need some help to clarify something. 
I have the number of hospital beds for a given neighborhood, let's say neighborhood A has 250 hospital beds. 
How do I convert the number of hospital beds into ""per population""?
Do I divide the number of hospital beds by the number of people and multiply the result by 1000? Thanks in advance!!!"
Why is my function (in R) returning NA?,4,0,False,False,False,statistics,1503725561,True,"I have create a function to return the mode of a vector, it works with character and numeric type vectors. For example if the input is [1,2,2], then the function should return [2], if the input is [1,2,1,2], then the function should return [1,2].

For some reason when I take the code out of the function and run it line-by-line the code returns correct results. Once I put the code into a function format it returns ""NA"" for any input. How come?

	Mode &lt;- function(v){
      uniqv &lt;- unique(v) # removes duplicate values
      matchv &lt;- match(v, uniqv) # returns a vector of the positions of (first) matches of its first argument in its second.
      tabulatev &lt;- tabulate(matchv) # counts the number of times each item occurs
      
      # Check if the frequency of all elements in the same - returns vector
      if(max(tabulatev) == min(tabulatev)){
        # Since the occurance of all items are the same
        # return the vector of unqiue items
        return(uniqv)
        
        # Check if there are more than one same frequency - returns vector
        # Returns a vector of items with the highest frequency
      }else if(any(duplicated(tabulatev))){
        # Get the max item from tabulatev vector
        tabulatev.max &lt;- max(tabulatev)
        tabulatev.max
        
        max.items.indecies &lt;- c()
        # Find the index of all max items in the tabulatev vector and 
        # save them in the max.items.indecies vector
        for (i in 1:length(tabulatev)){
          if(tabulatev[i] == tabulatev.max){
            max.items.indecies &lt;- c(max.items.indecies, i)
          }
        }
        
        mode.items &lt;- c()
        # Create a vector of items, which have the highest frequency and
        # save them in the mode.items vector
        for(i in max.items.indecies){
          mode.items &lt;- c(mode.items, uniqv[i])
        }
        
        return(mode.items)
        
        # Else there is only one max frequency - returns vector of one item
      }else{
        which.maxv &lt;- which.max(tabulatev) # returns the index of the max item
        return(uniqv[which.maxv])
      }
    }"
PhD in Statistics or Data Science consultant?,0,1,False,False,False,statistics,1503752355,True,[removed]
Data Analysis Classroom - A free online platform for learning data analysis.,0,16,False,False,False,statistics,1503769252,False,
PhD in Statistics vs. Data Science/ML consultant,26,14,False,False,False,statistics,1503772455,True,"Hi guys,


I am a 25 year old student who is about to finish his Master's in Applied Statistics (southern europe). Beforehand I worked as a consultant at a moderately sized Statistical/Data Science consultancy. Now, at the end of my studies my professor has offered me a position at his chair to pursue a PhD in statistics (paid position). The statistical consultancy I previously worked at has also expressed interest in me coming back and working for them.


For a couple of weeks now I have been going back and forth and can't seem to decide what I want to do. On the one hand, it would be cool to do a PhD and work in the university context; which I really like. I love teaching and I like the freedom that pursuing a PhD provides; the freedom to go in any direction you like and be your own man. On the other hand, I fear that doing a PhD leads to me being too specialized on Statistics and unattractive for consultancies or companies, who do more applied stuff like Data Science or Machine Learning. If I would start working at the consultancy I would not obtain a PhD but have four years of Machine Learning work experience, which would make me highly attractive work force for future employers. But I would be way more bound to any projects that arise, and would end up learning a great deal less about statistics in general than I would when pursuing a PhD.


What do you think? Maybe someone who has done a PhD in Stats can share his/her experience."
How much do grad programs care about GRE?,6,0,False,False,False,statistics,1503779833,True,"I'm a senior math and statistics double major who is applying to statistics PhD programs this semester. I have a rounded 3.9 GPA and several different research experiences that have involved posters, oral presentations, and an unpublished paper. I have also had two different internship opportunities. I just took the GRE recently and scored 162 on both verbal and quantitative. I looked up charts that say I'm 91st percentile for verbal and only 81st percentile for quantitative. I'm really worried about good programs throwing my application away due to a not-stellar quant score.

Do I need to retake the GRE to try for a better quantitative score or should I be okay since I have a good GPA and resume?"
Test for statistical difference between three percentages?,13,1,False,False,False,statistics,1503781469,True,"Hi guys,

I want to test whether three percentages are statistically different to one another. They come from the same dataset but are from different samples, e.g. one percentage is 73.8% (166/225), the second is 71.7% (81/113) and the third is 72.9% (148/203). I thought an ANOVA test would be appropiate but I read that conducting an ANOVA test on percentage data is a bad idea. 

What's the most suitable test to see if they are statistically different?

Edit: I have narrowed the test down to a chi-test. I have found this website: https://home.ubalt.edu/ntsbarsh/Business-stat/otherapplets/ProporTest.htm which offers to test for the equality of several population proportions. Can anybody vouch for this?"
"How can I recode a variable in SPSS based on what another variable contains, rather than its value?",3,1,False,False,False,statistics,1503781473,True,"I hope this is the right place to ask this, and I know this is possible in Excel, but I can't figure out how to do this in SPSS (if it's even possible). I have a string variable, which is titles of videos. These titles are long and sometimes complex, but what I'm really interested in is teasing out key words within these titles. I'm looking to create new variables based on specific key words in those titles. So, for example, if I wanted to create a variable that identified cases where the video title contained the word ""red"", I'd create a new variable called ""Red"" and I'd make it a dummy variable, placing a 1 where the case has a title containing the word red and placing a 0 where it does not.

I know how to recode variables based on the value of another variable, but is there any way in SPSS to create a variable based on a specific word within that variable. I'm guessing there's maybe some conditional syntax I could create to do this. Any help is greatly appreciated!"
Is multicollinearity an issue in mediation analysis using the bootstrapping method?,0,1,False,False,False,statistics,1503782332,True,[removed]
I'm so lost on this: setting a sample size (crossposted from ELIF),11,1,False,False,False,statistics,1503784794,True,"I'm really stuck on this: I know that effect size, power, and alpha are needed, but what else? While I realize that power can be set at 0.8 and alpha at 0.05, what gets me is the effect size. Much of what I read is confusing, and it seems like you'd need effect size BEFORE doing a power analysis, though if you don't have any data, how can you do that?
PLEASE, how does a researcher determine sample size without prior data?"
I need help spotting my mistake showing that the computing formula for variance equals the defined formula,0,1,False,False,False,statistics,1503793833,False,[deleted]
Is it realistic to work full-time while also a full-time student in a Masters statistics program?,34,1,False,False,False,statistics,1503795331,True,"Is it realistic to work full-time while also a full-time student in a Masters statistics program? My school requires students get at least B's to pass the classes (is it that way at all schools)? Anyways, I would work part time, because I don't think it's realistic to do both full-time, but I don't think relevant jobs would hire me for just part time positions... do they? 

EDITED to add: my program will have 2 courses per semester as full time. "
I need help spotting my mistake showing that the computing formula for variance equals the defined formula,7,13,False,False,False,statistics,1503795621,False,
block design anova?,3,3,False,False,False,statistics,1503825123,True,Posting here because I cannot find an answer in my statistical textbooks. I have an ecological dataset from an experiment I did that comes from a study I did testing for differences in organism density after environmental restoration. The experiment is setup with 3 plots where the restoration took place and 3 adjacent controls. 12 replicates sampling organisms in each plot was taken. I figure a 2 way anova is appropriate here but do I use the total densities of each replicate as my input or should I sum across my replicates?
Is this a valid way of measuring the effect of a variable?,16,10,False,False,False,statistics,1503863842,True,"Hi! I know little about statistics and am trying to learn, but have been tasked with taking ownership of a marketing test at work. We are trying to figure out ""which marketing image performs best"" which is measured by CTR - click through rate. The question would be something like ""do images with a color tint perform better than images without a tint?"" or something like that. Here is the doc I have inherited.
http://imgur.com/a/OlyNA
The variable names are on top and the overall CTR for that image is next to each column. Each image has a different combination of variables and that combination is entirely up to the whim of the individuals on the marketing team -
 there is no systematic approach. Then what is happening is that each CTR is being averaged for all images which share a given variable. 

This seems like it would be entirely invalid as it completely ignores interaction effects and the presence of any other variable. It isn't actually measuring the effect of any individual variable nor is it measuring the effect of the combination of variables - is it? We only have until friday to complete the test and do not have time to test every legitimate permutation. Is this method valid on any level? Is there any way to make it valid?

I recommended at the very least just testing each variable individually, so for instance if there are 10 variables then just introduce each variable in isolation to the original image (which we use as a baseline) as opposed to a random collection of them at once. 

Of course, this is to say nothing of tests of significance! 

Thanks"
I need help figuring out type of analysis for duration of therapy and adverse outcomes,9,3,False,False,False,statistics,1503880774,True,"Hi everyone,

I have a prospective cohort of 90 subjects who were previously on drug X, now on drug Y and followed for 24 weeks. Continuous dependent laboratory variables were checked at week 0 and at week 24. 

1) What type of analysis can I run to evaluate how duration of drug X affected variables at week 0?

I'll be running a paired t-test to evaluate changes from week 0 to week 24 while on drug Y. However,

2) What kind of analysis can I run to evaluate how previous duration of drug X correlate with changes while on drug Y?

Should I create a categorical variable for duration of therapy on drug X based off quartiles? Linear regression? I'm not sure what to do.

Thanks in advance!"
Economicshelpdesk.com Offers Best World-Class Experts to Help Your Statistics Assignments,0,1,False,False,False,statistics,1503907265,False,
Data shows that we might be getting tired of Twitter but not getting tired of Facebook,10,23,False,False,False,statistics,1503921024,False,
The democratization of data science education,0,2,False,False,False,statistics,1503924640,False,
A few questions related to unsupervised learning,3,5,False,False,False,statistics,1503928588,True,"Hey guys,

I have a few questions related to unsupervised learning. I am reading [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf).

1) Page 376 - How can we rewrite 10.3 as `1/n * sum((z_i1)^2)`? Also, why is `1/n * sum(x_ij) = 0`? Is it because we set mean = 0?

2) Page 376 - Why do we even assume that column mean is 0?

3) Page 380 - How can we approximate `x_ij` as `z_ij * theta_ij`?

4) Page 382 - Why is total variance defined the way it is? That is, why do we go through every feature, sum its *squared* observation values and then divide it by the number of observations? I know that a definition is a definition, but there's probably some reasoning behind it.

5) Page 414, task 3c) - Do I compute the centroid by taking the mean of all X1 and X2 components in that class?

6) Page 413, task 2a) - How can I know the height at which the fusion occurs?

7) Page 413, task 2b) and 2c) - If I perform single-linkage clustering, do I first look at the lowest number in the correlation matrix, then fuse those two, then look at the second lowest number, then fuse the class I previously got with that observation and so on? For complete linkeage, is the process the same, only that I look for the highest numbers?

8) Page 414, task 4 - How can I know this? There isn't enough information?

9) Page 415, task 5 - For the first variable scaling, it will perform the scaling based on the number of socks people bought. For the second variable scaling, it will perform the scaling based on the people who bought both socks AND computers and those who bought only socks. For the third variable scaling, it will perform the scaling based on the people who didn't or did buy the computers. Am I correct?

10) Page 415, task 6) - Can someone give me hints on this? Also, what does it mean that the ""principal component explains X percent of the variacne?"". I know from the text that principal components point in the direction among which the data varies the most, but I don't know how could I word the quoted phrase differently.

Thanks in advance!"
Looking for a place for statistics about preference of modern living .,0,1,False,False,False,statistics,1503929912,True,[removed]
Im interested in computer simulations? where should I start?,4,10,False,False,False,statistics,1503930534,True,Recent developments (Improbable and Waymos Carcraft ) have got me interested in simulations. I have a solid stats background and decent CS. Where to next?
Basic question about odds of something happeneing,2,1,False,False,False,statistics,1503931462,True,[deleted]
Is it possible to make a transition from econometrics to mathematical statistics? [X-post from /r/academiceconomics,8,1,False,False,False,statistics,1503940272,True,"I posted this in the /r/academiceconomics earlier today but that doesn't seem like the most active of subreddits so I figured I'd ask here too.

I've been thinking a little recently about what to do with the bachelor's degree in economics I'm going to have after this semester. I really like economics and I want to pursue a master degree in economics with a focus on econometrics, but it seems like most jobs that I will be able to do, could just as well be done by a mathematical statistician or even a physicist without too much training.

So since it's a little late for me to switch major after my fifth semester and prepare myself for a master degree in something more quantitative, I'm trying to figure out a way move to mathematical statistics through econometrics, perhaps by learning the theoretical and mathematical side of statistics on my own. What do you think, would this be a possible way to compete with the more quantitative degrees, like mathemetical statistics? If so, would it be the *best* way, short of starting over with a bachelor's in mathematics?

Thanks in advance."
When I specify an uncorrelated random intercept and slope in R (lme4) I get strange output...,13,3,False,False,False,statistics,1503945155,True,"This is the model I'm running:

    RT ~ AoA + voice2 + bilabial + labdent + alveolar + palatal +      velar + stop + fricativ + affricat + nasal + Unique + Vcat +  
    (0 + Vcat | Subject) + (1 | Subject) + +(1 | Item)
I intend this to have a random subject and item intercept, as well as a random subject slope for variable 'Vcat' (dichotmous, coded 1 and 0). I also intend the random subject intercept and slope to be uncorrelated.




And this is the output regarding the random effects




    Random effects:
     Groups    Name        Variance  Std.Dev. Corr
     Item      (Intercept) 7.665e+03  87.5505     
     Subject   (Intercept) 3.036e-03   0.0551     
     Subject.1 Vcat0       1.013e+05 318.2952     
               Vcat1       9.723e+04 311.8163 1.00
     Residual              5.223e+05 722.6838     
    Number of obs: 3341, groups:  Item, 77; Subject, 53

I don't understand why I seem to be getting two random slope values? Does anything have an idea?"
Need help figuring what type of analysis I need for research project.,2,1,False,False,False,statistics,1503956963,True,"I am trying to compare two means (ipsi and contralateral side values) on the same subjects (31 total) at 4 different time points. If I was comparing at one time point I understand I would use a paired t test. If I was comparing just one mean over the 4 time points I would use repeated measures anova. However, how do I see if there is a difference between the two means over the 4 time points? I will be using SPSS. Thanks for any help! "
Will employers of data analysts/BI analysts/data scientists/market research analysts pay for MBA degrees? And is there opportunity to move to management in these jobs. do we just cap and stay at senior analyst forever? What is the hierarchy like for such professions?,14,15,False,False,False,statistics,1503957155,True,
Impossible Statistic,12,0,False,False,False,statistics,1503964802,True,How many bugs have been splattered on the front bumper/windshield since the advent of the automobile?
Normalization and the Scaling Problem in Correspondence Analysis,0,4,False,False,False,statistics,1503966876,False,
How do I import data from excel into R?,1,1,False,False,False,statistics,1503974095,True,[removed]
Need advice with limited statistics comparison data,0,1,False,False,False,statistics,1503975893,True,[removed]
Interpolation/Extrapolation,4,3,False,False,False,statistics,1503977290,True,"The Prince, travelling through his domains, noticed a man in the cheering crowd who bore a striking resemblance to himself.  He beckoned him over and asked: ""Was your mother ever employed in my palace?""
""No, Sire,"" the man replied.  ""But my father was.""

I need help understanding a comedy article about inferences written in terms of extrapolation and interpolation.  It goes on to describe a series:

A C E ... K M O 

Then defines interpolation: G I
And defines extrapolation: R T V

Here's where I need help.  The article states the Prince's question was extrapolation.  The man's reply interpolation.  How come?  "
Linear Regression solving through LibreOffice Calc Spreadsheet application in Linux/Windows,5,3,False,False,False,statistics,1504014679,False,
Randomization in a Difference-in-Difference design,14,6,False,False,False,statistics,1504017098,True,"Does it make sense to randomize treatment among subjects when you intend to do a difference-in-difference analysis.

Does it offer advantages over systematic simple randomization followed by a pre-post analysis?

Thanks!"
"I have to do a ""lunch and learn"" presentation on segmentation analysis at work. What is a catchy title and any good ideas for presentation material?",0,1,False,False,False,statistics,1504031068,True,[removed]
[Academic Survey] I am conducting a survey on interpretation of statistical results in scientific experiments. Your responses would be very helpful,1,0,False,False,False,statistics,1504032689,True,"My research team at the University of Michigan is studying the interpretation of scientific results. We are conducting a survey of statistics, researchers, grad students and professionals who use statistical methods in their field of work. The survey consists of 2 pages and takes approximately 15 minutes to complete. Please fill out the survey, your responses would be very helpful! 

https://umich.qualtrics.com/jfe/form/SV_0lh5C7uoTKWs1IF?surl=rs4

Eligibility : You must be 18 years or older to participate.

Reward : 5 participants will be randomly chosen to win a $25 Visa gift card.

I will share the results with the sub once the project is complete. In the meantime if you would like to learn more about the study, please comment or email the team:

Prof. Matthew Kay: mjskay@umich.edu

Abhraneel Sarma: abhsarma@umich.edu

Mauli Pandey: maupande@umich.edu"
Help with interpreting a simple slope,4,2,False,False,False,statistics,1504033995,True,"I am currently conducting some research in my spare time. As part of my analysis I have created a simple slope graph, however I am struggling to put into words what the graph shows. Furthermore when reporting the graph what stats would I have to state? I have attached the graph to this post. (https://ibb.co/f8MJsQ)
As a bit of background, Cohesion is my predictor/IV, Social Identity is my moderator and group performance is my DV.

My main hypothesis is that social identity will moderate the relationship between cohesion and performance. 
"
Simple question about delta,4,0,False,False,False,statistics,1504036674,True,"Hi guys, 

I'm new to stats and I have something that I need to do. I need to look at the change between two timepoints for several assessments and then see if the deltas for these changes correlate with one another. I'm doing a regression analysis between two assessments to look at the latter. 

So I have times A and B. I want to see the change between 1 and 2 for assessment X and assessment Y. Then I want to look at the relationship between the change in Assessment X and change in Assessment Y. Again doing regression analysis for the latter.

To calculate the changes in times A and B which should I do B-A or A-B? Afterwards do I need to do anything else? 

Thanks!
"
Understanding the Math of Correspondence Analysis with Examples in R,1,25,False,False,False,statistics,1504044105,False,
What would it be called when the statistically correct choice is wrong?,0,1,False,False,False,statistics,1504046599,True,[removed]
Imputation of multilevel and longitudinal data,2,1,False,False,False,statistics,1504059735,True,"Hypothetically, let's say I have datasets where the rows are countries and columns are years. These datasets can consist of the change of inflation in a country from one year to the other, or the total population of those employed, or it could even be the GDP of the country.

I plan on taking hierarchical approach (continent, NATO or not, UN or not) and a time series approach (box-jenkins or something of the like), in addition to trying MICE (pretty much just an MCMC method).

My first question: how do I do multilevel imputation in R? I've been messing around with Stef Van Buuren's ""mice"" package, but I can't quite figure out the multilevel implementation.

My second question: Is there any way to incorporate the hierarchical and longitudinal component? I read somewhere that time dependent methods are used for hyperparameters in an MCMC. Is that a viable method or overkill?

Any other thoughts or advice would be greatly appreciated!

EDIT: Another idea I've messed around with is manipulating the data. As it stands, there are GDP dataframe, inflation dataframe, employment dataframe, and some others. As stated earlier, all dataframes have countries as row name and year or quarter of year as column. I can handle any reshaping or combining or anything if data manipulation is part of your advice."
Wanting to learn R - where to start?,0,1,False,False,False,statistics,1504087196,True,[removed]
Running a gage R&amp;R on a two part adhesive system,12,1,False,False,False,statistics,1504100428,True,"We need to run an R&amp;R study on our testing methods, say gel time. 

We have a set of materials 3 resins and 3 hardeners. 

We've set the test up so 4 people run the 9 tests twice. 

So it would be 

H1 v R1 gel time
H1 v R2 gel time

Etc

Do I take each pair as one part having a total of 9 parts. 

Or because it's a two part system do I have to treat it differently?"
Biostatistics PhD Chances from non-math background,10,16,False,False,False,statistics,1504101990,True,"I plan on applying to biostatistics PhD programs this fall, and I'm hoping to get some insight from people in the field or other PhD students on realistic chances so I know how high I should aim with my school choices.

  &amp;nbsp;

My B.S. is in Cellular and Molecular Biology (GPA 3.93), and I also have an M.S.E in Biomedical Engineering (GPA 4.00).

GRE: 164 verb, 168 quant

 &amp;nbsp;

I've been working/doing research in drug development at a rotational program at a pharmaceutical company for the past 2 years.  I've solely been working across computational functions (clinical development, bioinformatics, and systems toxicology), so I've picked up a variety of computational skills (R, matlab, unix environments).  On the other hand, all of my research experience in college was in a wet-lab and not all that relevant.

 &amp;nbsp;

Math courses I've taken:

- 2 years of calculus (through multivariable calc and diff eqs). 

- 1 undergrad intro to stats course

- 1 graduate level probability course (uses multivariable calc)

- 1 advanced calculus course

- 1 grad statistics course

- I'm currently taking a linear algebra online to satisfy the prereq

 &amp;nbsp;

Do I have a good shot at getting into top biostatistics programs despite not having a math or stats B.S/M.S?  Or should I aim a bit lower with the majority of my apps?  I would of course still throw a few 'hail mary' applications at harvard, etc. 

 &amp;nbsp;

As a note, I know my background is more suited toward a computational biology or bioinformatics program, but I really want to get the rigorous statistical training that is a part of a biostats program.  So I'm willing to join a lesser school in biostats rather than the number one program in comp bio.  I'm mainly deciding between research in clinical trials or genetics in my biostats program."
"Weibull distribution, expected unit failure",2,1,False,False,False,statistics,1504104213,True,"eta=21260 hours

population=1600

at time = 4000 hours

constant hazard rate.

Unit failures at time 4000 hours?

- does constant failure rate mean B = 1?

- Am I finding f(t) or F(t)? If it's F(t) what is y?
"
Data Project: Stats Feedback Greatly Appreciated!(X-Post R Data Science),0,2,False,False,False,statistics,1504106443,True,"I am trying to increase my stats knowledge by creating data projects which investigate data questions. The question in this particular project is: Do Major League Baseball teams really go on ""hot"" or ""cold"" streaks or are wins more or less randomly distributed based on overall team strength?
The project isn't quite fully flushed out as of yet, but I figure I could really use some direction before going too far down some rabbit hole.

You can see the project here: https://rawgit.com/j-v-k/MLB/master/index.html
I am trying to increase my stats knowledge by creating data projects which investigate data questions. 
The question in this particular project is:
Do Major League Baseball teams really go on ""hot"" or ""cold"" streaks or are wins more or less randomly distributed based on overall team strength?

The project isn't quite fully flushed out as of yet, but I figure I could really use some direction before going too far down some rabbit hole. 

You can see the project here: https://rawgit.com/j-v-k/MLB/master/index.html

The specific questions I have are below, but feedback of any sort is very welcome!:

* Does my ""Average Streak Size"" metric make sense, and does it make sense to compare real ""Average Streak Size"" to simulated ""Average Streak Size""?

* Are there any statistical pitfalls that I missed when simulating my seasons over 10,000 games rather then 162?

* Is my conclusion that the difference in the correlation between |Winning % - .5| and Average Streak Size in the Real and simulated data is indicative of a real ""Streak Effect""? 

* Did I use and interpret Welch's T-test correctly?

* Any other omissions or statistical errors?"
Predictors and outcomes in SEM models,4,3,False,False,False,statistics,1504109911,True,"I'm looking for some good sources and could use some guidance about the impact of misspecification in which an outcome is treated as a predictor in a SEM model. Are there consistent effects? What clues in the model (perhaps modification indices, etc.) might clue the researcher into this issue? Any good simulation study papers you guys could recommend would be appreciated. I'm clear on the basics of SEM, so I'm not really looking for an overview type paper. Thanks! "
My professor asked us to solve this problem today.,0,1,False,False,False,statistics,1504118788,True,[removed]
Average Die Roll (Keep the Highest/Lowest),3,2,False,False,False,statistics,1504120137,True,"I am trying to figure out a formula for dice. I have looked around online and haven't quite found exactly what I am looking for. I am creating an excel sheet that calculates the highest roll of a certain number of dice.

* Roll X Dice with Y Sides
* What is the average Highest/Lowest Roll?

I am not looking for a specific number to be rolled...just what the average roll will be. Can someone help me? Thank-you."
SOS-homework help,2,0,False,False,False,statistics,1504121389,False,
Independent t-test help,3,1,False,False,False,statistics,1504128846,True,"Hi all,

I have trouble solving this independent t-test question without being given the population mean (u).

Here is the the question and answer from the textbook: [Question](http://i.imgur.com/zGU3krc.jpg)
 + [Answer](http://i.imgur.com/XmOhAW2.jpg)

The formula (Formula #1) in the textbook is this:
http://i.imgur.com/aWmAdHA.jpg?1

I couldn't figure out how to get ""u"" from the question. In the past, we were given the population mean in the question, which i put in for ""u"".

I looked online for an alternative formula (Formula #2), and found this gave me the same answer as the textbook: http://www.informatics.jax.org/silver/images/D13.jpg

Basically instead of (M1-M2)-(u1-u2), I just did ""(M1-M2)""

While I'm satisfied that I solved the problem, I still don't understand how I would use formula #1 to solve the problem. 

(M1-M2)-(u1-u2) = (7.46-11.33)-(u1-u2)

Can anyone with a better understanding of statistics help me understand this, conceptually?

Edit: In this video https://youtu.be/jyoO4i8yUag?t=194 he shows the formula i'm working with, and says the ""mew1 - mew2"" is zero. Why is that?




"
"A nice talk: ""The Importance of Statistics: Lessons From the Brain Sciences"" (2017 JSM Fisher lecture by Rob Kass)",2,27,False,False,False,statistics,1504134081,False,
"Okay r/statistics, whose car is this?",1,7,False,False,False,statistics,1504134836,False,[deleted]
"Question about percentiles! I am reading research for a thesis and I encountered the phrase: ""exercise stress test less than the 25th percentile for tetralogy of fallot.""",3,1,False,False,False,statistics,1504140555,True,"Does this mean that its a good thing as in this person did BETTER than 75% of people, or they did worse than 75%. It says ""stress test less than 25%"" so maybe that can mean only a small amount of people get this good of a score, OR only do that bad.

The wording isnt great, maybe anyone has ideas? Or maybe just badly written."
Calculating Z-Score for population,2,0,False,False,False,statistics,1504141107,True,"I've been trying to figure out how to calculate the z-score for a population data set, but I keep getting a value error in excel. Can someone please break this down for me, clearly I am missing something somewhere. "
HELP! Incorporating several datasets into a combined regression + using results to inform mixed effect model,0,1,False,False,False,statistics,1504147990,True,"Hi /r/statistics, I am a pharmacometrician and am getting outside my comfort zone.  For those who don't know, my field is based around non-linear mixed effects models used to fit repeated observations in clinical trials to see how things (e.g. drug levels, effect on body) evolve wrt time on an individual and population level (among many other things!) 

I have several data sets that inform different parts of my non-linear model.  For each of the datasets, I separately run a model prediction.  For all of the datasets + model predictions combined, I minimize a least squares regression function using minqa in R.  My understanding is that in this approach, all observations are weighted equally. It seems to work pretty good.  But I wonder if this is statistically thorough.  

My question is: What if I thought a data set was less reliable than another? What methods may I use to weigh the observations (or data set as a whole?)

I am using a penalized regression to prevent parameter estimates from deviating from my initial estimates.  This serves two purposes: a) some parameters are ""less"" identifiable (so this stabilizes the regression) and b) parameters are based on other results which carry a certainty.

My second question is: I am assuming a ML (LS -&gt; ML if error is normal) approach, how would you weigh the penalty for parameter deviation?  I have confidence levels in the initial estimates.... 

So, the above discussion pertains to mean-level (population), informing the model, as there are no mixed effects.  However, I have individual level data that I would like to use to a) inform the parts of the model that are identifiable from the data and b) inform random effects/variabilility.

My third question is how properly incorporate results from the mean level regression into the population model.  A couple of my colleagues use a frequentist prior, should I?  
"
Help with interpreting simple slope!,0,1,False,False,False,statistics,1504148298,True,[removed]
Interpolation: Finding the real range with given info,0,0,False,False,False,statistics,1504148602,True,"Hi all:

How would one find the **GPA range** for the following data:

PERCENT RANGE	|		GPA RANGE
:--|:--
85-89		|	4.0
80-84		|	3.7
77-79	|	3.3

I would like a GPA listed for each percent from 77-89. For example, the real value of 80 percent is 3.54 (not actually)

How would one go about doing this? I spent almost an hour trying implicit strategies and formulas from my stats 1 textbook but was unsuccessful."
"Question about calculating odds, given two normal distributions",12,4,False,False,False,statistics,1504150808,True,"This is from the non-fiction book ""The Undoing Project"" by Michael Lewis, pg 186. I was wondering how the odds were calculated:

&gt; The average heights of adult males and females in the US are, respectively, 5'10"" and 5'4"". Both distributions are approximately normal with SD = 2.5""

&gt; What are the odds that an investigator has selected the male population if

&gt; The sample consists of 1 person whose height is 5'10"" (the answer is 16:1)

&gt; The sample consists of 6 persons whose average height is 5'8"" (the answer is 29:1)

I'm not sure how to start solving this problem and would appreciate the help. It's not a homework problem in the sense that it's not homework, but it's formatted like one. "
Advice: should I go for biostats MS or PhD?,0,1,False,False,False,statistics,1504152033,True,[removed]
Analysing Factorial Design Data,0,1,False,False,False,statistics,1504158005,True,[removed]
Question on calculating odds based on variables.,0,0,False,False,False,statistics,1504158979,True,"I am in an NFL survivor pool with a $10 buy. There is no limit to how many entries you can enter. It is a winner take all format....
What is the calculation for best amount of entries buy in based on entries in the pool? (For example, if there are 100 entries, you could buy in 1 time and your odds are 1-100 to win $990, or you can buy in 20 times and your odds are 20-120 or 1/6 to win win that same $990 but at a $200 risk instead of $10 risk)"
23 people birthday paradox as a computer program. Is this the right way to program it?,8,2,False,False,False,statistics,1504176159,True,"https://betterexplained.com/articles/understanding-the-birthday-paradox/

I'm looking at the 23 people birthday paradox and wanted to write a program to prove it.

I was going to represent the days as 365 and pick 23 random numbers between 1 and 365.

Then compare those to see if any two matched.

Somehow that doesn't seem like it would give me the correct answer, but it seems logical.

The random picking of dates in a year, seems right.  Picking 23 is right.  So the only problem would be in comparing the 23 numbers.

But the odds of 23 random numbers between 1 and 365 having a match wouldn't be 50/50 would it?

Where does this go wrong?"
Sports analytics difference in mean tests,0,0,False,False,False,statistics,1504178808,True,"Hi all,

I'm doing a sports analytics project (baseball). 
I have a database in which I've derived player statistics over a along period of time. These statistics are stored in different tables based on different conditions. For example I have a table with a list of players and their statistics in park A, a similar table for park B, etc..

I wish to conduct difference on mean tests to determine whether players perform better (or worse) under each of these conditions. 

I have previously done tests like this using r and stata however that is with the raw data as inputs and not tables of calculated statistics.

Does anyone have any suggestions about the best/most efficient way to conduct many of these tests?"
Animation of bayesian updates of the Beta distribution,1,28,False,False,False,statistics,1504182777,False,
Help with interpreting simple slope,0,1,False,False,False,statistics,1504186287,True,[removed]
Understanding Classification in Machine Learning,0,11,False,False,False,statistics,1504193239,False,
Noob/layman here. Need some help selecting an appropriate statistical test.,4,2,False,False,False,statistics,1504193530,True,"I have grad school level experience with stats, but I don't use it every day and that was a long time ago. Something has come up and I could use some help. Let's say we have a sample of n=300. Let's say the group was asked to perform the same task 5 times. For example, they were shown pictures of five products (one at a time, in random order) and asked whether they would consider purchasing each product. The results are as follows:

    Respondents selecting ""Yes""           %
    Product 1                             70
    Product 2                             40
    Product 3                             10
    Product 4                              4
    Product 5                              2

What is the proper way to analyze this data? Are respondents exhibiting a preference for Product 3 over Product 4, or is this just due to random error? Etc.

Thanks in advance for your help.
"
Regression question,1,1,False,False,False,statistics,1504195344,True,[removed]
Sketchy Statistics?,2,2,False,False,False,statistics,1504206337,True,"The author of [this article](http://jonathansacramento.com/posts/20170416_churn_model.html) uses a Bayesian approach to build a model for user churn.

However, at one step he removes the term, P(x months of inactivity | churn). He argues that it does not make any sense, therefore can be removed. I know that the reversed term of Bayes Theorem can be weird at first glance, but isn't it incorrect to remove it? My thought is that it can still be calculated by looking at inactivity lengths of users that did in fact churn.

What are your thoughts?"
Noob/Layman here. Need help with rainfall data analysis project.,3,3,False,False,False,statistics,1504210068,True,"Sorry if this has been discussed before. So basically I am writing a python program to visually represent rainfall data last 15 years. 

My question is how would you predict average rainfall for next year/month from set of available data? 

Accuracy is not important here. This is a college project where I am supposed to analyze specific data and draw some meaningful conclusions from it. A very simple model would help predicting average rainfall for next month/year. 

You have data for average monthly rainfall of different states for last 15 years. 
For eg: Average Rainfall in August '17: 90 mm
What would be average rainfall in September '17?

From whatever little statistics I understand linear regression model would be simplest to use. Help me with applying linear regression on this set, please. 
Also explanation with an easy example would go long way in helping.

Sorry if wrong sub. Since I am mostly stuck on statistical part, I thought this sub would be better than r/LearnPython

Tl;dr: Need help with a simple rainfall forecasting model using linear regression and predict rainfall next month.
"
Do I have a chance for Masters in Statistics? (in Canada),0,1,False,False,False,statistics,1504214794,True,[removed]
"Core texts in Regression, Nonparametric Stats, et. al.",18,9,False,False,False,statistics,1504215347,True,"Hi guys, I'm compiling a list of textbooks that I could best call ""core"" texts, or maybe ""classics"" from Stats.  For instance I picked, for Mathematical Statistics, Wasserman's All of Statistics.  I picked it because it:  1) is highly recommended on all the web forums I've searched, 2) covers all of the standard material an undergrad is supposed to learn by the end of a degree program, 3) is readable by an undergrad, 4) uses a fairly standard approach to the subject, and 5) it's just a little old and has stood the test of time.  That's kind of the perfect example book for what I'm looking for.

Part of the point of the list is to serve as a reference so that a person can decide if they know a subject adequately for an undergrad education.  Basically ""If I understand what's in this book, I'm qualified to say I understand the subject.""  

For Regression Analysis (/ Econometrics) I couldn't find a satisfying text.  All the ones I've looked at seem too hard for the usual undergrad.  Same for Non-parametric Statistics (although I wonder if this is actually a purely graduate-level topic and that's why I can't find readable books on it).  I'd appreciate any advice on these or other subjects.  Here's how I've broken down the subjects and selected books so far:

**Introduction to Statistics** 

**Mathematical Statistics**

*All of Statistics* -- Wasserman

**Regression**

**Stochastic Processes**

**Computing with Data**

**Survey and Experiment Design**

**Time Series**

**Probability Theory**

*A First Course in Probability Theory* -- Ross

**Bayesian Statistics**

*Data Analysis Using Regression and Multilevel Modeling* -- Gelman ???

**Nonparametric Statistics**

**Causation**

*Causation* -- Pearl

If anyone's interested, I'm keeping the updated list at my website, [here](https://www.axiomtutor.com/linksresources)."
Using Bayesian Kalman Filter to predict positions of moving particles / objects in 2D (in R),0,13,False,False,False,statistics,1504216122,False,
im looking for statistics on this?,1,0,False,False,False,statistics,1504218535,True,"i checked google they dont have anything,im trying to find a graph that shows the level of mental depression(or psychologicaly depression) rates from the 1800s until now?or even before that."
Humble Bundle on Stats/Data science books,10,82,False,False,False,statistics,1504221886,False,
Homoskedastic Time Series,0,1,False,False,False,statistics,1504232133,True,[removed]
MANOVA or multiple regression?,0,1,False,False,False,statistics,1504232195,True,"Hi everybody! I'm not sure I should use MANOVA or multiple regression.

I'm looking at how four different health behaviors change after an intervention is administered. I have pre and post measures of the health behaviors. I think any change in behavior is likely related to changes in psychological symptoms. I've given one measure for this. I thought I would use MANOVA because I have one independent variable (the psych measure) and four dependent variables (the health behaviors). I have made a ""change variable"" for the psych measure by subtracting pre scores from post scores to get one number, which is how much scores on the measure changed. I planned to use this change variable as the independent variable in my analysis.

It feels like I'm on the right rack, but something isn't quite right here. Should I be using regression instead? Conceptually, I want to predict change in health behaviors based on how the psych measure changed (i.e., people who improved on the psych measure show decreases in the health behaviors, while people who didn't get better don't show those changes).

Thank you for help and advice!

cross-post r/askstatistics"
Computer science minor worth it?,0,1,False,False,False,statistics,1504232386,True,[removed]
"What's the probability that FRB 121102's radio burst could include a frequency value that contains four 11+ digit prime numbers, factorially, when pi is raised to the power of the frequency?",7,2,False,False,False,statistics,1504237133,True,[deleted]
Is the Central Limit Thereon Correct application,0,3,False,False,False,statistics,1504260588,True,"I have have run a AB Test and identified independence using a CHI Squared test.  Now that  I have a result. I have been asked to give a probability on whether the results of the AB test will be lower than the findings.  So, essentially, the AB test revealed 62% conversion rate for a population of 80333. IF this variation is released on 500,000 customers what is the probability that the version will be under 62%."
Probability &amp; Statistics for Engineers &amp; Scientists,0,1,False,False,False,statistics,1504266279,False,
"Analysis on test group before and after an event, and comparing this change to a control group",0,1,False,False,False,statistics,1504279199,True,[removed]
Best tests for EXPLORATORY data?,2,8,False,False,False,statistics,1504285668,True,"I am in the social/behavioral sciences and am curious for those that have done exploratory studies, what statistical tests do you commonly use and why?"
"What is the expectation of Y given a joint distribution of f(X,Y)?",11,4,False,False,False,statistics,1504291513,True,"Just failed an exam from this...I know how to calculate E(Y) given the pdf of y, but what if you are given a joint distribution of X and Y? I calculated the marginal densities of X and Y and noticed they are not independent (since their product did not equal the joint density), so it did not seem to me that it would make sense to calculate E(Y) by taking E(marginal Y)...since Y has a dependency on X. So I tried to take E(Y) by taking int (joint pdf * y) dy - but apparently this is wrong. 

No internet searches are proving helpful either. It's just people asking about how to find the expectation of a joint density.
"
Data in one of my treatment conditions is not normally distributed. Question about how to handle this,2,1,False,False,False,statistics,1504291529,True,"Hi guys, 

I'm doing a bivariate correlation between the delta of 2 variables and I have 2 treatment groups. The deltas I'm using are from the original values (Time A and Time B). Time A in one of my treatment groups is not normally distributed and I want to see how my results would change if it was normally distributed. So I take it that that would require a log transformation.

I'm using SPSS and the deltas were calculated from excel and then uploaded into SPSS. So not sure if I would have to do a log transformation on excel? 

I'm new to R but I can also handle this in R if someone could guide me.

Thanks. 

"
[Career Advice] [Software] R or Python or else,0,1,False,False,False,statistics,1504296384,True,[removed]
What is the formal definition of 'p*',0,1,False,False,False,statistics,1504297726,True,[removed]
Where to start for workforce intraday analysis?,0,5,False,False,False,statistics,1504300128,True,"So just graduated from college with my BA in Statistics and Economics, been applying to some jobs, got an offer for a workforce intraday analyst position at a call center. After my meeting/interview I find out that this team is actually just one person doing all this forecasting and analysis which is crazy! Turns out this one person doesn't even have a statistics degree but she is in charge of doing all this analysis. But she knows a decent amount of what she's talking about and what she's doing. I guess the problem is her explaining to me what she wants from me to help her figure out somethings. 


Thinking back at my education I don't think we ever talked about intraday analysis, but I get it, your just essentially forecasting call volume to determine how to schedule our workers on a daily basis per hour. 

Anyways enough rambling, I just wanted to post here to see what kind of resources can I find online to help me on this adventure into intraday analysis. Like I definitely want to do my own studying about it at home so that way I can impress the company."
Adjusting bias in random forest response values,5,9,False,False,False,statistics,1504302471,True,"I have a question about the statistical validity of something I'm thinking of doing - it seems too simple to be valid.

Random forests (and all bagging approaches?) perform fairly poorly in the extremes for continuous prediction, under and over predicting at the high and low end.  This is a natural outcome of their lumping data together.  Understood.  Even with nodesize set to the minimum (1), and fiddling around with other parameters, that tendency is still there due to the random predictors utilized (I assume).

I'd like to continue to use random forests for my problem (tree densities) for other reasons.  But, I'd also like to address this issue.  The predicted values are, as expected, higher than my validation points on the low end (e.g., it predicts 100 trees for a given site, that site only has 80) and lower on the high end (e.g., predicts 2000 trees when there was 2200).

The plot between predicted and observed is quite linear, running from over prediction at the low end to well predicted in the middle to under prediction on the high end.

The simplest way seems to be simply fit a linear regression to the predicted vs. observed plot and adjust that way (not extrapolating).  The linear regression gets r2 of &gt;0.95, all the diagnostic plots look great.  

I can then apply the linear model ""adjustment"" to the random forest prediction, which has the effect of mostly eliminating that bias (it's of course not perfect, but better).  It doesn't artificially reduce the variance, which is good.  I'm essentially just adjusting the slope and intercept of the random forest model outputs.

So what is wrong with that ad hoc sort of approach?  It's not very satisfying for sure, but I'm not finding a better option online - seems like this under/over bias is an inherent part of the method. Is there a better way?


Edit:  The question is also posed here by somebody else: https://stats.stackexchange.com/questions/28732/response-distribution-dependent-bias-in-random-forest-regression

The regression adjustment is called ""naive"" but then sort of walked back.  I guess I'm looking for a second opinion.  I am not extrapolating anything, so it avoids that problem."
Help Finding Examples &amp; Solutions,0,4,False,False,False,statistics,1504305169,True,"Hi Folks, anyone out there in statistics land that would know of a good book that has examples and solutions for Bayesian classification. I'm looking for more examples similar to those presented in these videos:

https://www.youtube.com/watch?time_continue=5&amp;v=GlmS_jox08s

https://www.youtube.com/watch?time_continue=66&amp;v=CNpSrdnYvbo

Thanks in advance folks!"
Using Particle Filter for tracking objects in 2D (in R),1,10,False,False,False,statistics,1504311677,False,
15 AdSense Facts and Stats You Wouldn’t Have Ever Found,0,1,False,False,False,statistics,1504343602,False,
Know of any cool examples of boxplot usage?,9,9,False,False,False,statistics,1504370253,True,"I'm not a statistician and was hoping for trivial/unusual/surprising/fun papers. More specifically, I'm trying to find poor or fantastic usage of boxplots... "
What is the job market/salary like for a BS stats major in the big cities? What are some good resources for finding this information?,3,0,False,False,False,statistics,1504382844,True,[deleted]
How can I tell if I'll love this field?,5,4,False,False,False,statistics,1504384801,True,"I'm going to study Math along with Statistics+Operations Research this year.

**Background:**
Mathematics have always been my strong side. It's a compensation for my weaker language side.
While I don't do Math (pun intended) in my free time, I've always loved it and loved taking tests for the fun and challenge of it.
I learned programming in high school, and creating the final project independently was difficult so while I didn't end up creating what I had in vision, my project won as one of the 3 best in our school. I've been skeptical about programming, and it's hard to be accepted due to a very high PET demand.
So I thought to myself ""Perhaps I will try Statistics+Operations Research where I can also advance to programming and data mining if I wish to"".

**TL;DR:**
But aside from articles over the net, this profession is a stranger to me, so I'd like to know the best way to get to know it. If there's any info or personal experiences you can provide me with, I'd be most glad.

Thanks"
Bracket that ranks all losers,1,1,False,False,False,statistics,1504405508,True,"I know this may not be exactly what this sub is for but its the only thing I could think of
 I'm looking to make a list ranking around 100 things by competing songs 2 at a time to find an ordered list of my top 100. Does anyone know how to make a bracket/any programs that do this, without me having to manually make like 20 losers brackets. 
Thanks "
"2 separate regressions with the same dependent variable but different independent variables, what can I infer from the regression coefficients?",5,0,False,False,False,statistics,1504407395,True,"I conducted two separate regressions using the same dependent variable but different independent variables. Let's call the independent variable in the first regression as X1 and the independent variable for the second as X2. The regression results provide me with a regression coefficient for X1 in the first regression. And a regression coefficient for X2 at the end of the second regression. Assume the regression coefficient for X1 is 0.02 and the regression coefficient for X2 is 0.003. Can I say that X1 is a better predictor for the dependent variable compared to X2 based on the regression coefficients? Can I push this even further and say that X1 is probably 6 times (0.02/0.003 = ~6) the better predictor compared to X2?

**For further clarification**

I'm essentially trying to see if revenue (dependent variable or y) is affected more by market A (independent variable in the first regression). Or, if it's affected more by market B (independent variable in the second regression). I'm looking at revenue and market potential (in terms of $) values across 13 years to determine this. I know based on industry experience that market 1 and market 2 have some influence on company revenue. I'm wondering if I can use the slopes of each test to make a statement as to whether market 2 has a feebler impact on revenue compared to market 1. Is that reasonable?

**Update - multivariate regression**

I tried doing a multiple regression using market A and market B as independent variables. Noticed that the regression coefficient for market B had a high p-value. Plus the coefficient for market B (while small just like the regular regression) was negative! But the very same coefficient was positive under the regular regression. Market B has a high p-value of 0.92 while Market A has a p-value of 0.047. Can I use this to say that Market A works as a better indicator of the company revenue as opposed to Market B - which happens to be noisy?"
Regression: Unadjusted=non-significant but adjusted=significant. Explanation?,16,6,False,False,False,statistics,1504410923,True,"I am working on a paper looking at level of suicidal thoughts  (outcome) by HIV status (positive vs negative). so my outcome is two ways a composite score (range from 0-6, higher=higher level of suicidal thoughts) and I am looking at a yes/no item (have you thought of committing suicide). Basically my research question is are HIV-positive patients more likely to have suicidal thoughts than HIV-negative?

So for the score, I am using ANOVA/ANCOVA for the score as an outcome variable and  the item I am using Logistic Regression.

For both cases, the unadjusted model (so predictor is just HIV status), it is non-significant/borderline significant 

But when I adjusted for stuff (Age, sex, education, depression..and more)... HIV status becomes significant.

I am unsure how to explain that. Can someone help me understand what is going on behind SAS that after accounting for covariates, the independent variable became significant? 

is this a case of confounding?
"
when we make graphs...,0,1,False,False,False,statistics,1504428639,False,[deleted]
How do I report bootstrap results from SPSS in APA format?,0,1,False,False,False,statistics,1504447791,True,[removed]
"finding the 'spread' for binary result, what 'formula' should i be looking at?",6,11,False,False,False,statistics,1504457037,True,"Sorry I am not able to explain clearly in the title, as I don't really know the term.

So I have a bunch of binary results (true and false) with X amount of test data.
Example: method A showed 50% accuracy and method B showed 60% accuracy, but there are only 10 test data. How do I find the 'spread' of each method to say if the difference is actually significant??

Thanks, sorry for the noob question"
"""Getting Started with Particle Metropolis-Hastings for Inference in Nonlinear Dynamical Models"", Dahlin &amp; Schön 2017",0,25,False,False,False,statistics,1504473366,False,
Question about equivalence testing with covariates for partial correlations and regression,0,2,False,False,False,statistics,1504476713,True,"Hi everyone,

I've been struggling with some analyses for a while and was hoping I could get some advice. I need to run some equivalence tests to test our hypothesis that our IV is not correlated with or predictive of our DV. However, I can't seem to find any info about how to do this when accounting for covariates. Has anyone had any experience with this that could offer some direction? Thanks!"
ELI5: How do I interpret a likelihood ratio,5,3,False,False,False,statistics,1504487640,True,I'm going through research articles and I don't understand what likelihood ratios are nor do I understand positive/negative likelihood ratios. Please help. 
"If a survey has been done before with, let's say 30 questions, based on which a score has been calculated, and the following survey contained a lot less questions, for instance 12, how can be checked whether (and how much) significant the change in the two scores is (statistically speaking)?",0,1,False,False,False,statistics,1504513729,True,[removed]
Analysing differences between 0-100% data,0,1,False,False,False,statistics,1504518644,True,[removed]
Books on R?,0,1,False,False,False,statistics,1504530959,True,[removed]
I suck at maths. Does this mean I will suck at statistics as well?,9,7,False,False,False,statistics,1504544912,True,"I have a background in *medical sciences*. Actually I have an MPharm but given the curriculum I'd place myself more towards the interdisciplinary side of the field.

Whereas I have a little knowledge about statistical inference and dabbled on my own with exploratory statistics, I do not feel I fully understand the underlying theory behind the how and when - I know how and when to apply some statistical methods but lack in understanding why use them and what exactly do their results tell me.

In the Uni our maths covered the subject up to the differential equations and singular integrals. And whereas I feel at home while doing anything molecular I feel...overwhelmed when I see equations in front of me, even though they represent nothing more or less than the molecular structures and/or formulae - pattern recognition and deduction.

I did start to work on understanding the more advanced topics of maths namely linear algebra series from 3blue1brown but the question whose answer eludes me still is - just how much math does one need to understand in order to understand even basic statistics? Are there any auxiliary branches / competences one should strengthen alongside pure statistics?


Cheers!"
How to handle data higher than measurement range,5,2,False,False,False,statistics,1504550561,True,"I have a dataset where one outcome I am measuring is a ""range"" - this will be either a number (between 0.1 and 20 microns), or ""greater than 20 microns"" because the range is bigger than our instrument is able to measure.

How should I handle this variable statistically? Should I make the entire variable categorical with binning, e.g. '0 to 5 microns', '5 to 10 microns', etc? Or is there a better way to handle this? 

(I am a grad student with access to R or other stats tools if need be.) Thank you so much in advance!"
Calculating Reliability of Subjects' Standard Deviation on a Test (Not Average),1,1,False,False,False,statistics,1504563371,True,"Hi all. I am trying to determine test-retest reliability for a test that involves 6 trials where subjects responses are angle from 0 degrees (ie, +1 degree or -1 degree). I have 35 subjects who completed the test two times (total of 12 trials, 6 trials for each time tested). However, mean response does not give much information on this test; someone who responds +6 and -6 (avg = 0 degrees) will look the same as someone who scored +1 and -1, despite performing completely differently on the task. Likewise, taking ""absolute value"" average performance collapses standard deviations so now someone who scored +6 and -6 looks like someone who scored +6 and +6 (more consistent). Therefore, I cannot rely on the test/retest reliability of the mean; I would like to calculate test/retest reliability of the subjects' standard deviations. All formulas for Standard Error of Measurement / ICC utilize Mean and not standard deviation; does anyone know what equations I can use to estimate a subjects' expected consistency instead of their expected accuracy?

TLDR: Average does not give as much information as standard deviation on a task, would like to calculate/estimate expected test/retest standard deviation, if possible."
Question about interaction terms and mediation in SEM,6,6,False,False,False,statistics,1504563845,True,"Dear statisticians of reddit. I have a question about how to interpret some results. For the sake of brevity, I have three variables A,B, and C. The argument is that A has direct effects on C but is also partially mediated through B. I take two modelling approaches. The first is linear regression model were C=A+B+(A*B). In the second approach, I use a path model to test the mediation effect. The path model shows a straightforward mediation effect. The regression model, however, shows a negative sign on the interaction term coefficient. Is this what one might expect with the interaction term approach?"
Help with survival analysis,0,1,False,False,False,statistics,1504566318,True,[deleted]
Gradient Boosting Machine book or paper recommendation?,1,1,False,False,False,statistics,1504567066,True,"Title.

Studying decision trees and such. I got random forest and cart down but GBM is a bit difficult would love a book for it.

Thanks!

update:

solved."
Looking for a good open source stats program or spreadsheet. Any recommendations?,8,5,False,False,False,statistics,1504567790,True,Thanks for any advice. 
Recommended Statistic MOOCs?,7,18,False,False,False,statistics,1504571099,True,"Hi. Anyone use MOOCs to as a refresher for statistics? If so, which ones would you recommend? Thanks!"
Please help,0,1,False,False,False,statistics,1504571454,True,[removed]
Can someone explain this 'risk' statistic?,4,1,False,False,False,statistics,1504573691,True,"From https://en.wikipedia.org/wiki/Heart_failure

&gt; In the year after diagnosis the risk of death is about 35% after which it decreases to below 10% each year.

I can read that a few different ways. Does it mean that after one year, the risk is below 10% and continues decreasing?

"
How to Intepret Survival Analysis Output ?,3,1,False,False,False,statistics,1504577293,True,"Hello all. I have been playing around with a STATA breast cancer dataset. I am focusing on the commands: 

    clear
     
    use http://www.stata-press.com/data/cggm3/bc_compete
     
    stset time, failure(status = 1) 
    stcrreg drug age, compete(status = 2)
    
    sts list

The regression gives me the output: 

    Competing-risks regression                       No. of obs       =        423
                                                     No. of subjects  =        423
    Failure event  : status == 1                     No. failed       =        135
    Competing event: status == 2                     No. competing    =         78
                                                     No. censored     =        210
    
                                                     Wald chi2(2)     =       5.07
    Log pseudolikelihood = -794.79099                Prob &gt; chi2      =     0.0794
    
    ------------------------------------------------------------------------------
                 |               Robust
              _t |        SHR   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
    -------------+----------------------------------------------------------------
            drug |   .6845084   .1215754    -2.13   0.033      .483278    .9695282
             age |   1.006517   .0107129     0.61   0.542     .9857373    1.027734
    ------------------------------------------------------------------------------

And I am interested in understanding how the coefficients affect what I assume is the baseline survival function ?


             failure _d:  status == 1
       analysis time _t:  time
    
               Beg.          Net            Survivor      Std.
      Time    Total   Fail   Lost           Function     Error     [95% Conf. Int.]
    -------------------------------------------------------------------------------
         3      423     38     11             0.9102    0.0139     0.8786    0.9338
         6      374     35     21             0.8250    0.0186     0.7850    0.8582
         9      318     27     10             0.7549    0.0214     0.7100    0.7939
        12      281     20     10             0.7012    0.0230     0.6535    0.7436
        15      251      2      2             0.6956    0.0231     0.6477    0.7384
        18      247      2      6             0.6900    0.0233     0.6418    0.7331
        21      239      2      2             0.6842    0.0234     0.6357    0.7277
        24      235      0      1             0.6842    0.0234     0.6357    0.7277
        27      234      1      5             0.6813    0.0235     0.6327    0.7249
        30      228      1      0             0.6783    0.0236     0.6295    0.7221
        33      227      0      2             0.6783    0.0236     0.6295    0.7221
        36      225      2      2             0.6723    0.0238     0.6232    0.7164
        39      221      1      1             0.6692    0.0239     0.6200    0.7136
        45      219      1      3             0.6662    0.0240     0.6168    0.7107
        48      215      2      0             0.6600    0.0241     0.6103    0.7049
        54      213      0      1             0.6600    0.0241     0.6103    0.7049
        60      212      1    211             0.6569    0.0242     0.6071    0.7020
    -------------------------------------------------------------------------------

  Essentially, now that I have estimated the parameters, how do I input data from an individual's characteristics to obtain a probability of him failing in time t ? 

Thanks !"
Here is a very interesting glossary of common terms in Statistics!,4,35,False,False,False,statistics,1504610482,False,
Why dont we use Degrees of freedom all the time in statistics?,2,2,False,False,False,statistics,1504611417,True,[deleted]
Interpolated median... but not,6,0,False,False,False,statistics,1504612187,True,"I have been looking at trying to calculate a median from a function for my distribution of two continuous variables. In this case, personal income by age.

I wish to calculate the median for different age groups but long story short, I cant use the sample data outside of my work environment due to confidentiality and producing medians for every conceivable age breakdown is inconvenient.

In R I have just been producing a function and integrating the age range of interest and solving for half the integral to find a median/midpoint in that section of the curve. It seems similar to an interpolated median because I can generate a median at an interpolated point on the function rather than using an actual sample value.

I realise I havent created a new method so my question is what the heck is this called ?

I cant seem to find reference to this technique anywhere.

TL;DR - What is it called when you use integrals to estimate the median of your distribution rather than use actual samples?"
5 Tips on How to Stay Organized As a Statistics Student with SAS Tutors Online,0,1,False,False,False,statistics,1504612989,False,
MANOVA: What does vectors of means mean?,0,1,False,False,False,statistics,1504616110,True,[removed]
Question about MANOVAs,0,1,False,False,False,statistics,1504616716,True,[removed]
"What websites do you read daily, or weekly?",5,2,False,False,False,statistics,1504627100,True,"Everyone has blogs, news sites, or content aggregators that they follow daily.  I typically follow Hacker News every day, look at Andrew Gelman's blog once or twice a week, and look at Arxiv.org when I remember to.  It's been a while since I've seen this asked on r/statistics and I want to find some new sources of good content to break my normal habits!"
"Can't land an internship, would independent projects boost my resume?",9,9,False,False,False,statistics,1504632443,True,"I'm currently in my last semester of an Applied Statistics M.S. degree at Purdue. I have great grades and a good looking resume and cover letter. However, I have had no luck in landing any sort of practical job experience. I'm thinking of doing a few independent projects to showcase my practical abilities. Do you think this would be effective for landing a job after I graduate? Or do you guys think I should keep trying for internships? Any advice would be appreciated! "
Career options for CFA/MS Stats?,0,1,False,False,False,statistics,1504632487,True,[removed]
"Wondering about the best way to see if people were more likely to choose Option A vs Option B, for multiple types of questions.",4,1,False,False,False,statistics,1504633402,True,"Let's say I present participants with pairs of animals (this isn't what I did in reality, but it will serve as an example). One member of each pair is always a cat, the other member is always a dog.

Their task is to choose the member of the pair that is the best match for a certain adjective. Let's say there are three different kinds of adjectives: those describing positive physical traits, those describing positive personality traits, and those describing positive mental traits.

My original intention was to analyze this data with mixed effects logistic regression. That is, I would separate the data into the three different subsets based on which kind of adjective was shown. For each adjective type I would run an intercepts only model in which choosing a dog was coded as 1, and choosing a cat was coded as 0. I would have a random subject intercept and a random item intercept (i.e., which pair they were shown; the same cats and dogs were always paired together). This would tell me if participants were more likely to choose dogs vs. cats as having positive physical traits, positive personality traits and positive mental traits.

I would then also be interested in adding predictors to the model to see how they affect things (e.g., whether the participant is a cat or a dog owner).

But, this strikes me as a very simple analysis. Is there a better way for me to analyze this data?

In addition, if this is the best approach, is there a way to see if the additional predictors remove the overall effect? Would their addition making the intercept insignificant tell me this?"
What is the accuracy of the Spaghetti hurricane models as a function of temporal distance into the future?,0,1,False,False,False,statistics,1504643032,True,"In other words, is there some kind of a statistical curve in which accuracy and temporal distance into the future are inversely related for these?"
Question about Reddit statistics,0,1,False,False,False,statistics,1504648272,True,[removed]
"Looking for crash course in standard deviations, z-scores and odds",6,19,False,False,False,statistics,1504653105,True,"I've built a website for players of a collectible card game to track their match stats and compare those with the general population. A player chooses their deck each matchup is against a random opponent with their own deck. 

In a sample world, on my site, a player would log a match where they played Deck A against Deck B and won.

Then the aggregated data would show that Deck A is 40% of all decks played and has a win % of 50% against another Deck A (by default), a 55% chance against Deck B and 48% against Deck C. Then Deck B is 30% of ""the meta"" and has a 60% win % against Deck C.

Now, I know how to calculate an expected win % based on that data (and I vaguely understand that those ""meta"" percentages should move towards equilibrium, but the nature of the game makes people irrational actors).

However, the object isn't always to maximize win percentage. There are 15 round tournaments where, in order to win, one must first advance to the top 8, which typically requires a 12-3 record or better. Having a 67% win rate doesn't amount to much with 0 variance, as a 10-5 record is ultimately as good as an 0-15 record.

My understanding to this point is that I can calculate how many standard deviations a deck is away from 12 wins as use that as a guide for which deck has the best chance to advance.

However, I'm not sure how to convert that to a odds.

Here's is what I've worked through so far:

* I calculated expected win rate and variance for each deck

* I calculated standard deviations to 12 wins with this math:

(12 wins needed - (Win % * 15 rounds))/(variance * 15 rounds)

* I calculated the mean and standard deviation of the set of standard deviations

* I used those to calculate a z-score.

The fewest standard deviations to 12 wins was 1.10 and that correlated to a z-score of -2.71. Looking at [this chart](https://en.wikipedia.org/wiki/Standard_score#/media/File:Normal_distribution_and_scales.gif), I'm guessing that means something like 1% to advance. I'm not sure how to get there mathematically and I'm not sure if that changes based on the size of the tournament or if the ""meta"" being incorporated in the expected win rate means that it scales perfectly.

So first, am I on the right track? If so, how do I get to the finish line? And if I'm not... help, please."
Alternatives to AUC,9,7,False,False,False,statistics,1504675370,True,"Hi all,

I am currently working on a model trying to reproduce the presence/absence pattern of an organism on a landscape. My model produces a raster of population growth rate, strictly positive. I validate the model with a raster of presence/absence, binary, using AUC. 

My issue is is that AUC doesn't tell the whole story: the model performs okay, but reproduces the pattern on the landscape very well, except in a single discrete area on the landscape, which drives down AUC. 

I of course will be reporting the mediocre AUC value, but I'd like to see if there's any alternative validation measures I can report as well. For example, is there a metric that summarizes pattern fidelity? 

Thanks!"
Help with deciding on what statistical test to use,20,4,False,False,False,statistics,1504689576,True,"Hello /r/statistics,

I am currently doing my internship at a company, and they have asked me to determine the influence of various variable parameters in the productionprocess on the particle-size of the end-product.
Unfortunatly, during my education statistics has not really been a widely covered subject, which is why I do not have a clue on which statistical test to use.
Can anyone suggest which statistical test is most suitable for my project and if possible, where I can find more information regarding this test so that I can educate myself on it.

Thanks in advance!"
ANOVA with multiple imputed data,0,1,False,False,False,statistics,1504694840,True,[removed]
How can I determine whether or not to apply the central limit theorem?,5,1,False,False,False,statistics,1504707466,True,"If the sample size n is 20,and the sampling distribution X~Bin(n=20,p=0.8),need to know P(x&gt;14).So I can't calculate the sample proportion and use the Z table to get the probability?CLT should only be applied when np and np(1-p) all larger than 5?What if the sample distribution is uniformed or something else?I'm a little bit confused. "
"Can someone ELI5 on the difference between Association, Causation, and Correlation?",13,13,False,False,False,statistics,1504708516,True,"I think I understand causation pretty good. It's when one variable directly impacts the outcome of another variable? 

In some text I find says association and correlation mean the same thing. But the way my professor was talking about them in class made it seem as if they have different meaning. 

Please eli5 thanks in advance. "
What’s the difference between Mathematics and Statistics?,13,0,False,False,False,statistics,1504720197,False,
How to test if gender impacts mortality rates when already considering age,0,1,False,False,False,statistics,1504722951,True,[removed]
Does my method work for controlling variables in a Linear Discriminant Analysis?,0,1,False,False,False,statistics,1504724333,True,"Say you have IQ and Academic Achievement (AA) and you create a partial correlation with both of those variables and covary them with Socioeconomic Status (SES). 

From this video ( https://www.youtube.com/watch?v=GUNPXLRk_60 ), I learned that, if you want to graph that partial regression in SPSS, then you can use the residuals of two regressions, IQ with SES and AA with SES. 

Thus, if I am understanding this correctly, the residual of each of those regressions is the value of the dependent variables (IQ and AA) controlled for SES, correct?

This leads me to my current question. Can you control for variables in a linear discriminant analysis? 
In SPSS, there is no option to, which makes sense because of the nature of the classification function. However, what if I regressed each of the individual dependent variables in the LDA on typical covariates, such as age, sex, and race, then used those residuals of the dependent variables for the discriminant function? 

Would that technically control for the variables or is there some problem with this that I am not seeing?"
A Better Ranking System than the one currently used in College Football [COLLEGE FOOTBALL APPLICATION],0,1,False,False,False,statistics,1504725024,True,[removed]
Advice for recent grad who is bad at programming,23,13,False,False,False,statistics,1504726664,True,"I just recently got my Bachelors in Stats and most of the stat related jobs I can find require heavy programming abilities. Unfortunately besides having taken not that many programming classes, I'm in general just bad at programming. Any advice for any other jobs or careers I should look into?"
Alternative statistical theory texts to Bain and Englehardt,5,1,False,False,False,statistics,1504732620,True,[deleted]
How to choose parametric family of a parametric survival model's baseline hazard?,2,3,False,False,False,statistics,1504735920,True,"Hey everyone, I'm currently in Puerto Rico holding on for dear life. I have been reading about survival analysis because it's the perfect tool for a problem in tackling currently. 


Initially, I thought a Cox proportional hazards model would be great because it seems to be the favorite model out there. However, I intend to use the results to project currently censored individuals to the future and that is not possible with Cox  (since baseline is never really specified).


Because I am interested in inputing covariates getting output ""probability of failure at time t"", it seems a parametric survival model will be optimal. Most of the literature I have found indicates that a simple exponential distribution works remarkably well for tha vast majority of uses, so I'm inclined to try that. However, for my precise problem, there will be competing risks and I believe that they work fundamentally different across time. 


A bit of background on the issue. I am trying to estimate probability of departure from a company for each individual in it given a vector of covariates which include: division, wage, age, position...The competing risks are that a person can leave by will, be booted or retire. I expect retirement hazard specifically to be very low at the beginning of tenure and to spike up at the end of the span. 


What parametric family works for what type of hazard ? Is there a systematic way of determining the specification based off the data, or is it just intuition ? 


Im sorry if this is stupid, I started reading about survival analysis yesterday. Also, sorry for the spelling mistakes, I have no electricity and typed this on my phone. 


Thanks a lot for your help. Hope everyone affected by Irma stays safe. 

"
Help settle an argument?,0,1,False,False,False,statistics,1504740502,True,[deleted]
Settle an argument?,18,1,False,False,False,statistics,1504741132,True,"I'm going to preface this by mentioning that I know very little statistics. Obviously arguing about whether or not god exists is pointless, but my friend made a claim that bothered me so I thought I'd post it. He said that all religions are equally likely, including the absence of a god. Is that accurate? We were assuming infinite religions, not just the ones practiced currently, if that makes any difference."
How long do you have to go without causing a car accident to be a statistically better than average driver?,13,8,False,False,False,statistics,1504747191,True,"Basically if a 16 year old got their license yesterday, they (hopefully) won't have ever caused an accident.  However, their lack of accidents is just because they don't have enough ""data points"" yet rather than their superior driving skills.  At what age do you have enough driving history to be a better than average driver if you've never caused an accident?

I don't know much about statistics, but I would think it could be answered in two ways.  One would be once you pass the average time between accidents.  

Another answer could be once you pass the average age for people's first accident.  Assuming you become a better driver with age you might at the current moment still be worse than the ""average"" driver but on track to be a better ""lifetime driver"" if that makes sense.  "
Supplemental materials for text,2,1,False,False,False,statistics,1504753265,True,"I'm taking a course using the text ""probability and statistics: the science of uncertianty"" by evans and rosenthal and i find that the text lacks explanation. Can anyone suggest supplimental materials to help fill in the holes? 

During calculus i really enjoyed using pauls notes from lamar uni. If you know of and could share something similar i would really appreciate it! 

Your's, at least until shit gets weird, trully

L to the tum tum "
Extra Course Work for MS in Statistics?,2,1,False,False,False,statistics,1504760698,True,"I'm currently a senior Finance major about to graduate in the spring. I want to go on into a graduate degree in statistics eventually, however I don't think I have the mathematical background to get into any good programs. I have 2 years of programming experience during undergrad, as I was a double major for a while, but somehow I have not taken linear algebra or any advanced calculus. Only discrete math and a basic statistics course. My question is, in order to make my application more acceptable, should I enroll in some of these classes I need at a University after I graduate, and before I prepare my application? Or will this not have an effect? Also, are there specific statistics programs that best prepare someone for the field of data science? I've researched the top schools for statistics, but don't exactly know how well their programs translate into the real world. Thanks!"
Is time a discrete or continuous variable?,0,1,False,False,False,statistics,1504760734,True,[removed]
JMP 10 does not work with OS 10.12.6,0,1,False,False,False,statistics,1504761541,True,[removed]
Creating Layered Data Visualizations Using R and Plotly,0,1,False,False,False,statistics,1504768093,False,
"If correlation doesn't imply causation, how can you determine that the correlation does imply causation?",51,16,False,False,False,statistics,1504771968,True,"I did a first year stats course a few years ago, didn't find it too hard or put in too much effort (I'm a maths major so it's a similar style of thinking), but I can't remember much at all from it. 

Help please?"
Question regarding interacting dummy variables,3,4,False,False,False,statistics,1504785927,True,"I'm having a hard time understanding why we do it? Right now I'm assuming it's because it's the additional 'benefit' when both dummies equal 1. Example: regressing wage on gender education and marital status. Say we have an observation that is male (=1) and is married (=1) we include the coefficients for both those dummies. Male dummy coefficient is the benefit/loss to wage. And the same is for marriage. My question is when we interact the two, is it the coefficient the  'bonus' from being male AND married? Any info will be useful thanks"
Forecasting future currency exchange rates with recurrent neural networks,0,17,False,False,False,statistics,1504788055,False,
Proper test to compare two data sets of center of pressure distribution over time?,1,1,False,False,False,statistics,1504794947,True,"I am looking for the best analysis for comparing two different devices that record measures related to balance (center of pressure sway) over time. 

Essentially I would have two data sets in say a 30 second period with coordinates, and I want to see how each coordinate correlates to each other but over that entire period. I know a Pearson correlation can be used, but I feel like I've heard of a rather specific test that can be used to compare two large data sets that compare point by point over time. 

So for example data point 1 in device A vs point 1 in point B at say .01 seconds, 2 to 2 at .02 seconds, 3 to 3 at .03 seconds, etc. "
How rare is it?,1,0,False,False,False,statistics,1504796481,True,[deleted]
Where can I find the most comprehensive set of data on Deaths Demographics?,3,1,False,False,False,statistics,1504801735,True,"
I am looking to do a statistical analysis on Deaths going back as far as possible, with respect to day, time, age of death, and cause.  Does anyone know where I would find such data?"
I am not smart enough for graduate school in Applied Statistics,45,32,False,False,False,statistics,1504803959,True,"This week I’ve had my two graduate courses (Applied Statistics – using the book: Introduction to the Practice of Statistics by Moore, McCabe, Craig) and (Probability Distributions – using the book: Introduction to Mathematical Statistics by Hogg, Allen, Craig). I’m a bit worried about the Probability Distributions class since I have no experience with mathematical proofs. In undergraduate, I’ve done a minor in Mathematics (Calculus 1-3, Linear Algebra, Differential Equations, Numerical Analysis, Probability and Statistics), but none of it was taught with proofs, only applications. Now I’m worried about graduate school because the professor gave us 3/5 proof-based questions for homework. One question about unions and intersections and limits of nondecreasing and nonincreasing sets. Another question about a set integrated over a sample space in 3-dimensions (triple integral with spherical coordinates). Another question about proving the inclusion-exclusion formula for the union of three sets and also a generalized version for a finite amount of sets. One question on probability and a final one on a proof of the “continuity theorem of probability”. 


It’s hard for me. I get lost and confused with all of the notation, especially with the generalized inclusion-exclusion formula. Professor is not very helpful, he just said to use induction, but I was never really taught all that. I tried to look this up online how to do the problem and there are always more than one solutions or no solutions at all. I am ashamed to say that it took me over 10 hours to understand and prove Demorgan’s Laws. Maybe I don’t deserve to do a master degree in Applied Statistics. I worry that if it took me so long to understand something simple like Demorgan’s Laws then it will be next to impossible to understand complicated topics in statistics. I don’t want to fail out of graduate school and disappoint my parents and waste their money. I am sad thinking about this. 
"
JMP vs Minitab (for scripting),0,1,False,False,False,statistics,1504805177,True,"I need to script some routine statistical reporting for some control charts.  Anyone have experience with both of these scripting languages and have an opinion on which they prefer?  I vaguely prefer Minitab in this case because I find it's quality control stuff a bit more robust, but I have other business reasons for considering JMP.

The script won't be doing anything too crazy, just pull data from a server excel file which will also include constants for limits/point estimators

I have a decent amount of experience coding.

Also, for business reasons R and Python are not alternatives."
How to find Pearson correlation for arrays using SPSS?,0,1,False,False,False,statistics,1504805646,True,[deleted]
M.S. in Stats worth it?,0,1,False,False,False,statistics,1504815059,True,[removed]
"I need help determining if the following example is Experimental, or quasi-experimental",0,1,False,False,False,statistics,1504815546,True,"I have come across this example and I am having difficulty determining. This is not for homework, but is more a philosophical question about the nature of designs. If you have an answer I would love a source so I can read up on it. I have found myself enjoying philosophy behind statistics more than anything else. Here it goes. 


IV: Racial non-minorities vs racial minorities; Gender; Time
CV: Years of having been a therapist 
DV: Depression reduction in symptoms


Design: people are assigned to therapists that either match or not match their gender and racial status, with the possibility of 4 groups. 


Gender Match and Race Match
Gender Match and Race mismatch
Gender Mismatch and Race Match
Gender Mismatch and Race Mismatch


They are given a pre and post test and years of having been a therapist is held constant. 

I am scratching my head. My gut tells me non-experimental but I could be wrong. 

Again I appreciate you taking the time to read. And sources would be much appreciated as I am very much intrigued. 
"
MS in Stats worth it?,0,1,False,False,False,statistics,1504816461,True,[removed]
Calculating average,0,1,False,False,False,statistics,1504820427,True,[removed]
"Is there a way to generate p-value given two means, along with their 95% confidence intervals?",7,5,False,False,False,statistics,1504821156,True,I don't have access to the raw data - just the mean with a 95% low/high confidence internval. Is there a way I can generate a p-value or some sort of metric like the p-value?
Probability theory help,7,1,False,False,False,statistics,1504821442,True,"There's another post on here that sounds similar to my situation. I got into my PhD program for biostats through unconventional means (molecular biology background, but I'm decent at programming and am technically-oriented). So now I'm taking an intro to theory of statistics course and it feels like the carpet has been whipped out from underneath me. I did a minor in math in undergrad (which was, now, several years ago), I've always been ok at math, but these probability theory questions are curve balls from every direction. 

We are using Introduction to Probability by Blitzstein and Hwang. I've been doing all the reading, watching his lectures on Youtube, and trying to go through and prove to myself the concepts that he shows in the text. I can mostly follow when a problem is explained retrospectively, but I have trouble applying these concepts to isomorphic problems. I've spoken to the professor and he says he'll sit with me during office hours to go through questions I have, but we go so fast that I don't think this will be enough. I've already failed the first two quizzes of the class, and there is a screening exam next summer that will test my understanding of these things.

Do you know of resources online that have practice questions with solutions that break down why a problem was solved in a certain way? I think I just need to practice the same kinds of questions over and over until it sticks. I'll do some Googling on my own, but I figured you all would probably know better than I.

Any help is appreciated. "
"Using bootstraps to get CVs in SAS, R, or STATA, which is best?",4,2,False,False,False,statistics,1504838668,True,"I have survey data with 1000 bootstraps already attached.

So far I have extremely complex programs in SAS to get the CVs of the **change** between the means of two monthly estimates.

I know there's proc surveymeans but that doesn't seem to work for proportions.

**Anyone know if R or STATA is better than SAS for conducting these types of bootstrap tests?**"
What is a term for the data that stem from the first phase of fielding something?,5,0,False,False,False,statistics,1504871620,True,"I am going to analyze data over a period of two years. I am planning to not implement the first 6 months, as the particular field I am studying is just fielded, and I dont want my data to be affected by the initial low experience level. How can I describe this? Are there better academic terms to use?"
"Do the number of dummy variables in a regression have an impact, (specifically an increase), in the multicollinearity/VIF?",0,1,False,False,False,statistics,1504876131,True,[removed]
"Assistance/advice needed for one-way ANOVA, randomized block design, with multiple measures per treatment in each block",5,3,False,False,False,statistics,1504885805,True,"Hello,

I've been having trouble figuring out how to analyze the data I have collected in an experiment. I can't find any instance suggesting how to treat replicates in my data set.

In brief, I have a continuous output variable, four treatment groups, and four blocks. Within each block there is a full/balanced data set (3, 3, 3, 3 or else 4, 4, 4, 4 measurements). The problem I face is how to deal with these replicate measurements. I note that it's very straightforward when there's only 1 measurement per treatment per block.

I think it's reasonable to go ahead with calculating the mean of the replicates and then using these means to 'pretend' there was only one measurement each. However, I'm wondering if there is a better way of doing this that makes better use of all the data I've collected? Is there a fair way I can make my F-test degrees of freedom go from F(3,9) to F(3,49) or something like that?

Thanks so much."
Two different groups have different shaped distributions - which test to use?,18,7,False,False,False,statistics,1504905742,True,"So today I gathered a whole lot of data. I have two groups and I want to determine if those two sets of data are significantly different from each other. Based on the experimental design, I felt that t-test would be most appropriate. So, I checked the ""rules"" before using t-test and as it's probably well known, ""A t-test is most commonly applied when the test statistic would follow a normal distribution"". However, in my case, the data in one group conforms to normal distribution, while the data in other does not. As such, I believe I am not to use the t-test. What is the correct approach, then? "
Unbalanced factorial design-ANOVA?,1,3,False,False,False,statistics,1504914508,True,"I have a 2x2x2 factorial design. However,  I am missing a treatment data altogether.  My study is store x type of fat x presence of stirring on the amount of crystals. I am missing the data for a fat type from one store. How do I analyze this data? I cannot redo the experiment. I have used a 3-way ANOVA for the other balanced experiment. There is no literature data that I can assume. 

If I were to individually compare the each fat type (in this case 3, since one is missing) by one way ANOVA? or arrange the data as 6 data sets and analyze the entire set by one-WAY ANOVA ?
Store1-fat2-stirring1
Store1-fat2-stirring3
Store2-fat1-stirring1
Store2-fat2-stirring2
Store3-fat1-stirring1
Store3-fat2-stirring2
I am missing Store1-fat1 data"
Elementary Statistics : Probability Rules : Treatment of Experimental Da...,0,1,False,False,False,statistics,1504914615,False,
"Alternative names to risk, risk difference, and odds ratio",0,2,False,False,False,statistics,1504923747,True,"I am curious if anyone has seen alternative names for risk, risk difference, and odds ratio when looking at data that takes the 2x2 table form. For me it seems the ""risk"" does not capture the generality of the formula. ""Risk"" makes sense in biostats, but is an odd word for marketing. It seems that a more appropriate word would be rate. Has anyone seen alternative terms?"
Resources for understanding the goals and reasoning behind linear and mixed effect models for someone with basic stats background,4,21,False,False,False,statistics,1504928122,True,"Hi all, would anyone have any recommended books, lectures or anything of the sort to getting into and understanding what linear and mixed effect models (emphasis on the mixed) are?"
Comparing whether a second test is as good as the first one,7,1,False,False,False,statistics,1504930449,True,"If I have two tests, with test A being treated as the gold standard aka the truth, and I have the number of true positives, true negatives, false positives, and false negatives, how do I figure out how good the second test is in comparison to the first test aka the gold standard?"
What is Data Science?,0,0,False,False,False,statistics,1504942798,False,
Introduction to Statistics using NumPy,4,7,False,False,False,statistics,1504966855,False,
Interpreting overlap in credible intervals between parameters and their transformations,0,1,False,False,False,statistics,1504971459,True,[deleted]
Bernoulli trials and their basics.,0,5,False,False,False,statistics,1504977753,False,
multicollinearity and interaction terms,7,1,False,False,False,statistics,1504986038,True,"I have a few models where I am testing for moderation by using interaction terms, where y=x1+x2+x1*x2. Unfortunately, the interaction term is showing a negative sign. This appears to be because of multicollinearity with x1 and x2. When I run a model just with the interaction term the sign is positive as predicted. I have already centered the variables by standardizing. What are some strategies for dealing with multicollinearity? "
are the linear regression coefficients for a factor in a two way anova identical (or related in anyway) to the fixed effect estimates for that same factor in a mixed model?,2,13,False,False,False,statistics,1504986567,True,"from what I understand, the coefficients in linear regression are calculated using OLS and the fixed effect estimates from a mixed model are determined using ML (which gives the same results as OLS depending on the assumptions), so assuming I'm using the same dataset and specifying the correct model then I should be getting the same values for the coefficients right?
"
Learning the normal distribution through interactive sampling,0,1,False,False,False,statistics,1504995084,False,
Checking for Population growth,0,1,False,False,False,statistics,1504996237,True,[removed]
Need to take 3 of the following courses for a Stats minor. What should I take?,15,5,False,False,False,statistics,1505007034,True,"If it helps I plan on using this to help mainly with industry, and not so much theory. 

* Regression Analysis

* Applied Bayesian Statistics 

* Experimental Design and Analysis

* Surveys, Sampling and Observational Data

* Stochastic Processes

* Structural Equation Models

* Applied Multivariate Statistics

* Statistical Computation

"
"Is there a ""best practice"" for selecting random effects variables in a mixed model?",9,13,False,False,False,statistics,1505011137,True,"I've learned a bit about mixed models recently, but one subject that still eludes me is the ""best"" way to determine what random effects to include. I have a mountain of variables that *could* improve the model I'm working with, but I don't want to do anything until I have a better idea of what to do with them. Right now the only thing I've included as a random intercept and slope is subject. I've gathered that that's an appropriate thing to do in a repeated measure design. 

Does it make sense to construct a model from the bottom up, testing variables one at a time, rejecting ones that fail to improve the model? Do I test every permutation of model and select the one that best balances accuracy and complexity? Is there a package for R that helps test random effects? Finally, are there any major pitfalls that I should know to avoid before I start this endeavor?

thanks!"
how helpful would a statistics Masters degree be in landing jobs?,3,0,False,False,False,statistics,1505014199,True,"I will be graduating with a B.S degree in Finance from a no-name school and will move to live in San Francisco forever after graduation (parents live there). How helpful would a masters degree in statistics be for me to land jobs in BI, product analysis, market research analysis, risk analysis, corporate finance ,etc?  "
Stats jobs related to renewable energy,18,12,False,False,False,statistics,1505019274,True,"Hey guys i am curious if there are any jobs for statisticians that are related to renewable energy? I have 1.5 years left in my business mgmt degree and would love to find out which skills i should build. I dont want to be a manager and i dont want to have to deal with clients all day. Id rather be part of a team who rarely has to talk to clients but gets stuff done and works together... kind of like a bunch of electricians on a big job site who can joke around a bit and swear and dont need to worry about a customer coming in. 

I like graphs, numbers (if they have to do with renewable energy), and the like. However i dont think i want to be an accountant"
PCA and matrix algebra,4,7,False,False,False,statistics,1505029515,True,[deleted]
Undergrad advice: Stats/Econs or Stats/CompSci?,10,0,False,False,False,statistics,1505058157,True,"I think I have seen something like this before, but I don't think the advice really applies to me.

Basically, I'm majoring in stats. I'm currently thinking of either picking up a second major in econs or minoring in CompSci. You might ask, why major in one or minor in the other? Stats graduates often perform much better when they are proficient in a programming language (Python, R, SAS etc.) but I feel like the CompSci major at a university level focuses on more of the theory parts, such as understanding the workings of computer processes itself, and not learning languages which I would be more interested in.

Long story short, I'm leaning more towards taking up the Stats / Econs double major (which would greatly benefit me as I'm planning to go into actuarial science in the future, and self-learn the aforementioned programming languages) as opposed to Stats / CompSci. But then again, I'm open to all suggestions, so pls send them my way.

Thanks in advance guys, this community is awesome!"
"Other than Machine/Deep learning, what CS courses are best for applied stats/data analysis?",19,16,False,False,False,statistics,1505068510,True,"Barring introductory programming/data structures, obviously. 

I'm interested in doing applied statistics and machine learning, and thus I plan on taking CS courses in machine learning/deep learning. I'm still only a Sophomore in college, so I have plenty of time to do more CS classes. A lot of them seem at least somewhat interesting, but I'm not sure which ones are the best to take (both for direct usefulness in stats, and looking good to employers).
The CS courses are roughly broken down into three categories:
Theoretical, e.g. Theory of Computation/Algorithms: Seems interesting, though they seem less practical (especially the theory of computation).

Systems, e.g. Intro to Systems, Distributed, Operating, Security : These seem the most directly applied, though they don't seem to be directly useful for anything stats related. They seem the least interesting, and are by far the largest time commitment (20-30 hrs/week).

Other A.I. Courses, e.g. Computer Vision, Natural Language Processing: Seem semi-related to ML, but I'm not sure if the connection is strong enough to warrant taking them over others.

I'm not entirely sure what I should take, any thoughts?"
What's wrong with using just trend for time series analysis?,1,1,False,False,False,statistics,1505089659,True,"Been a while since I took a time series class... So here goes.

I remember in my old time series assignments I would remove trend and seasonality, do the model fitting and what not, then add the trend and seasonality back to see my results.

For my current analysis, I want to use one time series to predict another... one time series is popularity of a food item in one forum versus popularity of the same food item based on google trends.

So for example, one clothing item is a particular kind of frozen dessert. Obviously, there's seasonality here, so I remove that, but I'm actually interested in the trend - for this data I found that the google trends time series is fairly similar to the forum time series, just lagged a year later.


Bonus points if you have suggestions on how I can use one series to predict another..."
How do I calculate overall probability of students in an excel sheet? I keep getting an error....,1,0,False,False,False,statistics,1505090596,True,"I am supposed to calculate the following: 
The overall probability of students graduating at each of the three universities.
The overall probability of students having a publication at each of the three universities.

J.W. Blake	2560	2304	876
K.R. Cunningham	599	599	300
R.H. Doughty	472	425	162
L.M. Edwards	1027	904	434
W.H. Greiner	138	121	44
I.D. Jackson	817	809	307
O.P. Lawson	2218	2018	726

The first number after the name is standard for the # of students taught, the second one is students graduated and the third one is for publications.


I am typing the following information into the cell sheet =prob(c2:c15) but I keep getting an error. Can you help?"
Data Scientist Marketing Journal Suggestions,4,6,False,False,False,statistics,1505095181,True,I got a position in marketing data science. I am curious what some of the best journals in this field are. I am only aware of the asa journals. I would love to hear of a good journal to keep me sharp.
ESPN NFL Game Prediction Accuracy,0,1,False,False,False,statistics,1505099777,True,[removed]
ELI5: Covariance (I keep getting differing ideas),5,6,False,False,False,statistics,1505102652,True,"As far as I have been explained covariance (in terms of correlation and regression), the following have been explained to me:

Covariance is how two variables change together - if one goes up, the other does too. If one goes down, the other does, too.

Covariance is a third confounding factor. Like age when comparing gender to depression.

Covariance is also known as a covariant, which doesn't tell me anything.

I've googled, youtubed, and am still stuck. Sure, I may be a dumdum, but I would love anyone's opinion."
"When Correlation Is Not Causation, But Something Much More Screwy",4,70,False,False,False,statistics,1505113206,False,
Bayesian Updating Given Uniform Prior,8,1,False,False,False,statistics,1505116457,True,"Hey everyone, I had a question from my grad class that confused me a bunch. I know about and have googled Bayesian updating with priors, but everything I found so far has shown me how to update given the result of a few coin tosses, for instance. In the question i have, that part is missing and it is confusing me about how to proceed. Here's the question:

Suppose that a biased coin is thrown once. Estimate the probability of heads, theta, under the assumption of a uniform prior distribution on theta. (i.e. the pdf of prior distribution is g(theta) = 1 if 0 &lt;= theta &lt;= 1, and g(theta) = 0 otherwise.)

As you can see, it says the coin is tossed once but it doesn't say anything about whether it is heads or tails. If anyone can clarify what to do here, I would really appreciate it."
"How would I justify the use of intervals in classifying an individual as no evidence of nomophobia, at risk of nomophobia and nomophobia?",1,0,False,False,False,statistics,1505120598,True,"I have the following intervals for classifying an individual as no evidence of nomophobia, at risk of nomophobia and nomophobia?

as no evidence of nomophobia: 0 - 10
at risk of nomophobia: 11-25
and nomophobia: 26-50

I used a pilot graph and then made these intervals based on evaluation. I checked what other people may have done, but it was clear that they just made some intervals up. Can someone recommend a way to justify these intervals?

By the way, an individual is given a ten-item questionnaire from a 6-likert scale from 0-5. Individuals then choose answers and it is then added up. From the adding up of scores, they are then placed in one of these intervals. Someone with an added up score of 23 is in the risk of nomophobia category. 

Thanks."
What is anova in a simple linear regression comparing?,0,1,False,False,False,statistics,1505137616,True,[deleted]
How do you pick which schools to apply to for an MS in Stats?,1,0,False,False,False,statistics,1505141931,True,"So I'm applying for Masters in statistics this upcoming cycle and was wondering what criteria I should use to pick schools to apply to.

Here are my stats

Program: Statistics. Masters for the most part

Interests: Sports statistics and geospatial data for the most part, but I'm generally open to various topics.

Undergrad Institution: UNC Chapel Hill. Graduated in 2017

Undergrad GPA: 3.792

Undergrad Major: Statistics, Economics. Minor in Hispanic Literature and Cultures

GRE: Have not taken yet, but I'm a good test taker and did well on SATs in HS.

Quantitative Courses: Calc 1-3 (A), Discrete Math w/ proofs (A-), Differential Equations (A), Linear Algebra (B, the professor was the worst one I've ever had), Object Oriented Programming (A), Game Theory (A-), Deterministic Models (A), Probability (A), Econometrics (A-), Stochastic Modeling (A), Mathematical Statistics (B), R Programming (A), Time Series (A), Game Theory (A-).

I'm missing Real Analysis, which could be a big deal.

Research: I had one research assistant position at Duke, but it was completely unrelated to statistics. I also wrote a Senior Economics Thesis that is fairly quantitatively heavy and got highest honors. 

Letters of Rec: Two econ professors, who should be good. 1 stats professor that I'm not too sure about"
Confusion regarding P-values in hypothesis test,11,2,False,False,False,statistics,1505158547,True,"So say I'm performing a hypothesis test, with 95% confidence, which gives me the test statistic 1,8 (P=0,0359).

This information can tell me that i am NOT 95% confident that µ ≠ x . Because that requires the p-value to be less than 0,025

BUT at the same time i can say that ""I am 95% confident that µ &gt; x) , because for some reason the p-value just have to be less than 0,05

Doesn't that contradict itself? I mean, if its ""most likely more than x"", would that not also mean that its most likely (even more likely) to ""not equal"" to x? Been looking around on internet for a decent explanation but haven't found much. (I don't know what to search for honestly) 

Very thankful for any explanation or link helping me out here! "
Does anyone know of a good source to learn about fuzzy hypothesis testing?,0,8,False,False,False,statistics,1505163531,True,
Stat Question Help,1,0,False,False,False,statistics,1505172791,True,[deleted]
Project in Advance statistical methods,3,0,False,False,False,statistics,1505177767,True,"Hello guys, I am seeking some opinions for project in Advance statistical methods?

Thanks in advance "
Data on where Stanford Statistics Department faculty members received their PhDs,0,1,False,False,False,statistics,1505200435,True,[deleted]
Split plot in time with three factors,0,1,False,False,False,statistics,1505201918,True,[deleted]
Can I combine probabilities (negative predictive values) in this scenario?,21,2,False,False,False,statistics,1505203542,True,"Imagine I have two tests. One can detect diabetes in general, but doesn't give information about the type of diabetes. It has a negative predictive value (NPV) of 85%. I have another test that can detect diabetes type II with an NPV of 80%. 

If both tests are to be used, is there some way to combine these NPV probabilities in terms of diabetes in general? If both tests are negative, it seems like the NPV for ""diabetes"" would bit a bit higher than just 85%. But I'm not sure, since the 2nd test says nothing about type I diabetes. 

This is a theoretical question so you can also imagine it being applied for something where test 1 tests for ""leukemia"" and test 2 tests for ""leukemia of the AML type"" - basically any pair of tests where the 2nd test is for a subgroup of the first."
Data on where Stanford Statistics Department faculty members received their PhDs,1,30,False,False,False,statistics,1505207418,True,"If you frequent the math subreddit, you may have seen some posts about collecting data on where faculty members in math departments received their PhDs (if you're curious, here is the data for [Stanford](https://www.reddit.com/r/math/comments/5rqz9u/collected_data_on_where_stanford_mathematics/), and here is the data for [MIT](https://www.reddit.com/r/math/comments/6tyqp0/data_on_where_mit_math_department_faculty_members/)). I did these mainly because I was bored, I like data, and PhD data is easily accessible using the [Math Genealogy Project](https://genealogy.math.ndsu.nodak.edu/). I recently learned that the Math Genealogy project has data not just for Math departments, but also for Statistics and other fields, so I thought it would be fitting to do this data collection for a Statistics department. I chose Stanford to do this because it has been widely regarded as the 'top-ranked' Statistics Department for a long time, and I think it's interesting to see which universities produce the most faculty members that end up at top departments. Doing this sort of analysis for math departments also shed some light for me on just how difficult it is to become a tenure-track professor at a top institution. 

The following table shows data on where all tenure-track faculty (including Emeritus faculty) in the Statistics Department at Stanford received their PhDs (foreign universities have their country listed next to them):

Institution|Number of PhDs
:--|:-- 
Stanford|7
Berkeley|4
Harvard|3
Columbia|2
Cornell|2
McGill (Canada)|1
Montpellier (France)|1
Perugia (Italy)|1
Technion (Israel)|1
Washington|1
Wisconsin-Madison|1

If we include all post-docs, research associates, adjunct faculty, and other non-tenure track academic staff, we add the following numbers:

Institution|Number of PhDs
:--|:-- 
Berkeley|2
Stanford|2
Bern (Switzerland)|1
Chinese Academy of Sciences (China)|1
Duke|1
Florida State|1
Harvard|1
Iowa State|1
Leiden (Netherlands)|1
MIT|1
Princeton|1
Texas Tech|1
Tsinghua (China)|1
UChicago|1
UPenn|1
USC|1
UT Austin|1
Yale|1

Some comments:

1. Statistics Departments seem in general to be a lot smaller than Mathematics Departments, so there was less data for me to collect than when I did this analysis for math departments. For instance, Stanford only has 24 tenure-track faculty members in the Stat Department, but when I did this analysis for math departments, Stanford had 38 faculty members while MIT had 68. 

2. It probably isn't a surprise to you that Stanford and Berkeley, usually considered to be the top two Statistics Departments, are the two schools that produced the most Statistics faculty members at Stanford, with Stanford itself producing the highest number of faculty members. Harvard also has a very well-regarded Statistics Department, so it probably isn't too surprising to see Harvard in third place. However, I think there are a lot of fantastic Statistics Departments that are underrepresented/absent. For instance, UChicago, Carnegie Mellon, Duke, UPenn, etc. all have very respected and well-ranked Statistics Departments (all top 10 according to US News), but there are no faculty members from any of these schools. Columbia and Cornell are well-represented despite being ranked 20 and 24 respectively. 

3. Several faculty members/academic staff actually didn't get a PhD in Statistics. There were a fair number of people who got their PhDs in other affiliated fields, like Mathematics, Operations Research, Computer Science, etc. For instance, on the second table, there are schools like Princeton, MIT, etc. listed, and these schools don't even have a Statistics Department! One research associate in particular got a PhD in Government. There seems to be a wide range of disciplines that can lead to doing work in a Statistics Department.

3. The non-tenure track chart seems to have a more diverse selection of schools than the tenure-track chart. The tenure-track chart is a lot more 'top heavy' with mostly elite schools represented. This probably isn't surprising to you.

4. This is only one ""snapshot"" in time of the statistics faculty at Stanford. I make no claims about the historical trends of the PhD programs that faculty members there have attended. There aren't a great deal of data points due to the smaller faculty size, and the data set is probably too small to make any generalizations. I just did this mostly because I enjoy collecting and seeing this data, and I put it out there in case this data is interesting for you too. Someone better at programming than me could probably do this sort of data collection on a much wider scale, which I think it would be very interesting to see. "
2016 American Community Survey Content Test: Telephone Service,1,2,False,False,False,statistics,1505215618,False,
What are my chances of getting into grad school for Statistics with a 3.2?,11,7,False,False,False,statistics,1505217710,True,"I am a math major and took a few stat electives.

My coursework includes probability, statistics, regression, sampling theory, calculus 1-3, real analysis, diffeq, linear algebra, and numerical analysis. I've received B's in nearly every pure math class and A's in the statistics classes. I've also done research under a professor (not published) and relevant internships. 

[EDIT] Thanks for the advise everyone! 
"
Guide on building your own neural conversational agent,1,14,False,False,False,statistics,1505225007,False,
Problem setting up probability table,0,1,False,False,False,statistics,1505228621,True,[removed]
Where does the square come from in the population variance and standard deviation formula?,0,1,False,False,False,statistics,1505248403,True,[removed]
Understanding power and its relationship to Type 2 Error?,2,1,False,False,False,statistics,1505258807,True,"Hey guys,

So I was wondering what the intuition was between these two events and how to understand them. I know what a type 2 error is: probability of incorrectly accepting the null and I know what power is: probability that you correctly reject the null.

One thing I would like to know is the intuition between the two and how to visualize it. I know there are formulas but is there some explanation which ties these together?

Thanks for reading"
Getting into Grad School for Applied Stats,23,8,False,False,False,statistics,1505259407,True,"Hello /r/statistics. I am a recent graduate who has been struggling and trying to break into the data analysis field with no luck. I have since decided to try and get into the data entry field and no luck either. So I have decided to go get my Masters in the Stats for Fall 2018.

I got my BA in Applied Mathematics with a Concentration in Probability and Stats. What types of challenges will I face in making this jump? I graduated with a 3.0 and want to go to a state school. Will scoring higher on the GRE help me out even though the program I want to go to doesn't require it? I am currently working at a dead end movie theatre job where the Operations Manager is making it pretty unbearable to work at. My girlfriend wants me to stay but I want to leave and find employment elsewhere while getting my degree.

Any advice would be appreciated. Thank you."
Does there exist someone faster than Usain Bolt today?,1,0,False,False,False,statistics,1505267458,True,"There are (serious) problems with estimating the likelihood of faster runners than Mr. Bolt from record-setting dash times that are both obvious and subtle. Humor me by imagining none of them exist.

--------------

Usain Bolt is the fastest human measured for the 100m dash. However, given the small number of athletes, it seems likely that the ""true"" fastest human alive is sitting on a couch somewhere and has never attempted a competitive running career.

I am trying to use the fact that the difference between samples at the tails of the normal distribution become smaller and smaller. I'm using this to compute the likelihood there exists someone faster than Usain Bolt by comparing Usain to the 2nd fastest, 3rd fastest and so on.

To do this, I'm trying to compute the largest value that exists beyond ""Usain Bolt"" by taking the derivative of the normal distribution's CDF with respect to y, raising that to the nth (where n is about 7,000,000,000 or the number of samples less than the ""maximum"" - the logic behind this is described in the [German Tank Problem](https://en.wikipedia.org/wiki/German_tank_problem) Wikipedia page which generalizes among different distributions), e.g:

    $\int_{0}^{\infty}y f_{Y_N} (y)dy = \lambda n \int_{0}^{\infty} y \left [ \tfrac12\left[1 + \operatorname{erf}\left( \frac{y-\mu}{\sigma\sqrt{2}}\right)\right]  \right ]^{n-1} \frac{1}{\sqrt{2\pi\sigma^2}}\, e^{-\frac{(y - \mu)^2}{2 \sigma^2}}dy$

1. Is this a valid way to compute the probability that there exists someone faster than Usain Bolt?

2. Is there a name for this sort of question outside of ""German Tank Problem for other distributions""

3. Is there a good way to estimate standard deviation from the extreme samples of a distribution? Finding information about the fastest 100m dashes of all time is easy, finding averages &amp; variance is hard)

Thank you for your patience in dealing with a programmer without a background in the topic.

"
Daniel Kahneman replies to a critique of studies on priming,15,77,False,False,False,statistics,1505271827,False,
Need some guidance (undergrad stats major),0,1,False,False,False,statistics,1505271830,True,[removed]
Questions about linearization of a non- linear model?,1,1,False,False,False,statistics,1505274751,True,"I took statistics last year and I just wanted to refresh my self because we're linearizing data in Chemistry and had some questions to add on to it.

1. If you log just the y value but keep the x values the same, is it a power model or exponential model?

2. If you log both x and y, is it a power model or exponential model?

3. Why does using log (or ln????) linearize data? 

4. Is there any significant difference between -log and log of a model? (this may sound like a stupid question but I had to choose between which one was a better model in chemistry and it was -log because it made the regression formula positive not negative, -x)
"
"Statisticians and Biostatisticians in industry, how much did/does the ranking of your school play a role?",1,3,False,False,False,statistics,1505279401,True,"I always hear that the quality of your school only matters for your first job. When I browse linkedin profiles and team pages of companies that I would like to work for I see A LOT of well regarded schools (think top 25 USNews rankings, and especially top 5 biostatistics when it comes to biotech).


 Is mobility actually limited by your alma mater or is it more the confounding of the types of people that get into those schools in the first place?"
Correlation =/= causation. Then are correlated variables still considered as explanatory variables?,4,1,False,False,False,statistics,1505280440,True,
"Help, please!",0,1,False,False,False,statistics,1505307337,True,[removed]
Textbook Suggestions,3,0,False,False,False,statistics,1505311597,True,"I come from a an undergrad background in Math and consider myself well-versed in probability, but I haven't done much in terms of statistics. Currently, I'm beefing up my R, SQL, and Tableau skills, but I was wondering if anyone had any suggestions for textbooks (paid or otherwise) that don't shy away from the theory or math. I've done digging online, but most of the resources seem catered to people who don't like math or haven't had much formal education in it. Long story short, any book suggestions that are more on the applied side with theory baked in and isn't programming-focused would be greatly appreciated!"
Getting U.S. Energy Data from the Energy Information Administration,0,1,False,False,False,statistics,1505315971,False,
How do I find the probability of every one of my measurements to be within a certain range?,0,1,False,False,False,statistics,1505319447,True,[deleted]
How to find the increase factor between two non-linear variables.,0,1,False,False,False,statistics,1505323298,True,"Hello, 

I have a question related to one of my school assignments. I have two variables- time and temperature. I want to know how much temperature is increasing for each n+1 in time. What method should I use to get to the solution?

Time(s) | Temperature(C)
--------- | ----------------
0 | 20
4 | 57
8 | 89
12 | 108
16 | 131
20 | 145
30 | 176
40 | 191
50 | 204
60 | 210
70 | 215
80 | 217
120 | 220

Thank you very much in advance!!!"
How do I calculate the standard deviation of normalized values?,1,1,False,False,False,statistics,1505324014,True,[deleted]
Supplements on Liklihood Theory,3,1,False,False,False,statistics,1505330553,True,"I'm a new Ph.D student in biostats coming from applied math.  Consequently,  likelihood theory is new but not completely foreign to me.

Can someone suggest some readings to helo supplement my understanding?  Specifically, I'd like to read more on properties of the MLE (i.e. consistency, efficientcy etc) and regularity conditions."
2.77,7,6,False,False,False,statistics,1505334852,True,"I'm working on a stats project that is killing me.  Basically our organization has been using these kind of obscure ASTM tools for managing a gage variance monitoring system.

Most of it makes sense.  For instance there is standard deviation broken out by different levels.  Multiple site, multiple operators, etc..  good so far.

What doesn't make sense is this factor, 2.77, that is hidden inside one of the formulas.  It is (effectively) the z-score for one of those different levels of variance, and they say it's for the 95th percentile.  On it's face of course that makes no sense.  

Now in statistical process control there are these correction factors you use to unbias sigma.   So the smaller your sample size the more you (potentially) underestimate sigma, so you use this correction factor to unbias it.

But that is just too big for a correction factor to get there.  In an N=2 situation it only goes up to a factor of 2.5.  

But then there's this part of me that hears 2.77 and I feel like I know it from somewhere.  I actually spent some time this afternoon just googling that number.

Anyways.  Any thoughts on how you get a 95% area from sigma (or a biased point estimator of sigma) with a z score of 2.77? 


"
"PhD students, what is your focus, when did you figure it out?",15,13,False,False,False,statistics,1505337688,True,"Title says it all, but for further clarification:

I'm currently in the process of applying to PhD programs. Should I have any idea at the moment what I want to focus on when/if I pass qualifying exams? There are a lot of things I'm interested in, but I don't feel like I would even know where to start to narrow down a focus. Is this normal? (For the record, my undergrad is in stats as well.)

Anyway, I'd also love to hear what people are studying now and what it entails!"
Choosing a PhD Path,0,1,False,False,False,statistics,1505338488,True,[removed]
Looking for good resources on working with cell phone sensor data.,0,2,False,False,False,statistics,1505338941,True,"I'd post this in /r/DSP, but it's pretty much dead.

I'm working with sensor (accelerometer/gyroscope/magnetometer) data gathered from a mobile device.  In a nutshell, we have k drivers and we want to build a model that can determine which of the k drivers is driving.  There's a paper I went through that does a similar analysis, but there are some glaring flaws in it (e.g. they didn't take temporality into account when doing crossvalidation.)  It also uses data that's far less noisy than what we have.

What good books or resources can you recommend for building this kind of a model?  I looked into time series classification, and while it seems useful, it primarily deals with detecting events within a signal, rather than determining which of k individuals produced a given signal.

What are some good resources for learning to work with this kind of data?  There seems to be a dearth of information on the internet.  My ideal situation would be a few Applied Predictive Modeling-style example walkthroughs."
Handling incomplete information properly when using Bayes' theorem (specifically in the context of medical diagnosis),0,3,False,False,False,statistics,1505340586,False,
Assessing the accuracy of predictions of a mixed model?,1,3,False,False,False,statistics,1505341711,True,"I've been using lmer to fit a model with one data set, and I want to tests its accuracy using several similar data sets that we already have. I've gotten to the point where I have predicted values of my (continuous) DV as well as the actual values we collected. 

What's the best way to assess the accuracy of the prediction? R-squared? RMSE? If it helps, I intend the model to be predictive, not just explanatory."
Uncertain career path,0,1,False,False,False,statistics,1505351975,True,[removed]
How to present regression results when outcomes are binary?,12,2,False,False,False,statistics,1505352325,True,"The outputs I'm trying to predict are binary (i.e., alive or dead) but my model will output a probability (i.e., probability of death). Let's say there are multiple models I want to compare. What's the best way to determine which one is the best?

One way would be to threshold the probabilities at say 0.5 and convert the model outputs into binary, and then do classification error. And create an AUC out of that by varying the threshold.

But are there other ways? I feel like I'm losing information by thresholding but not sure what better way would be."
Interpreting odds ratios in logistic models,2,2,False,False,False,statistics,1505352646,True,"When running a logistic regression model, I need a way to measure effects that are partialed out when a mediator is added. The easiest way to do this in OLS is to observe the decline in the slope coefficient (e.g., a decline from 4 to 3 suggests a 25% decrease in effect size once the mediator is added). 

What I'm wondering is: can the same be done with odds ratios? Or would you need to convert to logged odds before calculating the reduction in effect size? "
Statistical Test on Probabilities?,0,1,False,False,False,statistics,1505355649,True,[removed]
what jobs do holders of Master's in Statistics get? What jobs did you get?,32,13,False,False,False,statistics,1505367254,True,
Good Feature Building Techniques - Tricks for Kaggle,0,1,False,False,False,statistics,1505378769,False,
Stats (Data Science) Vs Double Major (Stats and CompSci),6,1,False,False,False,statistics,1505394548,True,"I am currently majoring in stats, and my uni allows us to choose a field of specialisation. I have narrowed it down to either Data Science or Business Stats. However, I am also thinking if I should take up a second major in CompSci.

I understand that CompSci is also in high demand and is very complementary with stats. The good thing is that I am interested in creating programmes and writing code that can help with stats problems. At the same time, I would also like to do finance-related statistics which the Business Stats specialisation is all about. 

Hence, my question is: what are the pros and cons of sticking with stats and specialising in data science, against specialising in business stats and taking up a second major in compsci?"
"Designing a drug treatment/protein expression study, need help with choosing stats",0,1,False,False,False,statistics,1505406714,True,"HI guys, 

I have a project that I am interested in and the easiest way to describe it is that I am going to treat cells with vehicle, drug A, and drug B for 2 hours and also vehicle, drug A, and drug B for 48 hours and look at protein expression changes.

 I want to compare responses to:

drug A vs drug B at each timepoint. 

drug A at 2 hours vs drug A at 48 hours and the same for drug B.

I also would like to be able to compare each drug to vehicle at each timepoint. 

There are a lot more comparisons going on here than I am used to. We usually just do simple t tests and repeated measures ANOVA in our lab. My stats are pretty weak, so I am wondering if someone could help point me in the right direction to help me figure out what kind of tests I would need to perform here. Thanks!
"
Understanding finding the P value of the critical value,5,3,False,False,False,statistics,1505407460,True,"I am having issues with finding the p value of the critical value on ti-84. Here is the problem I am working on:

The director of research and development is testing a new medicine. She wants to know if there is evidence at the 0.05. level that the medicine relieves pain in more than 349 seconds. For a sample of 69 patients, the mean time in which the medicine relieved pain was 352 seconds. Assume the population standard deviation is 21. Find the P-value of the test statistic. Round your answer to four decimal places.

using z table on ti-84 = 1.187

then im going to tcdf lower limit 1.187, upper: 10000000000, df 68 = .1196

But the actual answer is: 0.1170.

Not sure what I am doing wrong on the calculator.

Thanks
"
"I lost my TI Nspire CX CAS and a Statistics test is coming, is it worth replacing? (xover w r/Nspire)",2,1,False,False,False,statistics,1505415032,True,"Hello, a bit of context.

So I'm a freshman in college, and my teacher in high school said we should get the highest end calculator so we could get used to it for college. So my folks got a Ti Nspire CX CAS for my future math since I wanted to do a scientific degree. I grew to have a good knowledge of the device and loved working with it. Surprisingly enough once I was in college I found that my degree (nursing) only had me take elementary Statistics. 

The first test is on Monday and we're allowed to bring a calculator. So I look to bring my Nspire buddy and I can't find him anywhere. I could potentially if I order it now get a new one in time for the test, or we have scientific calculator (looking at the TI-36X Pro Scientific Calculator) which the professor said was all we needed for the test. So I'm asking you reddit, is it worth the 177.99 dollars to get a new Nspire?"
What are great videos to get back into statistics?,14,13,False,False,False,statistics,1505415478,True,"Hey Redditstaticians,

My girlfriend is starting a study where statistics play a major role. She used to be really good at it, but finds it daunting to start again after a couple years not using the knowledge. Ofcourse she has her books and everything, but I think she needs someone explaining it to her in a TL;DR fashion.

So do you guys have some youtubevideos she can watch to get it going again? Preferably by a professor that explains it in a somewhat pleasant way (some people have that gift). 
Her study is in health/psychology research, so it would be best if the video is somewhat aimed in that direction."
"First major project, seeking guidance from experienced statisticians/researchers",14,4,False,False,False,statistics,1505418519,True,"Hi guys, 

Background: I've been given the opportunity to lead an evaluation for a two year pilot that will contrast outcomes from two different approaches to delivering a specific type of social service. I am the closest resource in house with the skills/education to lead the evaluation. I do not claim to be a statistician or scientist of any kind, although I have taken research methods and statistics courses at the undergraduate and graduate level with success. I also have experience with data analysis/performance management in the same industry. I have never formally lead the design of any study, and every evaluation I have taken part in until now has been formative or summative, focusing on stakeholder needs.

Question: I find myself presented with the opportunity to conduct an RCT, however, while I am very excited, I am also very nervous. What are some of the steps I should be cognizant of when putting together the overall study design? 

One thing I am nervous about is controlling for co-linearity or confounding variables; as it stands, differences in outcome could stem from multiple factors and not solely from the main intervention; what would would be an effective/pragmatic approach to dealing with these kinds of issues, and when (i.e. during the design phase, or during the analysis phase)?    

The researchers from the original study that this evaluation is based on used a mixed methods effects analysis, a term I have only come across recently. I have found some resources explaining mixed method analyses and am reviewing them now. Are there other tests I should look into that are more suited to repeated measures studies?

Apologies for the long winded post, any insight is greatly appreciated. Cheers all."
Hello! So I'm retaking a statistics course that I had to drop last year. I'm studying all my old quizzes. Does someone know the answer to this question and why? (Obviously the answer is not C haha).,2,2,False,False,False,statistics,1505426787,False,
Have most of you self taught yourself in R or other statistical languages?,16,2,False,False,False,statistics,1505431419,True,"Hey guys, 

Just got a quick question. I am soon to be starting an **MSc in Social Anthropology** later this month in which I will have the option of choosing 3 optional modules. 1 module I have been considering is called **Statistics and Causal Analysis for Qualitative Social Scientists**  



My question to you, is if there is any point in me choosing this as a module choice when there appears to be a colossal amount of online resources that I could do in my own time to learn R. 



There are quite a few other interesting and relevant modules that I would like to enrol on (I'm planning on getting onto a research degree after this year with a focus on conflict, security and terrorism) and have been wondering if it would be a waste of a module choice to choose statistics when I can do it in my own time. 



Let me know your opinions, especially if you feel that it would be much better for me to get a proper grounding of the language within an academic environment. The module is described with the following on my university's module choice directory: 



>This course introduces statistics and the R language from their very basics. The course assumes no background knowledge of either statistics or statistical software. Topics covered include an introduction to statistics in R, distributions, hypothesis
testing (t-tests, proportion tests, ANOVA), correlation, linear regression, multivariate statistics (multiple regression, PCA, discriminant analysis) and logistic regression.""

Thank you!"
Rating the restaurant raters: Yelp vs. Google vs. Facebook,0,0,False,False,False,statistics,1505441270,False,
What is a controversial opinion that you have?,154,58,False,False,False,statistics,1505441414,True,Anything vaguely related to statistics. 
Difference of Two Proportions Hypothesis Test with Weighted Sample Data,6,3,False,False,False,statistics,1505461247,True,"I have survey data which contain respondents' answers to several questions. As the survey contained a disproportionate number of people from certain demographic groups, the survey results are weighted by race, sex and age.

I have responses to the same questions for two years (eg. 2016 and 2017), and am trying to find out if the proportion of people who responded ""yes"" to a particular question has fallen. That is, I have calculated the weighted agreement rate to the question in 2016 (p1) and the weighted agreement rate to the question in 2017 (p2), and am trying to see if p1 - p2 = 0.

I think I have a good idea of how to perform the simple hypothesis test for a difference between two proportions is  (described at https://onlinecourses.science.psu.edu/stat414/node/268). However, I do not have a formal background in statistics (only have some experience in introductory college courses and econometrics). Thus, I am wondering if: 

1) Weighting the samples by demographic variables has changed the standard error; thus, a more complicated hypothesis test formula is required. If so, what is this formula?

2) Whether there are other methods, other than applying this possibly more complicated formula, to rigorously test for a difference between the two proportions. For instance, are there non parametric hypothesis tests that can be used?

Thanks for your help!"
Newbie Question - Calculate probability of getting into car accident on way to work,6,1,False,False,False,statistics,1505490668,True,"Beginning learning statistics and probabilities. Was wondering on way into work how I would calculate my probability of getting into wreck. I figured since there is a car in front of me, in back and on each side,  and if this configuration remains for the entire trip, wouldn't my probability be 1 in 4? or 1 in 5 if you include NOT hitting any car? Be gentle, i'm just learning :)
"
Panel Data Question,0,1,False,False,False,statistics,1505491021,True,"Hi Everyone,

I'm working on a research project that is part of a randomized control trial. A basic description is that patients are given tablets in a hospital. Some patients are left alone to do as they please and other patients are shown how to use the tablet. For starters, I'm just looking at how frequently each patient uses the tablet on each day of their stay.

Here's the part I'm unsure about modeling. The treatment effect (education) is not given before the patient gets the tablet, it is given at a non-standardized time after the patient has already started using the tablet. I thought I would look at the data as a panel. If anyone has insight that would be cool! Here are the big questions I have.

1) Can I keep patients with different panel lengths in the same model? Say a patient with day 0 to day 3 and a patient with day 0 to day 15?

2) Is it acceptable to have the treatment effects happening at different time intervals? 

3) The usage data appears to have some sort of natural trend - like high usage in the first day followed by usage that trails off over time. Will mixing different stay lengths, treatment times, and natural trends cause additional headaches?

If you have an ideas, I'd be happy to hear them!"
Time dependent linear models in sklearn python,0,1,False,False,False,statistics,1505499621,False,
Creating a predictive model using survey data,7,7,False,False,False,statistics,1505502052,True,"Hey all,

I was wondering if this is plausible. Let's say I do a political survey where I ask people if they are reactionary/progressive, liberal/conservative, for or against social engineering, and finally whether they are republican or democrat. 

Can I create a model to explain how much variance each question accounts for in political identity? Instead of merely giving percentages on how many progressives are democrats, can I use a model to show which factor plays the biggest role in predicting or accounting for a democrat affiliation than a republican one?

Thanks!"
Quickie: Statistical Paradoxes,0,19,False,False,False,statistics,1505503152,False,
How to calculate the power of a t test?,2,3,False,False,False,statistics,1505503262,True,"Hey, 

I was wondering how you would calculate the power of a one sided and two sided t test and whether it differed from the z test.

I know for a z test it is p(z &lt; z_alpha - abs(ua - u0)/(s/sqrt(n))) for one sided and for two sided you replace the z_alpha by z_alpha/2.

For a t test would it be the same thing but the z_alpha part is replaced by t_alpha or is there something else that differs?

Also on a side not how would you do it in R?

Thank you for reading"
"When working on a big project in R, how many lines do you like your scripts? 500? 2000?",15,5,False,False,False,statistics,1505520869,True,
N-size for simple mean difference comparison,11,3,False,False,False,statistics,1505530608,True,"I am comparing the difference in scores for students in a pre-test and a post-test (matched samples). I am not doing a t-test or anything like that; it's a simple mean difference in score. For statistical significant tests, there are many guidelines for calculating sample size required. However, for such a simple scenario like mine, are there any guidelines for a minimum n-size that would make the mean difference somewhat reliable?"
"Hello everyone, I just want some advice. Recently came to the realization that I enjoy probability theory more than Applied Statistics. What options do I have as a career?",10,12,False,False,False,statistics,1505537261,True,
Resources for buffing up on structural equation models?,0,1,False,False,False,statistics,1505568885,True,[deleted]
Types of statistical distributions,3,234,False,False,False,statistics,1505569044,False,
Statistics over Input Distributions,0,1,False,False,False,statistics,1505590733,True,"Is there a name for statistical methods which directly operate over distributions rather than point samples from distributions? For example, say you wanted to model the price of an item. You could return a point estimate by returning a MLE for all buyer prices. However, this may not account for the shape of the input distribution.

Let's say the market can be characterized as bullish by looking at the shape of the input distribution. Knowing a market is bullish would result in a different price for items than if it was not. Is there a methodology for utilizing the shape of the input distribution as an input? Can it deal with distributions of different sizes? Are priors needed?

The machine learning community uses convolutional neural networks to operate over distributions of pixels. However, techniques like this still require a prior on the dimensions of the inputs. Are there techniques better suited for variable length distributions?

Thanks!"
Sportstatist.com - detailed statistics on different sports,0,0,False,False,False,statistics,1505591063,False,[deleted]
Maximum possible percentage above a sample mean?,0,1,False,False,False,statistics,1505594185,True,[removed]
Comparing data from Pie Graphs?,0,1,False,False,False,statistics,1505597173,True,[removed]
Newbie not sure which tests to use for SPSS,0,1,False,False,False,statistics,1505598145,True,"Hey, I have a big spreadsheet of data with observations recorded.

For each event, there is one supervisor (Mr. X, Mr. Y, or Mr. Z) and one student (Mr. A, Mr. B, or Mr. C) recording their observations. The names are not recorded into this spreadsheet. Both the supervisor and the student observe 3 different phases of the event and record a score for each phase (e.g. 0, 25, 50, 75, 100) for a scoring system (we'll call it Barney) as well as a score for each phase (e.g. 0, 1, 2, 3, 4) for another scoring system (we'll call it Gumby)

My end goal is to

1. compare interobserver agreement between supervisors and students for each scoring system (Barney and Gumbo) and possibly compare this interobserver agreement between the 2 systems to see which system has better interobserver agreement

2. correlate/prove there is agreement between the Barney and Gumby systems. For instance, event #6 scored a Supervisor-Student average of 51 on the Barney scale which is statistically significantly close to the Supervisor-Student average of 2.1 on the  Gumby scale.


I'm thinking fleiss kappa might be good here for #1 but I'm not sure. "
All about Binomial distribution,0,0,False,False,False,statistics,1505598265,False,
"g, a Statistical Myth",0,11,False,False,False,statistics,1505599887,False,
"I just had this idea, would wonder if you guys could help me find something like that :)",2,0,False,False,False,statistics,1505609777,True,"Hey,

Just thought about if there's a statistic of comparison between the average relationship length from people with a big and people with a small dick. That would show if penis length actually matters. I couldn't anything."
"Was curious, how exactly do people figure out these critical values for all these test? (z, t, chi-square, f, etc.)",2,0,False,False,False,statistics,1505610023,True,
What are the benefits of the z-score interpretation of probit regression coefficients?,6,2,False,False,False,statistics,1505612244,True,"Logit vs. probit is often a big debate. Many prefer logit simply because the coefficients can easily be converted into odds ratios, which are ""more intuitive"" to interpret than the z-score interpretation of probit regression analyses. 

Of course, many probit users bypass this issue by calculating predicted probabilities instead, but I'm curious to hear if there are any benefits to the raw z-score interpretation. Does the latter have any merits over the odds ratios of the logistic regression, in terms of interpretation of the model?"
How to convert Survival Analysis Parametric Regression Coefficients to hazard estimates,3,1,False,False,False,statistics,1505613486,True,"Hello all. If I have the following coefficients from a survival analysis regression and I wan to turn them into a vector of probabilities that a person with certain covariates will experience failure in steps t+1...t+n, how do I do that ?
For background, i'm estimating attrition from a company. div's are dummies for the division a person is in and startsal is starting salary.
Here are the coefficients, i'm using STATA.
    
                failure _d:  fail
       analysis time _t:  tenure
    Fitting full model:
    
    Iteration 0:   log likelihood = -8322.0526  
    Iteration 1:   log likelihood = -4849.4285  
    Iteration 2:   log likelihood =  -3487.921  
    Iteration 3:   log likelihood = -3453.8074  
    Iteration 4:   log likelihood = -3453.4414  
    Iteration 5:   log likelihood = -3453.4414  
    
    Gompertz regression -- log relative-hazard form 
    
    No. of subjects =        3,963                  Number of obs    =       3,963
    No. of failures =        2,685
    Time at risk    =       798141
                                                    Wald chi2(18)    =    24795.65
    Log likelihood  =   -3453.4414                  Prob &gt; chi2      =      0.0000
    
    ------------------------------------------------------------------------------
              _t | Haz. Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
    -------------+----------------------------------------------------------------
        startsal |   .9999875   6.27e-07   -19.89   0.000     .9999863    .9999888
           divd1 |   .0019133    .000213   -56.23   0.000     .0015382    .0023797
           divd2 |   .0011844    .000378   -21.12   0.000     .0006337    .0022138
           divd3 |   .0011969   .0001174   -68.62   0.000     .0009877    .0014505
           divd4 |   .0011721   .0000727  -108.86   0.000      .001038    .0013236
           divd5 |   .0013884    .000143   -63.86   0.000     .0011345     .001699
           divd6 |   .0012063   .0001062   -76.36   0.000     .0010152    .0014334
           divd7 |    .001033   .0000625  -113.58   0.000     .0009175    .0011631
           divd8 |    .001443   .0001554   -60.75   0.000     .0011685    .0017821
           divd9 |   .0013139   .0001296   -67.25   0.000     .0010829    .0015942
          divd10 |   .0015388   .0002064   -48.30   0.000     .0011831    .0020014
          divd11 |   .0022684   .0001593   -86.69   0.000     .0019767    .0026031
          divd12 |   .0031117   .0007373   -24.36   0.000     .0019558    .0049508
          divd13 |    .004918   .0006136   -42.60   0.000     .0038512    .0062803
          divd14 |   .0014269   .0001593   -58.69   0.000     .0011465    .0017759
          divd15 |   .0010063    .000095   -73.10   0.000     .0008364    .0012109
          divd16 |   .0010003    .000185   -37.35   0.000     .0006961    .0014373
          divd17 |   .0604247   .0061647   -27.51   0.000     .0494736    .0737999
    -------------+----------------------------------------------------------------
          /gamma |   .0097019   .0002192    44.25   0.000     .0092722    .0101316
    ------------------------------------------------------------------------------
    note: no constant term was estimated in the main equation"
Parameter vs statistic - Treatment of experimental data,0,1,False,False,False,statistics,1505614551,False,
SportStatist.com - fast statistics on different sports,2,6,False,False,False,statistics,1505649227,False,
Graduate Certificate in applied stats and data management,0,1,False,False,False,statistics,1505668782,True,[removed]
How hard is it to get a Data Analyst internship/job?,33,20,False,False,False,statistics,1505698952,True,"Hi everyone! I am currently a student in the Bay Area (San Jose, CA). I am studying Statistics, with a minor in Computer Science. My ultimate dream job is data scientist, but realistically speaking, it is quite hard for an undergraduate (and I consider myself an average student) to get such a job after getting my bachelor degree. So, my current plan is to earn work experience as a data analyst a few years, then go back to school to get a master in Statistics. Recently, my brother-in-law has had a talk with me. He said I should be mentally prepared that I will not be able to get a job with my major. I am sure he was not trying to bring me down or something. It is just because he had some friends with Math degrees (even Master) could not get a job, and he truly wanted to give me something to keep in mind. Before the talk, I had done some research, and what I've learned is that it is not too hard to find a Stat job in general as business always needs someone to analyze their data. So, that brought me here with some concerns.

1.Which of the 2 stories I mentioned seems to be relevant to reality, especially in the Bay Area? I know companies said they need Stat people, but what is their need of data analysts who mostly just have a bachelor degree?

2. Current data analysts or past data analysts. How is/was your job like?

3. What are some most popular technical skills desired by recruiters? I looked at Udacity Data Analyst degree, it said Excel, TAbleau, Python and SQL. In my case, I know R, Python, SAS and SQL. For Excel and Tableau, I think I can pick up quite quickly.

4. I know I will get a figure if I look this one up, but I still want to ask what is my expected salary as a data analyst? I don't expect to earn a lot at this point, but since I plan to go to grad school, so I need information to financially prepare from now.

Please let me know if you need additional information regarding my questions. I am hugely thankful for any thoughts, comments or feedback. Also, if you think you have a quite similar situation to me, would you mind if we get into a conversation later on? That will be great for me. Thank you!"
Name for a modified score statistic,0,3,False,False,False,statistics,1505705841,True,"I'm not too familiar with statistical terminology, and I can't find the information online.

I have a family of densities with 4 parameters, let's say parameter a, b and 2 others (c and d) which are location and scale parameters respectively.

If c and d are known, one possibility to test the null hypothesis that the i.i.d. data X_1,X_2,...,X_n have the density above with parameters a_0, b_0, c, d, is the Rao's score test, which would be based on the score statistic, namely the 2-component vector of the derivative (with respect to a and b) of the log-density, evaluated at a_0,b_0,c,d.

Now, assume that c and d are unknown.
I could create the same Rao's score test based on a modified score statistic where everything is the same except that c and d are replaced by their maximum likelihood estimators (under the null hypothesis).

Is there a common name for this ""modified score statistic"" and the consequent ""modified Rao's score test"" ?

Is this a common thing to do in practice ?

Can someone point me to some useful books on this subject ?

(I'm writing a journal article where I derive the asymptotic distribution of a modified score statistic.)
"
How to demonstrate that the difference between two independent and normally distributed sample means also has a normal distribution?,3,1,False,False,False,statistics,1505707512,True,
What should be expected trying to find a job in stats with a Mathematical Statistics BS?,11,14,False,False,False,statistics,1505713085,True,"Is a BS in mathematical statistics enough to open some opportunities or is an MS needed.

I will be graduating this spring with BS in both Mathematical Statistics and Physics and am currently working on my programming skills."
"Any ideas on how to ""cluster"" this data? Neuroscientist wants to go beyond an arbitrary R^2 threshold.",7,2,False,False,False,statistics,1505714532,True,"Hi,

I'm a neuroscientist. I've recorded time-series data, which is essentially a video of the activity of neurons. The data looks a awful lot like [this](https://www.youtube.com/watch?v=vqGPp4d3LGw). Each bright ""blob"" is one brain cell.

I want to know how the ""activity"" (which is essentially a brightness value which is a 16 bit integer) varies over time. When I start the data is a big R^3 matrix, which is 512 (pixels) wide, 512 (pixels) high and as many frames as a captured deep (something like 10000).

Software has already already automatically created ""Regions of interest"" (ROIs)  which are arbitrary bounding boxes placed around some of the pixels, and the average value of all those pixels in each frame is calculated, so I am left with a much smaller R^2 matrix, that is now &lt;number of ROIs&gt; by &lt;number of frames&gt;.

The problem is that the software sometimes splits the pixels that belong to one cell into two, three or even more ROIs. The data for each of these ROIs usually matches pretty well (real data [Here](https://i.imgur.com/B1nWZsf.png) each colour is a different ROI belonging to the same cell). However, it doesn't always match that perfectly.

What I have been doing is correlating all the data from every ROI with every other ROI and calculating a Pearson correlation coefficient. Then I arbitrarily set a threshold and say that all ROIs with a correlation coefficient above X (usually something like 0.7) belong to the same cell.

MY PROBLEM: This arbitrary correlation coefficient threshold is pretty gross. If I bump it from 0.7 to 0.71, sometimes I get different data, and because there are often 100s of ROIs it is very hard to go through manually and cluster by hand (cluster, as in, say that ROIs A, B and C all belong to the same cell). Is there a smarter way of doing this? I'm trying to imagine some cost function that I am trying to minimize, but most of the cost functions I can come up with are minimized when each ROI is treated as it's own cell (because I'm thinking the data for each group of ROIs is well clustered if each ROI does not deviate much from the mean of all the ROIs. And you achieve that perfectly if each ROI is in its own cluster). I suppose I should mention that sometimes each ROI IS it's own cluster (i.e. the software originally drew the ROI perfectly). I know there is lots of data published on 'clustering' but this isn't really the same thing. How can I more intelligently look at the data and say ""these arrays of numbers are very similar, and probably belong together""

Any ideas appreciated."
Which test to use?,3,10,False,False,False,statistics,1505728502,True,"I had never a statistics course and i want to do it right. So i hope someone can help me and explain it to me, so I can learn. 

I need to do a test, I have 6 different conditions (n=5). I have to find out if there is an difference between these condtitions. Someone suggested to an ANOVA combined with Scheffe or Bonferonni. I would like to know why these? and what is the difference and what is the best to do?

I hope someone can help me"
Pspp help,7,8,False,False,False,statistics,1505733923,True,"evening fellas, i'm just trying to figure out how i make pspp display my means and standard deviation into 3 decimal places. i'm really getting frustrated by this, any help would be really appreciated."
What's the opposite of aggregated data? Non-aggregated?,6,1,False,False,False,statistics,1505742639,True,
Which test do I use?,5,2,False,False,False,statistics,1505749453,True,"Hi, I have four different samples of human cells which were treated with different things. For each sample we had to record how many of the cells had a specific abnormality. Each group analysed a different number of cells so I've recorded how often the abnormality was seen as a percentage. What statistical test could I do to compare these percentages to show if any of the treatments caused more abnormalities? Thanks :) "
Statistics Tutorial,0,1,False,False,False,statistics,1505752370,True,[removed]
How to find quartile locations (distances) on Excel?,1,1,False,False,False,statistics,1505759188,True,I need to find the location (distance) of quartiles using data on a table i have on Excel.  
Sample Size Calculation,0,1,False,False,False,statistics,1505760993,True,[removed]
Why is SPSS slowing down so much after deleting variables?,4,1,False,False,False,statistics,1505763934,True,"So I've got a huge dataset I'm working with on SPSS. It's got around 470,000 cases and 670 variables and sits at about 14 GB. I've noticed with this dataset that if I open it up and sort a variable, that variable sorts appropriately (albeit slowly). But if open up the dataset and first delete several variables (in this case, I deleted all but 11 of them) and then try sort the same variable, the process is excruciatingly slow. I also can't save, since that takes just as long. 

Does anyone know what's causing this? I would think the file would be much smaller after deleting 600-some variables, thereby making it run faster when sorting variables or saving, but the opposite is occurring. At this point, I'm stuck on this dataset until this issue is resolved, because the rate at which this crawls is unmanageable.

Oh, and if it does take too long, I end up getting an error in SPSS about the processor being unavailable. The weird thing is that I was able to work in this dataset before this week, and now I can't, no matter what computer I use.

Any help is greatly appreciated! Thanks!"
Debate on Reddit over the chances of drawing the same lottery numbers twice.,5,4,False,False,False,statistics,1505764248,True,"Here's the thread: https://www.reddit.com/r/todayilearned/comments/70un9r/til_that_in_2010_israels_weekly_state_lottery/

Anybody wish to weigh in on this? "
[Help] Probability Theory proof.,1,1,False,False,False,statistics,1505766022,True,"Let E(n) and F(n) be increasing sequences of events. 

i.e. E(1) is a subset of E(2), E(2) is a subset of E(3) and so on.

Where E(n) --&gt; E 

and F(n) --&gt; F, as n --&gt; inf

Additionally, let E(k) be independent of F(k) for all k = 1, 2, 3 ... 

Show that E is independent of F.

---------------------------------------------------------------------

So correct me if I'm wrong. We're basically given 

P(E(1) intersect F(1)) = P(E(1))*P(F(1) 

....

P(E(n) intersect F(n)) = P(E(n))*P(F(n))


....

and we have to show that P(E intersect F) = P(E)*P(F)

From P(E(n) intersect F(n)) = P(E(n))*P(F(n)), if we take the limit as n --&gt; inf then the right hand side is satisfied.

But how can I show E(n) intersect F(n) converges to E intersect F? 

Thanks in advanced guys. If this is the wrong place to post can someone direct me to a more appropriate site for questions? 

"
A day in the life of a statistical geneticist?,10,24,False,False,False,statistics,1505767890,True,What kinds of projects do you get to work on? And what were your qualifications (eg. I heard it's necessary to have a Masters and maybe a PhD)?
Question about risk ratios,1,3,False,False,False,statistics,1505771194,True,"When using a log-binomial regression model, how should I interpret the risk ratio (aka relative risk)?

Example:
Say I have a continuous predictor (number of kids), and a binary outcome (bankruptcy). I use a log-binomial regression model. If the risk ratio for ""number of kids"" is 1.5, I can say that someone with an extra kid has a 50% increased risk compared to someone without an extra kid. 

But what if someone has three extra kids? Would the relative risk be 1.5^3, just like it would if we were working with odds ratios? "
A series of paired t-tests...,0,2,False,False,False,statistics,1505772659,True,"**Straightforward question:**

We collected treatment and control data on 4 measures for each participant, and plan on running four paired t-test on them. We needed a minimum sample of n=40, but ended up with n=50 for some the measures due to participants not completing all 4 measures. 

So measure X has 40 pairs, measure Y has 50 pairs, and the measurements are coming from the same sample. For some reason, past RAs removed all data from Y when the participant didn't complete X, making the sample size 40 for all measures. 

Is there a reason one would do this? Obviously you need even treatment/control data for any given measure to run a paired t-test, but why would you throw out data for one measure if it was missing on the other? 

**Bonus:** 

The code I was provided includes two test for each measure - a standard paired t-test, and a two one-sided test (TOST) for equivalence. Usually, a positive TOST (Alternative hypothesis == equivalence) is associated in a failure to reject the null in the standard t-test, and a rejected null on the standard t-test is associated with failure to reject H0 in the TOST - but not always. Past reports have deemed results to be inconclusive if they don't match. It seems questionable to me to test similar hypotheses using two different tests and label the results inconclusive if they don't match, but I can't really find anything on the matter. Has anyone encountered this before?"
How to Create Viral Content with Studies in Neuroscience and Past Data,1,17,False,False,False,statistics,1505781234,False,
What test can you use to see if your new results invalidate your previous expectation for the distribution?,2,2,False,False,False,statistics,1505785899,True,As per the title if you have new results come in how can you tell if they invalidate the ex ante distribution either by size/ magnitude of the new inputs or by frequency? 
E[f(x)] =?= f(E[x]),7,1,False,False,False,statistics,1505793987,True,does this hold?
Statistics B.S,0,1,False,False,False,statistics,1505800027,True,[removed]
Do u prefer R or stata?,15,0,False,False,False,statistics,1505812006,True,"I have never used stata, I use R and MatLab, anyone of you do u use stata? Do u think it is worthy than R or Matlab?"
Has anybody ever tried Answerminer? Do you have any experience with it?,0,1,False,False,False,statistics,1505813366,True,"I've just come across this data analyzer tool and want to know that has anybody tried it and what is her/his opinion.

https://app.answerminer.com/auth/login"
Perception of Style Study - 57% of people think wearing a tie is no longer a necessity at a job interview,3,0,False,False,False,statistics,1505815395,False,
Is this graph Statistically correct? Surely it must be questionable!?,6,1,False,False,False,statistics,1505816333,False,
Empirical rule questions. Help!!,2,1,False,False,False,statistics,1505826864,True,[deleted]
How do I check whether a data set is a paired data or independent samples,2,6,False,False,False,statistics,1505834300,True,
Probability Help,6,4,False,False,False,statistics,1505834852,True,"Are there any good resources online to better understand probability? I'm currently stuck on the ""choosing"" nonsense.
For example: you draw five cards without replacement. What are the odds of getting two kings, a joker, and an ace?
That's just an example, but problems like that are my bane. Are they any suggestions on how to better understand them?"
Expected Value and Density,0,2,False,False,False,statistics,1505841075,True,[deleted]
Which statistical test to compare weight loss / weight gain on a weekly base?,0,1,False,False,False,statistics,1505842054,True,[removed]
Statistical test for non-ordered categories?,7,6,False,False,False,statistics,1505843260,True,"I have a dataset with messages sent among 3 groups. Each message is assigned one or more categories based on the content.

Is there a test for significance between what % of group A's messages were labeled category 1 vs. what % of group B's messages were labeled category 1? There is no expected distribution of categories for each group.

Here is an example:

  | Cat1 | Cat2 | Cat3 | Cat4 | Cat5
---------|----------|----------|----------|----------|----------
High Schoolers | 14.8% | 9.2%|6.6% |26.2% |7.4%
College students | 24.2%| 11.4%|10.6%|15.2%|12.9%|
Post-grad | 23.2%| 4.5%|2.3%|20.3%|7.9%|

In this case, I want to see if there is a significant difference between high schooler (14.8%) and Post-grad (23.2%) messages that were categorized as Cat1."
LOTTERY ODDS,0,1,False,False,False,statistics,1505853393,True,[removed]
What Standard Deviations Equate to Sample %?,6,3,False,False,False,statistics,1505855065,True,"I am trying to build a cost history report for my job, breaking down each bid we receive and using standard deviation to determine any tasks/projects that are significant. It's been a LONG time since statistics, so I'm trying to remember how do I determine what SD value equals a sample %'age?

If I'm not explaining myself correctly, I remember the 68-95-99.7 rule, but that's just too broad of a range for what I'm trying to accomplish. I am looking for something more like 10-30-45-60. Is there a calculator somewhere for this? Every Google search is not helping me!"
Basu’s Elephants [PDF],1,17,False,False,False,statistics,1505856582,False,
How do you Analyze the Ratio of Two Time Series Variables?,1,8,False,False,False,statistics,1505871062,True,"Say you had two time series variables X and Y, and you were interested in performing a time series analysis for X / Y. For example Y = Number of customers at a store in a day and X = Number of units of a product sold in a day, and you were interested in X / Y = rate of units sold per customer per day. What would be some good approaches for constructing a time series model on the rate? The purpose of this model would be to make predictions of the rate/anomaly detection.

Some ideas I had:

* Naively construct an AR model on the rates directly.
* Construct an AR model on X and Y jointly. Then predict X and Y and use those predictions to construct a prediction for X / Y. Then, I can use the estimated covariance matrix in the model to estimate CoV(X / Y) to do anomaly detection.

I feel like the the first approach is probably the incorrect approach and that the second one is the better one, but I'm not convinced it's entirely principled. Also, I would love to hear about other suggestions. Further, how would this analysis change if we knew that X / Y was in the interval [0, 1]? Conceivably, you could construct a model according to method 2 above where we predict X &gt; Y. Thanks in advance."
Significance in a Randomized Block Designs,0,0,False,False,False,statistics,1505880554,True,I am currently testing whether seasons has an significant effect on motor-vehicle fatalities within Australia from 1989 to 2016. I have conducted an ANOVA and found a significant result (p = 0.0469) but when conducting a post-hoc Tukey's test I found no significant difference between any of the seasons. What can I conclude? 
Independent events...re: probability,0,1,False,False,False,statistics,1505881614,True,[deleted]
Odds or probability for an &gt; 7 Richter scale earthquake happening 32 years after in the same place,1,1,False,False,False,statistics,1505882993,True,[removed]
Need help on finding the inventor of statisical theories.,6,0,False,False,False,statistics,1505893996,True,"Dear redditors, I would greatly appreciate it if I could find the original papers where these following theories were first introduced. 

I am having trouble finding the inventors of these theories. I would appreciate on tips on how to find it. 

K-nearest neighbors classifier 

Linear Regression 

Clustering analysis

Decision Tree

"
Likelihood Ratio Test vs Coefficient Significance,10,2,False,False,False,statistics,1505896022,True,"
	

I ran a regression on some data I collected. The data has one independent variable and looks as if a quadratic function would fit it well.

I fit both a quadratic and a quartic model to the data and performed a likelihood ratio test. Seems that I should prefer the quartic model (p&lt;0.001).

However, the coefficients of my quartic model are not statistically significant.

Should I prefer the quadratic model because it's coefficients are statistically significant, or prefer the quartic model from the results of the liklihood ratio test?
"
Have Date and time on the same axis in R studio.,2,1,False,False,False,statistics,1505897091,True,"This is a moisture data. I want to have date and time on X axis and moisture on Y axis. Thanks
Example data

    Date	           W5	        W7
    6/24/2017 12:00 AM	0.333	0.326
    6/24/2017 12:30 AM	0.333	0.332
    6/24/2017 1:00 AM	0.334	0.351
    6/24/2017 1:30 AM	0.334	0.351
    6/24/2017 2:00 AM	0.334	0.352
    6/24/2017 2:30 AM	0.334	0.352
    6/24/2017 3:00 AM	0.335	0.352
    6/24/2017 3:30 AM	0.335	0.351
    6/24/2017 4:00 AM	0.335	0.351
    6/24/2017 4:30 AM	0.335	0.351
    6/24/2017 5:00 AM	0.336	0.350
    6/24/2017 5:30 AM	0.336	0.350
    6/24/2017 6:00 AM	0.336	0.349
    6/24/2017 6:30 AM	0.336	0.349
    6/24/2017 7:00 AM	0.336	0.348
    6/24/2017 7:30 AM	0.336	0.348
    6/24/2017 8:00 AM	0.336	0.348
    6/24/2017 8:30 AM	0.336	0.347
    6/24/2017 9:00 AM	0.336	0.347


The script I am using right now just for date

    ggplot(data, aes(x = Date))+ geom_line(aes(y = W5, colour = ""W5""))+ geom_line(aes(y = W7, colour = ""W7""))
"
An Efron-Stein Like Lower bound for Variance. Is this new?,3,13,False,False,False,statistics,1505900102,False,
Comparing two scatter plots,4,1,False,False,False,statistics,1505923641,True,"Data was collected at multiple time points for two sets of subjects. From the data, I've made two scatters plots of, let's say, subject health vs. time.

It's fairly trivial to calculate if there is a significant difference between each (or any) time point using multiple t-tests. However, is there a way to test for a significant different between the plots as a whole? I can't seem to find an answer but I may be asking the wrong questions.

p.s. Apologies if this should be posted elsewhere. Just let me know. It's work-work not homework, if that makes a lick of difference."
Question about categorizing data for regression,7,3,False,False,False,statistics,1505927989,True,"So im a beginner here, student, trying my best to learn but i'm stumped on what to do here and im sure its a simple fix:

I have some data on whether an individual smokes or not, where 1 - yes, 2 - no, 3 - don't know, 4 - did not respond.

I want to regress distress level against smokers, so i only care if you smoke or not. 

I thought i could just put 0 for all answers that are not yes (1) but apparently this is wrong.

What would you do?"
Error bars?,2,0,False,False,False,statistics,1505932127,True,"(I posted a similar-ish post recently so sorry if this seems familiar to anyone!) 

So i've been treating 4 groups of human cells (all originally from the same batch) with different doses of a chemical. I was then looking to see how many of the cells were normal after treatment (had 46 chromosomes) or were abnormal (had more or less than 46 chromosomes) in each of the treatment groups. So i now have lists of how many chromosomes were in each of the cells that were analysed. I thought the best way to present this would be to make a graph showing the percentage of normal cells (those with 46) found within the 4 groups. How do i put error bars on my graph? Do I calculate the standard deviation for each group from the raw data (list of chromosome numbers) and then use the SD value when i specify the error values? I have no idea! I haven't done anything to do with statistics for like 10 years and even then i sucked at it! I hope this makes sense and sorry for the stupid question "
Question about observational studies,2,1,False,False,False,statistics,1505937583,True,"Hello everyone, I am a undergraduate student and in one of my class we are asked to do statistical analysis on an observational study to find if smokers and non smokers have higher chance respiratory viral infections in a certain demographic. I have no idea how to do one and was never taught how to do one and wanted to ask if a someone here could help with this predicament. I personally am not collecting the data but I have to guide another group on what kind of data to collect. My question is what kind of different observational studies are there that would work with my study, and what requirements with the data are to be met to have a do a good analysis on the data? Also any other general tips when conducting an observational study?"
Announcing the First-Ever Police Data Challenge,0,25,False,False,False,statistics,1505938038,False,
A Family of Efron-Stein Like Lower bounds for Variance,0,2,False,False,False,statistics,1505950837,False,
Difference between z-score and empirical rule,3,2,False,False,False,statistics,1505964021,True,"The empirical rule says that 1 standard deviation is equal to 68% of the data. But if you have a z-score of 1, it is not equal to 68%, but 84.13%. Why do they have different values if they both equal to 1 standard deviation?"
"Question: How to ""Balance"" Scores from Low Sample of People [Read for more info]",3,3,False,False,False,statistics,1505966002,True,"I'm trying to compare boardgame scores between people.

Some people played a lot of games; some played very few. For the ""few games played"" people, their average score is essentially their actual score from playing the game once. Is there a way to compare the scores of the many played games vs. the few played games? I can of course say, I'm only comparing the scores of people who've played 5 times but the number 5 is ultimately arbitrary? What's the none-arbitrary number that would work as the cutoff? Is there way for me to use the data in a comparison of the few games played people?"
Question on Mean &amp; Standard deviation,7,4,False,False,False,statistics,1505969215,True,"The posted speed limit in a neighborhood is 45 mph, we are given the mean (49.83 mph) and the Standard deviation (6.28mph)

How do I find out:
-What percent of cars travel at or below the posted speed limit?

-What percent of cars that travel through this neighborhood do so at speeds between the speed limit and 5 mph over the speed limit?

What would be the method to solving this? Thank you!
 "
Dimensionality Reduction Using t-SNE,0,1,False,False,False,statistics,1505972064,False,
Interpreting significance based on confidence interval,0,1,False,False,False,statistics,1505997566,True,[removed]
The Ten Fallacies of Data Science – Towards Data Science – Medium,20,31,False,False,False,statistics,1506000962,False,
Modelling the probability that a cancer patient will survive vs die from cancer,26,5,False,False,False,statistics,1506002397,True,"Hi everyone,

I am trying to model a phenomenon.  It isnt actually about cancer patient, but this is the best analogy I can come up with.

~~I am wondering what would be the best way to model if a cancer patient will survive a cancer given a set of variables, such as time since beginning of treatment and other characteristics of the patient. ~~

I don't care about the duration of the treatment, only if the persons will  beat the cancer or die.

I have multiple observations for each patient: one per year, until they either die or beat cancer.

Should I just do a logistic regression, or should I do some survival analysis ?   If I do a logistic regression, how would you treat the fact that patients are there multiple observations per patients?   If survival analysis, can you point me in the right direction?

When predicting, I will want to take into account if a patient has been treated for 1 year or 10.

best,



-----
edit: 
I'm afraid cancer patient was a bad example .   Let's try this instead:

I have a technical ticket open.   It's a ""minor case"".  I want to know how likely we will be able to close that as a ""minor case"",vs seeing it degenerate as a ""major case"" before we can close it.  

Again, a case can last for multiple time periods, and will have observations for each time periods where y will be one of these :

- censored (open, currently estimated as either minor or major)
- 1 (closed as major)
- 0 (closed as minor)



ticket id|date|status|outcome (estimate if not closed)|month since start|x1|x2|
:--|:--|:--|:--|:--|:--|:--|
AA|jan1|open|minor|0|1|1|
AA|feb1|open|major|1|1|2|
AA|mar1|closed|major|2|1|2|
BB|jan1|open|minor|0|2|2|
BB|feb1|closed|minor|1|2|2|

So my actual data set will look like this.  The point is that today I have a data set with a bunch of tickets and I want to predict the most likely outcome (status when closed).  
"
Correlating Energy Data Sets: The Right Way and the Wrong Way,0,2,False,False,False,statistics,1506007884,False,
Starting an 'R Club' - ideas?,8,7,False,False,False,statistics,1506012970,True,[deleted]
Please help me compare these two logistic regression models!,3,1,False,False,False,statistics,1506013504,True,"Hi All, thanks for any assistance! 

I have a binary outcome of interest, and I have 9 binary predictors - the predictors are 9 different events having occurred (or not). So I want to ask the question 'did the customer buy anything?', with the predictors of 1) whether or not they got a phone call, 2) a letter, 3) met with representative in a store, etc (clearly they could have had multiples of these events happen). In the first model, I entered all the predictors as class variables. In model 2 I summed their binary occurrence, and entered that summative predictor by itself (reflecting for example, that any person might have been exposed to 0-9 of the 9 things).

I want to compare these models - not the coefficients but the model fit (I'm actually trying to answer the question of which method of treating the predictor is better). 

Can I use the chi-square distribution for the difference in -2LL? Some references say the models must be nested (I cant tell if these count as nested or not), while other references only say 'compare deviance between other models fitted in the same data', which these clearly are.

The models produce very large deviance statistics, and very large differences between them (-2LLmodel1 (-) -2LLmodel2 = 358565, with delta df = 10).

Is this really indicating that the models are meaningfully different in terms of fit? The c-statistics do vary but only on order of about .02-.03.

(I have a very large sample if that makes a difference (&gt;25,000)).

Thanks for reading. "
Confused about Autocorrelation?,2,1,False,False,False,statistics,1506014073,True,"Hi Guys,

I have a question regarding this [paper](http://people.stern.nyu.edu/churvich/Forecasting/Handouts/Chapt3.1.pdf). I'm confused with [this section](https://imgur.com/a/YEkPr). If there is a strong autocorrelation between the series Xt and Xt-j, then we would expect that the forecast for Xt would go up/down depending on Xt-j and on whether the autocorrelation was positive or negative. But if this is the case, how can the E[Xt] still be equal to 0? Was hoping someone could clear this up. Thanks guys. "
Between/Within vs Overall (standard deviation/capability analysis),0,1,False,False,False,statistics,1506015314,True,"I'm working on a statistical quality control tool and realizing I need to be using a ""between/within"" (I-MR-R) chart.  One of the issues with that is that you use a few different forms of variance in it.  

The first is the ""within"" variance, so that would be the variance within a specific subgroup/batch of data.  Second you have the ""between"" variance, or the variance between each subgroup/batch.  Then you have the Between/Within variance, which is the combination of those two.

Finally you have the ""Overall"" variance, and that's the one I am having some difficulty with.  It seems that often times the Overall variance is larger than the Between/Within variance.  Apparently this difference is supposed to be a sign that there is something wrong with your system (not in control). 

But what I don't understand is what other variance there could be other than Between/within.  Or is it meant to be kind of like how an ANOVA is calculated, and saying that if the overall variance of the system can't be explained by the Between/Within variance then you have a non-random factor at play causing the difference seen? 

Any thoughts?


Ed:  also, I've run a Nested Anova on this data, the way it's structured it makes sense to me that the Between level is the top level, and the within level is the bottom/Nested level, but the numbers I'm getting are slightly different than expected (and there doesn't seem to be an ""overall"" version).  "
Is there anything statisticians can't do?,10,0,False,False,False,statistics,1506021211,True,"Every college in my university has an intro stats class. The engineers need one, the bio majors need one, even the sociologists have one. 

Every subject uses or has latched onto it. Criminology used to be hopelessly qualitative until Gary Becker introduced a rational choice framework, already used in economic models.  

Every team of scientists needs someone that, at the very least, understands how to run an ANOVA, and a peer reviewer in journals that can okay it. 

Financial firms of all kinds are hooking up with big data analysis and machine learning, which are old statistical methods rehashed. 

Statisticians are kind of like the modern world's renaissance man? "
Can you find the expected value of a continuous function?,1,1,False,False,False,statistics,1506022824,True,[deleted]
Is there an analog of the data processing inequality for minimum error (Bayes) classifiers?,0,1,False,False,False,statistics,1506023124,True,"In information theory, the data processing inequality states:

&gt; if  X-&gt;Y-&gt;Z is a Markov chain, then I(X,Z) &lt;= I(X,Y), where I is mutual information

I'm wondering if there is an analogous theorem for performance (percent correct) of a minimum error classifier (discrete case). Let Px(Z) be the probability of correctly predicting which X produced Z, using a Bayesian (minimum error) classifier. Does the following hold:

&gt; If X-&gt;Y-&gt;Z is a Markov chain, then Px(Z) &lt;= Py(Z)

This seems intuitively obvious, but I have not seen it proven.
"
How to study statistics/probability,13,10,False,False,False,statistics,1506023142,True,"Hi Guys. 

Student taking an intro probability and statistics class for engineers.
While the class makes sense, the homework is quite daunting and I just can't seem to wrap my head around the wording of problem and the problem solving approach. I have tried reading our textbook but it seems to be at a much more advanced level than our class. 

Could you offer some statistics video lectures or textbooks that are a little more beginner friendly. I am struggling quite a bit and don't want to be forced to look up answers. I really want to understand what I am doing more, but I seem to hit a brick wall. 

I am most confused by conditional probability at the moment. 

Thanks! "
Math in Spring,0,1,False,False,False,statistics,1506026998,True,[removed]
Statistics vs Biostatistics,12,20,False,False,False,statistics,1506037302,True,"I'm looking to go back to school and start a new career after being a stay at home mom for too long and I would really like to be a biostatistician. I don't want to uproot my family in order to pursue grad school at a different school, but the local university does not have a biostatistics program (but they do have a MS in statistical computing). Does it matter? How much easier would it be to get a job in biostats if I had a biostats degree instead of a regular stats degree? 

Another element to my question is I know I would do much better in a classroom setting, but if I *really* wanted to get a biostats degree and not move my family, there is an online program available. I much prefer to be in the classroom though. Besides my preference, I'm concerned about how people - especially the people that do the hiring - in the field of biostats feel about online degree graduates. Not long ago online degrees seemed to be viewed as something negative, but it seems to me this has been changing (and I also imagine it's dependent on the industry). I'm hoping some of you could provide me with some insight. 

TL;DR I want to be a biostatistician. My options are an ONLINE MS in Biostatistics or a traditional MS in statistical computing. Can I get easily hired as a biostatistician with either of these? Or is going for a traditional (as opposed to online) MS in biostats highly advised? "
Question about box plot,2,0,False,False,False,statistics,1506038343,True,[deleted]
The p-value argument. What is your best explanation in lamons terms of the p-value?,11,0,False,False,False,statistics,1506056762,True,[deleted]
Specialization in College,1,2,False,False,False,statistics,1506060241,True,Hello! I am a freshman in college and I am considering pursuing a career in statistics. I was wondering what the best plan of attack is for specializing in agricultural statistics. In this scenario know I want to get a MS in Applied Statistics after my undergrad. Is it unwise to get my bachelors degree in an unrelated field (related to agriculture /bio) that I want to apply my later Masters degree to? I would take the required prerequisites for grad school in this scenario. Or on the other hand should I double up on statistics for both my undergrad and graduate degree? Thank you so much for any input.
Market Overview on App Development Tools,0,1,False,False,False,statistics,1506061402,False,
The Media Has A Probability Problem,22,68,False,False,False,statistics,1506087150,False,
"Good Online Course for ""Advanced"" Undergraduate Statistics (Psychology)",1,2,False,False,False,statistics,1506091416,True,"I just started my first semester of my Masters and am a few weeks into a graduate level course in ANOVA. It has been several years since my undergraduate data analysis courses and I am having a hard time keeping up. I am debating dropping the course and spending the semester reviewing the stuff I am supposed to already know. Is anyone able to recommend a good online course that perhaps does a quick run through of introductory concepts and then moves into the type of material you would find in an upper-level undergraduate social sciences data analysis course? Ideally it would be something geared towards psych (though this isn't critical) that I could complete by the end of December. Thanks!



TL;DR I'm looking for an online upper level social sciences stats course."
Should I take a summer off work during undergrad to learn SQL/some python?,2,1,False,False,False,statistics,1506097350,True,"I'm a business major, but want to move more into a data analyst type role. However, I've found our university courses to really be lacking and think I'd be better off taking courses through Course era / Khan Academy or something like that.

How negatively would that period be viewed if I'm not working at an internship or anything?"
Strange Probability Question,7,5,False,False,False,statistics,1506098775,True,"I'm stuck with something in a side project I'm working on, but I think I can figure it out if I can solve the following simplified problem.

I have a bunch of machines. Each machine contains a deck of 10 numbered cards which also have a color, red or green. Cards 1-4 are always green, and cards 5-10 are always red. The probability that each machine will dispense each card varies across machines. To use a machine, you press the button and it dispenses a card. If it's a green card, you get one point and must press again. If it's a red card, you lose a point and can no longer press the button. 

Once a card has been dispensed from the machine, it cannot be dispensed again, but the relative probabilities among the remaining cards stay constant. I don't think this is relevant, but in the actual scenario the initial probabilities for the green cards will all be higher than those for the red cards.

I want to know expected value of each machine. I'm not particularly experienced with probability problems so I'm not even sure how to begin modeling the stopping effect of a red card, in part because the impact of stopping depends largely on how many green cards you have already gotten. Thanks for any help!"
Modelling counts of people,0,1,False,False,False,statistics,1506107920,True,[deleted]
Statistics on infant mortality rate,1,1,False,False,False,statistics,1506109875,True,"My first baby is due coming up in November and I keep hearing in parenthood circles people echoing sentiments they've seen in the media that America has a horribly high infant mortality rate compared to other wealthy developed countries. In looking at the collected data i'm seeing some strange metrics that people are touting as statistics but it doesn't seem like these data sets would normally pass for credible data. I would love it if someone who is slightly more versed in statistical theory could validate or reject my thoughts on it.



1.) Infant mortality rate is calculated as number of infants that perish per thousand, among children under the age of one. The sample size of the United States is almost 4,000,000 vs the sample size of Japan being just over 1,000,000 or vs Luxembourg with a sample size of just over 6,000. I'm under the impression that you need equivalent sample sizes to begin to rule out anomalies and accurately predict trends and patterns.



2.) Infant mortality rate is not calculated using the same metrics in many other countries that the United States is being compared to. This leads to an inconsistency in how the data is collected. Due to many varying ways the data is measured it makes it difficult to determine whether the numbers are comparable. For instance, in many countries we are compared with, babies born under 1lb or before 22 weeks of gestation are not considered to be part of the infant mortality rate statistics and are considered still birth instead. Also, in the United States babies at even 22 weeks of gestation are considered viable to try to save, where in many other countries we are compared with, they are not considered viable and the attempt to save isn't made further increasing our negative statistic.



3.) Reporting of the infant mortality rate from other countries is collected differently, leading to inaccuracies in the data. Reporting data for many other countries is collected in different, possibly faulty ways than it is collected in the United States. Some countries we are compared to collect data after the fact, sometimes after a significant length of time by in home interview or voluntary data. It is to note, that this likely isn't the case with many countries like the UK, or France who similarly document infant mortality at the hospital where it occurred.



As an aside to the entire question, even with Mexico having one of the higher listed infant mortality rates at 13.2 deaths per 1000 infants under the age of one, this is still only 1.32% of all infants born will perish before the age of one which seems like a very low percentage. Expressed as the actual number it's about 31,000 per 2,353,000 babies born will perish before the age of one. This last paragraph is purely subjective, but one I felt like including.

TL;DR The statistics surrounding the Infant mortality rate among countries seems to be flawed as a valid statistical analysis. Please validate or reject my theory."
**Question and Answer,0,1,False,False,False,statistics,1506117955,True,[removed]
"Does anyone know how to ""bound"" a dependent variable in R?",9,0,False,False,False,statistics,1506119898,True,"Please help

EDIT: I am not sure, the question was not clearly explained.
I have a regression model - lm() - and i have four independents, one is a categorical midpoint (age), the other three are binary, (smoker, drinker, gender). The dependent variable is the distress level.
The question states: ""The dependent variable is bound at 0, run a summary lm(), how do the results change?"""
Which undergrad math courses help the most with preparing for a masters/PhD in statistics?,6,2,False,False,False,statistics,1506122812,True,
I'd like to use a BS degree in Statistics to get into the sports industry.,9,22,False,False,False,statistics,1506149951,True,"I am going to be majoring in statistics. How can I use a degree in statistics to enter the basketball/baseball/sports industries, more specifically making analysis about players, how much they are worth etc.... "
Why 24 out of 24 is mentioned as 99% success than 100% in HIV study/test sampling?,1,0,False,False,False,statistics,1506151539,True,"Recently Scientists [had found a vaccine for HIV](https://www.youtube.com/watch?v=bBOO4lJonjI).
In the sample test/study, they tested it with 24 monkeys, and it worked fine for all 24 monkeys. But they are [reporting it](http://www.bbc.com/news/health-41351159) as 99%. Can you clarify/explain it ?"
SPSS problem with descriptives,0,1,False,False,False,statistics,1506153948,True,[removed]
I'm a recent psychology graduate who would like a job in data science - asking for advice,16,0,False,False,False,statistics,1506167532,True,"I've just finished my MSc in social psychology and, in the future, would like to be a data scientist. As part of my psych degrees I had to learn statistics, and only then did I discover something I was good at and thoroughly enjoyed. The thing is that my stats training in my degrees aren't good enough qualifications for a data science job. A large part of me laments not doing a maths undergrad, but I was 18 and didn't even know what data science was.

My quantitative skills are strong, I have a knack for maths and coding/logic and am eager to learn. Is there anything you guys might recommend? I'm teaching myself python at the moment and have been desperately looking for jobs where I can use my stats skills (no success thus far) but apart from that, I'm not sure what to do."
Understanding data types - quantitative (Continuous or discrete),0,1,False,False,False,statistics,1506183840,True,[removed]
Computer Help-STATA/SPSS,2,0,False,False,False,statistics,1506188736,True,"Hello, I've been using STATA and SPSS on my laptop for about three years, but it recently died, and the people doing my tech support don't think that it can come back to life.

I was just wondering what people use for a desktop setup? RAM and Processor specifically. 

Sorry if this is not appropriate just not sure where to ask this!!!"
Does multilevel polynomial regression exist?,3,9,False,False,False,statistics,1506195586,True,"Hi all,

I've heard of growth curve models, where change takes place over a function of time, I've also heard of polynomial regression and multilevel modelling.

My question is, can I use the same mathematical principals of growth curve but have my lowest level independent variable be something else? Also, can I have multiple first level variables in this set up?

Thanks in advance for your answers."
Excel question,14,0,False,False,False,statistics,1506198791,True,[deleted]
This Font Makes Graphics Out Of Numbers In Seconds,1,55,False,False,False,statistics,1506202525,False,
How To Find Sample Size Using Student T's Distribution,0,1,False,False,False,statistics,1506203576,True,[deleted]
Online Stats Class,1,0,False,False,False,statistics,1506225806,True,[deleted]
"Why is it that in relative grading, scoring above average in tests is believed to be safer?",4,0,False,False,False,statistics,1506230969,True,"Like people seem terrified of getting below the class average in a test. I don't have a clear understanding of this ""All good as long as you get above the class average"" concept. A difference of 5 marks should be the same whether the class average is in between this range or not right? 

My apologies if it's super dumb but I'm just really curious about the statistical implications."
Choosing the correct number,27,0,False,False,False,statistics,1506232395,True,"Hi, had an issue that has been bothering me all week. At work I/we were analyzing a bottleneck step in our workflow.  We pulled the last 90 days of data (n=1200ish) and the results were mode = 14 minutes, median = 18 minutes, mean = 21 minutes, std. dev = 43 minutes and I think the 3rd quartile was 60 minutes. We are trying to set the baseline number to determine cost, allocation of resources, and any additional training. As the previous number was set at 8, several years prior. VP of Mfg wanted (mode) 14 minutes and CEO wanted (median) 18 minutes and both actually got into a pretty heated debate. They looked to me for an opinion and I suggested a split at 16 (but I don’t make decisions, I just pull the data and put it in histogram form). Right now we are settled on 16 but you can sense that this will change. 

How would a statistician resolve this?  Is there other formulas that could resolve the optimal number or at this point we just need to choose based on our own experience and knowledge the mean, median, or mode? Thanks in advance, this has literally bothered me all week but figured I would reach out to the experts to see how this should be handled."
Is preparation time discrete or continuous variable?,0,1,False,False,False,statistics,1506240200,True,[removed]
Help in Statistics Homework (Description in the attached picture),3,0,False,False,False,statistics,1506241999,False,[deleted]
Statistics/Data Science in Canada,0,1,False,False,False,statistics,1506242510,True,[removed]
The chance of rolling the same number twice in a row on a six-sided die in 100 throws.,4,1,False,False,False,statistics,1506246614,True,"Sorry if this is out of place, but it has come up multiple times recently between myself and friends about the probability of something happening twice in a row. In the actual discussion we have been having the chances are much slimmer than 1/6, but I figured I could just apply what I learn here to the bigger numbers.

(I looked online for the formula, but was unable to find it.)

Thank you in advance."
Help comparing three sets of time data,1,3,False,False,False,statistics,1506267128,True,"I need some help considering the best statistical test to compare three sets of 24-hr data. What I am interested in looking at the number is occurrences of an event during each hour over a 24-hr span, for three different groups. I want to know if there is a significant difference between the hourly occurrences of the event among the three groups (and if possible, on an individual group basis e.g. group 3 v group 1 or group 1 v group 2). I tried a chi-squared contingency table but can't figure out why that won't work.

group1 &lt;- c('0'=2, '1'=3, '2'=1, '3'=2, '4'=3, '5'=0, '6'=2, '7'=1, '8'=1, '9'=0, '10'=1, '11'=0, '12'=1, '13'=0, '14'=0, '15'=1, '16'=0, '17'=1, '18'=1, '19'=1, '20'=1, '21'=1, '22'=4, '23'=3)

group2 &lt;- c('0'=2, '1'=0, '2'=2, '3'=5, '4'=3, '5'=1, '6'=2, '7'=3, '8'=1, '9'=0, '10'=4, '11'=0, '12'=0, '13'=1, '14'=2, '15'=0, '16'=0, '17'=2, '18'=3, '19'=0, '20'=3, '21'=2, '22'=1, '23'=2)

group3 &lt;- c('0'=1, '1'=0, '2'=5, '3'=5, '4'=5, '5'=3, '6'=2, '7'=2, '8'=0, '9'=1, '10'=0, '11'=0, '12'=0, '13'=0, '14'=0, '15'=0, '16'=0, '17'=0, '18'=1, '19'=3, '20'=1, '21'=2, '22'=1, '23'=0)

Am I organizing the data correctly? Any advice on a good statistical test to use, or how to format the data? I am using R for analysis, if that helps.

Thanks!"
Study of Self-Esteem to # of Followers,0,1,False,False,False,statistics,1506271995,True,[removed]
Check similarity/consistancy between five training sets,4,8,False,False,False,statistics,1506276868,True,"Hi guys,
do you guys know a fast method to check if a dataset (out of currently five) is similar two the other datasets. I'd like to check if a part of the data should or should not be considered as training data.

Best Regards"
Maximum likelihood estimation and probability,0,1,False,False,False,statistics,1506290553,True,[removed]
"How do you know if a Masters program is ""accredited""?",1,0,False,False,False,statistics,1506304410,True,[deleted]
What statistical method/analysis should I use?,9,1,False,False,False,statistics,1506304721,True,"Hey! I'm running an experiment and need help selecting what statistical analysis method I should use. :)

Experiment is looking at vision - specifically colour discrimination at varying visual field angles.

Independent Variables (both are categorical):

- IV.1 - Colour [x4]

- IV.2 - Angle of Visual Field [x4]

Dependent Variables (both are ratio/scale variables):

- DV.1 - Stimulus Seen

- DV.2 - Color Identified

Main Aims/Hypotheses

1 - Are there any significant differences between when the stimulus was seen [DV1] and when the color was identified [DV2] - do this for each colour [IV1] at every angle [IV2]

2 - Are there any signifiant differences between when the color was identified [DV2] based on the angle the shape was presented [DV2] - do this for each colour [IV1]

Any idea what statistical test would be best used to solve this? I want to avoid running multiple tests that increase the risk of making a Type 1 Error."
How to Export CSPro Data File to STATA,0,1,False,False,False,statistics,1506307047,False,
What's a good way to move forward if you are unsure if a stats career is something you want to do?,3,6,False,False,False,statistics,1506308025,True,"I heard a lot of stats careers needs Masters degrees at least, but if you're not sure you want to go that route yet, and you don't want to waste time, what are some things you can do in the mean time? Is doing online courses in stats something good to have on your CV or does it mean practically nothing to employers unless you have that Masters?

Also, would you recommend learning straight stats, or learning a ton of different applications (like biostats)? I'm just not sure where to start, or if I'm wasting my time."
Statistics Homework Help,0,1,False,False,False,statistics,1506309832,False,
What is the error of an averaged measurement?,1,2,False,False,False,statistics,1506311195,True,[deleted]
Youngest PhD Statistician,0,1,False,False,False,statistics,1506324988,True,[removed]
ARIMA for Classification problems,3,7,False,False,False,statistics,1506326097,True,"Note: I am a physicist by profession so pardon for my mistakes/unclear explanation.

I have a large set of data where each entry corresponds to a [waveform](https://imgur.com/a/07k2r). I am still trying to wrap my head around this topic with the usage of ARIMA in statsmodels in python. 

Someone suggested to me I can use ARIMA for this classification problem of mine. However I am still unable to wrap my head around it how it works. 

Any expert here could give a word of advice on this issue?"
SPSS qualitative coding question,3,7,False,False,False,statistics,1506337763,True,"I am having some trouble with qualitative data in my thesis. I have got five variables with qualitative responses, the questions were 'list things you find stressful' (over five different time periods). I am not concerned regarding the responses themselves rather if they wrote a response or left the question blank.

Is there a way to create a new variable which shows their level of response, for example they would be given a value of '1' if they only responded to one of the five stress related questions.

I thought possibly giving the empty responses a '1' value then creating a new value with the mean of all five questions. However it did not work because the data is both string and numerical.

The aim is to exclude people who completed equal to or less than 2 of the responses. 

I am aware I can manually code this however there are over 300 responses so I am wondering if there is a faster way.

Thank you! If this is the wrong forum let me know and I will remove! "
WHAT ARE YOUR EYELIDS,7,0,False,False,False,statistics,1506346042,True,[removed]
How to fit a probability distribution to binned data?,0,1,False,False,False,statistics,1506346921,True,[removed]
using a z score to calculate a percentile,0,1,False,False,False,statistics,1506351126,True,[removed]
Gaussian Process Regression for Large Datasets,13,7,False,False,False,statistics,1506357373,True,"My boss specifically wants to do Guassian Process Regression for our dataset. It uses time to predict course grades. We want more accurate confidence intervals to model the uncertainty around timeslots that we don't have data for. (For example classes start at 8am and 9am but we don't have any data for classes that start at 8:30am) 

However all the Gaussian Process Regression package only work with maybe sub 750 data points and after that it's just too slow. Are there any python or R packages that I can use for this? The dataset is around 1 million entries. Is this even possible? I've been googling around a lot but haven't figured anything out. "
Can you help me with Preparing Panel Data in Excel for Gravity Model in STATA?,2,4,False,False,False,statistics,1506365221,True,"Hi,
I want to perform a gravity model for migration using panel data in STATA, but I am having a hard time preparing data in Excel. This is not my first time using STATA, I am familiar with the basic panel models, but I have never encountered a problem with bilateral flows. From what I have read so far, many have used the count panel data models, which I think I will manage to estimate once I arrange data to be imported. 

My model consists of the following variables:
Dependent variable - Immigration inflow by citizenship from country i (origin) to country j (destination) in period 1998-2015
Explanatory:
1. GDP of country i in period 1995-2016
2. GDP of country j in period 1995-2016
3. Unemployment in country i in period 1995-2016
4. Unemployment in country j in period 1995-2016
5. Life expectancy in country i in period 1995-2016
6. Young people in country i in period 1995-2016
7. Number of foreigners in country j from country i in period 1998-2015
8. Dummy if countries are neighbors (time invariant)
9. Distance between countries (time invariant)

As you can see, there are a lot of variables of different types, so I would be extremely grateful for any advice and/or Excel example on how to prepare it all for importing in STATA.

P.S. I can send you my data if it helps.

Thanks in advance!
"
Traffic Accident Factors: what test(s) to use?,1,1,False,False,False,statistics,1506369402,True,[removed]
Can anybody help me doing this?,2,1,False,False,False,statistics,1506382839,False,[deleted]
How to deal with a regression where I have an unbalanced design with an unequal number of replicates between dependent and independent variables.,0,5,False,False,False,statistics,1506382875,True,"OK, first off, I wasn't involved in the research methods/data analysis planning process. I was brought in after the data was collected, when they found out they didn't have anyone who could actually perform the analysis (i.e. &gt;5 yr ago). 

So, I kinda threw out their plan for the analysis they wrote into the research plan, because I didn't think it was the best way to go about answering the question they wanted answered. (that and because they typically use softwares, such as excel or systat, which are my non-preferred tools)

**Description of the Data:**

* The goal was to determine the effects of a drug *in vivo* based on an *in vitro* assay. 
* Both the *in vivo* and *in vitro* data is discretely measured with a scale [0,+infinity), but the cutoff for clinical efficacy is values &lt;6
* The data was collected in healthy volunteers and in patients. So there are three major groups of subjects:
   1. patients whose in vitro says they are 'sensitive' and  the in vivo matches (because they are on a correct dose)
   2. patients whose in vitro says they are 'sensitive' and the in vivo doesn't match (because they are on too low of a dose)
   3. patients whose in vitro says they are 'resistant' and the in vivo ""should"" never match (mechanism of drug resistance means that no increase in dose will help)
* They pulled the in vivo data in quadruplicate, but the in vitro data in triplicate (I don't know why)
* The healthy volunteers have in vitro data taken from when on and off the drug; whereas the patients only have in vitro data taken when on the drug 

**My Current Plan:**

* Was going to do a regression in R (they originally only planned for an ANOVA/ANCOVA, but seeing how the goal was to make predictions, I figured a full multivariate regression model was more appropriate)
* Was going to build the model that basically looked like:

         lme(InVivo ~ InVitro * Dose,
             random = 1|ID) 
* considering also doing a logistic model for resistant/sensitive based on the clinical cutoff we use, but figured that a regression on the actual values would be more appropriate, then leave the clinical decision for whoever is doing the prediction

**Major Questions:**

1. How do I account for the fact that, because the the number of InVivo measures doesn't match the InVitro measures for a given covariate (mainly dose)? As of now I've wrangled the data into a long format with both In vivo and In vitro measures all in the same column, but marked by a 1/0 flag indicator in a ""Vitro_1"" column of the data frame.

2. How can I modify the random effect portion of the lme model to account for intra-assay variability?

3. Are there any other considerations that might be appropriate for this analysis? (I'm currently considering post-hoc tests for temporal effects, but they *should* be nominal given a washouts between dosing level changes)



P.S. I'm so sorry about how long this is, I just want to make sure that whatever superhero decides this post is worth responding to has all the info they might need. I really appreciate any and all help; I work with data analysis that is similar to this, but I kinda picked up this project as a charity case. That and it was supposed to be a quick way to get a publication for myself haha. 

"
How can a mediator explain over 100% of a relationship?,5,1,False,False,False,statistics,1506383803,True,"After adding several potential mediators to a regression model, I've found that the coefficient on my main explanatory variable has decreased by over 100% and is now negative. How am I to interpret this finding? 

The purpose of my model is to measure attenuation in coefficients as potential mediators are added to the regression, but I can't plausibly say that the mediators explain ""over 100%"" of the relationship between x and y..."
"I have a mean of 12 ad a standard deviation of 3,how should I transform the values to get a mean of 75 and a standard deviation of 12?",3,0,False,False,False,statistics,1506384825,True,[deleted]
Can anybody help me doing this?,0,0,False,False,False,statistics,1506385425,False,[deleted]
Anybody can help me doing this?,0,1,False,False,False,statistics,1506390010,False,[deleted]
Help please,2,0,False,False,False,statistics,1506390946,False,[deleted]
"If I add the same value to all the values,for example 5,will the new median just be the old one+5?",1,0,False,False,False,statistics,1506391757,True,[deleted]
[COX regression] How to calculate baseline hazard rate not using means of covariates in SPSS.,13,9,False,False,False,statistics,1506417522,True,"Hi everybody. 
First of all, please show mercy, I'm just a simple med student trying to do some basic research. 

I'm trying to look at certain risk factors and how they affect the 10-year risk of developing a certain form of cancer. To do this I'm using the Cox regression model to estimate hazard ratios for my risk factors, or covariates. 

As far as I understand it; if I can calculate the HR for these risk factors, or covariates, and find out the baseline 10 year cumulative hazard, where these covariates have the lowest possible value, I can estimate the 10-year hazard rate given different combinations of covariates.

However, I have no clue how to calculate the baseline cumulative risk, as SPSS only allows me to do this at the 'means of covariates'. 

How is the hazard function for 'means of covariates' connected to the hazard ratios? It was my understanding that the hazard ratios were a comparison between the absence(0) and the presence(1) of the covariates I entered. To estimate the absolute risk increase I have no use of the baseline hazard rate for covariate means, right?

I hope I made any sense with this. I appreciate any help possible."
Good example of 1-tailed t-test,38,3,False,False,False,statistics,1506427625,True,"When I teach my intro stats course I tell my students that you should almost never use a 1-tailed t-test, that the 2-tailed version is almost always more appropriate.  Nevertheless I feel like I should give them an example of where it is appropriate, but I can't find any on the web, and I'd prefer to use a real-life example if possible.

Does anyone on here have a good example of a 1-tailed t-test that is appropriately used?  Every example I find on the web seems contrived to demonstrate the math, and not the concept."
Bayesian learning for statistical classification to improve your model,0,20,False,False,False,statistics,1506433280,False,
How do you solve this question. Currently in exam preperation and i cant figure out how they come to their solution...,0,1,False,False,False,statistics,1506436449,False,[deleted]
Ovulation cycles of women,4,0,False,False,False,statistics,1506438828,True,"It's a well known fact that women are more sexual during ovulation days. I also heard that women in the same environment tend to synchronize their ovulation cycle with other women. Does this sychronization forms into some sort of macro pattern, and could this be represented by common (median) ovulation days per country?

For science of course."
Bayesian Statistics,0,1,False,False,False,statistics,1506439766,True,[removed]
"If I want to have 95% chance that less than 1% objects are faulty, how many samples do I need?",5,6,False,False,False,statistics,1506444791,False,
ELI5 Tolerance Intervals (plus confidence and reliability),1,1,False,False,False,statistics,1506446042,True,"I never learned any of this in school but I've been using these in my job for a couple years but I feel like a need a really simple explanation to tie my understanding all together.  

Playing around in Minitab it seems like a tolerance interval with 50% confidence/ 50% reliability is just the average of a set of data but I don't feel like I understand how each confidence and reliability affects the output.  

Does a 95% reliability mean my result will always be above the result 95% of the time, but only with 95% confidence that the data that gave me that result was correct?  "
Most BS way usage of statistics on Healthcare costs,2,5,False,False,False,statistics,1506455810,True,"http://www.businessinsider.com/healthcare-spending-out-of-pocket-by-state-2017-9

This article touts that they analyzed over 2 million customer banking accounts and from this data they derrived that Healthcare out of pocket costs do not exceed $1,000. This is complete bullshit way of using data. The data set only contains a fraction of required data to make such analysis and is lacking some obvious ways their customers could have spent money out of pocket such as credit cards at other banks or FSA which is deducted from paycheck and never reaches the bank. And they even have the audacity to say they are over estimating the total out of pocket cost... Also there is no data on the high cost of monthly payments.

This will only make those burried in medical bills depressed by lying that their neighbors aren't experiencing similar difficulties."
Best practices of orchestrating Python and R code in machine learning projects,0,9,False,False,False,statistics,1506459040,True,[removed]
"Do these numbers make sense? Stats for survival for pancreatic ""IPMN"" cysts when resected (operated on) vs. not resected. How can 5 years survival be over 50% if median survival is 2 years or less? Thx. x-post from R/badstats",13,1,False,False,False,statistics,1506460321,True,"The median survival was 21.5 months for patients with resected IPMNs, ranging from 2 to 124 months, and 14 months in non-resected IPMN patients, ranging from 5.5 to 70 months. There is no significant survival difference between the resected and non-resected groups, with a 5-year survival of 69.8% in resected IPMNs and 59.8% in non-resected IPMNs, P = 0.347. https://link.springer.com/article/10.1007/s00268-005-0035-8"
Vanward Statistical Consulting - vanwardstat.com,0,1,False,False,False,statistics,1506463150,False,
Data analysis consulting services - vanwardstat.com,0,1,False,False,False,statistics,1506463866,False,
How to test for significant difference in five-star ratings?,8,3,False,False,False,statistics,1506464889,True,"For example, the local McDonalds has a 4.5 rating on Yelp. How can we see if a 4.2 rating over the last 7 days is statistically significant? "
Debunking a “Junk Science” Survey of Student Views on Free Speech,9,12,False,False,False,statistics,1506465663,False,
Probability Hmwk... I'm panicking!,0,1,False,False,False,statistics,1506469925,True,[deleted]
A few probability hmwk questions - Any help would be greatly appreciated!,1,0,False,False,False,statistics,1506470523,True,[removed]
"I have an interesting idea, but I'm not sure how to model it",1,0,False,False,False,statistics,1506486097,True,"This was conceived on the way home so it's not fully fleshed out but it's getting there.  

I'd like to take existing information on out-of-the-money options from the last few years and regress them on a variety of factors to get to a probability regression(if that exists) or model that can explain what affects an out of the money option contract's value and to what degree as well as the probability of the option coming in-the-money when the contract expires.  I'm not sure what type of variable I'd use or if this would require the creation of one.

What do you guys think it would take to build a model for this?"
Boyfriend and I are arguing about his toe,36,6,False,False,False,statistics,1506489265,True,"So he had a procedure done on his toe where the chance of it failing is 1%. It wound up failing, and he says if he gets it done again it'll be a 1% chance of failure overall. 
I'm saying that it would be a .0001% chance of failure overall, since the procedure being done twice combines the chances. He says it resets and that it's a 1% chance. 
Who is correct here? We realize this is the dumbest debate ever, but we're both stubborn people. :| "
P value is worthless?,20,3,False,False,False,statistics,1506490115,True,"Attended a stats seminar this weekend, and the presenter spent a good fifteen minutes explaining there has been a decades old argument that a ‘p’ value does not provide a good measure of evidence regarding a model or hypothesis.  That given the same ratio in the variables, the more you increase the sample size, the larger larger the differences are, the larger the p-value is, which would then indicate the HO is true.

Maybe I'm not completely understanding the nuts and bolts here...can someone shed some light?"
LPT: Everything you do must be reproducible,20,72,False,False,False,statistics,1506496724,True,"I don't really come from a statistics/data science background, and don't have a lot of formal training outside online classes, but have ended up in a position where I do a ton of data analysis on a daily basis. And goddamn do I wish someone sat me down and explained this to me sooner.

The reality is it is SO EASY to mess something up and end up with data that looks right but isn't. Forgetting to enable the weights in a survey, messing up the filter in a SQL query, using count instead of distinct count in a pivot table, etc.

I've had clients come back to me four months after a project saying they got different results when they ran the data. Fuck if I know, time to spend 5 hours re-running everything. SAVE YOUR WORK.

I once got to the end of 6 weeks worth of data runs and analyses for a client. Went to double check the data in the final report and couldn't reproduce it. I had no idea what I had run and didn't know if I messed up early or was messing up now. Turns out it was earlier and had to re-do everything. SAVE YOUR WORK.

Most of my day to day work is in excel and SPSS. Unfortunately that means a lot of it is in GUIs that can't be reproduced, so I've been slowly integrating more and more R.

I know is one of things that most of you are going to be like ""duh,"" but on a deadline or just doing exploratory analysis, it's so easy to take shortcuts and forget to do it."
Check Your Residual Plots to Ensure Trustworthy Regression Results!,6,13,False,False,False,statistics,1506499501,False,
Does this sound correct? R2 values,2,2,False,False,False,statistics,1506508382,True,"Hi guys, I'm fine with everything else other than the statistics part in.. generally everything. Anyway I've got to describe the r2 values from my regression models and I'm not entirely sure as to how or exactly what they mean and I'm not sure if this is correct. I've said:

""Although all values are significant, all four response values had weak R2 values. The lowest being albedo (Figure 3.1, R2=0.073) and vegetation cover (Figure 1.1, R2=0.0772) whilst biomass (Figure 4.1, R2=0.122) and FHD (Figure 2.1, R2=0.167) were slightly higher they did not perfectly describe the presentation of the regression models as they don’t explain the variability due to the low R2 values""

Also, is the F value the individual response between two response variables? If anyone can point me in the right direction or give me any help at all, that would be very much obliged. Sorry if I'm a bit confusing but I'm even more confused! 

T.I.A guys, any help will be much appreciated. I think my brains about to explode."
Recent PhD career advice,0,1,False,False,False,statistics,1506513504,True,[removed]
Statistical consulting services - vanwardstat.com,0,1,False,False,False,statistics,1506520283,False,
Help with creating a two variable function from table?,1,1,False,False,False,statistics,1506521654,False,
"Searching for someone with a PHD in statistics, especially focused on financials",0,1,False,False,False,statistics,1506524122,True,[removed]
"Why it's common to remove insignificant coefficients from a linear model, not accounting for Type 2 Error?",1,1,False,False,False,statistics,1506527741,True,"In hypothesis testing normally we fix an Type I Error threshold (alpha) and try to reject the Null Hypothesis; failing to reject the Null Hypothesis means we can't conclude anything, since we're only accounting for Type I Error. 

Why is this conclusion not used in the context of Linear Models?"
What is it like to work at Statistics Canada? What type of work does an analyst/economist/statistician do there?,6,30,False,False,False,statistics,1506528288,True,
Simpson’s Paradox: Two HR examples with R code.,2,15,False,False,False,statistics,1506534965,False,
Is RMSEA a standardized or unstandarized measure?,0,1,False,False,False,statistics,1506539299,True,"I've always considered a standardized measure to be one where the units were transformed via division, and where the units are no longer in the original units. The formula for RMSEA divides by df, but also contains a radical. It's bounded by 0 and 1, so it's not going to be in the original units either. 

So I'm reading a paper and it was called an unstandardized effect size and I was hoping someone could explain that to me a bit. It's a Chinese paper, so I was wondering if the author maybe meant that it's simply less interpretable since it's so affected by sample size (a fact I've read many times). Or is it really an unstandardized measure? "
RMS error vs normalized,0,1,False,False,False,statistics,1506544680,True,[deleted]
Difference of means but no correlation?,2,5,False,False,False,statistics,1506553490,True,"Hi guys.  
I'm running a quick study at my job. We are trying to evaluate the validity of a placement exam that we have for students that want to jump up a couple of courses in the academic program.  
What I'm doing is basically compare¿ing the results of two groups of students:  
1. Students that got to the course via regular path (ie. going through every course prior to the course of evaluation).  
2. Students that got to the course via placement (took the placement exam and landed on the course of evaluation).  


The data is population data, and it was not a normal distribution so I ran a Mann-Whitney U test on SPSS to test mean difference. In all courses, the students that got there via placement exam had better grades. And the difference was significant (though I don't know how important significance is in a mean test if I'm working with population data, not a sample).  

Anyway, it was pretty clear that placement was a factor that favoured students in terms of their academic achievement, but when I ran Spearman RHO to check correlations between the course score and a dychotomic variable (0 regular students; 1 placement students), all of them had minimal coefficients (between 0.05 and 0.15).  


I'm really confused by this. If there's a significant difference of means between groups, them being a regular or placement student should have some sort of relation to grades, right?  
Thanks for your help."
Best statistics jobs for introverts?,17,9,False,False,False,statistics,1506567395,True,"Of course, there are many different jobs and different workplaces, but in general, which statistics jobs seem to have the least contact with people?"
Avail Best Statistics Assignment Help in Australia by Experts,0,0,False,False,False,statistics,1506595326,False,
"Best statistics job for *extroverts* (sorry, had to)",15,17,False,False,False,statistics,1506595786,True,I have the opposite question as a previous poster. 
Z-test for proportions?,5,3,False,False,False,statistics,1506603967,True,"Hi, I want to compare whether cells treated with a chemical produced more abnormal cells at the end of the treatment compared to when treatment began. I have the percentage of abnormal cells found at both the beginning and end of the treatment, can I use a z-test for 2 proportions to compare these two percentages and see if there was a significant difference in the amount of abnormalities? Many thanks :) "
Shapiro-wilk test when sample=population,12,5,False,False,False,statistics,1506604261,True,"I have two pieces of equipment(E1 and E2) for taking measurements and i wish to compare them. I have the real measurements made by much more precise equipment so i can actually create for instance a data sheet with the errors from E1 and E2. 
All good, i was happy, but i was told it was necessary to make shapiro-wilk test to my samples. So i studied a little bit of R, watched some videos about it on youtube but one doubt still remains.. since i haven't take that many measurements, i'm using my whole sample in the tests... so isn't my population and my sample the same thing? is this test still valid?


as you can see, i'm lost hahah. any help is much appreciated"
PAM dataset,0,1,False,False,False,statistics,1506607748,True,"I have to plot data on a map, but it is packed in a PAM dataset (or at least I think it is). How can I transform this into something that I can use in Carto? 

A csv file for example. Or SVG/shapefile.

Link to dataset in question: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/29634

Many thanks!"
Some help needed in representing small difference in time series,4,2,False,False,False,statistics,1506617949,True,"So I have a problem trying to highlight the difference of two similar time series but with small distinctions between them

In a perfect world scenario, the time series I have will have difference that is noticeable at small scale , [a zoom in portion of the whole series](https://imgur.com/lYaYlLE) 

However in reality the time series I have will have noise, [so it end up something like this](https://imgur.com/cJPnJbN)

I look into various methods in represent the time series, autoregression, lag plots, etc.

I cannot seem to find a solid method that magnifies the theoretic difference between a noised series.

Any idea or suggestions how I can approach this problem."
Best way to survey people in-person when considering long data entry?,3,2,False,False,False,statistics,1506620800,True,"I'm sure many people here have dealt with putting data from a paper survey. Doing this can be very time consuming, especially if the survey is a couple pages long.

I'm trying to research what are the best methods to survey people in person and somehow getting the data on a computer as fast as possible. Another huge factor to consider is that these people are being asked to fill out a survey after the class finishes. We want them to get out fast, so we usually give them about 20 minutes to complete the surveys and hand them to us. We want to have this same efficiency.

I know one method is possibly using scantron sheets. Though this may be a more expensive route. If anyone has more info on this, please let me know.

Another method is to use tablets and have people take the survey on an app such as Survey Monkey which can put the data online really easily. The problem here is that we may have to use several ipads, and people may have to wait their turn for ipads. It's a little annoying. "
Anyone Here Using Alteryx?,8,0,False,False,False,statistics,1506622120,True,"My team is strongly considering Alteryx as an analytics tool. I don't know anyone that uses it. Normally I'd advocate for R but based on what I have learned so far, Alteryx seems like a better product for this team as they are not quants by training and this looks quite user friendly. 

I'd like to get some candid perspective from a user so I can help this team ramp up and get the most value out of Alteryx.

Some key considerations for me:

1. Can it function as a database / data warehouse or does it have to sit on top of one?

2. How are its visualization capabilities without Tableau?

3. What kind of statistical tools come prepackaged?

4. How is the linking compared to SQL joining?

5. How easy is it to refresh workflows with new data? For example if a workflow has a 2016 data set and one wants to swap in a 2017 data set, does the workflow take it in seamlessly or is redevelopment needed?

Thanks in advance, trying to pierce through the marketing material is very time consuming."
"Having a mean of 266 days of pregnancy and a standard deviation of 16 days,what percents of pregnancies last between 240 and 270 days?how long do the longest 20% of pregnancies last?",3,0,False,False,False,statistics,1506642762,True,[deleted]
Critique on my resume for Data Analyst internship,19,15,False,False,False,statistics,1506655265,True,"Here is my resume: https://imgur.com/a/mt8ma
(Typo: Related experience should be just ""experience"" as I talk about non-academic stuff)

Hi everyone! I have recently asked the question: ""How hard is it to get a Data Analyst internship/job?"" at https://redd.it/70ruk9. I am very thankful for our r/statistics community, as I received a lot of sincere and valuable feedback. If you think you also have some thoughts about that question, you are more than welcome to add to it. I strongly appreciate that!
And, today, to make my dream of getting an internship this Summer, I again need your critiques on my resume. While I will definitely visit career centers (and I did once), I believe it is greatly beneficial to hear from experienced people like you.There are a few points that I particularly worry about:

1. For my list of technical skills, I am referring to the languages that I can perform tasks from basic to intermediate (R, more advanced for Java) and the languages that I am on the process on learning them, and I see good progress, but of course in the end are still basic to slightly intermediate (Python, Excel, MySQL). For instance, if you refer to ddply or numpy, I can perform some task about them, but for deeper stuff, you still have a lot to learn. So should I just keep them as I wrote, or should I be more honest that: ""I know a few for X, a little more for Y, etc."" 

2. I don't have a project about data analysis at this point, which I am afraid, they will question my ability. I am thinking of doing a few now, but as my schedule gets crazy, that may be a bit rushing. Do you think it is okay for the project part right now? Do the experience part balance things out a little bit

3. How do you feel about the Career Objective? I kinda feel here and there with it!

Thank you so much for your time and input. I am grateful for everything you gave.

   
   "
Performing a within-subjects t-test for which each pair of scores are two correlation coefficients,7,3,False,False,False,statistics,1506658978,True,"Hi,

I have a single pool of 60 human subjects for this experiment. Each subject will complete 8 trials during the experiment. After each trial, I will take measurements for 3 dependent variables from each subject. The first DV is a reference variable. The other two DVs are correlated with this reference variable (and also correlated with one another). My goal is to determine which of these latter two DVs is more strongly correlated with the reference DV. 

I know that one way I can run the analysis is with Williams' Modification of Hotelling's Test. This is a special type of t-test that compares two different correlation coefficients which share one variable (*r*(1,2) vs *r*(1,3)) and are drawn from the same sample. However, this would require computing the two correlation coefficients using each subject's mean score across the 8 trials (for each of the respective DVs). The separate measurements effectively serve to reduce error variance here. 

Alternatively, I could use the 8 separate measurements to generate a unique pair of correlation coefficients for each subject. Then, this pair of coefficients would serve as each subject's scores for the two respective samples in a within-subjects t-test. The t-test would compare the mean correlation coefficient for the 60 subjects for *r*(1,2) to the mean correlation coefficient for the same 60 subjects for *r*(1,3).

My question is whether this second technique is valid given the assumptions of a t-test. If so, any opinions are welcome regarding which is more appropriate. I believe the second would have much greater power. 

P.S.: I would never have designed a study this way purposefully -- this is a partial replication and I am trying to work around the original paradigm as best I can."
Gaussian Process models within hierarchical Bayesian regression?,0,1,False,False,False,statistics,1506664232,True,[deleted]
Going for Masters in Stats after Econ Bachelor’s,14,2,False,False,False,statistics,1506664420,True,"I’m currently a junior in my undergrad in Economics, but I’ve been shifting my interest to Statistics. Instead of switching majors and practically starting over, I’m thinking of doing my upper-level electives in Stats and Math classes I’ve seen that’s needed for most Masters in Statistics degree. 

Will these courses prepare me enough for a Masters in Statistics? 

Probability and Statistics
Applied Regression Analysis
Calc I
Calc II
Foundations of Math
Foundation of Analysis
Real Analysis I
Real Analysis II"
Best Statistics Assignment Help written by Expert Writers,0,1,False,False,False,statistics,1506668314,False,
Comparison of non-parametric data,0,1,False,False,False,statistics,1506674372,True,[removed]
Merging two samples?,8,5,False,False,False,statistics,1506691218,True,"For some odd reason, I'm drawing a blank at the moment. I have data from two surveys. Each were taken by students at two different universities. The survey asked Male and Female students if they liked golf.

--------------------------------------------

Survey for School 1: Male Students = 3 liked golf and 98 didn't.  
Number of Males (nm1) = 101  
Proportion of Males that liked golf: 3/101= 0.03  
      
Female Students = 11 liked golf and 125 didn't.  
Number of Females (nf1) = 136   
Proportion of Females that liked golf: 11/136 = 0.081   

--------------------------------------------------


Survey for School 2: Male Students = 3 liked golf and 142 didn't.   
Number of Males (nm2) = 145            
Proportion of Males that liked golf: 3/145= 0.021       
    
Female Students = 18 liked golf and 238 didn't.             
Number of Females (nf2) = 256             
Proportion of Females that liked golf: 18/256 = 0.07              

-----------------------------------------------------

If I wanted to compare the proportions of all male students liking golf to all female students liking golf, can I combine the data of males and females from the different universities? 

If so, is there a statistical test I need to perform to see if I can merge the sets?

Any help is much appreciated!"
Correlations of small count data,2,5,False,False,False,statistics,1506696543,True,"When working a small sample (usually less than 30) of count data that are themselves very small (&lt;15), can you apply the basic correlation formula between the features?"
Best place to learn statistics,13,6,False,False,False,statistics,1506697462,True,I have an awful professor for statistics and I was hoping I could get some recommendations for a good place to learn stats.
Where Can I Find a Breakdown of the U.S.A Defense Budget?,1,0,False,False,False,statistics,1506700303,True,"  I have been struggling to get this information, and I need it for a research paper. I am trying to support the United States spending big on the army. "
DistLM (Primer/Permanova) with single species data - biological application,0,3,False,False,False,statistics,1506702013,True,"Hello,

I've been asked to look over some analysis - it's an application of DistLM in the permanova add-on for primer. I wanted to ask opinions about using this with single species data - I'm not really familiar with primer as I mainly use R but from what I've read around the subject &amp; other applications in literature, this model is mostly used with distance matrices of response and predictor data, where the response is community (multi-species) data. There doesn't seem to be anything to stop someone doing this with single species data but I was wondering if anyone had any insight into how valid or otherwise this might be?  

I'm a bit dubious about how useful this is, if you have just one species, every time you have the same count or zero in two sites it'll say there is no distance between them, which must influence the model in some way.  

As I said, not really an experienced primer user, just throwing this out there for any thoughts."
Statistics vs Physics major in undergrad?,14,9,False,False,False,statistics,1506703298,True,"A bit of background, I am a junior in a college in the US, and currently a Physics/Math double major. I chose physics because I liked doing it, and I still do, but I never intended on going to grad school for it to be honest. Actually I didn't know what I wanted to do with it, I was just going to see what jobs were out there when I graduated. 

However, a few things have changed for me since I chose physics. For one thing, I started dabbling in machine learning and statistics over the past two summers at my internship (a small quantitative financial firm) and freaking loved it. The more I learned, the more excited I got about learning more and thinking about the potential the field has to solve so many problems. The other thing that changed was the physics work picked up dramatically. Can I do it? Yes. Am I enthusiastic about doing nothing but coursework every waking hour when I know I won't continue with physics after undergrad? Not at all. I don't have time to do research (I have a position with a physics professor), I don't have time to learn and practice programming, I don't have time to learn more ML, and I especially don't have free time, save for maybe a few hours on Saturday some weeks which I spend doing errands anyway.

Herein lies my dilemma: switch to stats or stay in physics (I would keep the math major regardless)? I like doing physics more but I'm starting to feel the pull of wanting to make a career out of stats/CS. I also know almost everyone in the physics department but know little about the stats department, besides the fact that the grad program is ranked a little higher than physics at my school.

I'd also like to note that switching to stats would not delay my graduation, it actually would reduce it by a semester, so that isn't a factor.

This ended up being a bit longer than I had expected, but I'd appreciate any advice from people here. Thanks!"
Statistics about reddit?,0,8,False,False,False,statistics,1506703694,True,[removed]
NFL Confidence Pool - creating an optimal pick set based on win probability and user pick distribution data.,0,1,False,False,False,statistics,1506704809,True,[removed]
Estimate Standard Deviation in a Normal Distribution with a known Mean,12,4,False,False,False,statistics,1506707707,True,"Google isn't helping me find what I'm looking for.  Is it possible to know/estimate the standard deviation of a normal distribution with a known mean?  If so, how?  Thanks."
Need help: hour comparison,0,1,False,False,False,statistics,1506709733,True,[removed]
Difference in writing up the results of an ANOVA that include random effects?,8,4,False,False,False,statistics,1506712205,True,"I'm looking at the following two models:

anova(lm(Z ~ X*Y, data = data))

anova(lmer(Z ~ X*Y + (1|ID), data = data))

The error terms of the second model are smaller than the first, but there is no difference between the two as far as significance of the main effects and interactions (X is significant, Y is not, and X:Y is significant). Would I interpret/write the results in same way, acknowledging that I included a random factor if I use the second? Are there additional considerations when using mixed effect models?"
Which ANOVA run in SAS?,0,1,False,False,False,statistics,1506718071,True,[removed]
"I often hear, ""If you do statistics, you can work in any field"". Why is this?",14,8,False,False,False,statistics,1506732978,True,"I thought you'd need education in stats PLUS whatever area you're putting it with (eg. biology for biostats).

It seems that for things like biomedical engineering (bio + engineering) you need to be equally strong in both areas. So why is it that in stats people tend to say you can go into any area even if you are purely a statistician?

ETA: [here](https://en.m.wikipedia.org/wiki/List_of_fields_of_application_of_statistics) is a nice list of areas which use stats. If you were just good at stats, could you move from area to area quite easily throughout your career? Or would you need to pick one area and stick with it?"
What is the difference between random slopes by group and interacting group with beta in a regular OLS?,7,7,False,False,False,statistics,1506736061,True,I've been starting to experiment with random slopes because I think treatment effect might vary according to a group variable. How is this different from interacting the treatment with the group in a vanilla OLS? Are these different ways of estimating the same thing (treatment heterogeneity)? Or are they substantively different?
I am taking a Statistics class through FOX MBA and need data for a project. All help is appreciated and completely anonymous!,0,1,False,False,False,statistics,1506741262,False,[deleted]
"I’m taking a Statistics class through Fox School of Business and need data! All help is appreciated and it’s completely anonymous, thanks!",2,0,False,False,False,statistics,1506741862,False,[deleted]
Adjusting pValues for High-replicate Cancer Data,7,2,False,False,False,statistics,1506742062,True,"I am currently working with TCGA cancer data and have a question regarding the calculation of pvalues.  I am looking at the expression of a single gene in two different conditions (tumor and normal tissue).  The tumor group has an n=1090 and the normal group has n=113.  Given the large number of replicates, my pvalues end up being very small.  Super! Right?  My question is whether I need to include some kind of multiple testing correction.  From what I have read, it sounds like this is necessary when many comparisons are made, but I am not sure if I am making 1090 comparisons, or only two.  Any help would be greatly appreciated.  I am also happy to make the data available, if necessary.  Below is a link to a simple graph of my data.

https://i.imgur.com/q03BDob.jpg"
I want to switch my major to statistics/QM. Have a few questions...,8,3,False,False,False,statistics,1506744462,True,"Currently doing accounting but I find the major to be boring. My school offers a Statistics and Quantitative Modeling. So here's what I want to know:
1. I'm a big fan of computers and am also learning programming on my own so would a minor in CIS make me stand out more? (CS not offered at my school)
2. Where can this degree take me?
3. How's the pay?
4. Is the degree in demand? I live in the east coast but plan on moving to the west.
5. Any advice on what I should get my masters in?
6. Is the work boring?"
What method should I use to organize all of my data?,5,2,False,False,False,statistics,1506752297,True,"Hey everyone.

I did a correlational survey in my school. Our hypothesis was ""Meat eaters with high GPAs are more likely to be sexually active.""

I'm really not sure how to organize all of the variables into a scatterplot or in a graph form.

The variables are: GPA, eating meat, and being sexually active.

I was thinking doing a scatter plot with my X and Y being GPA and eating meat and using a different colored dot for if the respondent said that they were sexually active?

Thanks for the help."
Mann-Whitney U Test,0,1,False,False,False,statistics,1506780481,True,[removed]
MSc in Statistics from European institution,17,13,False,False,False,statistics,1506785040,True,"I'm final year math undergrad at EU institution researching options for masters in statistics. Due to my own financial situation I must look where is cheapest, so probably be Belgium, Germany or Netherlands. How ever I don't necessarily want to work in these countries and would probably be at disadvantage as I only speak English.

 Wanted to ask if anyone could tell me if all European universities would be recognized when looking for statistic jobs in say UK or US?

Can anyone recommend institutions or programmes? 

Would be great to hear form graduates form Masters of statistics form institution in either Germany, Belgium or Netherlands. Or someone who as hired( or chosen not to hire) graduates from these countries."
Anyone have case studies and practical examples of how to model something using statistics? How do I actually *do* statistics?,13,5,False,False,False,statistics,1506792726,True,"Most of the problems I've been dealing with in my undergrad courses so far are about probability distributions and related topics. The math is just abstract, there are examples here and there, but they always assume some form of data, or the problem always works out nicely. 

Any redditor who is an employed statistician, researcher, or have modeled real world problems for someone shed some light on the process of how the work is done? 

"
"What jobs can I get with graduate coursework in stats, R programming skills, and a MS in a science field? (hopefully SF Bay Area)",0,1,False,False,False,statistics,1506813515,True,[removed]
"Monty Hall, Host Of 'Let's Make A Deal,' Dies At 96",13,62,False,False,False,statistics,1506825090,False,
Submission due in &lt; 2 hours - need help please!!,0,1,False,False,False,statistics,1506835476,True,[removed]
Newbie question about PCA,0,1,False,False,False,statistics,1506888734,True,[removed]
Thinking about dropping out of a Masters of Applied Statistics...,10,9,False,False,False,statistics,1506901188,True,"Hi,

I hold a BS in Math (graduated in 2005) and a Masters in Math Ed (2008). I taught math at the community college and lower-levels at the university for a span of ten years and I could never land a permanent position and decided to leave the field. 

I earned a GIS certificate in 2016, and then moved to Colorado. I stayed with family in Fort Collins while looking for work, but I could not find anything that would pay the bills and/or relevant to science/math/gis. I'm now doing electric and gas designs for distribution/services in Denver. I tried my best to avoid Denver. I spent most my life in the PNW and I'm just not a fan of Denver, the cost of living, all the people, etc. I want to go back to Oregon and live in a small town of Eugene or Bend. 

I've been taking online courses at CSU and I'm just not at all motivated to learn this stuff. The idea was to take some stats courses and that would potentially open doors for me in terms of work. However, I'm not seeing many jobs that are relevant to stats in the towns I want to live in. Given that I'm (kind of) half-stepping in my coursework, working full-time and lack time to focus, and not seeing the jobs in towns I want to live in, I'm thinking about dropping out. I just don't want to accumulate $30K in debt and not have a job or be miserable in a big where I don't want to be just to have a relevant job in stats. Plus, I'm 36 years old, and this program is taking ALL of my free time. It absolutely sucks having suck a busy schedule with work and school at this age. I can't enjoy my down time, go hiking, enjoy my dog, etc. 

I want to to drop, but I feel like a sucker if I do. Any life advice, suggestions, outlook/perspective of life about a masters in stats, any reality checks, etc? Anything would be helpful.


Thank you."
I have a small portfolio of consulting projects I've worked on. Will employers care about this when I apply for internships and jobs?,4,3,False,False,False,statistics,1506901799,True,"I'm a third year PhD student who wants to apply for internships. I've been doing private consulting work for other grad students for about 6 months. With the exception of one project I worked on, I never really do anything exciting: lots of ANOVA and linear models for underpowered experiments. I always present my clients with a nice writeup of what I did for them.

If I compiled some of these projects into a portfolio would anyone care? It would illustrate that I know what I'm doing, but at the same time it wouldn't be exciting. If a portfolio would help, what's the best way to share it with employers?"
"I interviewed my friend Rachael Maltiel who dropped out of her Stats PhD, she now heads an analytics team at eBay [video]",15,63,False,False,False,statistics,1506907973,False,
Data Sampling,3,3,False,False,False,statistics,1506910792,True,"So I have been tasked with making a marketing model for locations at work. We have our current location and two new ones that we are looking to expand into. I am setting up the model to analyze foot traffic and vehicle traffic. Without getting too much into it, I am curious to know if there is any statistical difference in taking sample sizes of 1 minutes or 10 minutes or 1 hour. "
"Chi-Squared, t test or ANOVA?",1,0,False,False,False,statistics,1506913688,True,[removed]
Assumption of independence in the stock market?,16,3,False,False,False,statistics,1506948087,True,"Hello,
I'm going to make a few generalizations and simplifications to try and be on point as to what I'm asking. To be clear, I'm asking out of pure curiosity.

If I flipped a fair coin yesterday, whatever it came out had no impact on the result of flipping the coin again today, due to the independence of events.

Regarding the stock market, analysts and consultants always say 'past performance does not indicate future performance'. That's fair enough and easy to understand.
However, there's no reason to assume independence of events, when for example the events are how a certain stock performed yesterday and how it will perform today. There are momentums, rallies, over inflated expectations that correct themselves, etc.
So let's say that I take performance history of all the stocks traded in a certain stock market, break them down to daily performance, and find there's a correlation coefficient of .7 (for example, anything ≠ 0) between the performance in t-1 and t (t is period in days). 

And let's say all companies have the same number of stocks and all stocks are $1 each.

Does this mean that if I invest in X stocks before the market opens today (when X is large enough) based on the direction of their performance yesterday, about 70% of my investments will be profitable? (excluding outlier events).

I assume there's something off with my logic. Please help me understand. "
Technique for comparing metrics,0,1,False,False,False,statistics,1506954858,True,[removed]
Creating a Dataset for Repeated Measurements - SPSS,0,1,False,False,False,statistics,1506955693,True,[removed]
Want to pursue a graduate degree in Statistics. Need Advice.,15,16,False,False,False,statistics,1506963430,True,"I'm currently a senior in college studying statistics and operations management and am seriously considering pursuing a graduate degree in stats sometime in the future. That being said, I'm quite unsure about what my chances are and could use some advice on how to go about making myself competitive for a solid program over the next few years.

Ideally, I would like to take some time off to work in the real world and return to school afterward. As far as qualifications go, I'm sitting on a 3.5 GPA at Wharton Undergrad and will have taken the following relevant courses by graduation:
- Multivariable Calculus for Business Students (MATH110)
- Probability Theory (STAT 430)
- Intro to Business Statistics (STAT101 &amp;STAT102)
- Intro to R Programming (STAT 405)
- Data Analytics and Statistical Computing in R (STAT470)
- Predictive Analytics (STAT 422)
- Modern Data Mining (STAT 471)

As you may have guessed, one of my main concerns is my lack of coursework. Our majors (called ""concentrations"") are more similar to minors in terms of length and commitment, and I have yet to take any classes in linear algebra, numerical analysis, topography, stochastic processes, or other high-level, theoretical maths. I am also unsure whether my GPA will be competitive. 

Given this, are there any specific steps you would recommend for someone who is looking to go down this path but generally uninformed about stats graduate school admissions in general?"
How many people are needed in a survey on order to show an overall population trend?,9,5,False,False,False,statistics,1506971037,True,Say for a country wide survey.
Need a method for inspecting paperwork for quality assurance.,2,1,False,False,False,statistics,1506978254,True,"Allow me to explain my situation, hoping this will help clarify what I'm looking for assistance with...

I work for a production lab in the Quality Assurance department.  Our Quality Assurance method doesn't involve checking the product itself, but ""document checks"" (checks on paperwork filed with each product that more or less describes manufacturing parameters.)  This documentation is either a ""Pass"" or ""Fail"" based on our inspection.

We currently perform these ""Document Checks"" on 100% of the products we complete.  However, a recent change to our lab's manning has resulted in a reduction to our QA manning.

We've decided to switch from 100% documentation checks to a sample based system.

I'm no statistician and my research has yielded several different ideas, but most of these seem focused on customer's accepting / rejecting entire batches of product based on % of errors from sampling.  This is not a concern in the field of work I'm in.

My main question is this.  I have several years worth of data showing the total number of ""Products"" we have completed every month.  I also have how many ""Passed"" and ""Failed"" document checks we found during the individual months.  The highest ever recorded was a 5% documentation failure rate.

Is there a statistical way to solve this data where I can reach a level of specified confidence (Let's say 95%) that we'd still be catching 99 to 100% of these errors.

TL;DR:  Let's say I produce a minimum of 2455 items every month.  Of these products, 5% fail minimum quality specifications.

Is there a formula I could use to where I could solve for a % of product requires inspections that would result in 95% confidence that I've caught 99 or 100 percent of the erroneous product?

(I should specify the production amount and failure rates (2455 products a month / 5% quality failure rate) listed above are the very EXTREME of my line of work.  I know that changes to our production amount each would change these sample requirements, so I'm giving the extremes of the numbers to generate a safety net, accounting for changing variables.)"
Russian Medals Problem • r/UkrainianConflict,6,5,False,False,False,statistics,1506981454,False,
Grouping cases in R,0,1,False,False,False,statistics,1506989808,True,[removed]
Why do these two chi-squared calculators give different answers?,2,1,False,False,False,statistics,1506999535,True,"This is out of curiosity more than anything. Usually I wouldn't use online calculators for stats, but I couldn't be bothered to open a stats package out and about. But to my surprise the two chi-squared calculators I found through Google give different results.

For example, I plugged in an observed pattern of 27:13 with an Expected Pattern of 20:20, and both results came differently.

GraphPAD
https://www.graphpad.com/quickcalcs/chisquared2/

came up with a p value of 0.0269, and a statistic value of 4.9;

while this one:

http://www.socscistatistics.com/tests/chisquare/Default2.aspx

came up with a stat value of 2.5274 anf p-value 0.111884 i.e. not significant. What's with the difference?"
Need help with hazard ratios and attributable risk!,1,5,False,False,False,statistics,1506999936,True,"Is there a way to go backwards from a hazard ratio (or adjusted hazard ratio) to attributable risk? 

I'm analyzing a paper published in a medical journal and trying to figure out how to take the adjusted hazard ratio the authors provided and convert it to attributable risk so I can then calculate number needed to harm. "
How to make infographics interactive,0,1,False,False,False,statistics,1507001284,False,
SPSS Version Question,2,3,False,False,False,statistics,1507002383,True,"I currently have SPSS 23 on my computer. I know that 25 is the latest version, but our professor only has SPSS 24 on our syllabus. How much different is SPSS 23 from 24? I am a Masters student, and this will be the only stats class I ever have to take because I don't plan on pursuing research as a career."
Ensembling,3,2,False,False,False,statistics,1507009392,True,"Is there a method of ensembling that would make sense to apply weights to models based on their predictive power? If so, or if such a method exists, how do you determine the non-zero weights. I imagine it would be super easy to program into an algorithm, hence my curiosity. I'm super new to predictive modeling by the way; outside of basic concepts taught in undergraduate statistics. "
BayesFit: A module for fitting models to psychophysical data using PyStan and Stan.,0,3,False,False,False,statistics,1507029124,False,
A/B Testing: Impact of Alpha &amp; Beta on Required Sample Size,3,3,False,False,False,statistics,1507034196,True,"I'm trying to scope out a future A/B test.

Let's assume the control group has a 1.5% conversion rate. We think the test group will have a 1.65% conversion rate (10% lift). Given these parameters, I want to ensure significance of the test.

Two levers are Alpha (probability of Type 1 Error) and Beta (probability of Type 2 error).

https://i.gyazo.com/641161aec37650f037c036195a80d2ac.png

Where I get confused in the image I made, is comparing (Alpha 0.1; Beta 0.1) against (Alpha 0.05; Beta 0.15). If my sample size is higher (247k vs 236k), shouldn't my Beta be lower?

Shouldn't Alpha and Beta be a deterministic outcome based on sample size?

I used this tool to produce the sample size requirements: http://clincalc.com/stats/samplesize.aspx (To get Beta, I computed 1-Power)"
Exploring variance among repeats in repeated measures ANOVA design,3,5,False,False,False,statistics,1507037493,True,"This post got no love on cross validated, so I'm hoping I'm not making a fool of myself. 

I have a repeated measures design with five repeats, and a two-way randomized block ANOVA with repeated measures did not reveal my treatment as a significant predictor of the dependent variable. However, when I run a one-way ANOVA for the first repeat, I find a significant result. Considering that this is the first repeat (and independent), would it be acceptable to only perform a one-way ANOVA for this one repeat? 

If it is unacceptable, or just silly, to do a one-way ANOVA on the first repeat, what are my other options for exploring this variability? What would be the relevant post-hoc tests? 

"
"beginner question, is this right?",8,0,False,False,False,statistics,1507041233,False,
How Is Work Like?,11,19,False,False,False,statistics,1507041450,True,"Do statisticians work in groups? Solitary?
What is a day/week/month/etc's schedule like, regarding work, meetings, team work, etc?
I know there are jobs where you work in a team, and jobs which are more solitary.
Anything else I should know?

Thanks"
"Hypothetically, what would be the best tests to run on a salary spreadsheet?",4,0,False,False,False,statistics,1507044980,True,"We can assume that the information on the sheet includes some type of unique identifier, role, years of experience and their annual salary."
What statistical test would be used to determine whether the composition of a legislature (with n parties) differs from population party composition due to chance?,11,16,False,False,False,statistics,1507047666,True,"Legislatures are essentially samples of the population (in both senses of the word ""population""). Ideally, you'd want the sample (Legislature) to be representative of the population.

I recently started learning about chi-square tests, f-tests, t-tests, etc, and I was wondering which test should be used in this scenario. It's relevant because the supreme court of the US will soon decide a case on Gerrymandering, and I just read that the justices say they have ""no way to establish a criterion for whether voting districts are skewed."" I am very skeptical of that assertion, given how powerful I am finding these statistical tests to be. I know that no test is 100% certain; let's assume a 95% confidence level."
"Annualized standard deviation using monthly returns, since inception 5+ years of data",2,4,False,False,False,statistics,1507052980,True,"Hi everyone, I feel like I'm missing something here that I could use straightening out.

I have monthly returns that I want to get the annualized standard deviation. I have exactly 99 data monthly return numbers. In excel, I put all the data in 1 column, and I did the following equation:  =STDEV.P(F4:F999) , where F4 is my first monthly return number. I used population since this is since inception and there's not a sample but the whole population (logical reasoning?). This gives me 2.06%. Now, for annualized, I did =((1+T21)^12) - 1 , where T21 is my 2.06%.  Which gives me 27.66%. Doesn't this seem rather high? I feel like I'm missing something here.... please help me out! "
Betting question,6,4,False,False,False,statistics,1507057490,True,"Let's suppose that we have a really big statistical sample of a given sports event (e.g. the Premier League). We know that for example, for the past 50 years, in all played games, we have 40% home wins (1), 30% draws (X) and 30% away wins (2).
Lets also say that an odd from 1X2 (let's say X) delays for a number Y of games (for example, the last 20 games are chronologically something like 112X122X2X11X2X1X22X1X112221121211221), meaning the odd X has ""delayed"".
Is it reasonable to expect, that the propability for the next game to end a draw, is higher than the respective propability inside a statistical normality, where the odd X hasn't delayed significantly?"
Assessing Accuracy of Point Estimates,3,4,False,False,False,statistics,1507059151,True,"Hi All!

I'm working with different point estimates of a variable with an unknown distribution, and I'm struggling to evaluate different projections for accuracy. Basically, I'm trying to see which system is best at predicting a certain value (Like, the temperature tomorrow, for instance)

What I have done so far is get a bunch of different data points (i), and record what the projections say for the expected value (Pi). I then go back and record what the actual value was (Xi). Typically, I'd try to compare predicted percentages vs realized percentages, but I'm not sure how to best do that when I'm given only point estimates, particularly when I don't know what the distribution is.


I've tried doing a linear regression of Pi and Xi, but that just feels wrong, as I don't get good results even if I ascribe that X is a normal variable with a mean of P and a fixed variance. Do any more statistically inclined redditors have better ideas? Am I making any sense?


Thanks in advance!"
Simple Question,0,1,False,False,False,statistics,1507061448,True,How do I create a relative frequency table in Mac Numbers? I have been watching a few tutorials online and nothing is working for me!! 
What is a semi experimental study?,3,1,False,False,False,statistics,1507064405,True,Which category does the fall?
Stan vs PyMc3 (vs Edward),0,4,False,False,False,statistics,1507069808,False,
focus on advanced statistics or econometrics?,5,3,False,False,False,statistics,1507069812,True,"Hello everyone 

I have some questions regarding statistics and econometrics (also posting this in r/econometrics to get their opinion).

Over 2 years ago I graduated in applied economics (in a European uni). This education covered many different fields (micro and macroeconomics, finance, marketing, ...).

When I graduated, I was really interested in macroeconomics and pursued a job/phd in that area. Unfortunately I didn't find it. After a while I found a job at a public company. I have a very decent job here, I only have overtime during specific times of the year, the job is low stress in general, I have lots of leave (even for European standards) and the pay is good.

However, lately I've been feeling that I am capable of much more. So I dug up my maths and statistics courses (and realized that I forgot a lot). I'm almost through my courses in statistics and will be learning R and perhaps SPSS because I saw those during my education.

I'm not really looking for a job as a macroeconomist anymore (although I'd accept it immediately), but a more quantitative job in general. The company I work for is very big and has quantitative jobs available, mainly in HR, finance and supply chain management. But I would consider leaving the company for an interesting job.

So, to increase my job opportunities, I'm not sure whether I should focus on econometrics or look for more advanced courses in statistics (my nephew studies maths and can give me all his notes and slides from his statistics courses), bayesian statistics also looks very interesting.

Studying both advanced statistics and advanced econometrics would be best, I know, but I work full time so I have to make a choice. I remember my econometrics professor saying that knowing econometrics is a very important asset as an economist. I'll probably stay with my current company so the quantitative knowledge would be related to business economics.
However, I also remember going to a job fair where only 1 of the alumni used econometrics during their jobs (he's a macroeconomist at the central bank) and even employers stated that having a good knowledge of statistics was enough for their jobs, stating that econometrics was only used my macro- and micro econometricians in central banks and universities. 

I'm more inclined to study econometrics because I'm interested in economics, and this would give me a better opportunity to find a job I'm really interested in, but I assume that studying statistics in general would give me more opportunities to find a job in general.

Thanks for your help.

tl;dr should I focus on more advanced statistics or econometrics to improve my odds of finding a quantitative job?"
Interpretation of SEM figure.,0,1,False,False,False,statistics,1507093524,True,[deleted]
Nothing to see here.,13,23,False,False,False,statistics,1507099357,False,
"What are the odds of being injured in a mass shooting, excluding domestic violence?",10,2,False,False,False,statistics,1507128554,True,"This question seems to be coming up a lot after Vegas but I know that most statistics I see count domestic scenarios like the one this month in Plano, Texas in their numbers which in my mind skews the numbers when discussing this kind of attack.


 I have no idea how to figure this out so I hope you guys can help. Would the numbers be easiest discussed as odds over your lifetime vs at any one time, particularly comparing to other data points (lightning strikes, lottery jackpots, etc)?"
Covariance between two exogenous variables is 0.06. What does this mean?,1,1,False,False,False,statistics,1507129922,True,"Is there a rule of thumb for covariances values? I'm feeling kind of lost. 
I'm using structural equation modeling (SEM) in AMOS and I have a control variable (age) that I treated as another IV (exogenous variable) and covaried it with anxiety (my other IV). The covariance between these two is 0.06. But I don't know if this means the covariance is significant or how to explain this. 
Can anybody help me?
Thank you! "
What kind of control chart should I use?,3,4,False,False,False,statistics,1507134562,True,I want to use a control chart to illustrate that a process is out of control. I have a list of average weights taken and the acceptable standard deviation is +- 1.2 pounds with the ideal weight being 50 pounds. What type of chart would you recommend? I'm using Minitab but I could also use excel.
What if a regression model has low p values but low R^2 as well?,8,9,False,False,False,statistics,1507134733,True,"What would you do in this situation if the goal is prediction? Clearly, low R^2 means you are not explaining much of the variance in the response.

But does the low p value for the predictors mean they are significant but the model is not right? For example, perhaps a transformation of the response and/or predictors is in order? 

The situation I described might happen when, say, one of the predictors have a quadratic relationship with the response but you have no such term in the model, right?

Thanks"
Should researchers make sure they understand the precise definition of a p-value?,62,31,False,False,False,statistics,1507135779,False,
Resources Roundup: 2017 Police Data Challenge,0,3,False,False,False,statistics,1507139994,False,
Alternative to Teleforms for automating form-generation?,0,2,False,False,False,statistics,1507147924,True,"I just got a new job, and a big part of it is creating documents for people to fill in using Teleforms. Which, I'm learning quickly, is an extremely tedious task. Given that most of the information that needs to be encoded onto a form is sent to us in a spreadsheet, it seems ripe for automation (even if not fully so).

Does anyone know of programs similar to Teleforms that can be parameterized? The only thing that comes to mind is LaTeX, but I'm not sure if that'd be the best way to do it.

Thanks"
Movie_Partner - Fill this 2 min Google Form to find your perfect movie partner.,0,1,False,False,False,statistics,1507148761,True,[removed]
Drink-spiking data - Help coming up with an empirical strategy,0,1,False,False,False,statistics,1507148911,True,[removed]
Question about lab using measurements and statistics,0,1,False,False,False,statistics,1507150794,True,[removed]
Book/resource recommendations for Clinical Trial Design and Analysis,2,4,False,False,False,statistics,1507151275,True,"Hi all,

I will be transitioning from a role analyzing purely observational epidemiological data to a role analyzing and designing clinical trials. I was wondering if you may have any recommendations for books to brush up on design and analysis of clinical trials?

Thanks!"
Interesting Problem for Serious Geeks: Self-correcting Random Walks,0,10,False,False,False,statistics,1507159077,False,
"DOF with dummy variable, and bonus question",3,2,False,False,False,statistics,1507159787,True,"I have two questions: 

1. Lets say you want to get the degrees of freedom to do a confidence interval. In your regression equation you have a dummy variable D(constant)*X(1 or zero). If your dummy variable for the given equation is zero, would you change your degrees of freedom because technically you have one less independent variable, thus raising your DOF by 1?

2. This is hard to describe in words, but if you were to graph a regression line with a point above it: the distance from the average(mean) Ys and the regression would make up your SSR, and the distance from the regression to the point would be your SSE. My question is: what components would be SSR and SSE if the point is inbetween the regression line and the mean y values? What would they be if the point would was below the mean y values?My guess is all SSE but I want to be sure. 

Thanks!"
How do decide which variable is the exploratory variable and what's response variable,0,1,False,False,False,statistics,1507160166,True,[deleted]
A question about the field and potentially majoring,6,3,False,False,False,statistics,1507163171,True,"I am considering a degree in stats, but I was wondering what work was like for statisticians. Along with a degree in stats, I am considering a degree in sociology. Sociology is like drugs. It deals heavily in the fields I want to work, helping to do some (perceived) good in the worl, but stats seems to be the tool that sociologist Wield to accomplish those task. I'm wondering how closely statisticians are to the topics and fields they run numbers for. How knowledgable are they?

Would becoming a math major for stats mean that I would be a less adept social scholar, even if that is the field I end up working in?"
EILI5 standard error,13,13,False,False,False,statistics,1507164764,True,"Smarter folk,

I'm fairly a novice at stats and have repeatedly failed to fully grasp the application of standard error and it's relation to standard deviation, no matter how many times I read it. Explain it like I'm five, please!

Best,

Weenie Stats Jr."
Deep Learning vs Bayesian Learning,13,1,False,False,False,statistics,1507171317,False,
What are your thoughts on pursuing MS in Biostatistics with a BS in Biochemistry?,0,1,False,False,False,statistics,1507178725,True,[removed]
Tutor?,2,0,False,False,False,statistics,1507193771,True,[deleted]
How to: Compare 5 non linear regression data set,5,0,False,False,False,statistics,1507194215,True,thank you
Test of equality of two probabilities in a multinomial distribution,1,3,False,False,False,statistics,1507194524,True,"Hello! I have a problem that arose while playing games with my wife, and I found two solutions that I'd like comments on, to know if the reasonning is correct, and if not, what would be the fix. 

**Problem**: two persons play a series of N matches of a certain game, of which the outcomes can be player A wins, player B wins, or a draw. We wish to test the hypothesis that the players are of different strength (ie. that they have a different probability of winning).

We assume that the outcome of one match follows a categorical distribution with parameters p_A, p_B and p_draw (with p_A + p_B + p_draw = 1), and therefore the number of wins and draws after N matches follow a multinomial distribution with parameters (N, p_A, p_B, p_draw). Let's denote them x_A, x_B and x_draw with x_A + x_B + x_draw = N.

**First approach: Chi-square test**

* Null hypothesis: p_A = p_B = p.
* Alternative hypothesis: p_A != p_B.

Under H0, we have 2 * p + p_draw = 1, or p = (1 - p_draw)/2. We estimate p and p_draw using the usual sample means:

* p_draw_estimate = x_draw / N
* p_estimate = (1 - p_draw_estimate) / 2 = (x_A + x_B) / (2*N)

We can then perform a Chi-square test:

* Observed: x_A, x_B, x_draw
* Expected: N * p_estimate, N * p_estimate, N * p_draw_estimate

The second line expands to: (x_A + x_B)/2, (x_A + x_B)/2, x_draw

The first two terms of the Chi^2 statistic are equal (x_A and x_B are symmetrical around their average), and the third is 0

* Chi^2 = 2 * (x_A - (x_A + x_B)/2)^2 / (x_A + x_B)/2 
* Chi^2 = (x_A - x_B)^2 / (x_A + x_B)

And then we compare the Chi^2 statistic with the desired threshold value of a Chi^2 distribution with 2 degrees of freedom, as usual.

Two doubts here:

* Since the expected values are estimated from sampling, does the Chi^2 test apply? This is not exactly the same as a test of goodness to fit where the probabilities would be known beforehand. I have the idea that Pearson proved that the difference between known probabilities and their estimates can be ignored.
* The number of degrees of freedom. Usually for three values that have a known sum, there is one reduction, but in this case we assume that two of the probabilities are equal. Would that reduce the dofs by one more? Plus, the third term of the Chi^2 statistic is always zero, so that would perhaps confirm this suspicion?

**Second approach: z-test on the difference**

Same hypotheses. Assuming N large enough, the random variables X_A, X_B and X_draw are normal:

* X_A ~ N(N * p_A, N * p_A * (1-p_A))
* X_B ~ N(N * p_B, N * p_B * (1-p_B))
* X_draw ~ N(N * p_draw, N * p_draw * (1-p_draw))

And have non-zero covariances:

* Cov(X_A, X_B) = - N * p_A * p_B

Under H0, p_A = p_B = p and these become:

* X_A and X_B ~ N(N * p, N * p * (1-p))
* Cov(X_A, X_B) = - N * p^2

We consider the difference of the means: D = (X_A - X_B) / N, which is normal with mean 0 and variance:

* Var(D) = 1/N^2 * (Var(X_A) + Var(X_B) - 2 * Cov(X_A, X_B)) = 1/N^2 * (2 * N * p(1-p) + 2 * N * p^2)
* Var(D) = 2 * p / N

Finally, we estimate p from the samples, and deduce an estimate for Var(D):

* p_estimate = (x_A + x_B) / (2*N)
* Var(D)_estimate = (x_A + x_B) / N^2

We can then perform a z-test for the mean of D:

* z = (0 - (x_A - x_B)) / N / sqrt((x_A + x_B) / N^2) 
* z = (x_A - x_B) / sqrt(x_A + x_B)

To compare to the desired threshold of a standard normal distribution, two-tailed.

**Comments**

* The fact that the results are identical (Chi^2 = z^2) suggest that both approach are valid (or both are invalid...)
* This also suggests that in the Chi^2 test, only one degree of freedom should be used, as the Chi^2 distribution with 1 dof is exactly the standard normal distribution squared, which would make both tests identical.

Would anyone be able to confirm that this is correct? Or why it is wrong? Thanks in advance!"
What's your take on coefficient of variation?,7,6,False,False,False,statistics,1507210469,True,"Definition: sd(x) / mean(x)

I hear about this metric a lot in the supply chain world, and it's used as a measure of relative variation. What are some of the key assumptions necessary for this to be statistically valid, and what are some suggestions or cautionary advice you would provide around using it?"
Singular Value Decomposition Method with Examples and Applications,0,30,False,False,False,statistics,1507212692,False,
Time Series or Poisson Regression for Prediction,10,8,False,False,False,statistics,1507212716,True,"I've been picking at a forecasting model for a gym in town.  They tweet out how many people are in their gym and want to determine if they can use the data to predict usage for present day.

This smells of time series to me, but it also smells of Poisson Regression (these are counts after all).  PR would be easy, and I could do some inference, but I don't know how PR works for prediction.  Likewise,  time series is relatively easy to implement, however the gym is open between  6:00am 11:30 pm so I'm not sure how I can approach the closure period.

Right now, I just use polynomial regression and am off between 9 and 15 individuals on any given day.

Interested to hear your thoughts on the implementation of either method."
"Undergrad in Marketing, Masters in Statistics?",3,1,False,False,False,statistics,1507219089,True,"Is this a viable option? I went to a highly ranked business school, where our marketing program was highly statistics and analytics based."
Transitioning from school to work,0,1,False,False,False,statistics,1507229302,True,[removed]
Need to hire a bio statistician studen,0,1,False,False,False,statistics,1507231097,True,[removed]
Size of Correlation Coefficient for X and Y is Strongly Correlated with Sample Mean for X.,4,6,False,False,False,statistics,1507232401,True,"TL;DR: Does the situation in the title reflect a normal mathematical property of all correlations -- or does it tell me something important about the relationship between *X* and *Y* (my guess: heteroscedasticity)?

Was reading an article in which variables *X* and *Y* were measured for the same individuals at several points in time. The authors also included a correlation coefficient for *X* and *Y* at each of these points. The authors showed a line graph of the means of *X* and *Y* over time, and it appeared that they were affected by various events administered at specific points in time. There was a small to moderate negative correlation, at all points, which was statistically significant at almost all of them.

I was interested in whether these events also had an effect on the size of the correlation between *X* and *Y*, so I made [my own line graph](https://i.imgur.com/u5KoVXj.png) in excel which plotted the means of *X* and *Y* and their correlation coefficients at each of the measurement times. To make this more readable, I converted each of these variables to *Z*-scores.

Upon examining the line graph I noticed something that struck me as odd. The line representing the size of the correlation between *X* and *Y* was extremely similar to the line representing the mean value of *X*. Regardless of when the measurement was taken (with respect to the timing of the events, or the passage of time in general), larger mean values for *X* seemed to be associated with larger correlation coefficients between *X* and *Y*. While there were only 8 points for each of the lines representing the mean *X* and the correlation between *X* and Y*, I did run a correlation just for a ballpark, and found a positive correlation with an *R*^2 of ~.68 (and the *R*^2 for mean *Y* and correlation between *X* and *Y* was ~.34, negative correlation).

Now without the raw data (even the mean values had to be interpolated from a graph in the paper), I can't do any typical analyses to see what's going on behind the scenes. This could totally be a normal mathematical property that I am not aware of. However I don't think that's what I'm seeing. It looks as though the strength of the negative relationship between *X* and *Y* is highly dependent on how high *X* is and to a lesser extent, how low *Y* is. In other words, the relationship is strongest at one end of the spectrum, and gets weaker as it approaches the other. I suspect that what I have observed might be a manifestation of heteroscedasticity (again, without the data I can't just check that for myself). 

So, the question is, normal mathematical property, or indicative of something else about the relationship between *X* and *Y*?"
I need help with statistics problem finding confidence interval.,3,0,False,False,False,statistics,1507233306,True,I have three sets of data from a lab experiment and I need to find the confidence interval. Im using excel. I am very lost and this is my first class Ive needed statistics for. If anyone can explain what CI is that would be helpful as well.
What statistical analysis test do I use?,0,1,False,False,False,statistics,1507233642,True,[removed]
The Role of Statistics in the Data Revolution?[2001],1,7,False,False,False,statistics,1507234277,False,
What are typical opportunities for working from home?,1,1,False,False,False,statistics,1507240411,True,[removed]
Simple Poker Calculation,6,2,False,False,False,statistics,1507274217,True,"Looking for odds of certain starting hand odds for no-limit poker. Am I correct in thinking there are 3 categories of odds? Pocket pairs, any 2 suited (Ace King Suited), and any two off-suit (Ace King off).


Am I Calculating these correctly?

Pocket Pairs= (4/52)*(3/51) = 0.452%

Ace King Suited= (4/52)*(1/51) = 0.151%

Ace King off= (4/52)*(4/51) = 0.603%


"
Need help spss multiple response,1,2,False,False,False,statistics,1507274698,True,"I have around 10 risk factors for a certain condition and what I want to do is to analyze the no. Of risk factor if it's associated with outcome (ex outcome of people with 1, 2,3,4 or above risk factors) so how should I do it? Forgot to mention the risk factors have been recorded into 0 and 1 where 1 means yes but some of the risk factors have  missing values don't know if it will affect outcome. Thankyou in advance everyone. "
How can I use a calculator to benefit me in a statistics exam?,9,0,False,False,False,statistics,1507283492,True,"I am very inadequately prepared for my upcoming statistic exam. I figured that rather then learning to do all of the caluclations/notations manually it would be far easier to learn to do them on a calculator. (this is actually allowed) Im not sure where to start with learning this or if the functions I need will be in the calculator by default or if I will need to make my own programs for the calculator. Any help on this is massively appreciated as its a huge source of stress right now. Although I am for the most part not familiar with the terms I have listed they are what will likely be on the exam if that helps in giving advice. 

calculate P-values, mean,median, mode, variance, sample and populations, confidence intervals, null and alternative hypothesis, test statistic, two/one sample t-test, binomial distributions, using excel commands, two sample proportion test, chi square test, reference distribution, random variables"
Graph to compare three independent variables,2,2,False,False,False,statistics,1507289122,True,I want to see the relation of three continuous independent variables through graph. What kind of graph should I use using SPSS? 
Is it possible for me to calculate my curved score based on data I have?,10,1,False,False,False,statistics,1507301665,True,"Hey, so I'm taking my first Stats class this semester, and this is more of a general question since the class has got me thinking. No this isn't homework, yes I'm weird and am inquiring into statistic, on reddit, in my free time.

So my Econ class just had a test, but we did so poorly on it that the teacher is curving our grade. I approached him after class, and he gave me some raw data for how we did on the test. I'm wondering if it's possible for me to calculate my final grad from only the info he gave me. I tried looking online, but none of the resources I found gave instructions on how to calculate specific scores.

My Grade: 72

Class mean: 53.55

Class median: 52

Lowest grade: 24%

Highest grade: 96%

Population/Sample Size: 92 students

He never said what he was curving it to (I'm assuming an 80? idk what college teachers usually curve to), and I'm also assuming it's going to be a normal bell curve.

My stats class has gone over standard deviations, bell curves, normal distribution, and all that sorta of jazz so far, but we've never applied to anything like grade curves or normalizing one bell curve to another. Any insight would be very appreciated "
Population Statistics Distribution Question (need help ASAP),0,1,False,False,False,statistics,1507305389,True,[removed]
PCA and what Axis 1 and 2 represent,3,0,False,False,False,statistics,1507311808,True,"I did an experiment on plants with three groups (control, drought, chemical) and measured their reflectance to calculate reflectance indices. So I have 3 groups (n = 8) and 18 reflectance indices calculated for each.

[Here you can see some of the data in the table, the resulting PCA graph, % variance, and the coordinates.](https://imgur.com/a/yXLTo)

My question is how to explain what is in Axis 1 and 2. What is causing the separation between my SD and SR groups?"
How to run ANOVA of non-linear regression curvefit values,0,1,False,False,False,statistics,1507314266,True,[removed]
How to run ANOVA of non-linear regression curvefit values,2,5,False,False,False,statistics,1507315912,True,"I have a data set of concentration response curves that I've used to calculate EC50 and Emax values for different treatment groups. If I want to run an ANOVA of these values, should I take the calculated values for the curvefit of all data and the calculated standard error, or do curvefits of each individual replicate and enter those individual values as replicates for each mean?"
F test for two means rejection regions?,2,3,False,False,False,statistics,1507321170,True,"Hey,

Could someone tell me the rejection regions for the F test when comparing two means for various $H_0$ ?

H_0: mu1 = mu2

H_A: mu_1 != mu_2


H_0: mu_1 &gt;= mu_2

H_A: mu_1 &lt; mu_2


H_0: mu_1 &lt;= mu_2

H_A: mu_1 &gt; mu_2

I'm confused on how the rejection region would look for the other two cases where it is greater than or less than.

Thank you for reading


"
Stat Help,0,1,False,False,False,statistics,1507327535,True,[removed]
Looking to gain some insight on the statistics of what people are looking for in their financial futures,0,0,False,False,False,statistics,1507336113,False,
Is this appropriate for repeated measures?,7,4,False,False,False,statistics,1507340668,True,"I conducted an experiment, replicated 6 times, in a randomized complete block design. I took plant height weekly, along with several other measurements. I had 10 species, some with 3 cultivars, some with 1 cultivar, for a total of 24 experimental units. I also progressively added salt to the soil to evaluate salt tolerance. Is it appropriate to run repeated measures on these data? If not what is the best approach to b evaluate the relative salt tolerance?"
Survey Question for my STAT class. Takes only a few seconds. Thanks!,0,0,False,False,False,statistics,1507342286,False,
15 AdWords Stats To Drive More Traffic,0,1,False,False,False,statistics,1507368365,False,
I'm pursing graduate school in social psychology and want to learn R to improve my application and overall statistics skills. What's the best way to learn R for beginners with no background in programming?,38,55,False,False,False,statistics,1507380104,True,[deleted]
"Class interval, limits, boundaries, width and midpoint - Treatment of Ex...",0,0,False,False,False,statistics,1507417000,False,
Advice for my first Data Cleaning and Visualization project,5,8,False,False,False,statistics,1507432425,True,"Hi everyone! I am Junior college student with Statistics major and Computer Science minor. My plan is to work as a data analyst after graduation, and then, in the next few years when I've gained decent work experience as well as earned my Master Degree in Statistics, I will try to be officially a Data Scientist.
For this Summer, I am trying to apply for a Data Analytics internship. I've talked to a few people on Reddit about my qualifications to apply, and they said it would be great if I can put a project using statistical programming languages. Now, as I only have the knowledge equivalent to the first semester of a year-long Applied Probability and Statistics course, I don't think I can do much with fancy data analysis or machine learning. Hence, I try to focus on Data Cleaning and Visualization, which I can compensate the lack of knowledge by my coding skill.
For the project, I intend to perform a study of 10 - 15 graphs, charts and their applications. For instance, when to use boxplot and what we can learn from it. I will choose some not-too-dirty data sets to perform basic cleaning as well. And all the code for graphs, charts and cleaning work will be available in both R and Python. There will also be brief analysis regarding descriptive statistics for each graph. With that idea being said, do you have any advice or idea on which kind of graphs I should include and which datasets I should use to make those graphs? It would be great if we have a website that provides comprehensive information about various kinds of graphs.
Also, as a beginner, I am still a bit unclear about data cleaning. I know dirty data may be missingness, incorrect data, or bad-formatted data, etc. but overall, given a dataset, I may not know how to conclude when the data is clean enough for me to do all the work. So, I definitely need some advice on that too.
Please help me if you can. I value our kinds of input you gave. The project itself is a beautiful learning experience for me, which helps me a lot in the future. Thank you so much!"
Repeated measures ANOVA help,1,1,False,False,False,statistics,1507443721,True,[removed]
Descriptive statistics of panel data,0,1,False,False,False,statistics,1507445713,True,[removed]
Measurement error of a ratio.,4,8,False,False,False,statistics,1507450574,True,"Lets say I have a weight that weighs with a measurement error of +-3 kg

My weight is measured as: 100 kg
My girlfriends weight is measured as: 50 kg

The ratio of my weight compared to my girlfriends is 2.0

What is the measurement error of this ratio?

Would the same apply if calculate the ratio between to measures that I know the 95% CI for?"
Are these good reasons for pursuing stats?,4,6,False,False,False,statistics,1507453067,True,"I am a 2nd year statistics student in university, and I'm wondering if I have a realistic picture of a statistics career. Here are my reasons for studying stats.

Firstly, data seems to be ubiquitous now, and I really like the potential of choosing any industry I want to work in, within reason. On a related note, I feel like studying statistics makes you understand other fields a little better. For example, if you understand data, maybe you'd have an easier time reading a scientific study in physics, compared to someone who can't interpret data. I know this is overly-simplistic, as you also need to know physics to read a physics article...

Also, I'm attracted to the idea of working ""behind-the-scenes"", or in a support role. I'm not sure why. So for example, while I've never wanted to become a doctor, I am interested in biostatistics and experimental design, and working alongside medical scientists in that respect. I also think signals processing is cool, having read about SS applications in medicine, satellites, engineering etc. 

I would like to eventually do data work in the sciences/engineering fields, rather than something like finance or social science. Taking all of this into account, am I being too naive? Will I be disappointed? "
What statistical analysis would I need to perform if I wanted to determine the magnitude of the contribution of my predictor variables on the variance of my response variable?,3,4,False,False,False,statistics,1507474038,True,"For example, let's say I have performed an experiment over 4 locations, each an RCBD with 3 replicates and 20 treatments and I wanted to determine the magnitude of influence of the treatment, location and replicate effect on the variance of my response. From my understanding this is a 'combined analysis'.

Sorry if my terminology isn't quite clear - I can elaborate if need be.
"
Options for multivariate total least squares regression in R (preferred) or Matlab?,1,3,False,False,False,statistics,1507485055,True,"Just looking for packages that can implement total least squares/orthogonal regression. R is my main language, but I have some experience in Matlab if it would be easier to do it there. 

 Trying to fit data that uses the same assay to determine measurements ex vivo and in vitro. The overall goal is to determine the ex vivo measurements based on the in vitro. I assumed that with the same kind of assay error contaminating both IV and DV, orthogonal regression was the way to go. 

Fixed effects: &lt;Dose of drug when measurements were taken&gt;; &lt;in vitro measurement&gt;

Random effects: &lt;Patient ID&gt;


many thanks, 
more_momus"
Box Plot from Frequency Table?,6,2,False,False,False,statistics,1507493859,True,"Hi, 

I'm working on a research project where I performed an analysis of some (physical) samples and I now have frequency table for each. I'd like to plot the samples on a single graph in box-plot format, however I can't seem to find a simple way of doing this. I use origin and matlab. Both of these take raw data. I can't convert my freq table into raw data as this would produce an unimaginably large data-set. Does anybody have any suggestions how I can go from freq distribution table to box plot without much hassle? Any help would be very much appreciated!!"
"Should I go ahead and take Cal 1, 2, and 3 if I want to get a MSc in Applied Stats or MS in Data Analytics?",9,3,False,False,False,statistics,1507496538,True,"I’m an undergraduate sociology major in my fall semester of junior year. This semester I’ve really discovered that I really like statistics and analysis (in the class I’ve been in, we’ve been analyzing data through a social research focus especially). I think data analysis is something I’d want to turn into a career. 

My University doesn’t offer a stats major and from what I can tell from the math department I could take cal 1 and cal 2 before graduation and maybe linear algebra or cal 3 if I can find a way to pay for it during the summer semester. 

I’ve only taken math up to Pre-calculus in college and have never liked math much at all but stats has become something I really have an interest in. 

Basically, are cal 1, 2, and 3 really necessary for a grad program in applied stats or data analysis? Obviously for applied stats it might be necessary, but I’m not so sure for data analysis programs. 

I’ve been in what’s basically an introductory statistics class, so I haven’t been introduced to higher level curriculum, so I’m not sure what I would need for graduate school. We’ve been using STATA but from what I can tell, R seems to be more pervasive, so should I learn that as well?

Sorry if this seems scattered, but I’d appreciate the guidance!
"
How well do Bayesian Mixed Clustering Models account for dependence of features?,3,15,False,False,False,statistics,1507499613,True,"I'm working on clustering pitches thrown by MLB pitchers over the last 10 years on a per-pitcher and cross pitcher basis. I've written a scraping algorithm to capture the release point (in horizontal and vertical space), location of the ball hitting the strike zone (again horizontal and vertical space), spin rate, spin direction, break angle, break length, start speed, and end speed of every pitch thrown over the last 10 years. MLB's classification using the pitch grip (four-seas fastball/sinker/slider/etc) is not fulfilling since different pitches have different purposes and one player's slide will be extremely different form another's. 

So I'm looking into unsupervised clustering algorithms from a Bayesian perspective. I'm not sure of the exact algorithm I'll use yet (looking at the Finite Mixture Model in section 2.2.2 of [this paper](http://www2.stat.duke.edu/~kheller/thesis.pdf)). But, many of my input variables will have major dependencies on each other. Things like start and end speed could be changed to start speed and end speed - start speed, but some of the dependencies are harder to simplify, like spin direction and break angle. I could do a PCA, but would prefer not to since I want the limit to which the model feels like a black box. 

The paper I linked did not mention how well a Finite Mixture Model handles these dependencies, so if somebody could point me in the right direction, that'd be great. 

Thanks! "
Reference for poor sampler mixing in large bayesian models,2,12,False,False,False,statistics,1507555666,False,
"What are the loss functions which are based on Mahalanobis distance, Hellinger Distance, Bregman divergence and Bhattarcharya distance respectively?",4,3,False,False,False,statistics,1507565512,True,"For example, RMSE is based on the Euclidean distances between the points, similarly what the equivalent for these"
What would be the best way to analyze data like this?,9,7,False,False,False,statistics,1507586015,False,
College majors survey! no name required,6,0,False,False,False,statistics,1507591625,False,
"If you flip a coin 1,000 times and end up with net H heads (# heads minus # tails), what is the probability that at some point during the procedure, you were up by at least 1.25H heads?",13,11,False,False,False,statistics,1507601731,True,"Can this be answered with a generic formula as I framed the question? Obviously it depends on the net number of heads you end up with. If you have 750 net heads (875 heads, 125 tails) - of course extremely improbable - there is zero chance that you were ever up by 1.25H = 938 heads. But if you ended up with H=4 (502 heads, 498 tails), it is highly likely that you were up by at least 5 heads at some point."
Using t-SNE to understand Middle Eastern Politics,0,1,False,False,False,statistics,1507616172,False,
How many groups should i use when performing Hosmer-Lemeshow goodness of fit?,0,1,False,False,False,statistics,1507624395,True,"I am performing a Hosmer-Lemeshow goodness-of-fit test.

As I understand it, by tradition one uses 10 groups, but this group number is arbitrary. 
I only have 39 observations, so dividing these into 10 groups seems a bit excessive. I cant find any guidelines on choosing the number of groups.
Do you have any advice in this regard?
Cheers"
Why minimize NLL instead of maximizing LL?,4,1,False,False,False,statistics,1507627955,True,"So I'm looking into the details of maximum likelihood estimates, and this is bothering me. I understand that we take log because it is computationally easier and outputs smaller numbers, but I don't understand why we have to minimize Negative Log Likelihood, when we could just maximize Log Likelihood. 

To me these methods seem equally hard to compute. In both cases we would take the derivative and set equal to 0, and both methods would yield the same result. Why add the extra step of converting LL to NLL by NLL = -LL ?"
Visualizing Data Distribution: Here some Box Plot variations you might know.,0,1,False,False,False,statistics,1507628102,False,[deleted]
Visualizing Data Distribution: Here some Box Plot variations you might not know yet,8,49,False,False,False,statistics,1507628708,False,
"Ordinal Regression, Exponents and Odds Ratio",1,1,False,False,False,statistics,1507635116,True,[removed]
Checking multicollinearity with categorical and numerical variable s,0,1,False,False,False,statistics,1507640561,True,[removed]
Quick Question about Multiple Imputation in SPSS,2,1,False,False,False,statistics,1507645482,True,[deleted]
The Impressive Growth of R,11,46,False,False,False,statistics,1507647293,False,
More p-value decimal places in R,5,2,False,False,False,statistics,1507656436,True,"Hi everyone, hopefully this is a quick and easy question but I'm having trouble finding good advice via Google--I'm running mediation models using R (mediation) and the results return p-values with insufficient decimal places (0.00). How can I get R to give me more decimal places?

My repeated failures: https://imgur.com/a/5kRKy"
Variable standardization in logistic regression,0,1,False,False,False,statistics,1507662391,True,"I am working on fitting logistic regression models using LASSO, which requires covariates to be mean centered and unit variance. I have both categorical and continuous variables in the model. If I want to include interactions in my model (lets say the interaction of x1 and x2), do I use the product of two standardized variables or do I standardize the new interaction effect x1*x2? What if I am using interactions between a categorical variable and a continuous variable? For example, I have M/F in the model and a continuous variable x. Do I need to standardize x*M and x*F separately or just standardize x and use x_s*M and x_s*F? Thank you for your help!
"
Does something like a biunivocal/symmetric Mahalanobis distance exist?,1,1,False,False,False,statistics,1507664240,True,"Say we have two multidimensional vectors X1 and X2, each associated to a covariance matrix, C1 and C2. Y can measure de Mahalanobis distance between the vectors using C1 or C2:

D1 = (X1 - X2)' C1⁻¹ (X1 - X2) or

D2 = (X1 - X2)' C2⁻¹ (X1 - X2)

Is there a way to measure a distance between the vectors taking into account both covariance matrices? It seems to me that this is related to the subject of total least squares but can't get my head around it."
Questions about Excel and SQL in Data Analyst Internship,4,1,False,False,False,statistics,1507665535,True,"Hi everyone, I am trying to apply for a Data Analyst internship this Summer. Base on my searching from job descriptions so far, most Data Analyst internships require Excel and SQL, some even specifically say ""deep expertise"" or ""proficient."" Some of them also say it would be a plus if the candidate knows R, Python. 

In my case, my college coursework is all about Math, Statistics and Computer Science, so I have not had many experience with Excel. I do know how to do write functions and perform basic tasks in Excel, just not an expert. About SQL, I am taking it next semester. Rather, I am more comfortable with R and Python, as I tried to learn them over the Summer, and I had a good programming background, too. 

With Excel and SQL, I think if I have a few months, I think I can pick them up. But, as the deadline for Summer internship is close, I don't want to miss out and decide to just apply. So I guess my questions are:

1. Should I just list R and Python as my related skills? Or how can I explain that I am not proficient at them now, but I can make sure I will catch up from this point to the internship time?

2. I also read some other posts on Reddit. They said that it is true Excel and SQL are important and used at work, but they are ultimately just the tools. As long as you have good knowledge of Statistics and analytical thinking skill, the companies can help you pick up the tools pretty fast. Is that correct? I personally somehow see that as I learned Python and R pretty fast. Turning knowledge into practice is truly the deal.

3. Seem to rephrase question 2, but do recruiter really expect all the qualifications in the job posting from me? Essentially, I am still a college student, and data analyst position does not require advanced knowledge like data scientist, so it is fair for me that I don't know everything yet. That should be the point of internship, helping fresh people get into the profession, am I right?

Basically, just those skills make me worry. I am doing good in my Statistics and Math classes. I have good programming skill. I usually do free tutor for my friends, and they like me, so I also think my interpersonal skills are okay, too. I really want to get work experience, so the internship is definitely a dream comes true for me.

Please share your thoughts, comments, critique, or encouragement if you can. I value all of that. Thank you so much! 

 "
How Instacart uses Monte Carlo simulations to balance supply &amp; demand in a complex on-demand marketplace,2,42,False,False,False,statistics,1507670249,False,
"I am preparing for a test in an elementary statistics class, and need help with a probability question",4,0,False,False,False,statistics,1507679688,True,"The question is as follows - 

The numbers 1 through 13 are written in separate slips of paper, and the slips are placed into a box. Then, 4 of these slips are drawn at random.

What is the probability that the drawn slips are ""1"", ""2"", ""3"", and ""4"", in that order?

I'm not searching for an easy answer here. I'm struggling to find the proper method for solving it. Thus far, I have calculated the following

p(drawing a 1)=1/13=.0769

p(drawing a 2)=1/12=.0833

p(drawing a 3)=1/11=.0909

p(drawing a 4)=1/10=.1


Now, I am struggling to remember what to do with those numbers. There is a formula here I am just failing to execute, I'm sure.

Note - I have no idea if this is a normal kind of post on /r/statistics, so I apologize if this is not the place to ask."
Trouble understanding and interpreting results of variance components analysis.,0,0,False,False,False,statistics,1507679891,True,"I am trying to figure out the variance components of this model : yijr = ai + bj + repr(j) + eijr using my test data :


Linear mixed model fit by REML ['merModLmerTest']
Formula: y ~ (1 | a) + (1 | b/rep) + (1 | a:b)
   Data: t
REML criterion at convergence: 4738.329
Random effects:
 Groups   Name        Std.Dev.
 a:b      (Intercept)  0.000  
 a        (Intercept) 26.118  
 rep:b    (Intercept) 17.424  
 b        (Intercept) 11.209  
 Residual              3.025  
Number of obs: 810, groups:  a:b, 270; a, 90; rep:b, 9; b, 3
Fixed Effects:
(Intercept)  
      120.5  

So from my understanding, if I set up this data and gave 'true' variances to my factors assuming I have the proper model specified, I should expect the variances for each of my factors to be close as possible to my 'true' variances, right? If I have misunderstood the variance components, how would one interpret the output to my above model?

The true variances being : 
a - 682
b - 150
rep - 204
error - 8.8



"
What would be the best way to visualize data on a scatter plot?,1,0,False,False,False,statistics,1507680507,True,"stemming from this thread here: https://www.reddit.com/r/Amd/comments/75jbir/can_you_guys_help_me_raise_awareness_for_review/do708aw/?context=1

It's been a long time since I've taken a statistics course, and now I'm starting to second-guess myself.  Is the OP right, or am I?"
Statistics : Calculating Variance : Treatment of Experimental Data,0,1,False,False,False,statistics,1507685649,False,
"Standard Deviation, Average +/- or Something Else?",0,1,False,False,False,statistics,1507689528,True,[removed]
What would be the outcome of applying a proportion test to a continuous variable?,2,3,False,False,False,statistics,1507698305,True,"I'm aware proportion tests are only meant for binomial data, but if one did apply a two sample z test to a continuous variable, for example, how would this alter results? 

The context here is that some marketers within my org have been applying z tests two non-binomial data and have been getting wonky results, but I'm trying to get a better understanding of how data can be affected."
Statistics vs Mathematics with stats focus for undergrad?,8,7,False,False,False,statistics,1507707127,True,"I started out as a maths student in university, but found myself more and more drawn to statistics; I realized that I enjoy mathematics the most when I'm applying it to something else. So, naturally, I switched my program to statistics. 

Eventually, I'd like to get a masters degree in this field. This sounds like a dumb question, but in terms of preparing for grad school, would it have been better to just stick with maths, and just choose a bunch of stats electives, rather than going right into stats? I'm a bit worried because all my statistics professors seem to have studied pure math as undergrads, only to focus on stats in grad school. Maybe I'm just being neurotic; for some reason, I regularly fear that I'm unknowingly screwing up my future and education."
Using R's Survey Library for Stratified Mean,0,1,False,False,False,statistics,1507721722,True,[removed]
"[HIRING] Tampa Bay Rays: Baseball Research and Development Analyst, Junior Analyst and Intern",2,10,False,False,False,statistics,1507728827,True,[removed]
Can KL Divergence be used as way of PPC?,1,1,False,False,False,statistics,1507733217,True,[deleted]
What sampling method would this be? First year HS stat student.,4,0,False,False,False,statistics,1507743424,False,
"Discrete distribution, symmetric, finite support and variable variance?",2,2,False,False,False,statistics,1507745740,True,"What discrete distribution satisfies these properties? 
1) A finite support, say from 0,1,2...,M. 
2) Symmetric about the mean, which is located at M/2. 
3) A free parameter to change the variance, from 0 (if M is even) or 1/4 (if M is odd) up to some value.

"
Determine probability density function?,4,0,False,False,False,statistics,1507745859,True,"I receive summary data from my measurement machine: Mean, median, standard deviation, and interquartile range. To my knowledge, I cannot extract the raw data. Can I use these parameters to estimate the pdf? Or determine the family of distributions? Mean≠median for these data. Thanks."
Propagating Uncertainty when Averaging Measurements,5,6,False,False,False,statistics,1507747060,True,"I'm having trouble wrapping my head around how to account for uncertainty/std.dev. when averaging a set of values with their own uncertainty or std.dev.  For example, if I have a set of measurements:  

10+/- 0.5  
10+/- 1  
75 +/- 5  

The standard dev. of the average should be pretty high, but if I propagate my uncertainty via RMS, that value doesn't reflect a high std.dev.  

Like wise, if I have a set:  
10+/-0.5  
10+/-0.1  
10+/-5  

Taking a simple average of those values will give me a St.dev that doesn't reflect the accuracy of the measurements initially.

What is the proper way to approach this problem?"
Best way to test if method of count estimation is appropriate?,1,3,False,False,False,statistics,1507750469,True,"So, working through some data where I have to essentially count the number of particles in each replicate. Unfortunately, sometimes there are so many particles that I need to count that it takes an unrealistic amount of time to count all of them. Because of them, I'm working through a new method where I'll estimate the total count by subsampling. I want to make sure that this subsampling method is accurate. 

To do this, I'm thinking about counting a bunch of the replicates and then subsampling them using my estimation method. Then I'll calculate the CV for the full counts and the percent error for the estimation method. Would this give me a good indication of whether this method is accurate if PE is less than CV%? I'm trying to establish if the measurement error within replicates is less than the natural variability among replicates.

So in summation if:

Percent error (estimation method) &lt; CV % (full count) 

Then I'm good to go?"
"When using a Bayesian approach to determine a regression coefficient, I am unclear on exactly what the likelihood distribution is and how it is constructed.",4,9,False,False,False,statistics,1507750622,True,"My understanding is that when using a Bayesian approach to get an estimate of a regression coefficient, that coefficient's value is sampled from a posterior distribution, which is a combination of: the prior distribution (representing prior beliefs about the likelihood of different coefficient values) and the likelihood distribution (representing the likelihood of coefficient values given the observed data).

I'm unclear on exactly what that likelihood distribution is.

Suppose I have an experiment in which I want to know if men and women differ in their preference for blue vs. red. I ask 1000 people which colour they prefer. Their choice (blue vs. red) becomes the dependent variable. I have a coefficient representing whether a person was a male or a female. How exactly would this sample of data be used to construct a likelihood distribution? My understanding is that such a distribution would be the likelihood of different coefficient values given the observed data? Is that right? 

Edit:

Also, is the likelihood distribution: the likelihood of the data given various parameter values? or the likelihood of different parameter values given the data?

Edit 2: 

It seems ilke it is the former (i.e., the likelihood of the data given various parameter values)."
Uncertainty with Random Sampling [Highschool/First Year STAT],6,2,False,False,False,statistics,1507751135,True,"I'm having some difficulty determining whether these scenarios are considered random simple sampling or not. (SRS)

1) Say a marketing researcher collected opinions from shoppers in a mall who are willing to stop and fill a questionnaire.  Would she have produced a simple random sample because she was in a shopping mall?  


2) A question that my professor had asked in class today to think about was since Canada has a population of roughly 37 million and the US, 326 million (much larger).  Is it true that random samples of Americans should be larger than samples of Canadians?  


I'm thinking about Cluster, Stratified, Systematic and SRS..  I believe when taking a stratified sample, you have to have proportional samples of each strata..  Sorry, I'm becoming confused.

P.S First time seeking help here, sorry if this wasn't the right place to post."
Question about TwoRavens web application.,0,1,False,False,False,statistics,1507755754,True,"CCES released their study here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910/DVN/GDF6Z0

If explored online through their tool, I cannot figure out how to link multiple variables to see their frequency. Does anybody have some insight on how to do this? It also has a weight variable ""commonpost_vv"" that needs to be applied.

For example, suppose I want to filter the sample down to an age bracket and state of origin. It doesn't update the subsets when I select them so I'm sure I am linking improperly. Thank you for any help with this. "
How much weight does a Masters in Statistics hold over a Masters in Analytics?,19,17,False,False,False,statistics,1507759917,True,Looking into Masters in Analytics programs currently but there seems to be a bit of a negative connotation towards these programs. I currently have my undergrad in Econ and graduated in 2015.  
High standard deviation on an exam,3,1,False,False,False,statistics,1507763861,True,"I am not a math or stats student.  What does it mean, from a final grade point of view, when you take an exam with a standard deviation of about 20 points and you score like a point below average."
FDR in business and marketing,2,0,False,False,False,statistics,1507775103,True,"I know that FDR is used in microarray data. I was curious if anyone had any papers on FDR being used in marketing/business. One issue I see is that a lot of marketing data is passively collected instead of through experimental design. This puts it in the wold of post selective inference. Still, I am curious about non health applications of FDR."
NEED HELP WITH HW QUESTION,0,0,False,False,False,statistics,1507778631,True,[removed]
Regression with Bimodal Distributions - not really sure how one would deal with this,4,1,False,False,False,statistics,1507778874,True,"I've been struggling over this question for a few days. Essentially, I have a dataset with a count outcome variable (bounded 0-7, indicating number of days in a week X behavior was enacted) that I would like to predict with a multiple regression model, but it's distributed very [bimodally](https://imgur.com/a/nRvns). I've never dealt with this before, and I have some ideas about what to do but no expertise on which choice is best:

- dichotomize the outcome variable. I'd really rather not do this if I can avoid it, because the numerical variation in there is important to our theory. 

- a negative binomial regression (the overdispersed solution to poisson, but would this still work with the clustering at both ends? Am I allowed to do this with bounded data?)

- Turn the count data into a proportion, so that the outcome is proportion of the week behavior was engaged in,  and then use a binomial regression (did a [simple linear trend line](https://imgur.com/a/9bo2c) earlier just to visualize, and it does look like the S-shape could fit the data better). But can I do this when the variable isn't dichotomous, even though it's [0,1] bounded?

- some sort of nonparametric test, though I don't know where to start on that because I've never done one for multiple regression before. 

Do you have any insight on this particular problem, or how to address bimodal outcome variables more generally? 
"
"People who do statistic analysis for other scientists (like the statistician in a research institute), do you like it? It sounds like my dream job but so many people disparage it",4,5,False,False,False,statistics,1507796758,True,"I'm nearing the end of my phd in bioinformatics, and I've been thinking a lot about next steps. I know how to do a little bit of every thing: programming, statistics, data visualization, and I know I'm not really interested in continuing to be an academic researcher. Although I'm not trained in statistics formally, I imagine I'd love to munge data and find meaning out of other people's data using different statistical techniques. Why do so many people I talk to say it's a bad job? Is it (for ex) the lack of appreciation from the people you do it for? I'm imagining being exposed to lots of different stats (which I like) and like not having to worry about ultimately writing it into a paper (which I dislike ).  "
SAS Question: Link has an example for 2 groups. What if i need to work on 5 groups?,3,5,False,False,False,statistics,1507797480,False,
Bayesian Nonparametrics: Dirichlet process and its applications,3,41,False,False,False,statistics,1507816759,False,
Two Sigma statisticians share thoughts on various papers and talks at JSM 2017,0,4,False,False,False,statistics,1507819274,False,
Is there an effective way I can graph a ratio of three numbers?,11,2,False,False,False,statistics,1507819878,True,"For example, if I am measuring the reproductive output of a plant that produces buds, flowers, and fruit in a linear fashion (so that different days would have a different ratio of buds:flowers:fruit length), how can I graph this information?

In other words, is there an effective way, or perhaps a mathematical formula, that standardizes 3 number ratios so I can compare and plot 2:4:3 vs 1:7:6 on the same axis? Maybe something that will take these numbers and give me a value on an index from 0 to 1 or the like?"
Would this need to be determined experimentally or could it be calculated statistically?,0,1,False,False,False,statistics,1507824662,True,"I'm essentially trying to figure out the optimal way to rank lists of things. Normally, I would prefer using random samples of people that would give 5/5 ratings, with 1 is the most below average, 3 being average and 5 being most above average because that seems like it would be relatively simply and repeatable. However, stuff like submission comments, large/complex software systems, college courses, etc, seem like they would be too specific or time-consuming to reasonably learn what “average” means. For such things, I figure it would be better to use aggregated pairwise comparisons because that also seems like could be relatively easy, simply and repeatable. Ostensibly, as good way to test how close pairwise rankings would be to averaged ratings would be using both methods on a large set of songs.

The goal here would be getting acceptable repeat-ability using the least user effort. However, there are a lot of variables to pairwise comparisons.

The method: round-robin comparisons of all items against all other items, Swiss tournament comparisons where winners are compared against winners and losers are compared against losers, and randomized tournaments that determine rankings based on the number of wins after a “to-be-determined” amount of comparisons, (which could vary as necessary I suppose). The optimal users-per-comparison ratio. Whether higher certainty of individual comparisons is necessary for higher certainty of overall rankings. Whether/when to utilize short ranked lists: where #1 wins against #2, #3, #4, etc, and #2 wins against #3, #4, etc. Whether ranks should be determined by wins or by the aggregated ratio of favored vs unfavored “votes.” Whether ties should be included or excluded.

...Does all that seem like it could be determined mathematically/statistically beforehand or would it require experimentation?
"
Uncommon Data Transformations,0,1,False,False,False,statistics,1507832628,True,[removed]
Exploring a career change. Is a career in statistics viable?,6,1,False,False,False,statistics,1507833251,True,"Yesterday, I posted on [/r/actuary/](https://www.reddit.com/r/actuary/) looking at actuarial science. Posting here first would have been better.

Anyways, my background is a BA in Geography and a MS in City Planning from a Midwest state school. I wasn't able to break into the city planning profession as I had hoped. Turns out I was more interested in learning the subject instead of being in the profession.

I recently took a skills assessment package with my university and statistician came up as a career choice. I enjoyed taking 3 stats classes in college and have used SPSS and Stata software packages before. I'll have to learn R and brush up on my Python skills, but I know I'm deficient in a stats background at this moment.

Any input? Thanks."
Need help finding the standard devation,0,1,False,False,False,statistics,1507844112,True,[deleted]
Question about building a rubric for ranking things,0,1,False,False,False,statistics,1507848143,True,[removed]
B.S. in Math or Stats or B.A. in Finance - pursuing actuarial science,0,1,False,False,False,statistics,1507856087,True,[removed]
Limitless Pulse Pod System Kit Vape Pen - $29.9 - 1118 - Ave40,0,1,False,False,False,statistics,1507866424,False,
Biostatistics Book,6,4,False,False,False,statistics,1507904256,True,"Sorry guys, I know this has been posted a thousand times before, but I did look through past posts and couldn't really find the advice I was looking for. I'm a recent college grad involved in biomedical research and am looking to learn R and have a stronger grasp on statistics. I took a basic statistics and a regression course during my undergraduate time, but feel I'd be beneficial to start at the basics again. I'll be teaching myself everything through books. 

First question, is an intro to biostatistics book all that different from a standard statistics book? Which one should I rely on? The two I'm currently looking at are:

Intuitive Biostatistics: A Nonmathematical Guide to Statistical Thinking by Moltusky

Statistics in Plain English by Urdan

As far as R goes, I'm looking at:

R Cookbook by Teetor

Discovering Statistics using R by Miles

Any advice or other recommendations would be awesome. The eventual goal is to get involved in more clinical studies. Thanks in advance. "
Lots of data and a little overwhelmed,17,12,False,False,False,statistics,1507907532,True,"I have collected 4 years of data on a piece of equipment (4 years of breaks in one specific system) as well as 4 years of humidity data to go along with it. I have my data set up going day by day from the 1st of January 2013 to present and each day it shows if the system broke or if the system didn't break. The column next to it shows that days humidity percentage. What tests can I run to see if this data has some kind of relationship with one another?

Any help on this would be greatly appreciated. "
Timeseries: ARIMA,1,5,False,False,False,statistics,1507910500,True,"Hello,

Background: Continuous process, measurements every hour on the product.

I conducted a auto correlation analysis on one quality parameter of the product (&gt;100 quality parameters on the product). Can I use the same model for all the other quality parameters?

Friendly Regards
// Aftonhoran"
Standard Deviations and Percentages,2,3,False,False,False,statistics,1507919786,True,"If I conduct an experiment where each data point is either a success or a failure, there are N data points, and a success rate of P%, what is the standard deviation of P? Sorry if this is poorly worded, but it's been a while since my last Stat class.

EDIT: Kinda shitty to downvote a question that's at 1, so that no one answers it. I was kind of hoping for some help."
Logistic regression explained,0,1,False,False,False,statistics,1507922054,False,
What 2 values can I add?,3,3,False,False,False,statistics,1507925569,True,"I been messing with this question since yesterday a colleague asked me. What 2 values can I add to a data set that does not affect both the mean and standard deviation? 
I can prove that adding the mean to a data set does not affect the mean.
I am pretty sure this is related but I am at a wall. 
Any hints,  thoughts?  Thank you for reading."
Two group comparisons with covariates for non-normally distributed data in SPSS,6,6,False,False,False,statistics,1507927523,True,"Hey all,

I'm working on a project at my graduate school on the effects of motivation for cannabis users and their cognitive performance on standardized neuropsychological measures. A few of our variables are not normally distributed, and thus require some sort of nonparametric test. We have tried to transform the variables so that they are normally distributed; however, that was not successful. 

What analyses (nonparametric) could we run for a two group comparison with covariates for this sample in SPSS? I've found a few on other statistics blogs, but they didn't account for covariates. 

Any thoughts or suggestions would be greatly appreciated. "
What exactly does convergence mean in the context of Bayesian parameter estimation?,11,10,False,False,False,statistics,1507931832,True,"I have the general sense that it means the result is ""trustable"" and wouldn't change with more chains being run. But what exactly does this mean? And, a bonus question: how does Rhat measure this?

Edit: I have a new question, and in the interest of not flooding the sub I thought I would add it here. My understanding is that, when using a Bayesian approach to estimate parameter values:

* The posterior distribution is the combination of the prior distribution and the likelihood distribution.
* We simulate this by generating a sample from the posterior distribution (e.g., using a Metroplis-Hasting algorithm to generate values, and accept them if they are above a certain threshold of probability to belong to the posterior distribution).
* Once we have generated this sample, we use it as an approximation of the posterior distribution.

But, I feel like I must be missing something. It sounds like we have a posterior distribution and then sample from it, and then use that sample as an approximation of the posterior distribution. But if we have the posterior distribution to begin with why do we need to sample from it to approximate it? What am I missing?"
z scores and Likert scale survey results,0,1,False,False,False,statistics,1507932779,True,[removed]
Calculator for density curve online?,5,0,False,False,False,statistics,1507944899,True,"My minitab trial expired and I have problems on my homework asking for probability that something is in the top percentile, is there any way for me to calculate this online?
"
Gambling question,4,1,False,False,False,statistics,1507949160,True,If you have a dice game where you roll 2 dice and the goal is to get a 7 or 12 and after you roll the two dice you can double down for a third dice to get a 7 or 12 what the the odds of winning the game?
Simple probability question [confused],1,1,False,False,False,statistics,1507969441,True,[deleted]
"Freedman Statistics, 2nd Edition ( in contrast to the 4th edition )",0,9,False,False,False,statistics,1507972926,True,"I've just got this book as I've read it explains things well, however I ordered the 2nd edition. 

I'm just wondering if anyone happens to know what the differences are between the second and fourth editions? 

Is there anything that I'm particularly missing out on or, ? 

cheers"
Online Assistance in Econometrics Homework Help,1,1,False,False,False,statistics,1507990117,False,
Question regarding p-value reports to 3 decimal places,11,7,False,False,False,statistics,1507991136,True,"I am studying Psychology, and have been told that the APA style of lab reports calls for p to be reported to 3 decimal places. I don't have a problem with this, and the question it raised may not be an issue, but is as follows:

If I take α=0.050 and, after conducting my statistical test of choice appropriate to the data, p≤0.050, this is sufficient to reject the null hypothesis. Obviously, this covers p=0.0499, which in the APA style would be reported as 0.050 due to rounding to 3 decimal places.

However, what if p=0.0501? In the APA style this would be reported as 0.050, which would fit the criteria for rejecting the null hypothesis, even though in this case p&gt;0.050.

How would I report this? Is it ever an issue?

I understand that if this occurs, it suggests a repeat of the experiment with a larger sample size would likely produce p≤0.050 as a true result, rather than as a result of rounding to 3 decimal places.


Thanks!"
Statistics Assignment Help from Expert Writers in the USA,1,1,False,False,False,statistics,1507992322,False,
"Is a p-value of 0.01 ""more significant"" than a p-value of 0.001? If both are significant, does it make sense to say one is more significant than the other?",69,43,False,False,False,statistics,1508007945,True,"Scientific papers often report significance with * or ** or *** depending on how small the p-value is. But someone recently told me that asterisks are useless because we defined a threshold below which the p-value is significant and to say that the p-value is tiny small does not give more information than saying that it is below the threshold. Like you're either pregnant or not but you cannot be ""more pregnant"". What do you say about this?

Thank you!
"
What was your ( Undergraduate ) final year project on?,20,2,False,False,False,statistics,1508011765,True,"Most people do a  project in their final year at university, what was your project about? It would be interesting  to hear what people did their project on. 


In the UK this is referred to as a dissertation ( and the PhD will have a thesis ), but these terms may change / switch between countries. 

thanks :) 

 "
Non-speculative currency trading,1,0,False,False,False,statistics,1508012887,True,[removed]
A question about probability (playing a game),6,4,False,False,False,statistics,1508035817,True,"Okay let's say there's a panel with five pairs of buttons, each pair has a left and right button. Only one of each button is the correct button, and you're goal is to get 3 out of the 5 picks correct. You don't know if you're right or wrong for any of them until you pick them all. Is it statistically a better chance to meet the goal by picking all the buttons on one side, ie all left, as opposed to randomly?

I want to say it doesn't increase your chances since each button press is independent, but if you pick all left or all right aren't you theoretically guraneteed a 50% chance since one side will have to have at least three correct?"
"Ran an ANCOVA with baseline values as covariates, need to know if findings make sense",0,1,False,False,False,statistics,1508046865,True,[deleted]
A question about mediation and moderation from an undergraduate in psychology.,2,2,False,False,False,statistics,1508058290,True,"http://psych.wisc.edu/henriques/mediator.html basically summarizes my understanding of mediation and moderation, but I still feel like I don't grasp the concepts, and that I can't fully think through a study design because of it. Specifically, I am having trouble figuring out how the two types of variables are different. The following is my thought process...

1. Mediators are said to be variables that account for the relationship between the IV and DV.
     A. What is the precise definition of ""account""?

2. Moderator variables are variables that affect the direction and/or strength of the relation between an IV and DV.

If premises 1 &amp; 2 are met, then how are all moderators not mediators and all mediators not moderators?

By changing the strength and/or direction of the relation between an IV and DV (2), is a moderator not ""accounting"" for the relationship between an IV and DV, thus making it a mediator as well?

To me, ""accounting"" for the relationship between an IV and DV entails changing the strength or direction between an IV and DV. I say this because changing strength/direction seem to me like necessary conditions for influencing a relationship at all, and influencing a relationship seems like a sufficient condition for ""accounting"" for it. 

The general test for mediation is to examine the relation between the IV and DV, between the IV and the mediator, and between the mediator and DV. How could these conditions ever be met without the mediator not also affecting the strength or direction of the relationship between IV and DV, thus making it a moderator as well?

The way I understand their definitions, they are circular - and I am thus unable to figure out if a given variable in a study is a mediator or moderator. Can you point out the flaw in my logic or definition? How can I go beyond a pure statistical understanding of these concepts to know, without data to play with, whether a variable is a moderator or mediator by simply using a qualitative list of characteristics?"
Statistics &amp; Applied Economics Consulting,0,0,False,False,False,statistics,1508058939,False,
Structural Equation Modeling using Amos and Spss.,1,0,False,False,False,statistics,1508059684,False,
Structural Equation Modeling,1,0,False,False,False,statistics,1508062197,False,
Data Analysis,0,0,False,False,False,statistics,1508063123,False,
How do you decide which correlation coefficient (Pearson’s or Spearman’s) is best to use for a specific data set?,6,25,False,False,False,statistics,1508073461,True,"Hi!

I am currently researching and doing some work on this. One thing however is confusing me. I have a question that asks me to comment on the difference between the values of the two correlation coefficients which I have worked out, and then, with the help of a scatter diagram, discuss which is more appropriate for this occasion.

What factors would influence this decision? In addition, what makes a Spearman’s test “valid” or a good choice for a set of data?

Thanks in advance, I was struggling to comprehend the information online so hopefully someone here can explain it :)

My teacher had told me that for Spearman’s to be a valid test, the data must fit a bivariate normal distribution, but after researching a little bit this seems to be incorrect, right.?"
Is a p value of p &lt; 0.024 significant? (Paired samples T-test),11,1,False,False,False,statistics,1508082331,True,"Sorry I would glaze over in stats class feel pretty dumb: 

The paired t-test for dependent samples was used to statistically analyze the pre- and post-test scores. Results of the analysis showed significant difference between the pretest scores, t(12) = -2.58, p &lt; 0.024.

Thanks again!"
Hire Experts to give statistics assignment help online in Topup writer,1,1,False,False,False,statistics,1508096300,False,
Has anybody done work like this on predicting or correcting for reproducibility? [Better description inside...],1,4,False,False,False,statistics,1508105939,True,"Hey /r/Statistics,

So I'm a grad student in machine learning. I know that reproducibility is a big issue, especially in fields such as psychology. However, I am increasingly concerned about it in fields such as my own, where industry hype and tweetable pre-print findings are rapidly gaining traction.

I'm kind of curious to know if anybody has used scientific metadata to predict or correct for the reproducibility of various findings.

For example, I think the following features may be useful for predicting the reproducibility of a given study (or group of findings):

* Amount of research funding allocated to a particular field or avenue of research (more funding --&gt; more experiments --&gt; higher probability of significance due to random chance).
* Number of published and unpublished findings, both null and significant (probably need to re-weight this by, e.g. using citation networks to construct a proxy representation of scientific interest).
* Average cost per experiment (higher costs --&gt; fewer independent experiments --&gt; lower probability of significance due to random chance).
* Number of PhD students graduated per year (or, better yet, number of working researchers).

Perhaps a statistical model, trained on features like the ones listed above, could assist in the interpretation of scientific results. A significant finding of p&lt;0.05 in a field with high experimental costs, low scientific interest, and a high null-publication rate may have a higher probability of successful reproduction than a finding of p&lt;0.05 in a field with low experimental costs, high scientific interest, low null-publication rates.

**Core Questions/Discussion Points:**

1. Does anyone know if there are processes in place to collect the sort of scientific metadata I've described above? I am concerned that, if this data even exists, it may be inaccessible to statisticians.
2. Does anyone know of any attempts to predict reproducibility using features similar to the examples above? 
3. I have nearly zero knowledge of standard meta-analysis practices. Do meta-analyses usually consider these sorts of research-population-level issues as part of their analysis? If so, how? Can somebody link me?
4. Are there are any highly Google-able fancy academic words you would recommend to a novice interested in finding out more about this sort of issue?

To anybody who has read this far, thank you for reading and considering what I've written here! :)"
Statistical Significance of,7,3,False,False,False,statistics,1508106222,True,"I have an e-commerce website that's been running a split test for a few weeks on pricing.  I have data on how many opportunities there were for a conversion as well as the conversions that did happen and at what pricing.  I'm not really concerned with which pricing has the highest conversion rate, but rather which price results in the highest revenue (ie could have lower conversion rate, but higher price, resulting in more revenue).

How can I determine whether or not the results are statistically significant?  I've seen online calculators that will look at the conversion rate, but in the case of total value, I'm not sure where to start.  I've done a bit of searching online, but haven't come up with much.

Any ideas?"
0 EXPERIENCE WITH STATS REQUESTING ANY HELP AT ALL ASAP,0,1,False,False,False,statistics,1508107535,True,[deleted]
Statistics : Multinomial Distribution : Treatment of Experimental Data #2,4,8,False,False,False,statistics,1508117706,False,
compare similarity of binned data - what test?,5,8,False,False,False,statistics,1508161322,True,"i have two datasets - one containing the percentage of particles in three depth bins (top, middle and bottom) based on observations and one containing the percentage of particles in these bins based on a computer simulation. I want to compare them to determine if the computer simulation is producing statistically similar distribution patterns to those observed. what is the best way to go about this? I have looked into paired KS tests but as i’m using percentages i am always getting that the two datasets are from the same continuous distribution."
Finding probability of getting a given value,0,1,False,False,False,statistics,1508167835,True,[removed]
I hate this field so much,12,0,False,False,False,statistics,1508174036,True,"I'm in my 5th year as an undergrad, and god damn it do I hate this field. Every day I want to quit school. I mean why not? I sure don't want to do anything with data after college. Plus I hate going to class, I hate my profs, the whole vibe around the field seems unhealthy. I know this post doesn't make sense, and isn't well written, but I don't care. If I've learned anything in college it's that effort, logic and clarity are highly overrated. 

So those are my thoughts."
Statistics Question for You!,0,1,False,False,False,statistics,1508175107,True,[removed]
Am I in the wrong here (simply probability problem),0,1,False,False,False,statistics,1508175651,True,[removed]
Am I in the wrong here? (Simple probability problem),16,1,False,False,False,statistics,1508177632,True,"I TA for an intro to probability course. There was a question on the exam that was: Find the probability of rolling 2 dice and getting a sum of 3, given one of the dice you rolled was a 1.

Anyway, I know how to solve this problem pretty simply. There are 11 possible ways to have at least one of the dice roll a 1, and 2 ways to get a 3 out of those 11. Since all outcomes are equally likely, it is 2/11.

Some kid in the class asked me a question during the exam and asked if his defined sample space was correct (he wrote 36). I told him the way I would think of the problem is that the sample space is each distinct possible sum 2 through 12. He then changed his sample space to 12 (even though it would be 11 in my case). Then he proceeded to start replace all the 36's he included in his calculations to 12's. I realized while he was doing this it was clearly incorrect (both since he wrote the wrong sample space of 12 and he was using sample spaces incorrectly in his calculations), but I did not say anything since I felt I would be interfering too much with his exam.

With all that said, he still managed to get the correct answer (2/12)/(11/12) = 2/11 even though his method was wrong. Although this did not affect the final answer, am I wrong in telling him to think of sample spaces the way I did? In my mind, if we were using weighted dice, it would make more sense to assume the sample space is 11 instead of 36 so that you are not as careless simply inserting the sample space into your calculations."
"""Total of Unpaid Contributions to UNESCO by Country as of October 3, 2017..."" ...",1,0,False,False,False,statistics,1508178814,False,
Value of a statistics minor,4,2,False,False,False,statistics,1508184479,True,"I am a graduating senior with a software developer job already lined up. I've already completed my computer science major and have been on track to add a statistics minor. I really love coding in R and working with real datasets, but I absolutely despise the theoretical classes (probability, general statistics, Bayesian crap, etc.).

Right now I am in a probability course that is kicking my ass. I hate having to spend time on it because it keeps me from working on other interesting software development projects that I'm involved with right now. I get that theory gives one a good foundation as a statistician but what I really want to do is learn how to work with data using tech (i.e. R). Next semester I have to take another theoretical statistics course, but with these two courses I will be able to enroll in a statistical computing class that I _really_ want to take (but will likely be unable to take if I drop the theoretical classes because of prerequisite requirements).

I am considering withdrawing from my probability class and dropping the minor. This will make my senior year much more enjoyable, will be much better for my GPA (I would like to go to a computer science grad program one day), and will get rid of classes that I feel will be irrelevant to the work I will be doing as a software developer. However, I am worried that the value of a statistics minor outweighs the uselessness and difficulty of these theoretical classes.

In your experience, is a statistics minor useful? I will be working a lot with data and visualizing geospatial data in my future job, so should I just deal with the crap and finish out the minor, even if it means I take less classes that I'm actually interested in?

Any advice is appreciated. Thanks!"
Proper density function,0,1,False,False,False,statistics,1508192106,True,[removed]
How to approach analysis?,2,2,False,False,False,statistics,1508195523,True,"Hi, I'm a programmer, working on some code, and trying to figure out how to approach a specific problem.

I have a bunch of text documents, and I have specific terms that may or may not occur in each document (""marketing agency"", ""your company"", etc).

For the terms that I am looking at, I know what percentage of the documents the term occurs in (i.e., ""marketing agency"" occurs in 33%), and I can also determine a matrix of correlations for all terms (i.e., when ""marketing agency"" is present, ""your company"" is also present 22% of the time).

In general, more matches means a better chance that a document is a match, but I'm struggling with is how to use those correlations to arrive at a final ""score"" for a random document which would indicate that it is a match for the type of information I am looking for.

I recognize this question might be more appropriate for a machine learning discussion, but I thought I'd try here first in the event that it's a straight shot from a stats perspective. Thanks!"
Is this 538 puzzle correct? How many coins flips until 95% confidence?,5,15,False,False,False,statistics,1508197006,True,"A recent 538 riddle asked, essentially, how many flips would you need (flipping both) to be 95% confident in the difference between a fair coin and one that hit heads with p = 60%? The [answer they gave](https://fivethirtyeight.com/features/can-you-beat-the-game-show/), which is based on a recent [econ. paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3034686), says to create a joint PMF, and then ""sum over all of the cases where the biased coin has more heads than the maximum heads of any fair coin.""  

The problem, though, as some have pointed out, is that there are also cases where the biased coin has the **same** number of heads as the fair coin. Summing those with a 50% probability reduces the number of flips needed from 143 to 134. 

So the question: which method is correct? To make things easier, I wrote R code to show the two methods, which I will place in a comment below. "
How to deal with percentages using z scores,1,0,False,False,False,statistics,1508197306,True,[deleted]
Understanding an LDA Analysis using R,0,1,False,False,False,statistics,1508197469,False,
"glm offset expirement, what went wrong?",4,4,False,False,False,statistics,1508200109,True,"One of my analyst come up to ask why his Gamma/link=log glm with offsets was always overstating his observed data points.

I was able to reproduce the behavior in R with intercept only glm using offsets. For some reason, when you use offsets with a Gamma/link=log your predictions are overstated.

That's not all, your offsets have to different per observation.

See this code :

    set.seed(12345)
    x &lt;- rep(1,5)
    
    #Gaussian
    offs &lt;- runif(5,0,1)
    y &lt;- runif(5,-5,5)+offs
    model1 &lt;- glm(y~1+offset(offs),family=gaussian)
    model2 &lt;- glm(y-offs~1,family=gaussian)
    (cbind(mean(y),mean(predict(model1,type=""response"")),mean(predict(model2,type=""response"")+offs)))

    ##            [,1]       [,2]       [,3]
    ## [1,] 0.03836621 0.03836621 0.03836621
    
    #Gamma
    offs &lt;- runif(5,50,150)
    y &lt;- runif(5,1,100)+offs
    model1 &lt;- glm(y~1+offset(1/offs),family=Gamma)
    model2 &lt;- glm(y-offs~1,family=Gamma)
    (cbind(mean(y),mean(predict(model1,type=""response"")),mean(predict(model2,type=""response"")+offs)))

    ##          [,1]     [,2]     [,3]
    ## [1,] 136.8067 136.8067 136.8067
    
    #Poisson
    offs &lt;- rpois(5,5)
    y &lt;- rpois(5,1)+offs
    model1 &lt;- glm(y~1+offset(log(offs)),family=poisson)
    model2 &lt;- glm(y-offs~1,family=poisson)
    (cbind(mean(y),mean(predict(model1,type=""response"")),mean(predict(model2,type=""response"")+offs)))

    ##      [,1] [,2] [,3]
    ## [1,]  7.2  7.2  7.2
    
    #Gamma-log - Offset same on each observation
    offs &lt;- rep(runif(1,50,150),5)
    y &lt;- runif(5,1,100)+offs
    model1 &lt;- glm(y~1+offset(log(offs)),family=Gamma(link=""log""))
    model2 &lt;- glm(y-offs~1,family=Gamma(link=""log""))
    (cbind(mean(y),mean(predict(model1,type=""response"")),mean(predict(model2,type=""response"")+offs)))

    ##          [,1]     [,2]     [,3]
    ## [1,] 129.2581 129.2581 129.2581
    
    #Gamma-log - Offset different per observation
    offs &lt;- runif(5,50,150)
    y &lt;- runif(5,1,100)+offs
    model1 &lt;- glm(y~1+offset(log(offs)),family=Gamma(link=""log""))
    model2 &lt;- glm(y-offs~1,family=Gamma(link=""log""))
    (cbind(mean(y),mean(predict(model1,type=""response"")),mean(predict(model2,type=""response"")+offs))) 

    ##          [,1]     [,2]     [,3]
    ## [1,] 160.1253 164.3708 160.1253

Why am I getting a different value in the last one on [,2]. From the definition of the glm formula, it should be the same, no?

I’ve debugged the glm.fit function and the code works fine, its not returning the expected result.

Thanks"
all common pattern/types of relational graphs - for highly knowledgeable and experienced users,0,2,False,False,False,statistics,1508201429,True,"**link to all common pattern/types of common relational graphs?**

the only one i know of is a linear graph where 1 thing affects the other thing on a 1:1 basis/scale

im sure that the highly knowledgeable and experienced users know that graph are like 1-line summary of an essay/book/story/movie/anything

so all the common pattern/types of graphs are one of the most important thigns for every single person to learn

---

a different example, if something like a film has 100 ratings with an outcome of 4.0/5.0

if you vote 5, how much would it affect the outcome? is there a pattern to it? and if there was, what graph is it? what does the graph look like? what's the name for this common graph? 

---

what to google to find all the common patterened/types of  graphs?"
The mean of medians,7,2,False,False,False,statistics,1508215563,True,"I'm working with some census data, and I'd like to summarize that data from the individual census districts up to the county level. One of the things I want to summarize is the median age of those census districts. If I have for example 10 median ages, is it even close to statistically relevant to use the mean of those districts to represent the county?"
need help deciding which test to run in excel,0,1,False,False,False,statistics,1508216433,True,[removed]
Trust in numbers - David Spiegelhalter,0,2,False,False,False,statistics,1508229108,False,
"IBM SPSS 25, MacOS Sierra 10.12.6 help!",1,0,False,False,False,statistics,1508230108,True,"I downloaded spss 25, but it doesnt open. It installs fine and launches, but it doesnt go any further. It also says when closing spss quit unexpectedly. I have spss 25 on a mac sierra 10.12. Does anyone know what I can do? "
forecast sales using price,6,3,False,False,False,statistics,1508240979,True,"Hi All

I have historical sales data by week for the last 2 years for all SKUs, and also have an average price for the week for each SKU for the same period.

I have been earlier using Holt Winters method that I had developed in excel that used to forecast without taking price into account and just using sales history and time periods (months) and that used to work reasonably well. I used to forecast the category total and then just applied manually the split per SKU based on judgement and history.

However, this time I need to incorporate price into the equation as the SKUs are price sensitive, and not only are they price elastic by themselves, the relative price between SKUs of the same category can cannibalize SKUs in the category that are slightly higher priced.

Is there a way that I can implement this forecast in any logical way? I only have Excel as a tool and nothing else, and I am someone who prefers a simple solution that gets me there 80% rather than a overly complex one that might get me to 100%.

If there is a way, please let me know. It would be appreciated.

Thank you."
Chi-square test help,11,5,False,False,False,statistics,1508253584,True,"Hi I posted this somewhere else but was suggested to post it here. I'm trying to figure out how to use a chi-square test to analyse some data. So basically there were 4 batches of cells (human cells originating from the same place) and each batch was treated with a different concentration of a chemical. The number of abnormal cells (cells with less or more than 46 chromosomes) was then counted. So what i want to do is test and see if the concentration had a significant effect on the number of abnormal cells, I've seen other research papers use chi-square for similar situations so im hoping it is a good test here? But I really can't get my head around how to apply my data to it.                               An example of my data includes:                            Treatment 1 (1mM) - 37 abnormal cells vs 1163. Treatment 2 (2mM)- 76 abnormal vs 1124 normal.     Treatment 3 (3mM)- 126 abnormal vs 1074 normal. Treatment 4 (4mM)- 152 abnormal vs 1048 normal. (Made up numbers but pretty similar). Is there any amazing person out there who could talk me through the test, basically how I need to lay out the data in a proper table to test it etc (like what to put in what columns and rows) I'm getting so confused and don't want to get it wrong! - im not just trying to get someone to do it for me and give me the answers! I really just want to understand properly how I can use it in this situation so I can use it again in the future too :) i'd be so so grateful! Also I'm sure this is probably a very simple test so apologies if i seem really stupid!! "
The Supreme Court Is Allergic To Math,20,77,False,False,False,statistics,1508253900,False,
Help with determining an expected result?,0,1,False,False,False,statistics,1508254263,True,"I'm trying to determine whether or not an observed difference for yes/no data is statistically different from another, and it is my understanding a Chi-squared test does well for this.  My problem is that I am not sure what my expected result should be as there is no probability I can use.

My question is: is the rate of a detected bacterial infection in group 1 different than the rates of detected bacterial infection in group 2.  All of these groups ARE infected, but the difference is ability to detect it.

Any help is really appreciated!"
Grad-school Ph.D in statistics,3,0,False,False,False,statistics,1508257614,True,"Thanks for your insights in advance. To begin, gradcafe is practically empty, I've posted there for a week+ and only one person has looked at my profile. All historical posts here have been more informative and genuinely constructive. Gradcafe usually has candidates giving insight to candidates... not necessarily good advice (nor necessarily bad either), but I'd prefer more feedback overall.


&amp;nbsp;


**Major**: Mathematics, Economics (from a midwest university, known for hockey and aviation)


&amp;nbsp;


**Minor**: Statistics


&amp;nbsp;


**Cum.GPA**: 3.81 **Major GPA**: (mathematics): 3.83


&amp;nbsp;


Math/statistics and applicable economics Courses: 
**Undergraduate courses**: Calc 1, 2, 3 (AB exam pass, **B**, **A**), intro differential eqns (**A**), Applied stat. methods (**A**),  Business Statistics (**A**, econ), Set theory and Logic (**A**), intro to linear algebra (**A**), Linear Algebra (**A**), Theory of probability (**A**), Statistical Theory 1, 2 (**A**,**A**), Intro to Real analysis (**B** present), Capstone Math (**A**), Econometrics 1, 2 (**A**, forecasting an **A**, econ)


&amp;nbsp;


**graduate course**: (*Spring 2018*) Linear Statistical Models (**A**) 


&amp;nbsp;


**GRE**: V: 161, Q: 165, W: 5. (**88%, 89%, 93%**) 


&amp;nbsp;


**REU, research programs, and recommenders**: Carnegie Mellon University summer REU  (paper to be submitted soon), McNair Program -&gt; research in statistics (KDE &amp; quantile regression) (2.5 years at time of graduation), NSF funded research program (1 year long). I'll have decently strong recommenders, and 3/4 have published recently, the unpublished one is from my REU at CMU, but we'll probably be published in the next 6 months


&amp;nbsp;


The quant is low, so I'm realistic about not applying to schools whose GRE range I do not fall in for Quant. I'm looking at: Carnegie Mellon, UPenn, Chapel Hill, Cornell, UChicago, and UMich Ann Arbor—likewise all the same schools for econ Ph.Ds. Any thoughts?

I'll put my Vitae on imgur if you guys wanna see it, it's more thorough. edit; typos
"
Can you get an effect size if you only have one group?,11,0,False,False,False,statistics,1508258217,True,
From Power Calculations to P-Values: A/B Testing at Stack Overflow,1,3,False,False,False,statistics,1508266298,False,
What does it mean if configural invariance fails?,0,1,False,False,False,statistics,1508273026,True,[removed]
How Advanced Analytics is changing the way organizations turn data into strategic security intelligence.,0,3,False,False,False,statistics,1508284445,False,
MAPE - Splitting out Positive and Negative Results,0,1,False,False,False,statistics,1508286215,True,[removed]
"If it takes 4 years to get a master's, will your resume be tossed instantly?",0,1,False,False,False,statistics,1508287008,True,[removed]
"Is it true that as the quantity of random numbers chosen from a distribution that’s capped at zero but unbounded positively (whatever this is called technically?), the mean of all the numbers chosen increases?",10,3,False,False,False,statistics,1508291724,True,"My thinking is that as the quantity of random numbers chosen from a distribution that is unbounded positively increases, the expected ith highest number from those choices increases (whether that’s the highest number, 2nd highest number, 3rd highest number, etc.), but if the distribution is capped at zero, there are no negative numbers to bring down the increases in the expected values for the positive values.  Does this look correct?   "
Muslims are approximately 5% of the UK population but are 90% of those convicted for grooming gang crimes,19,0,False,False,False,statistics,1508292964,False,
How do we determine to reject or accept a null hypothesis based on a p-value?,10,0,False,False,False,statistics,1508293291,True,"Lets say we are given an alpha of 0.05. Is this the criteria we use for the p-value? 

"
Best technique for comparing groups of means?,2,1,False,False,False,statistics,1508305029,True,"I have cross sectional images of lung tissue, with 5 samples each prepared in a different way. I want to see which sample preparation technique produced the best images in terms of contrast.

I have done some basic stats classes but they were a long time ago. I know what a p-value is and I have a rough and basic understanding of things like ANOVA.

Each sample produces about 1500 images in total. I used a random number generator to select 30 images from each sample. For each image I used ImageJ to plot an intensity profile across the image. This gives me an intensity for each pixel, so about 2400 data points for each image. For each image in the sample I have the mean and standard deviation of these data points. (I also have the raw data but it is tedious to use).

So effectively I have 5 groups of means and associated standard deviations. I want to show that the image contrast for each sample really is different and that my results did not happen by chance.

What would be the best way to go about it? It sounds like some sort of ANOVA would be the way to go but I'm a little confused as I have groups of means rather than groups of raw data. I also just wanted to check here before I spend hours revising the wrong technique.

EDIT: Would it work to perform one way ANOVA on each group to determine the means are the same within the group, and then perform one way ANOVA between groups using the grand mean? I'm confused as."
"""One or more values were set to system-missing"" despite appearing perfectly fine in the text import wizard preview. What is going on?",6,2,False,False,False,statistics,1508327103,False,
Good Review Sheets For Job Interview Prep,0,1,False,False,False,statistics,1508331284,True,[removed]
My gf needs help with this question. She's lost and doesn't even know what to Google to figure it out,1,1,False,False,False,statistics,1508349488,False,[deleted]
How did you learn stats?,30,51,False,False,False,statistics,1508360289,True,"So I want to be a stats pro but I suck at math.  I've been programming Python for 2+ years in a science lab and have a solid grasp on fundamentals.  I've also used scipy for a machine learning project I created for myself &amp; actually completed it.  **I have all these ideas for analyses that I'd want to do but I haven't the faintest idea how I would conceptualize them into a statistical model.**  I've done t-tests (independent &amp; paired), regression, correlations, &amp; understand variability and basic descriptive statistics.  Basically, if I want to get a job doing this stuff, what else do I need to know and to all those self taught, where did you start?  Any good book recommendations/resources?   "
Could you help me with this?,4,0,False,False,False,statistics,1508372772,False,[deleted]
Biostatisticians,0,1,False,False,False,statistics,1508375387,True,[removed]
Noob question: how do sports leagues (MLB/NHL/NFL) keep track of niche records?,4,8,False,False,False,statistics,1508378274,True,"I log and track stats for a small hockey league, and would like to provide some fun stats/records for the players, but do not know how to start. I have 10 seasons of team/player data.

I assume the first rule is, that if you want a stat like ""Most goals scored on a Thursday night game in the rain"", you need to track goals, day of the week, time of day, and weather. So assuming that I've been tracking all the data that I want to report on...

... how do I go about analyzing it for these types of records? It feels like there could be literally thousands of different records out there, but is there some way for me to apply analysis to my data to identify anything that appears to be an outlier by 1 or 2 SDs, and then investigate that?

I'm just lost. Sorry if this is a huge noob question. Appreciate any answers!"
Is my supervisor misleading customers?,17,11,False,False,False,statistics,1508384040,True,"For the purposes of this question I'm going to leave out any specifics, to protect myself and my employer.

I was recently hired at a supplement company which provides a product that is proven to improve an aspect of a dog's health. The aspect is not important to this question.

The company conducted a number of studies about the effectiveness of this product. The studies include a six month study, and a ten year study. These studies resulted in an average 4%, and 1.3% improvement in each dog.

In training, the question of one year results came up. My supervisor said that I could just take the six month results and multiply by two.

I said to her that I wouldn't extrapolate the results of the six month study.

Confused, she asked why not? 

I said that we don't know the effects of the product after six months. That the effectiveness of the product could drop off dramatically after six months, so multiplying by two would be inaccurate.

Additionally, as the longer term study does not match up with this extrapolation method, it's another reason to not use it.

I don't have a scientific background, but my gut is telling me that multiplying out the results of a short term study is not good practice. And I'm not in a position where I can outright say to my supervisor that I think she is wrong, so for the moment this is what I have to tell our customers.

TL;DR - supervisor said that in order to find product effectiveness over one year, we can take six month study results and multiply by two.

I think this is bad practice as that doesn't match up with results from a longer term study.
"
Recommended books for Competitive Exam (Statistics and Probability),2,4,False,False,False,statistics,1508384673,True,"I'm looking for advice on how to prepare for a competitive exam for a statistics Master's program that I'm applying to.

The difficulty level of the questions is fairly high. Here's a sample:
https://imgur.com/a/Frg75
The questions can be answered using knowledge acquired from undergraduate stats courses.

I'm a Physics student with a statistics minor, and I'm pretty sure I can solve 90% of the questions in a few days' time. However, the exam is only 2 hours long, so I cant give each question more than 15 minutes if I want to score well.

So, in order to do that I need to practice, and for that I need to find books to practice questions from. The questions in most introductory books are fairly easy and don't prepare me well enough. So I need recommendations for books that have challenging problem sets.

Additional advice: It would also be great if someone can recommend be books for the real analysis and linear algebra questions that come up in these papers. Here's a link to the full set of past papers:
http://www.isical.ac.in/~admission/IsiAdmission2017/PreviousQuestion/Questions-MStat.html

**TL;DR
Need book recommendations to practice problems of this level:
https://imgur.com/a/Frg75
**

"
struggling to find interest in my stats major,9,4,False,False,False,statistics,1508389935,True,"I should start this out by saying that I haven't taken many classes yet, so my experience isn't the most well rounded. However, I'm a junior and I was considering changing my major to focus on statistics and don't really have much time to just see if i like it. So far, the classes I've taken have been all about plugging numbers into a formula and I'm getting pretty bored of it. The professor practically read the textbook and the exams are very basic and somewhat tedious. I was wondering if a career in stats is also very much just plugging things into a formula. "
Sampling of articles on Scopus,10,4,False,False,False,statistics,1508405538,True,"Hello to everyone, I hope I'm not breaking some rules, but I have a question that I can't seem to answer.

So, I have to do an analysis of articles on certain topic in the last few years. With each year, the number of articles increases. The number of articles is too big to include them all in the study, so I have to sample them somehow to get representative results. I was thinking of taking the first X% of articles for each year sorted by relevance by Scopus algorithms. Scopus can also sort by other parameters, but I don't think it would be a good idea to do that. 

So my question is: if this is a viable strategy to sample the results, what should my X be? The more the better or?

If that is not a good strategy, what could be a good one?

On the other hand, if I want to do the same for patents on Orbit, should I proceed in a similar manner? Orbit has a different relevance filter, it basically lists documents in the order of percentage of relevance to the searched topic. So would it be a good idea to use a cutoff value of lets say 90% below which I would discard the documents?
"
"If dropping a factor leads to a better AIC, can you conclude that the factor did not have a significant effect (as measured by a chi2 test)?",9,5,False,False,False,statistics,1508415506,True,"I know the information criteria measure something rather different from frequentists' significance, and this would obviously depend on your threshold for significance, but intuitively it's hard to imagine how dropping a factor with a significant effect could lead to a better fit, and I haven't come across a case where it would.

Is it possible to construct a counter-example for arbitrary high significance level? Are there natural counter-examples for alpha=.05?

The main reason I ask, beyond curiosity, is because I wonder whether I need to report that a factor is n.s. after showing that dropping it improves AIC.

Also, what about BIC? Since it punishes extra parameters more heavily, I guess it would be more likely to have counter-examples."
Is there a simpler formula for the chance of rolling a specific value on at least 1 of n dice?,2,1,False,False,False,statistics,1508417679,True,"I haven't used statistics in years, so sorry if I am not using conventional methods to represent statistical information. I shall describe the problem my curiosity conjured from the beginning and solve things along the way to show how I got to the final question.

##***Skip to the last section to see my question. This stuff is easy, and I let my curiosity run free one evening.***

---
---

###The Context

We have a die to roll in a board game. We want to roll a five or six. That's a 2/6 = 1/3 chance for a single die roll.

Let's say there are two boons we can attain in this board game. One boon raises the value of the die by one, effectively giving the roll a 3/6 = 1/2 chance of success. The other boon allows a reroll of the single die, thus providing a second 1/3 chance to occur.

I wondered which of these boons is the better one to choose before the die is initially rolled.

---

The first boon is a solid 1/2 chance.

The second boon requires some work. We have two dice with 1/3 chance each. This makes 3^2 = 3*3 = 9 possible permutations. Simplifying it down to rolling a 3-sided die and aiming to get a 3, by manually working it out, we get the following:

&gt; 1 1

&gt; 1 2

&gt; 1 3 *

&gt; 2 1

&gt; 2 2

&gt; 2 3 *

&gt; 3 1 *

&gt; 3 2 *

&gt; 3 3 *

That's 5 out of 9 permutations that have a 3. Therefore, the final answer is 5/9.

This makes the second boon, the reroll, a better choice since 5/9 &gt; 1/2 by a margin of 1/18.

---
---

But how do we determine the number of successful permutations--in this case, 5--without manually working it out? After my Google-fu has failed me, I tried to find a universal formula.

---

Concerning coin flips with 1/2 chance heads or tails, two coins have a 3/4 chance (using the same method above) to get at least one heads. For three coins, it becomes 7/8. With four coins, it becomes 15/16. This means the next fraction will always be:

&gt; F(n) = (p - 1)/p

where

&gt; p is the number of permutations

&gt; n is the number of coins

---

Considering a three-sided die with 1/3 chance, two dice will have 5/9. Three dice will have 19/27. Four dice will have 65/81. This turns out to be:

&gt; F(n) = F(n-1) + (B(n) / p)

where

&gt; F(0) = 0

&gt; B(n) = B(n-1) * 2

&gt; B(1) = 1

&gt; p is the number of permutations

&gt; n is the number of dice

(B(n) is a binary number.)

---

Considering a four-sided die with 1/4 chance, two dice will havee 7/16. Three dice will have 37/64. This turns out to be:

&gt; F(n) = M(n) / p

where

&gt; M(n) = (M(n-1) * s) + p - M(n-1)

&gt; M(0) = 0

&gt; p is the number of permutations

&gt; s is the number of sides

&gt; n is the number of dice

(M(n) is the nominator.)

---

That formula works with the three-sided dice and the two-sided coins. I succeeded!

**I found a universal way to determine the number of successful permutations for any number of uniform dice of any number of sides when trying to obtain at least one specific roll value.**

You can probably see my logic with how I got to this formula. I used an Excel spreadsheet to count the successful permutations, and this formula simply occurred to me from all the copying and pasting.

---

`f(n) = g(n) / number of permutations`

`g(n) = g(n-1) * number of sides + number of permutations - g(n-1)`

`g(0) = 0`

`where n is the number of dice`

Is there a simpler version of this formula? Perhaps without recursion? Does it currently exist in statistics? I'm sure it does, but I couldn't find it.

Thanks!"
Improving Real-Time Object Detection with YOLO,1,18,False,False,False,statistics,1508421423,False,
ELO: why is expected score of 2 equal players 1 and not 0.5?,7,1,False,False,False,statistics,1508421540,True,"I feel like I must be missing something.
The formula for expected score for player A is: 

expected score = 1/(1+10^((rB-rA)/400))

Where:
rB is player B rating 
rA is player A rating. 

If the players are equal skill (rB=rA), then this formula yields a value of 1 (basically 100% chance of player A winning?) 

Shouldnt this formula yield 0.5 (50% chance of player A winning, since they are both equal skill)? 

"
What is MULTI-STAGE STRATIFIED sampling? Can you provide and example?,2,1,False,False,False,statistics,1508424808,True,"Been watching YouTube videos, googling, still stuck and don't have a clear understanding. 

What is multi-stage stratified random sampling? Could someone please clarify and provide an example. "
Maximum likelihood estimation inquiry,0,1,False,False,False,statistics,1508429735,True,[removed]
What is Neyman's Allocation sampling?,7,0,False,False,False,statistics,1508432509,True,
"Major in Biochemistry, Minor is Statistics. Hoping to go to grad school for Stats/Biostats. Advice Needed.",5,1,False,False,False,statistics,1508432609,True,"I am a sophomore studying biochemistry, but I know I don't want to go into pure chemistry as a career. My school doesn't offer a stats major, and the rest of the math department is pretty small and underfunded. I still like chemistry and biology, and I have started biology research, so I will have that experience. I want to be a statistician working alongside medical/scientific researchers. I want to go to an MS program is Biostatistics/Statistics, but I am concerned about my lack of math course work. Here are some of the classes that I have/will have by the time I finish:  

Calc 1/Calc 2/Calc 3  

Linear Algrebra  

Applied Stats 1 &amp; 2  

Intro to Computer Science 1 &amp; 2 (both 4 credits, includes programming)  

Discrete Math  

Differential Equations  

(maybe) Mathematical Stats 1

Will I have a decent application for grad school? Will my biology research (it's microbiology, mostly genome annotating with some culturing work) help me at all?"
Suggested business setting cases studies books,2,4,False,False,False,statistics,1508433130,True,I'm looking for books with statistical analysis case studies to read. Any suggestions?
Establishing Statistically Significant Tiers,0,1,False,False,False,statistics,1508435721,True,[removed]
Gaining actual understanding of statistical principles,5,4,False,False,False,statistics,1508436884,True,"Hi all. Hopefully this is the right place for this question. I'm an experienced aerospace engineer engaged in flight test and for a master's degree I am taking a statistics course. It is the first dedicated statistics course I have ever taken (statistics and probability were touched on in a course on engineering analyses and another in experimental methods). I'm enjoying the course and doing very well, but like a lot of courses you take as a busy adult, it isn't clear why I'm doing some of what I'm doing. I'm good at the mechanics (thank God, I remember as much linear algebra as I do) of working the problems, but I can't say I'm understanding the concepts as well as I'd like. 

Are there any resources (books, YouTube channels, podcasts) that I can use to get a better understanding of the basic principles? I hope that isn't a dumb question. Generally, I'm not convinced that test data at my job (or in the industry as a whole) is treated as rigorously as it should be. I also don't think that experiments are designed with statistics in mind. But, I'm not smart enough to see where and to make an intelligent argument for improvements (which I hope will improve understanding and save money). I guess, that's what I'm looking for in the long run.

Edit: it's an honest question. Maybe rather than downvoting you could point me in the right direction?"
Alpha correction: Multiple Analyses vs. Different Items?,3,1,False,False,False,statistics,1508448730,True,"**Example:**

10 participants try a wide selection of red and white wines.

I run a within-subjects repeated-measures ANOVA for their red wine scores, looking at how several common wine characteristics affected scoring.

Separately, I also run the same analysis for their white wine scores. In other words, there are two separate ANOVAs, covering a different set of scores.

**My question:**

Should alpha be corrected here? If I were running multiple ANOVA on the red wine scores, I would think so. But given that we are tackling two separate wines, should this be corrected here too?"
Error bars,2,2,False,False,False,statistics,1508452094,True,"Hi :) so i just plotted a graph for some data i've got with standard deviation error bars. The error bars are huge and theres big overlap in the bars! Statistical tests indicate there is statistical differences in my data, but is there any point in using such tests if the variability in my data is so large? Could i say significance was found but because of the variability the results may not be reliable? Thanks :) "
Stats Help!,0,1,False,False,False,statistics,1508454321,True,[removed]
Currently in college for statistics and don't know what to do.,12,17,False,False,False,statistics,1508465552,True,"I never knew what to do with my life until I worked with a data analyst team in my dads company. I really liked what they were doing and I was a long time subscriber of /r/dataisbeautiful and love looking at sports statistics so I thought I figured out my future career. I'm in my sophomore in college taking my first stats class and I hate it. All we're doing is learning probability of things and it lasts the whole course. I also bombed all the homework so far and the test. So I feel like I'm just back where I started and don't know what I want to do for a career. So I wanted to ask all of you if any of you got off to a ""slow start"" with stats or if any of this probability stuff I would actually use in a stats career field? Really just any advice to someone genuinely interested in stats but can't quite grasp the material would be greatly appreciated. Thanks y'all. "
Example of placebo effects,10,1,False,False,False,statistics,1508469192,True,"I have a AP Statistics test tomorrow and have a question regarding placebo effect.

I know that a placebo effect is a fake treatment, but what are some examples of it?

In the problem on my review sheet, it says ""a study involving 28 patients who bypass patients found that the 14 randomly assigned patients who took Vitamin E for 2 weeks before their operations had significantly better heart function after the procedure **than the 14 randomly-assigned patients who took placebos**. 

In the context of that problem, what would a plausible placebo be? Taking a different type of vitamin? Taking a different medication? Maybe just drinking water? 

Also, part A of the question asks ""Explain why this is an experiment and not an observational study"". 

Should my answer say ""it's an experiment because the 14-randomly assigned patients took vitamin E before their response was measured and no data was gathered on them as they originally were""

or should i also acknowledge the 14 randomly assigned patients who took placebos (especially since a placebo is a **fake treatment**)? I think I should ackknowledge those who took placebos since although their treatment was ""fake"" or ""different"" it was still a treatment

Appreciate the help

TLDR: I want to know some examples of placebos (you can give examples in the context of the problem) as it will help me understand the concept better. I also have a quick question about a homework problem

**Also, I may post here multiple times tonight as I am studying for the test. If I am a burden, or breaking any rules by posting too much here, let me know.**

"
Anova P Value help please,0,1,False,False,False,statistics,1508473587,True,[deleted]
[Discussion] treat Likert Scale data (not individual items) as ordinal or interval during analysis?,3,3,False,False,False,statistics,1508490943,True,"I am currently working on my thesis for the MSc Marketing and because I wish to understand the impact on consumer-brand engagement (or how attached people feel to brands) of video content on social media (particularly Facebook), I figure Likert Scale responses would be a good way to understand consumer's attitudes. In my research on Likert Scales I have come across some debate over treating Likert Scale responses as ordinal versus interval data which then requires a different set of analysis procedures (ANOVA, Pearson's r test, etc.)

The questionnaire I have created collects consumers' responses to video content on Facebook and picture content on Facebook to find some difference and asks a series of 15 questions for each which relates to the marketing theory surrounding consumer-brand engagement (they are randomized and a legend was created to allow me to see what questions relate to which concept of the theory). Survey link: https://www.surveymonkey.com/r/P8LP9KM 

What do you believe would be most suitable given my questionnaire and data collected?
"
Need help matching a partially observed function to a density,4,2,False,False,False,statistics,1508504687,True,"I have an interesting problem where I have a finite number of observations of some unknown function, f(x). My data is of the form [(x,f(x))]. For example, a set of 3 observations could look like [(1,f(1)=4),(4,f(4)=3),(7,f(7)=1)]. Suppose I want to find a density that is a member from a parametric family, g(x|theta), of my choosing that is closest to the observed data (i.e. min h(f(x), g(x|theta)). Closest is defined by minimizing some distance, h, such as KL distance.

The function f, may or may not integrate to 1. I would imagine it would be best to first normalize the observations from f in some way so it would integrate to 1. The density, g, could be continuous or discrete.

I am not sure what kind of problem I have here or where to start looking into the literature. Any advice would be greatly appreciated.

edit - To clarify, my observations [(x,f(x))] are NOT a random sample from some probability distribution. They are fixed observations along the support of x."
Martingales,0,21,False,False,False,statistics,1508510672,False,
Help me describe my statistical expertise for a faculty position cover letter? It would be greatly appreciated!,5,1,False,False,False,statistics,1508512425,True,"Hi everyone, I am preparing a cover letter to complement my cv for a faculty position (I'm mid career). I was pretty deeply and extensively trained in theory of GLM (ANOVA &amp; multiple regression for continuous outcomes). After that i am essentially self-taught in HLM for longitudinal modeling (growth curve modeling - I taught myself from the Willett &amp; Singer book), and then I was asked to teach the MLM course for cross sectional data, so I taught myself that from the Bryk &amp; Raudenbush book (both of these applications I am really only comfortable with continuous outcomes). 

Also, in my years of actually analyzing data, I have of course done plenty of analyses with logistic regression, and understand the underlying principles pretty well but was not trained in it. I also am only superficially understand the underlying of logic of non OLS-based estimators, even though I use and interpret results. (I have been teaching basic biostats for a long time and am an expert in the logic of (traditional) hypothesis testing).  

But I don't want to oversell myself as a 'statistician'. I'm using the term 'applied data analysis'. 

Does the following seem appropriate (and complete?)  

""expertise in all applications of general linear models and most applications of generalized linear models. I have expertise in growth curve modeling, in addition to hierarchical linear modeling for cross-sectional designs"" 
"
"Multinomial Logistic Regression Interpretation of Parameters Estimates, very large Exp(B)",1,1,False,False,False,statistics,1508513125,True,[removed]
Question about stats major/prospects after college,8,4,False,False,False,statistics,1508515009,True,"Hi everyone, I'm currently a freshman studying civil engineering, but I'm thinking about switching to statistics. My college really puts a lot of focus on actuarial science, but I know that I don't want to work in finance. So my question is: is it possible/easy to pursue a career in statistics that isn't actuarial? More along the lines of data analysis, big data, research, stuff like that. Also (this is really specific, sorry) the whole reason I'm currently in civil engineering is because I'm really interested in transportation (mass transit and rail specifically). Is there a way to be involved in the transportation industry with a statistics degree? Thanks!"
Calculus App - A free app for calculus formulae,1,1,False,False,False,statistics,1508515719,False,
"MS but no stats experience, luckily I found this entry level job that I definitely qualify for.....wait...",33,5,False,False,False,statistics,1508521554,False,
What statistic to use to measure reliable change?,1,0,False,False,False,statistics,1508531226,True,"Hi there. I'm a PhD student and I'm trying to assess whether my patient's change in scores on a 10-point Likert-type pain scale represents clinically significant and reliable change. This measure is simply a self-report of daily subjective pain, and there are not psychometrics out there for me to calculate a typical Reliable Change Index (RCI) with the measure's standard error.

Is there a way for me to calculate reliable change based on the patient's own variability? I have over 100 data points (this person rated pain daily)."
How to model residual effects of web server requests on response time performance,0,1,False,False,False,statistics,1508531825,True,"up vote
0
down vote
favorite
I have a web application interface for an application that frequently experiences crippling performance under normal usage, meaning it's unrelated to an organic spike in volume. I'm trying to form a model and come up with an applied method to detect an anomalous request that perhaps itself does not exhibit the slow performance, but more about the residual effects on other requests which are independent. The uptick in response times are orders of magnitude larger, so fairly obvious to spot, but not tied to any specific time of day or other seasonality.

I've ruled out using random forests because of the time series element approach I expect to take.

I've also ruled out normalized cross-correlation because I'm not trying to find correlated time series patterns.

Also, nothing about it strikes me about finding anomalous behavior with regard to some seasonality, e.g. requests normally not seen on a late Saturday night.

I'm thinking of each unique URL as a vector within the timeframe, e.g.:

* vector 1 for: ""GET myapi.com/URL/A""
* vector 2 for: ""GET myapi.com/URL/B""
* (etc. there are thousands of unique URL endpoints)
The residual effect could include the actual vector or just subsequent calls within some time interval.

**How would I go about trying to detect which specific web requests may be contributing the overall instability after it's run, based on the web server logs that I have with a few weeks of data?** Struggling to find a model to apply here."
"Looking to apply for a data analyst job, but I'm very insecure. What should I be prepared for?",9,5,False,False,False,statistics,1508542684,True,"Hello everyone,

can you share with me what a job in statistics looks like? I'm about to major in social science (on a German university with a quantitative emphasis) but I feel like my knowledge is rather limited. Linear regression, logistic regression, factor analysis (exploratory/confirmatory) and all the basics required to understanding the respective output is basically where I'm at, while being mediocre at Stata and R. I honestly don't feel good in applying for a job unless I know whether my skills fit the requirements of the company. Sadly, job offers on the internet tend to be a little vague (analyze our data and advise us how to move forward!). What do entry-level data analysts generally do? And what is required of them? It feels like the level of knowledge I got is rather easily and quickly attainable, which is why I feel insecure, on top of not having STEM background."
"Trying to represent a ""consistency"" percentage using standard deviation",2,2,False,False,False,statistics,1508544780,True,"Hi, new to statistics and thought i'd apply standard deviation to a spreadsheet i'm using for playing golf. I have 5 golf shots using the same club under the same conditions, and record a percentage for how accurately the ball travels to my intended target.  If it's dead on it's 100%, if it's way off it's 0%. So I calculated the average of 5 shots and put that in it's own column. I then noticed standard deviation (in a column beside average accuracy) was a low percentage if there were no wild differences in individual shot percentages (which would be good if the average accuracy was high). So I decided ""consistency"" would be 100% minus the STD of accuracy in my data set. Hope that makes sense. I'll also add a link with screenshots of the 2 sheets in action. If there is an error in my approach or a better way, any help is appreciated. Thanks!

Sheet for a single club, recording 5 shots:
https://drive.google.com/file/d/0BxOANpQZUHGhZmhuTjhWdWVEakU/view?usp=sharing


Master page of all clubs:
https://drive.google.com/file/d/0BxOANpQZUHGhS2pWc2pNRUo5bGM/view?usp=sharing

Note: full, half, and qtr are swings (so a full swing with that iron, a half swing etc)"
Adding percentage improvements in a two-step statistical modeling setting,6,1,False,False,False,statistics,1508546988,True,"I'm building a model to predict mortality in a grassland system, mortality which is driven by broad scale climate which sets constraints (system only dies in certain climates) and fine-scale topography which influences which areas die within those ""bad climate"" places.

So I built a climate model to predict areas of mortality - it's pretty good, placing 90% of the mortality in those bad climate areas, and only 10% of mortality happening outside those climate conditions.  

I then built a model using only points within that ""bad climate"" space to model topography.  That one only manages about 70% accuracy.  So within a ""bad climate"" I can correctly classify about 70% of the dying places as dying places.

How do I add those together to get a total classification accuracy?  It's not quite a Baysian hierarchical model, or a mixed model formally, more a two-step model improvement method (there is probably a name for it, I just don't know what it is).

I know I can actually project these models, look at the observations, and calculate accuracy that way, but I'm interested in knowing if it's possible via the probabilities alone - I imagine it's fairly simple, I just don't know how to phrase the question correctly.  Any help is appreciated."
Did I do this correctly? (Double blind &amp; matched pairs sampling),1,10,False,False,False,statistics,1508561622,True,"I had an AP Statistics test today

I'm not sure if I got some of the answers correct; hence I am asking you guys. 

I asked you guys for some help while studying yesterday and it helped me tremendously, so I am hoping you could possibly verify some of my answers.

Question 1: A scientist wants to see if a certain medicine improves memory. She has 45 men and 36 women volunteers. She wants a placebo group, and 2 groups receiving the medicine - one receiving a large dose and one receiving a small dose.

Let me know if these are correct: 

**Explanatory variable:** medicine

**Response variable:** memory


**Create a design for the experiment:** I made a randomized block design. The question didn't specify which design to use (Would it work?) The thing that concerns me about this is that she said she wanted two groups receiving the medicine and in my diagram there are 4, but 2 of each block

Here is my diagram (recreated on Photoshop since I couldn't take pictures of the actual test): https://s1.postimg.org/1qz19cxvyn/diagram.jpg

I'm not sure if using a block design would work because if I recall correctly, the question didn't state that men and women would be expected to yield different effects/results from the medicine. However, I feel like we could still see how it affected memory for men and women separately and if there is an overall positive connotation. Let me know if I am good to go.

**The next part of the question asked to say how the experiment could be a double blind experiment.**

I said that a double blind experiment is where the volunteers have no idea what sample they are using and neither do the scientists. In order to make that specific experiment a double blind experiment, the scientist could put the treatment in identical containers and randomly give them to the volunteers without seeing what the contents inside were. A friend could keep note of who had which treatment and only let the scientist know after the experiment was over so he could analyze the data.

Would that answer suffice?

Question 2: (this one is a one-parter)

**A dude has a new compound that does stuff to grape vines after rain (don't recall the exact details). He is given permission by someone to use 10 vineyards in Napa to test the compound. He wants to use a matched pairs design for the experiment. Describe how you would design one.**

I said he would pick the two vineyards which received (or were affected by - don't remember my exact wording) the most and least rain. He would then randomly pick some vines to test the compound on.

Would this answer work? Of all the questions on the test, this is the one I am doubting most.

I have a couple other questions which i will post later this weekend so i don't overwhelm you guys all at once.

Appreciate the help!

"
Graph selection with 4 parameters,3,3,False,False,False,statistics,1508566478,True,"I have four parameter 

Location, yield(values), Crop and year(time). What kind of graphical representation is good for this kind of data. I used [bar graph](https://imgur.com/a/5H6y1) but its was not that clear since every year and place didnt have good, similar values.

Here is the example


    Sites       yield(kg)       Crop  Year
    Devitar T   4.5       Beans   (2015)
    Devitar NT  5.5       Beans   (2015)
    Kavre T     45        Beans   (2015)
    Kavre NT    35        Beans   (2015)
    Panch 1 T   5         Beans   (2015)
    Panch 1 NT  5         Beans   (2015)
    Panch 2 T   5         Beans   (2015)
    Panch 2 NT  5         Beans   (2015)  
    Anaikot T   4.5       Beans   (2015)
    Anaikot NT  3.5       Beans   (2015)
    Devitar T   48        Beans   (2016)
    Devitar NT  46        Beans   (2016)
    Kavre T     45        Beans   (2016)
    Kavre NT    55        Beans   (2016)
    Panch 1 T   5.8       Beans   (2016)
    Panch 1 NT  3.75      Beans   (2016)
    Panch 2 T   8         Beans   (2016)  
    Panch 2 NT  2         Beans   (2016) 
    Anaikot T   0         Beans   (2016)
    Anaikot NT  0         Beans   (2016)  
    Devitar T   246       Cauliflower  (2016)
    Devitar NT  220       Cauliflower  (2016)
    Kavre T     225       Cauliflower  (2016) 
    Kavre NT    162       Cauliflower  (2016)
    Panch 1 T   17        Cauliflower  (2016)
    Panch 1 NT  21        Cauliflower  (2016)
    Panch 2 T   16        Cauliflower  (2016)
    Panch 2 NT  9         Cauliflower  (2016)
    Anaikot T   63        Cauliflower  (2016)
    Anaikot NT  56        Cauliflower  (2016)
    Devitar T   26        Soybean   (2017)
    Devitar NT  29        Soybean   (2017)
    Panch 1 T   1.125     Soybean   (2017)
    Panch 1 NT  1.125     Soybean   (2017)
    Devitar T   19        Soybean   (2017) 
    Devitar NT  23        Soybean   (2017)
    Devitar T   204       Cauliflower   (2017)
    Devitar NT  222       Cauliflower   (2017)
    Kavre T     312.5     Cauliflower   (2017)
    Kavre NT    307.5     Cauliflower   (2017)
    Panch 1 T   6.5       Cauliflower   (2017)
    Panch 1 NT  7.5       Cauliflower   (2017)
    Panch 2 T   5         Cauliflower   (2017)
    Panch 2 NT  2.5       Cauliflower   (2017)
    Anaikot T   0         Cauliflower   (2017)
    Anaikot NT  0         Cauliflower   (2017)"
How to show a more rapid rise-and-decline in one group compared to another statistically?,5,2,False,False,False,statistics,1508567826,True,"I'm analyzing data of serum vitamin D levels for people living in two different locations. Graphically, it's obvious that group 1 has a more rapid increase and decrease in mean-monthly serum levels than group 2.

Group 1 serum-levels rise to a peak in two months, which is one month, then fall rapidly to baseline over two months. The rest of the measurements are at (or close to) this baseline.

Group 2 rises steadily over the course of four months, reaches a two month peak, then declines steadily for six months.

I want to show that group 1 has a sharp rise-and-decline compared to group 2. What can I use to show this?

I've considered splitting the results into before and after peak values, then using the r^2 and linear equation to show this. However, I'm not sure if this is valid because for group 1, n=2 in both cases while for group 2, n=4 and n=6 respectively. Thus I'm not sure of the strength of conclusion I could draw from this."
Men commits mass shootings,2,0,False,False,False,statistics,1508594667,False,
Wondering what fields id be getting into,2,1,False,False,False,statistics,1508595716,True,Hi so I'm going into stats because i enjoyed stats in high school and felt like a stats degree will never not be needed i just wanted to what kind of jobs id be looking at
Choosing the correct test...,4,5,False,False,False,statistics,1508606533,True,"Hey guys, I have recently enrolled in a statistics course on my university, and we have a homework assignment. As for the guidelines: I am not asking for an answer here for someone else to calculate all the results, I am asking for specific knowledge on choosing a certain test and the reasoning. I hope this allows for some leniency, as I can perform the tests and calculate the results myself.

One of the questions asks us to test two hypotheses, but among the students, we used completely different methods to test these two hypotheses. And I honestly don't know which one to choose right now:

&gt; The researchers think that content higher in vividness leads to a more positive attitude towards the brand. However, they think that this also depends on the type of content. Specifically, they want to know whether attitudes towards the brand will be more positive when content is entertaining than when content is informative. 
&gt; 
&gt; Test the following hypotheses:
&gt; 
&gt; * H3a: Attitude towards the brand will be more positive in content that is high in vividness than in low vividness content
&gt; * H3b: When content is high in vividness, entertaining content will lead to a more positive brand attitude than informative content
 
Basically we have 2 independent variables: Vividness (levels: high, low) and content-type (levels: entertaining, informative) and a single dependent variable (Combined mean of 6 7-point Likert scales on brand attitude, completely disagree - agree).

Now my reasoning was that it makes the most sense to test the first hypothesis with an independent t-test, and the second one with a two-way ANOVA. However, others of my group used a single two-way ANOVA for both hypotheses. The independent t-test gives a non-significant result (for H3a), while the two-way ANOVA gives a significant result for both H3a and H3b.

But are you allowed to test H3a here with a two-way ANOVA? Aren't you adding the content-type variable in the mix here to test H3a, which is something you shouldn't do since we aren't testing for that...?
"
Categorical data analysis - how to present data and combine repeat experiments.,5,3,False,False,False,statistics,1508606626,True,"My question is about the analysis of categorical data. I have data in the form of a 3x2 table i.e.

A1	A2	A3

B1	B2	B3

Where A is one type of worm and B is another type while 1, 2, and 3 are the phenotypes I measured.

And, since this is a biology experiment, I have repeated several times so I have several of these tables.

My plan is to represent that data as a bar chart (or dot plot) with X-axis being A1, A2, A3, B1, B2, B3 and Y-axis being the percentage value (e.g. A1/(A1+A2+A3)). Error bars will then represent the standard error since I have multiple measures from my repeats. Is this the most appropriate way to represent such data?

When I use statistics to ask whether the distribution of phenotypes is different between A and B, I was planning to use chi-square. For this, do I add all the values (e.g. all the A1s) from different experimental repeats together to get one single 3x2 table and then do the test on those numbers? Or is there some way to include in the test that I did many technical repeats? Is it important?

Any suggestions would be helpful."
Tricky wording to determine hypothesis testing.,4,5,False,False,False,statistics,1508607098,True,"Currently a student at UNT and thought I understood how to pick out correct hypothesis test based on word last like, more, less, different. Then I saw this problem. 

A university librarian at a university with a large multinational student body is interested in the proportion of students who would like the library search functions available in a language other than English. If over half of the student population favors the Multilanguage search option, the university administration will approve the cost of the new service. A random sample of 350 students was obtained and 186 students indicated they would like the new services. A hypothesis test was conducted using the KPK macro-based MS Excel output provided below to determine if a majority of students favor the new hours. A significance level of 5% was used.

The answer is Ho: u ≥ 0.5 and Ha: u &lt; 0.5

We have only ever been asked to pick out the wording but our practice exams are not hitting keywords. How do I learn what test to use by just reading?

Thank you"
Book Reccomendations,1,7,False,False,False,statistics,1508614841,True,"I'm looking for a good book on model fit. So far the only candidate I've found is ""Goodness of Fit Tests and Model Validity,"" by Huber-Carol et al. It looks promising, but unfortunately it's over $200, so something in the same vein, but cheaper would be ideal. "
I made a simple app to help less stats savvy people choose a Statistical Test for their data. Please don't be offended by the name!,23,121,False,False,False,statistics,1508624388,False,
Qualitative vs. Quantitative in taking notes,3,1,False,False,False,statistics,1508636228,True,Does anyone have tips on abbreviating or symbols for qualitative and quantitative when taking notes? I'm a psychology student and I find it very frustrating when I'm taking (and reading) my notes distinguishing between these. Just looking for any best practices.
Generating Examples of Simpson's Paradox,0,1,False,False,False,statistics,1508677749,False,
What analysis is used by the COMPARE method in SPSS when you apply a multiple-comparison correction?,5,1,False,False,False,statistics,1508678940,True,"Lets say that I carry out a two-way ANOVA in SPSS and find a significant interaction between my two factors. To find at which combinations of levels of my two factors there might be differences in my dependent variable, the SPSS website recommends using the EMMEANS command with the COMPARE sub-command to get the significance levels of the pairwise differences. 

Apparently, the default COMPARE sub-command uses Fischer's Least Significant Differences (LSD) test to get the significance values, but you can also add a further adjustment to account for multiple comparisons in the form of either a Sidak or Bonferroni adjustment.  

The thing that I can't really find any documentation for is what statistical test is being used when I decide to add either the Sidak or Bonferroni correction... is it still LSD, but with the correction, or is it calculating an F-statistic like in a one-way ANOVA, or something entirely different?"
What is a robust method for choosing variables for regression?,11,3,False,False,False,statistics,1508684449,True,"I recently came across a new method for selecting independent variables / covariates for regression (purposeful selection by Bursac et al., 2008), which made me wonder what everyone in this thread may suggest for variable selection. Other available methods that I am aware of include hierarchical, forced entry, and stepwise. 

With respect to conducting regressions from a medical standpoint, I would be inclined to choose variables based on biologic plausibility or support from previous literature. This reasoning may seem a bit flimsy, especially when there is limited literature or clinical knowledge on the variables I am exploring. What does everyone else think? I understand that the question is rather general. Thanks!"
What's a good comprehensive text book on statistics?,12,21,False,False,False,statistics,1508692544,True,"I know there are youtube videos and sites out there but I'm wondering if y'all can recommend a good comprehensive text book. Or more than 1. 

Thanks!"
What are some schools that will let you pay for a stats MS with teaching/research assistantships?,4,0,False,False,False,statistics,1508701719,True,
"Hi, can someone please tell me how do you preform mean centring on spectral data as a pre-treatment before PCA analysis ? Where do I find this option in XLSTAT ?﻿",0,1,False,False,False,statistics,1508718746,True,[removed]
Looking for some data behind a paywall,1,0,False,False,False,statistics,1508719646,True,"For my course culminating assignment in my data management class I'm looking at the revenue growth between different gaming industries, and I think I've found some data that will help me, but its locked behind a paywall. The data I want is [this chart](https://www.statista.com/statistics/412555/global-pc-mmo-revenues/) along with [this one](https://www.statista.com/statistics/278181/video-games-revenue-worldwide-from-2012-to-2015-by-source/) on statista. If anyone has a subscription and would be willing to send me a screenshot of the charts it would be greatly appreciated."
Need some help with2 Stat-200 homework q's,1,0,False,False,False,statistics,1508727195,True,"I kinda feel like a loser asking for help for 200 level stat hw questions but for some reason I'm not getting 2 of these questions and I need some help with the answers as well as understanding how to get there. These questions are regarding the Central Limit Theorem and finding probabilities. Thanks in advance! (Also, I apologize if this isn't the proper place to post this)



1. ’Snails!, Inc., a manufacturer of fasteners, makes a 12d common nail with a mean diameter of 0.14 inches and a standard deviation of 0.01 inches. If a random sample of 36 of these nails is taken, what is the probability that its mean will be greater than 0.143 inches? 
(Round your answer to four decimal places.)




2. The number of eggs that a female house fly lays during her lifetime is normally distributed with mean 790 and standard deviation 104. Random samples of size 93 are drawn from this population, and the mean of each sample is determined. What is the probability that the mean number of eggs laid would differ from 790 by less than 25? Round your answer to four decimal places."
Variance of binomial random variables,2,2,False,False,False,statistics,1508727934,True,"Given two binomial random variables, X ~ Bin(20, .4)and Y ~ Bin(30, .6), how would I get Var(2x-Y)? I originally thought that it would follow 2^2 Var(X) + Var(Y) - 2(2)Cov(XY) but I got stuck on the Cov(XY) part. 

I know Covariance can be calculated with E(XY) - E(X)E(Y) but then I'm stuck on the E(XY) part. I also know that E(XY) = 0 if they're independent but is there a way to figure that out in this situation other than just assuming? 

Or is there something that I'm completely missing? 

Thanks!"
Cant figure out the logic behind this very simple (children's)probability puzzle.,0,1,False,False,False,statistics,1508736847,True,[removed]
How to get relevant recommendations for MS/PhD after working for a few years in unrelated fields?,1,5,False,False,False,statistics,1508741776,True,"Background: Graduated with a BS in chemistry then worked in biotech for a few years. Tried to rebrand as a data analyst, but ended up as a technical writer for open source software.

The technical writing job is great for stepping into sysadmin/devops type roles, but it's moving away from where I want to be career-wise. I tend to gravitate around cluster setup, Hadoop, Spark, and analytics tools for writing. Also been doing small bug fixes with pandas.

Currently target is applying fall 2018. What are some ways to get involved with academia and make relevant references for applying in the future?

Thoughts so far:

 - Cold calling professors for (free contributions) to projects
 - Take online courses offered by an accredited university, do well
 - Ask open source project maintainers if they're willing to write a letter of recommendation

All of these sound rather roundabout so let me know if there is something more obvious."
Regression analysis on London and Amsterdam financial centres,0,1,False,False,False,statistics,1508753825,True,[removed]
Bayesian A/B test - using an updated prior based on collected data,6,4,False,False,False,statistics,1508769635,True,"I have a question about whether I would be adding bias to an A/B test by updating my prior based on combined A &amp; B data, and then running the A/B test on that prior.

My A/B test is click through rates, so I am using a beta-binomial distribution set-up.

I have estimated priors for my population based on previous data, but due to complications it may not accurately represent the population that the test is being run on. As an example, say that my prior is Beta(34, 726) distributed, but if I combine my A and B data (while controlling for sample rates) I can update that prior to Beta(26, 1032).

Would it be valid then to use Beta(26, 1032) as my prior for my A/B test, or am I adding bias by using prior based on my results?"
Need Stat 2000 help!,2,0,False,False,False,statistics,1508770013,True,"My prof. gave us back our tests and is letting us redo them for 10 extra points. The tests were scantron but he didn't give them back, just the test itself so we have no way of knowing which are right and which are wrong. Is there anyone who would be willing to review the test and tell me which ones I got wrong and how I can fix them?

PM me or just comment. It's 25 questions long.

Thank you!"
Analyzing subset of a simulation for statistical inference?,1,3,False,False,False,statistics,1508776293,True,"I'm using a third-party black box stochastic simulation model. Essentially, this model simulates the joint evolution of economic variables (GDP, inflation, yields etc) over some horizon, and translates this to the impact on a portfolio of investments. This simulates thousands of potential future scenarios with a range of realizations for the variables, which create a distribution used to calculate statistics (e.g probability of a loss greater than 90% of the portfolio's value).

Instead of these baseline results, I am interested in seeing the portfolio's statistics under a specific set of realized values for the variables (e.g GDP lower than 3% for the first three years, inflation lower than 0% for first ten years).

However, I can't modify any of the model's expected values or forecasts, which are all conditioned on historical information. I can only take the output as calculated under the baseline scenario.

A possible solution would be to take the simulated output, and filter them to select only the simulated paths that match my desired realizations. I could use this subset as an alternative distribution, and compare the statistics under this subset to the baseline output. Is this statistically valid or would this be nonsensical without changing the underlying variable expectations?"
M.S.: Biostatistics vs Applied Statistics,0,1,False,False,False,statistics,1508776965,True,[removed]
AP Statistics project - please fill out my survey,2,0,False,False,False,statistics,1508787762,False,
"When to prefer one model over another model? (say, SVM over logistic regression?)",10,15,False,False,False,statistics,1508797343,True,"We usually do either regression or classification. For regression, there are linear regression, decision tree, random forest, KNN, and I guess even neural networks.

For classification, there are logistic regression, SVM, decision tree, random forest, KNN, and neural networks as well. I guess linear discriminant analysis count too.

I have a good basic understanding of various approaches but I'm still not sure when is one model preferred over another? Can someone please give a rough overview of when to use what?

Thanks"
How to determine is a sample for the experiment is accurate representation of general population? And how to adjust if not?,1,1,False,False,False,statistics,1508797916,True,"During an interview, a case study involved classifying whether someone will get a certain disease. Predictors can include various physical attributes and lifestyle factors and so forth...

The samples are people who volunteered for the study. (We know who has the disease and who doesn't). 

A question is asked whether the sample under study is representative of the general population. In another word, can we apply the model to the general population. I answered no, because the samples are frequent hospital goers and/or because they volunteered, they are probably more health conscious than the general population.

Would you apply the model to the general population? If not, what would you do?

Thanks"
Two-way ANOVA involves a large number of means. Best way to approach post hoc comparisons?,7,9,False,False,False,statistics,1508802458,True,"The study is a 9 day repeated measure design with 4 different sites. There's a 3 day baseline period, a 3 day duty period, and a 3 day recovery period. So far, I've only performed a mixed effect ANOVA which showed a significant main effect for day, a significant interaction between day and site, and a non-significant main effect for site. I have a good idea of what mean comparisons I want to make, and have been looking for appropriate comparison. I have a number of questions, though:

1. I don't want to make comparisons between all 36 means. Does it make sense to do additional ANOVAs to rule out certain comparisons? Is there any reason to avoid doing an additional ANOVA on the 3 baseline days to rule out the need to make comparisons between them? 

2. If I can establish that there are no significant differences between sites, except for a single mean for one site, is it valid to average across each day across sites, except for the outlying mean, and then make pairwise comparisons? It'd be nice if I could make 6-7 comparisons instead of 24-28. "
Struggling writing this R code; finding admission decision and gender for each department.,0,1,False,False,False,statistics,1508803153,True,[removed]
"""Que""/""Cue"" GUI for R",0,1,False,False,False,statistics,1508809236,True,[removed]
Help me out !,0,1,False,False,False,statistics,1508821492,True,[removed]
Reduction of dataset,0,1,False,False,False,statistics,1508830899,True,[removed]
is Statistics masters good for me?,2,0,False,False,False,statistics,1508832495,True,I am a mechanical engineering undergraduate. now working as a analyst in a digital marketing firm. i wanted to do a masters in data science/analyst but internet told me it is a huge money grab and there is no clear relevance in this field. but a ms in math or statistics can give me a good chance in  entering into data science / data analyst jobs. how far is this true and how can i get a good college? is ms in statistics competitive?
How do you create an acceptance criteria range for multiple populations?,0,1,False,False,False,statistics,1508838501,True,[removed]
How do you actually go about making predictions in the most optimal way?,7,1,False,False,False,statistics,1508838711,True,"Say I have a large dataset for a bunch of different country's that's something like number of military engagements per month over several years. The end result is to make an informed guess about what happens in the future and the odds of an attack each month in each country.

How does one go about doing so? What are the different choices in how you would do it, what are the trade offs, and how do you make decisions involving those trade offs?

It seems that sort of thing would have no clear ""solved"" solution without assumptions. Is this intuition correct?

I thought this would be a simple question but I'm getting inconsistent answers and I can't find even how I began to look this up myself."
Can someone explain this to me?,6,4,False,False,False,statistics,1508839972,True,"The question is the following:
A new drug has been developed that is found to relieve nasal congestion in 90 percent of those with the condition. The new drug is administered to 300 patients with this condition. What is the probability that more than 265 patients will be relieved of the nasal congestion? 

I understand that P(success)=90% means that 270 patients will be cured, and I am looking for P(x&gt;265). Is this a normal distribution? If so, how do I apply the z-transformation?

Thank you"
Simpsons Paradox: Minute Physics,34,104,False,False,False,statistics,1508853065,False,
Differences between group in time series data,0,3,False,False,False,statistics,1508853433,True,"I have a large data set of about 90,000 observations that happened over a time period of 10 years on a daily basis. There are two groups in the data set and my hypothesis is that one group achieves higher scores of the continuous DV. As the data is not normally distributed, I used the non-parametric wilcoxon test (variance of the groups was different) and it supports my hypothesis.    
  
However, I find this quite unsatisfying as I have so many observations and I am looking for a more sophisticated statistical test to show the differences and maybe also control for certain events that happened in the ten year time period and are dummy coded.
  
My data set looks like this:
  
Group | Date | DV |Event 1
:---------:|:---------:|:---------:|:---------:
1 | 2008-01-01 | 33 | 0
2 | 2008-01-01 | 27 | 0
1 | 2008-01-02 | 35 | 0
2 | 2008-01-02 | 24 | 0
1 | 2008-01-03 | 55 | 1
2 | 2008-01-03 | 45 | 1
  
Thanks!
  
Edit: some more details"
"Career advice: any recommendations on what graduate scheme's are great for data analysis and statistics? UK based preferably, but not necessarily",0,1,False,False,False,statistics,1508855393,True,"
I just graduated MSc Conflict resolution, but had a fair share of quantitative analysis in my degree. Just wondering which statistical/ data analyst programmes are worthwhile consideration - my degree not being entirely numerate, it currently limits my employment capabilities in this sector."
Naive Bayes in 5 minutes - a simple explanation,0,0,False,False,False,statistics,1508858739,False,
"Anyone that has a PhD in stats (or related), can you tell me about your career?",0,11,False,False,False,statistics,1508860424,True,"What your background was, your current job (and location and/or salary if you wish), things you loved and hated about your PhD, the coolest paper you read, really anything!

I'm in the exploratory phase of figuring out my next step right now so all info is appreciated!"
Kackar and Harville correction,0,1,False,False,False,statistics,1508863625,True,"Can someone explain this to me like I'm 5? I've read over it, and it seems like it corrects for differences in variance based on sample size? "
Kurtosis and Skewness to predict wear,0,1,False,False,False,statistics,1508864196,True,[deleted]
BMI on SPSS v24,2,0,False,False,False,statistics,1508864814,True,"I have a data set with two variables (plus others). One with data in pounds and the other inches.

I want to compute a variable that calculates BMI using these two variables. But, it isn't working. 

So far I've tried: lb * 703 / inches x inches

Any help is greatly appreciated!"
Is topology useful/relevant in more advanced statistics topics?,8,6,False,False,False,statistics,1508864952,True,"If so, how and where is it used?"
"In cases where previous research cannot guide the choice of priors in a Bayesian analysis, what weakly informative prior do you generally use?",10,5,False,False,False,statistics,1508869933,True,"I should say that I have two models: in one the dependent variable is dichotomous, in the other in it continuous (i.e., I am running one logistic regression and one linear regression). In both cases I am also fitting random subject and item effects.

The package I am using (""brms"" in R) has a default half Student-t prior with 3 degrees of freedom for the random effects. It also defaults to a flat prior for the fixed effects, however I have read that this is not ideal. 

What I'm after are priors that are as uninformative as possible, for the fixed effects.

Are there some that you would recommend? And/or can you point me toward some good resources to help me make the decision?

Thanks!"
Question regarding the Chi-Squared test.,0,1,False,False,False,statistics,1508871907,True,[deleted]
Medical doctor looking for advice on how to get a job in biostatistics,15,3,False,False,False,statistics,1508873467,True,"I'm a medical doctor who has just finished med.school (6 year program, luckily in a country where education is free). The two last years I became more and more certain I should have chosen a different career path, because I was always happier when I worked in front of a computer doing programming, or maths and science in high school. However, I finished med.school, and now I'm looking for jobs in biostatistics and similar fields. Also PhDs in biostatistics.

I found out that almost all the jobs I have found are looking for someone with a masters degree in computer science or statistics. 

Does anyone have advice on have to change career path? Do I have to take a full bachelor's degree in science? Could it be enough to just spend a semester taking some statistics courses in university?"
How do I scale a normal distribution to be the same as another?,4,1,False,False,False,statistics,1508879310,True,"I have two normal distributions, Na and Nb. I'd like to move Nb's scale to Na's.

I tried calculating the z-scores for both distribution, but am unsure on how to convert Nb's scale to match Na's original scale. Any ideas?

EDIT: got it. Just rearrange the z score formula, solving for x..."
Estimating a normal distribution curve from a handful of data points?,0,1,False,False,False,statistics,1508887765,True,[deleted]
How to go about doing this question from my econometrics class?,4,0,False,False,False,statistics,1508889340,True,"In the question it’s given to us that “a poker hand (5 cards) is drawn from an ordinary deck of 52 cards. What is the chance of the following events?

A. The first 4 cards are the aces?
B. The first 2 cards and the last 2 cards are the aces?
C. The 4 aces are somewhere among the five cards?
D. “4 of a kind” (4 aces, or 4 kings, or 4 queens, ..., or 4 deuces?)”

Could someone explain to me the intuition behind the 4 parts to this question? Like in part a should we just take the P(first card is an ace), and so on and the P(the last card aka the fifth card isn’t an ace) and multiply those probabilities together? The thing is I’m not sure how to go about finding the probability of the first card being an ace so I’d like some help with that. Would it not be 5 out of 52 since we have 5 cards drawn from the ordinary deck of 52 cards and any one of them could be an ace? Thanks."
"What applied math courses to take (applied algebra, numerical analysis)?",5,3,False,False,False,statistics,1508892696,True,"This might sound like a silly question but, to my understanding having a strong background in linear algebra and multivariable is useful for statistics and data science.

Besides those two fields, what other useful, non-statistics applied math courses would you recommend taking? Specifically, will applied algebra or numerical analysis be relevant to statistics or data science?"
ELI'm a newb: What is functional data analysis?,3,9,False,False,False,statistics,1508892994,True,"I'm allowed to choose, among grad-school courses, a course on ""Functional Data Analysis"" (FDA). 

&gt; Theory and methods for analyzing functional data, which are high dimensional data resulted from discrete, error-contaminated measurements on smooth curves and images. The topics include kernel and spline smoothing, basis expansion, semiparametric regression, functional analysis of variance, covariance modeling and estimation, functional principal component analysis, functional generalization linear models, joint modeling, dimension reduction, classification and clustering functional data.

How is FDA different from, say, multivariate analysis + machine learning? (I will be taking a course based on *Elements of Statistical Learning* next semester for sure, I'm just not sure why I would want to take this other class.) "
"M.S. in Stats student, should I take a Stats or CS elective? (xpost r/datascience)",12,3,False,False,False,statistics,1508899264,True,"Hi guys,

A little background: current M.S. in stats student, have taken probability/hypothesis testing, regression methods (linear multiple, nonlinear, logistic), and R/SAS programming. I'm currently taking theory and linear modeling. End goal is to work as a data scientist.

Next semester, I unfortunately only have one elective choice due to conflicts so my adviser pointed me towards a possible CS course I can take. Unfortunately I'm not sure if it's worth foregoing my statistics elective to take the CS class. The descriptions are below:

* (Stats) : Categorical Data Analysis - Estimation, maximum likelihood, weighted least squares, log-linear models, logistic regression

* (CS) : Data Mining and Database Programming - Theory and practice of information management including PL/SQL; object and object-relational databases; data warehousing; data mining.

I know being an expert in SQL is very important in DS as nearly ever single job posting I've seen required it, and I am currently a novice at the language as I use basic SQL queries (Oracle) at work to pull in data. I'm just afraid of the possibility that if I forego that statistics course, that somewhere down the line I might need to know the theory and applications of categorical data analysis for data science or other statistical jobs.
Any recommendations?"
Supplementary Minors?,0,1,False,False,False,statistics,1508911421,True,"Hello, I am currently a freshman pursuing an Applied Statistics major, and I believe I will be able to fit in a CS minor as well as another minor of my choice. My plan is to minor in a field I hope to use my stats degree for, and some of the minors I'm considering include politcal science, biology, environmental science (geared toward forestry), psychology, and possibly education. I was wondering if any of you have experience in these fields, and if so any recommendations and insight is appreciated. I am especially interested in forestry because I love the outdoors and I would one day love to collect and analyze related data, or just get my foot in the door of the industry. Thanks!!"
marginal effects plot with glmer object,1,1,False,False,False,statistics,1508926786,True,"I'm running some hierarchical generalized linear models in R and I'm trying to plot the marginal effects. 
There's sjp.glmer which gives out beautiful plots but type = ""eff"" is not customizable and only provides face.grid plots. I couldn't figure out another way to do it. Anybody has an idea? Thanks!


I'm running the following model. 

- Outcome variable is binary (vote/not vote)

- I'm mainly interested in the effect of compulsory_voting which is a macro variable

modell1a &lt;- glmer(vote ~ education + age +  sex + compulsory_voting + (1|cantons), data=vox, family=binomial)"
You have a 30 sided dice and roll it three times,49,7,False,False,False,statistics,1508930109,True,"You get the number ""18"" three times in a row in a single attempt. You don't make any more attempts, so your ""sample size"" is 1. Is this an anomaly contrast to any other outcome? Would ""18, 12, 6"" be an anomaly as well? Or ""2, 29, 7""?

What makes it an ""anomaly""? Because to me, these outcomes are all equally likely. The only reason I would see someone consider the same number occurring three times over as anomalous is because they are either:

 - Construing an appearance of ""order"" as anomalous, since it does not ""appear"" random.

 - Asking the question ""Chances of getting this three times in a row"" for when you get three 18s, but not ""Chances of getting 18, 12, 6 in that order"" for when you get 18, 12, 6. Comparing ""getting three in a row"" to ""not getting three in a row,"" which is of course weighting probabilities and would make sense if you had a sample size of more than 1 attempt at rolling three times in a row.

Ultimately, each outcome is equally expected (or unexpected, given how slim their probabilities are), so would you consider it anomalous? If so, why? "
"You roll (3) 6-sided dice. Is the probability the same for rolling an ""exact result"" of 5-5-5 as it is to rolling an ""exact result"" 1-2-3? Does it matter if exact number contains doubles or triples?",9,3,False,False,False,statistics,1508955850,True,"Strange to see a dice related comment hours ago since I pondered the question then searched for this sub. What are the odds (pun intended but not my question here)?

I was wondering if the same rules of probability apply with doubles, triples and non-duplicate numbers when rolling for an exact number vs the general probability of doubles or triples appearing in a sample of random rolls.

To paraphrase for clarity, if you are rolling for an exact number would the probability be the same for rolling an exact three-digit number or would doubles and triples play a role in the probability?

"
Can someone explain this to me in layman a terms why this happens and what it means exactly?,50,25,False,False,False,statistics,1508957660,False,
Multiple regression - what do you think about my project?,12,5,False,False,False,statistics,1508958665,True,"Hi guys so I have a multiple regression project to do. I've always been interested about global issues and development and as such, I've decided to investigate what are the determinants of a nation's workforce productivity. I will be collecting data about different factors such as level of education, heath level, average salaries, average time for leisure for multiple countries (this part is going to be long). After, I will use multiple regression to identify the most important factors that influence productivity (as measured by GNP per capita). This will allow me to give recommandations to developing countries as to what strategies should be implemented to foster growth. What do you guys think? I'm so excited !!!"
Which statistics test is best for the scenario described?,7,1,False,False,False,statistics,1508964222,True,"Determining whether or not the difference in effects produced by two different independent variables is significant.    For example: There are two levels within the independent variable, and two dependent variables which the independent variable affects.  What test is most effective at determining whether the difference [between effects produced on dependent variables by each level of the independent variable] is significant?"
How to assign weight to studies using only sample size?,0,1,False,False,False,statistics,1508968368,True,[removed]
Survey for Statistical Methods Final Project (Not sure if I can post this here but worth a shot),0,2,False,False,False,statistics,1508971970,False,
Textbook Citation: how to convert q-values to p-values?,5,1,False,False,False,statistics,1508974551,True,"I'm interesting in converting q-values to p-values. 

The follow is a bit of R code from StackOverflow which gives an equation for doing this:

https://stats.stackexchange.com/questions/51070/how-can-i-convert-a-q-value-distribution-to-a-p-value-distribution

Here is the code:

    p_values  = qvalues * rank(qvalues) / (max(qvalues) * length(qvalues))

Is this procedure detailed somewhere in a textbook? I've been looking through my own statistics textbooks, and I haven't yet found how to do this procedure. "
Propagation of Error and Weighted Least Squares,2,6,False,False,False,statistics,1508976025,True,"Hi all,

I have a problem that seems like it should be straightforward but I'm having a terrible time searching for it.

I have a data set with known variances which is fit to a line with a weighted least squares calculation. The slope of the line from this data set is then used as a data point for further analysis, so it would be useful to have a good approximation of its variance.

However, the variance of the slope in the WLS algorithm seems to only account for the relative variances between points, and not on the overall variance. I.e., I can double the variance of each of the points and it will calculate the exact same variance for the slope of the line of best fit.

So I don't know how to propagate the overall error from the data set to the calculated slope from the WLS. What is the best way to account for this?"
Anova analysis help,1,0,False,False,False,statistics,1508984616,True,"https://imgur.com/2e1aLHd

I need some help with this anova analysis. Finding the MS(treatment) was easy enough (197.74) but how do i find the other 4? I'm pretty lost with this stuff"
Collection and Analysis,5,5,False,False,False,statistics,1508992336,True,"Hey everybody, I have a question about careers in data analysis. Are there any career paths/industries where you get to COLLECT the data and then apply your statistics knowledge to analyze it? I am in a situation where I think it would be interesting and exciting to do both, although I am unaware if these jobs exist. As far as context goes, I am a freshman majoring in applied statistics and planning to minor in business analytics and agriculture. "
Can someone Help me understand what would this distribution be?,1,2,False,False,False,statistics,1508992848,True,"Imagine you have a bag filled with 100 colored balls, and you know there are 10 different colors, and exactly 10 balls per color.

You grab 1 ball from the bag, you know you have 1 uniqued colored ball.

You grab 2 balls from the bag, how many unique colors do you have? (meaning if you have 2 blue balls, it's only 1 unique color)

You grab 3 balls from the bag, how many unique colors do you have? (if you have 2 blue balls and 1 red, you have 2 unique colors, i you have 1 blue, 1 red, 1 green, 3 uniques, etc.)

You take x balls, how many unique colors you have outside the bag?

How can I generalize this problem to any number of balls, colors and balls per color?"
survey on the changing work environment(already been on /r/sample size but I need more results,0,2,False,False,False,statistics,1509002291,False,
What kind of correlation coefficient test should I use for this?,5,3,False,False,False,statistics,1509002672,True,"I have to determine whether gender and belief about what economic impact immigrants have on a nation are related. I also have to decide whether and how respondent's gender describes/explains belief about what economic impact immigrants have on a nation.  

Gender is an independent categorical variable and belief about what economic impact immigrants have on a nation is a dependent quantitative variable because it's rated on a 1-10 scale. I'm still confused about what tests to run right now. I ran regression and used r and r2 to determine what the correlation is, but regression should be used when both variables are quantitative, so it cant be right.  

Can you help me with some directions? Does anyone have a good comprehensive table of how to decide which statistical tests to use?"
Question about my First Stats Project regarding normal distribution.,0,1,False,False,False,statistics,1509003513,True,[deleted]
A brief introduction to Slow Feature Analysis,0,6,False,False,False,statistics,1509008139,False,
What methodology should I do?,6,2,False,False,False,statistics,1509017598,True,"Let's say I have a variable Y and 3 explanatory variables X1 X2 X3. My goal is to check the following hypotheses :
1) as X1 increases Y increases and similarly for all other explanatory variables.

What methodology should I do ? Multiple regression isn't really approriate to check the individual effect of each variable. I believe that single regression for each pairs of variables would be better ?"
Should the choice between two sided or one sided test be based on sample's data?,5,2,False,False,False,statistics,1509019717,True,"So, I've had a test about hypothesis testing and there was a question that my teacher used the sample's data mean to choose the alternative hypothesis. But I saw a video saying that the choice should not be based on current sample's data (link down below).
https://www.youtube.com/watch?v=VP1bhopNP74 "
Aspiring biostatistician - should I bother with a machine learning class?,10,11,False,False,False,statistics,1509019864,True,"For my next semester, I have the option of choosing Statistics in Experimental Design or Machine Learning through our computer science department. I want to actually use my biostatistics degree for a biostatistics position (pharma, hospital, etc) rather than a generalized statistician or data scientist. What do you think is the merit of picking machine learning over experimental design statistics?

Masters of Biostatistics student"
Machine Learning Algorithms: Which One to Choose for Your Problem,1,15,False,False,False,statistics,1509025215,False,
Back-Transformation in an GEE model: What do I do?,0,2,False,False,False,statistics,1509033808,True,"Quick question for more advanced statisticiens out there.

I have a project, did an cubic-powered transformation of my outcome variable (so outcome^3) and used the transformed variable in a GEE model to reduce skewness.

Now I need to back-transform and present the raw estimates.

For the estimates, it is easy, I would just have to  take the cube root of it.

However, I am trying to figure out what to do for the 95% CI and the Standard Error of the means. I think my approach is correct, but I am not 100% sure... Therefore I am posting here to see if I can get some insight.

For 95% CI, I can just apply the cubic-root to get the 95% CI of the raw values. Is this correct?

For the standard error of the means, I have talked to Elodie and this is what we both think. However, we are not sure if we are 100% correct. The Standard Error of the Estimates=Standard Deviation/Sqrt(N). So when I transformed the data, I increase/decrease the standard deviation (Spread of the data) and the N remains the same. Thus, if I re-run the model using the original/untransformed values, I can use the standard error of the means of that model to present that with the back-transform GEE estimates, since we think the spread is will be the same. Is it correct?

Or can I just do the inverse-transformation for SE and CI?
"
"I'm taking Calc 2 and 3 in the upcoming semesters, then probability and statistics...",10,0,False,False,False,statistics,1509041586,True,Are there any topics in these two classes I need to pay close attention to. Once I am done with my bachelors I am going to be doing masters in machine learning. I'll be taking Linear Algebra after probability and statistics.
Helping a Fellow Student,0,1,False,False,False,statistics,1509042019,True,[removed]
Imputations vs excluding data,0,1,False,False,False,statistics,1509050006,True,[deleted]
How many sample sizes of 30 years are there in a 150 year period?,8,1,False,False,False,statistics,1509054432,True,"My instinct was 5, but someone made a good argument for there being 120 samples.  However that assumes that the sample periods can overlap and it just seems to me like you're using the same data points repeatedly."
Imputation vs. Exclusion,3,4,False,False,False,statistics,1509059039,True,[deleted]
Do we learn from the internet? - A quick survey,0,1,False,False,False,statistics,1509063112,True,[removed]
So simple... wait,38,215,False,False,False,statistics,1509064389,False,
Having difficulty understanding this problem,0,1,False,False,False,statistics,1509070220,True,[removed]
Is it possible to transition from an undergrad degree in CS to a MS in math/stats?,0,1,False,False,False,statistics,1509073137,True,[removed]
Important Things To Consider Before Buying Bitcoins,0,0,False,False,False,statistics,1509084186,False,
Frequentist or Bayesian in Psychology,9,0,False,False,False,statistics,1509087045,True,"I am taking up MA in Psychology and I am wondering about the use of statistics in the quantitative research of psychology. 

1) Why NHST used more in psychology than Bayesian?
2) How would one use Bayesian statistics in doing research? Are there such journal articles?
3) Are statistical inferences in Bayesian statistics more practical, reliable and accurate? What made them so?

It seems that the frequentist methodology is abused in psychology especially when the p-value is misused which is very critical in interpreting the results. Some say P&lt;0.05 is the ideal value. Why is that being used often?

I am asking these questions just to see if it is possible to use the Bayesian method instead of the frequentist method. 
"
Struggling in college level economic statistics... need resources because I'm struggling,2,1,False,False,False,statistics,1509092817,True,"I'm taking stats course for my econ major at the University of British Columbia and struggling hard. The textbook we're using is ""Statistics for Business and Economics"" by Newbold, Carlson, and Thorne. The book is useful, but still kinda confusing. Anywhere I can get this level of stats explained to me on youtube or something?"
Levene's test proof for different groups,6,1,False,False,False,statistics,1509096038,True,"To proof the inequality of variation assumption, the levene's test is used. This is straight forward. When the inequality is not proved, does this also proof that the two compared groups are significantly different? Or are there other conclusions that can be drawn from this?"
Help with my assignment? Don't think I'm doing it right/can't do it,0,1,False,False,False,statistics,1509096138,False,[deleted]
Seeking help with Statistics Assignment :(,0,1,False,False,False,statistics,1509101716,True,[removed]
How to increase B2B traffic by 192% in five months,0,0,False,False,False,statistics,1509104932,False,
"A Survey to determine the impact of video content posted on social networking sites like Facebook on customer engagement. Or to put it simply, how the videos we make for clients actually influence their customers. There are no studies that have yet been done about this on an academic level.",0,0,False,False,False,statistics,1509117709,False,
Conceptual questions I had on linear regression?,10,0,False,False,False,statistics,1509126984,True,"Hey,

So I've been learning linear regression, and multivariate linear regression, and ran into some conceptual questions. If possible could someone please help me understand these, I would appreciate it immensely.

Here are the questions.

For each unit change in x, how much is the change in E(y) and how much is the change in y?

Given observations x1, x2, . . . , xn from x and the corresponding observations y1, y2, . . . , yn for
y, is it true that yi = β0 + β1xi? Why or why not?

Given pairs of observations {(xi, yi), i = 1, . . . , n} which follow the model y = β0 + β1x + ε, what is the relationship between the uncertainty associated with each yi and ε?


Given pairs of observations {(xi, yi), i = 1, . . . , n} which follow the model y = β0 + β1x + ε, what is the relationship between the uncertainty associated with each yi and ε?


Any help is appreciated.

Thank you for reading"
Can someone please assist me in finding the answer to this? Thanks so much,0,0,False,False,False,statistics,1509162398,False,[deleted]
Did I do this correctly? (Part 2) AP Statistics Test Questions Clarifications,15,0,False,False,False,statistics,1509166548,True,"PART 1:

https://www.reddit.com/r/statistics/comments/77rirt/did_i_do_this_correctly_double_blind_matched/

I know I didn't get any responses for Part 1 (Free Response) on this subreddit, but I reposted on /r/AskStatistics, /r/HomeworkHelp, and /r/APStudents. Someone from /r/HomeworkHelp gave me clarifications

I will be reposting this on all those subreddits as well. 

I basically want clarifications on the Multiple Choice section of the test (I just want to make sure I got the questions correct). (The questions will be written in normal font and my **answers and commentary will be bolded**)

**1**

A group of scientists want to see if music affects probability. They observe 30 workers in an office, for one month with background music, and one month without. **This is an observational study, right? The question asked something along the lines of what type of study/experiment this is.**

_____

**2**

A dude wants to dial random phone numbers. He uses a random number dialing system and randomly picks numbers from the book. The numbers in the book are sorted by area code, though, and the number of phone numbers matches the area code. **This is a multistage sample, right? Yes, he chose phone numbers randomly, but the phone numbers were organized and grouped by area code.**

_______________

**3**

A farmer has 3 new farming methods. He has six new fields with 30 plots each. He wants to test the methods on each field with one of each method assigned to 10 of the 30 plots. **This would be a matched pairs design with each plot having it's own method, right?**

__________________

**4**


A scientist does an experiment. 50% of people said they voluntarily took ibuprofin and their disease decreased. It is valid to conclude that ibuprofin decreases the disease, right? **Yes, because 50% is so large that is unlikely to occur by chance. (That's the answer I put - idk if the question is correct though - I kind of forgot what it was. It has something to do with ibuprofin but I am not sure about the disease decreasing part. What would the problem have to state for my answer to be correct? 50% is in the question somewhere.)**

___________________________

**5**

A researcher wants to see if a medicine reduces headaches. He gives it to 25 people. Four hours later, 20 respond saying they felt better. Which is not a problem of the experiment? **Insufficient attention to placebo, right? The question didn't state anywhere that people would get a placebo so I believed everyone would get the same medicine. Other answers included something along the lines of - lack of response (or voluntary response), and some other things which i don't remember but I felt were a problem as well.**

____________

EDIT: It'd be great to get an extra set of eyes over Part 1 too"
Weighting shipping costs by location,3,0,False,False,False,statistics,1509210108,True,"I have a spreadsheet that has shipping costs from and to several locations in the US.

We need to figure out an average cost of shipping for any given UPC from a destination.

Problem is, we will ship to Alaska, but it only accounts for 10-15% of our business, so we want to weight that accordingly. 

Is it as simple as taking the shipping cost of shipping a UPC to Alaska and multiplying it by 10% or 15%? This is with the assumption that no other locations have this weighting/scaling factor b/c they aren't as expensive to ship to and account for a much larger percentage of our business.

I guess my question is - can I safely weight Alaska shipping costs as 10-15% without weighing the other destinations and still get an acceptable average?"
"Copula: standard families, association measures, 2D-plots, 3D-plots",0,1,False,False,False,statistics,1509210785,False,
"Learned stats long ago just for school, but tests and exams ruined it for me. How can I rekindle my love for stats?",26,23,False,False,False,statistics,1509212067,True,"I'm thinking of going back to basics. Thinking of what stats is really about, and trying to see its applications in the world. Simple things first. Rolling a die. Flipping a coin. No memorizing formulas just for the sake of passing an exam. 


Any recommendations for reading materials to rekindle my love of stats? I don't even mind books for children, as I really want to start over and re-train my brain to see stats as something interesting to learn, not as something to just memorize."
How do I present this in an excel graph? I don’t have the original dataset,0,1,False,False,False,statistics,1509215566,False,[deleted]
I am Terrible At Stats. What should I do?,2,0,False,False,False,statistics,1509217247,True,"I have been doing very poorly on the quizzes/tests even after going to multiple tutoring sessions.

Can't even catch a break really to catch up as quizzes are pretty much every week."
differences between SAS and SAS University.,8,3,False,False,False,statistics,1509230208,True,Someone asked me about the differences and I am not a University user. Does anyone have any experience here?
Two-Way ANOVA,0,1,False,False,False,statistics,1509232777,True,[removed]
LP Model,0,1,False,False,False,statistics,1509242166,True,[removed]
Breiman : Statistical Modeling: The Two Cultures,3,47,False,False,False,statistics,1509284247,False,
Need some tips on how to solve linear regression transformation problem,2,1,False,False,False,statistics,1509285599,True,"I'm having exam soon, just need some tips on how to solve this kind of transformation problem, thank you!!
Consider the following regression model:
y = β0 + β1*x1 + β2*x2 + u
(i) Transform the regression model above in such a way that you can use a t test to verify: (a) β1 = β2 (b) β1 + 2*β2 = 0 (c) β1 + β2 = 1 "
Does there exist a way to calculate correlation for the following problem?,0,1,False,False,False,statistics,1509294499,True,[removed]
What test would I perform as followup to my findings (one-way ANOVA project)?,4,1,False,False,False,statistics,1509296287,True,"I'm in an intermediate stats class working on an ANOVA project and while I don't have to followup with anything else, I'm curious as to what I would follow up with if I wanted to. I'd also like to suggest that I have an idea of what I'd do to follow up in my analysis.

I tested to see if the mean of something between different species differed between locations and found that it does. Some of the locations were closer together, so I'd like to test if the closer together the locations, the closer together the means. I found via multiple comparisons that there is some connection (similar locations = similar means) but is there some sort of test that would quantify something like the ratio between distance and similarities of means? Would I just do two-way ANOVA to test for interaction? (We're only just starting on two-way so I'm not entirely familiar with it)."
Please help,0,1,False,False,False,statistics,1509303588,True,[removed]
Parents I need your help for an AP Stats project,2,0,False,False,False,statistics,1509321095,True,It'd be a great help to me. I want to know how long your kid trick or treats for and how much candy they get. If you have multiple kids just list then separately. If they're lucky enough to get full size candy bars they're worth two pieces.
P-value,6,0,False,False,False,statistics,1509323452,True,"Can anyone explain why this happens?
For a college project I am doing regression analysis and one of the variables has a p-value &lt; 0.001 when it is by itself and it seems like it fits the model well. But when I do the partial F test with 2 more variables the p-value of that same variables goes to 0.95. My question for the project is to explain why that happens?"
"Probability Theory question... X,Y,Z are continuous RV's with unknown pdf but are i.i.d. Is it possible to find P(X&lt;Y&lt;Z)? If yes find the probability if no say what additional info is required to calculate the probability.",8,7,False,False,False,statistics,1509339506,True,"**SOLVED**

Sorry it's a bit lengthy. I tried googling but all I could find were similar questions with known pdf. 

In this question pdf is unknown. I feel like the answer is Yes because if the answer is No then all I have to say is the pdf is required and be done. 

Sorry poor English.. I dont ask for answers but any hints would be appreciated. 

Edit: Thanks everyone I learned something new today :D "
Help with regression containing probabilities as DV,2,2,False,False,False,statistics,1509346366,True,"A colleague and I are trying to determine whether one class does better than two others on a test (so percent correct ranges from ~30 to 100). The only independent variables are class identifiers (1, 2, and 3).

Is it ok to lump the two control groups together (2 and 3) and just do a difference of means between the one control group and the other two classes lumped together using a t-test in Excel?

Also, is it possible to use a logistic regression in this situation? I remember logistic was ok if the dependent variable is categorical/ordinal output, but maybe I'm missing something.   

Many thanks for any guidance"
AskReddit: dynamically determining sample size of statistical test,0,1,False,False,False,statistics,1509364738,True,[removed]
How do you figure out the variance with two correlated proportions?,0,1,False,False,False,statistics,1509373411,True,[removed]
Do I need statcrunch if I have microsoft excel?,0,1,False,False,False,statistics,1509379348,True,[removed]
"[Statistics Question] Using inequalities like Chebychev's or Cantelli's inequality, can we predict the bounds for an arbitrary distribution's quantiles knowing only the sample mean and std?",4,3,False,False,False,statistics,1509380786,True,"Using these inequalities it's possible to prove that the median is no more than 1 standard deviation away from the mean. I was curious if there is a way to do this for other quantiles as well?

I believe I am able to get an upper bound for the 90th percentile, but I am struggling a bit with lower bounds."
Why is Standard Error more affected by Sample Size Than Standard Deviation?,8,2,False,False,False,statistics,1509382468,True,
Comparing a function to real-world data with scaling,0,2,False,False,False,statistics,1509382738,True,"I have a function that predicts the probability a random variable X will take value x within a tolerance delta: f(x,delta) = P(|X-x|&lt;= delta). 

I'm attempting to compare to the function to real-world data. I have N realizations of X which ill denote by xn n=1...N. For chosen x and delta, M out of the N realizations satisfy |xn-x| &lt;= delta. An estimation of f(x,delta) for the real data is then festimate(x,delta) = M(x,delta)/N.

Plotting festimate and f on the same axis should show me how well the real world data matches my function. [This image](https://imgur.com/a/2slOb) gives an example of the raw results where red is f and blue is festimate. At first glance, the differences are too significant to assume that they match. I noticed, though, that if the functions are rescaled to have matching sums over x i.e. multiply f by constant A such that sum_over_x[festimate(x,delta)] = A*sum_over_x[f(x,delta)] then they appear to match. [This image](https://imgur.com/a/S5cG8) shows the results of scaling f to match festimate with the same data as the initial image.

Obviously more detailed analysis than an eye-test is required to say that they actually match, but am I justified in comparing the scaled functions instead of the raw functions? That is, if I perform analysis that finds the scaled functions do in fact match, does that mean my function matches the real world data? Or does the fact that I scaled one of them distort the results. What does the fact that they sum to different amounts imply anyway? Is it a result of the fact that I have finite N?

"
Senior Biostatistician Job opening,1,1,False,False,False,statistics,1509387621,True,[removed]
Statista.con,0,1,False,False,False,statistics,1509387674,True,[removed]
Help me understand the difference between incidence rate ratio and risk ratio?,2,1,False,False,False,statistics,1509392375,True,"I’m really not understanding the difference. If I’m looking at a 2X2 table, how Can I calculate each one? "
Need dire help calculating an Odds-Ratio.,9,2,False,False,False,statistics,1509392889,True,"Thanks for clicking. I'm a newbie research fellow working in a Pharmacology department at my local hospital. My current project is looking at the the development of osteoporosis after being on opioids for 5 years. This is the data that I currently have at the moment:

Opioids and developing osteoporosis after: 2,020 patients
- This subset includes both opioid dependence(OD) (67,296) and opioid dependence(OA) (17,898), which gives me the number below. I know the math isn't completely correct, and that's because I'm using a program known as PearlDiver which is used to find patient populations based on specific ICD-9 and ICD-10 codes. 

Total patients who are taking opioids: 81,479

Patients who developed osteoporosis that were NOT taking opioids before: 1,001,116
Patients in our overall sample size: 20,818,521 (20,900,000 - 81,479)

There are a few basic science papers which talk about opioids eventually leading to osteoporosis, but I'm having trouble figuring out how to get an Odds-Ratio from this. Any input is greatly appreciated. 

Edit: What I'm extremely bothered by is that taking opioids led to 2.5% chance of developing osteoporosis in patients who were either OD or OA; whereas in the general population you had a 48% chance. Seems like opioids have a protective effect, but I need the numbers to work in the opposite way, but I just don't know what I'm doing wrong here. "
Need help with k-nearest-neighbor leave-one-out cross validation,0,1,False,False,False,statistics,1509397620,True,[removed]
"Stupid question about Monte Carlo. How do we know it ""works""?",14,11,False,False,False,statistics,1509405834,True,"I'm familiar with *how* Monte Carlo works; I've been using it for two years in my position at an insurance company. But what I'm unsure of is how the scientific community became convinced that it works and produces useful results. I tried doing my own research but I can't find any arguments for *why* it works, just how it works and how to do it. 

Our relatively elementary model we use at work takes a matrix of 10,000 economic scenarios (with Treasury curves, credit spreads, equity returns, and other variables for each scenario) generated by a vendor solution, calculates the financial impact of each scenario on the company, and spits out a 1x10,000 distribution of those financial impacts. This distribution is always pretty close to a normal distribution, every time we update it with new information. Should we be concerned that it's always such a nicely behaved normal distribution? Did we unintentionally cook it up to behave like this? Is this a feature of Monte Carlo? Do MC models ever not ""converge"" like this? My exposure to the technique is limited to this model so I have nothing to compare it to. 

Thanks in advance for your patience with my dumb questions! "
is there a difference with statcrunch and excel?,3,3,False,False,False,statistics,1509405914,True,class using stat crunch and was wondering if I could just use excel? I have to buy the statcrunch software and just want to save money. 
Health Study Got This Bizarre Result By Using Wrong Statistics. No One Picked Up On It.,1,0,False,False,False,statistics,1509408908,False,
"Actuarial, probability of extinction for 2 lives",3,2,False,False,False,statistics,1509409506,True,"So i don't really seem to grasp how to apply this concept. I'm messing up something because i'm getting negative probabilities or values that are bigger than 1.  
Particularly, with this formula:  
t/q_\hat{xy}, t = {m, m+1, ..., m+n-1}.  
I understand that to calculate m/q_\hat{xy} it can be done with:  
mp_\hat{xy} - (m+1)p_\hat{xy}  
but i'm failing.

Anyway, I have a mortality table with q_x (Men) and q_y (Women) and wanted to calculate the premium they would have to pay. An easy enough formula that is just t/q_\hat{xy} times some financial mumbo jumbo and sum it all. But for the life of me i can't figure out what i'm doing wrong.

Let's say x=35 and y=33, i want to do from there until the youngest one reaches age 65. If both of them die before the youngest one reaches 65y.o. their son will be paid money from the insurance company. Ok, so i go to the table...  
p_xy = (1-q35(Men)) * (1-q33(Women))  
p_\hat{xy} = (1-q35(M)) + (1 - q33(F)) - p_xy  
Here is where i get stuck. Do i need more values, or are these 2 enough? Like i tried to apply it but

m=1
p_\hat{xy} - (2)p_\hat{xy}  
Negative obviously.

I think i'm messing up some concept really badly but not sure where exactly. Any help will be appreciated

Sidenote:
I also tried this:  
p_xy = (1-q35(Men)) * (1-q33(Women))  
p_\hat{xy} = (1-q35(M)) + (1 - q33(F)) - p_xy  
q_\hat{xy} = 1-p_\hat{xy}  

And repeat the whole process changing q35 to q36, and so on. Would this also be correct?

EDIT: Why are underscores disappearing? Why no math mode?"
Help me understand the difference between Nonresponse and Undercoverage (non-sampling errors),0,1,False,False,False,statistics,1509413965,True,"In an AP Statistics class and we’re discussing sampling methods and their errors. I know that Nonresponse is when you cannot reach certain participants due to refusal to participate or unable to connect to. I also know that undercoverage is when certain groups are left out of the process of choosing the sample size. 

What I don’t understand is the difference between the two in terms of impact on the endgame representation of the population. Wouldn’t both Nonresponse and Undercoverage result in groups being left out of the sample size? What’s the difference between for example a sample size of 100 and you are unable to reach 20 (Nonresponse) and the situation where a sample size of 100 and 20 people are left out?

Edit: after watching a few videos I think I may have it down. So in Nonresponse it’s completely random that the type of people are unable to be reached or refuse to participate but in Undercoverage there is a **certain** group being left out?"
Weakling in statistics needs help: Is this the right test?,0,0,False,False,False,statistics,1509416964,True,"I'm proposing a study design for a class and feel just so absolutely overwhelmed right now. I love that I'm learning, but feel like a dumdum. Can you let me know if I'm on the right track?

1st aim: Test the effects of learning compensation methods (IV's that are nominal) on learning satisfaction scale (continuous).

Initially I thought it would be a factorial ANOVA since I have many IV's. The IV's that are nominal are a list of compensation methods that someone can use to make themselves feel better. They can check none, one, up to six total items in the IV category (things they use to make themselves feel better). Technically this is 7 options (including none). I want to compare the groups that use none to the groups that use one or more. Would this be a factorial ANOVA? 

QUESTION: If I wanted to test the effect of each one: first choice, second choice, etc. independently in comparisons to none, would that still be a one-way ANOVA? Or Chi-Square test?

Aim 2: Test the compensation methods as a moderator of the relationship between learning difficulties and learning satisfaction.

Test: I am thinking multiple regression using interaction terms. The IV is learning difficulties, the DV is learning satisfaction, and I am testing learning compensation strategies (yes/no - yes, used at least one, no, used none) as a MODERATOR for the DV, learning satisfaction and comparing the two. I'm fuzzy at best on adding interaction terms and what they mean. For example: Learning Anxiety (specific IV) by Compensation would be one interaction term. I just don't know how this would look.

Any resources or help would be amazing. Thank you!"
Looking for a Statistician/Data Analyst to Interview,1,1,False,False,False,statistics,1509418640,True,"Hello r/statistics! I am currently writing a research paper on careers in statistics, and I need to interview someone for primary research. The issue is, I could not get in contact with anyone at Pandora (the company I chose), and time is running out to find someone to interview. If anyone wouldn't mind doing an informal phone/email ""interview"", I would be very grateful! I am specifically interested in statistics related to the music industry, psychology, and sports, although I can't be too picky at this point. Thanks!"
Chi Squared Test Hypothesis? Is this incorrect... I thought it was testing independence/association? This is from my TA...,5,1,False,False,False,statistics,1509420646,False,
How to test if my data follows a model?,5,1,False,False,False,statistics,1509428472,True,[deleted]
Basic question about OLS intercept,1,2,False,False,False,statistics,1509429635,True,"I’m in an intro to regression class so this will be covered in a few weeks, but when I look at some OLS regression models, there is an error term (notated as a, alpha, or e sometimes) and an intercept (notated as mu).

Why are there both? Wouldn’t they just be summed into a constant in the final model?"
Convexity of likelihood,8,1,False,False,False,statistics,1509452702,True,"I'm reading an [article](http://artem.sobolev.name/posts/2016-07-01-neural-variational-inference-classical-theory.html) about Variational Inference.

In the *Latent Variables Models* section, the author says that since we don't know the latent variable z we might marginalize that out, but that way *we would lose convexity*.

I don't understand why that would be the case. The author is implying that p(x,z|theta) is concave. That means that this is true for *all* x and z, but then the marginalization is just a sum of concave functions wrt theta which should still be concave.

edit: I think this isn't a problem of convexity but that, when we take the log to simplify computations, the log is ""blocked"" by the summation and so we can't convert the inner product into a summation.

That's why we need EM or similar algorithms. Do you agree or am I missing something?"
Can a null and alternative hypothesis both be false?,23,1,False,False,False,statistics,1509457751,True,[deleted]
Get probability that sub-group has higher than expected mean?,0,1,False,False,False,statistics,1509459850,True,[removed]
Simpsons Paradox: Are University admissions biased?,28,65,False,False,False,statistics,1509460989,False,
Non normality in OLS assumption,4,6,False,False,False,statistics,1509461802,True,"Gelman and Hill (2006) write on p46 that:

""The regression assumption that is generally least important is that the errors are normally distributed. In fact, for the purpose of estimating the regression line (as compared to predicting individual data points), the assumption of normality is barely important at all. Thus, in contrast to many regression textbooks, we do not recommend diagnostics of the normality of regression residuals.""

As I understand this it the normality assumption is so that the estimator of beta is MLE in addition to BLUE. My question is, how would we interpret a estimator of beta that is not the MLE but BLUE? Am I understanding that there is some estimator of beta that is better supported by the data, but is either biased or perhaps converges to the parameter beta more slowly?

Thanks."
Chi-square test for dependent data or different significance test?,0,1,False,False,False,statistics,1509463677,True,[removed]
How to do diagnostics for binary logistic regression?,1,1,False,False,False,statistics,1509486420,True,"We know for normal linear regression, we can do diagnostics by looking at the residuals. 

For logistic regression where for each predictor value there are many data points, we can look at Pearsons residual or the Deviance residual. Or maybe do some kind of Chi Squared test, based on the expected probability at each predictor value and the observed.

But what about binary logistic regression where there is only one data for each predictor value?

Thanks"
Means Square Percentage Error vs OLS,0,1,False,False,False,statistics,1509488430,True,[removed]
Any Insights Analysts willing to give advice to a potential intern for a game studio?,5,8,False,False,False,statistics,1509499147,True,"I have an interview with a large game studio about an Insights Analyst internship position. Lots of the questions will revolve around basic qualitative questions to understand my experience in maths with a focus on stats as well as going into developing plans around questions product managers have and figuring out what data to record and how to analyze them.

Does anyone have any insight into how much hard math I should know? I'm a junior in university studying game design with a marketing minor but I have yet to really dive into any insights analyst classes. I haven't even taken a stats course since AP Stats in high school. They gave me a sample question and it was pretty basic but I can see myself tripping up very easily because I've never been strong at math in class unless I could go home and just sit and do the problems.

I've analyzed data from usability tests but thats different.

I'm trying to just refresh my memory as much as possible and from what I remember stats math isn't too difficult mainly understanding what biases can occur and how to remedy them is the hard part. Can anyone confirm that?

What level of math do you need? With calculator or no calculator?

What kind of paradoxes are common and I should definitely know? The recruiter mentioned ""Simpson's Paradox."" Never heard about it until now -- should I have?"
Probability Contingency Table,1,0,False,False,False,statistics,1509500994,True,"I am new to statistics and I have no idea what I am doing. I am being tasked to fill out the table in the image based on these numbers. 

Total N = 1,000,000

A = 650,000				0.65 

B = 500,000				0.50

A n B intersection = 250,000	0.25

Need to fill out the following contingency table:

https://imgur.com/e8Z493T"
"Somewhat basic question, what method I should use to compare several regression models that use a different sample?",11,3,False,False,False,statistics,1509505015,True,"I have ten multivariate regression models, each one contains the same variables, but the each model uses data from a separate country. How can I compare these models? Do I have to do multilevel modeling?

I have been reading quite a few research papers and guides to get a clue about how to best do this but I am still kinda unsure. "
Interpretation of Varimax rotation in Principal Components Analysis,8,7,False,False,False,statistics,1509505183,True,"I'm currently running factor analysis on scans of a geological core sample. I believe that I should be using varimax rotation to simplify this data and improve the interpretation, however I'm finding that step difficult to understand.

I've attached images of the rotated and un-rotated solutions. You can see that if I attempt to write up the un-rotated analysis, I could say that ""BSi is loaded strongly on the second component"", for example. However this is no longer true in the Varimax rotated output. Which of these tables should I be relying on when trying to interpret the results?

https://imgur.com/a/9t33a"
Can someone help me understand how to read standard errors of coefficient estimates?,2,1,False,False,False,statistics,1509505523,True,"This is my first time using program PRESENCE, but I still don't understand how to interpret my data. We are doing detection probabilities where (p) = the probability that you detected the species of interest during each survey and (psi) = percent of habitat occupied by your species of interest. 

Here are the variables: 
•	Elevation: elevation at the sample site
•	HD index: an index of human disturbance at the sample site (incorporated people and domestic animals seen during surveys or captured on the cameras)
•	Road_dist: distance to the nearest road (measured using ArcGIS)
•	Trail_dist: distance to the nearest trail (measured using ArcGIS)
•	Grass_cover: the percentage of each sample plot that was covered with grasses/forbs
•	Shrub_cover: the percentage of each sample plot that was covered with shrubs
•	Rock_cover: the percentage of each sample plot that was covered with rocks
•	Slope: the slope (in degrees) at the sample location
•	Wash: If there was a wash present (1) or not (0) at the sample location
•	Water_dist: the distance to the nearest permanent water source (measured using ArcGIS)
•	Shrub_ht: average height of shrubs at the sample location


Questions: Is kit fox occupancy positively or negatively related to each variable? Why do you think that kit fox occupancy had these relationships with each variable (think about their biology/ecology)?

Standard errors and coefficient estimates: 
A1   psi                              :  -0.357223    0.345355
A2   psi.trail_dist                 :   0.416311    0.170759
A3   psi.shrub_cover            :  -0.677280    0.252126
A4   psi.slope                      :  -0.664279    0.394509
A5   psi.wash                      :  -0.409651    0.387987
B1   P[1]                            :   0.257518    0.136213

"
Can anyone help with this? It's driving me crazy but seems so simple...,2,0,False,False,False,statistics,1509512088,False,
Major in Math or Statistics?,16,7,False,False,False,statistics,1509514772,True,"Hi everyone,

I'm a lower sophomore who is  currently taking Calc II.  I had planned on declaring as a Mathematics major and then completing a Masters in Statistics.


I was speaking to some class mates and most have said that if I plan on doing a Masters in Statistics, that doing a bachelors in Stats would be better than doing a bachelors in Math.  The only classes that overlap between the two majors are the math requirements for the Statistics major that go up to Linear Algebra. 


However, I was advised by the statistics adviser that if I want to pursue a Masters in Sats, then sticking to the Math Major would be the best decision.


I'm very conflicted because my research has not yielded any significant answers that would sway my decision.  I was wondering if I could get some help from the readers of this sub on this decision.  Would it be best to major in Mathematics or Statistics if I plan on pursuing a Masters in Statistics?


Thanks for your time. 

Edit: Thank you everyone for your replies.  I will respond individually when I get home, but for now, I believe I will major in Math and minor in Computer Science since I would only need 3 more classes for the minor, one of which I was already planning to take next semester."
Behaviorism bf skinner statistics guidance,0,1,False,False,False,statistics,1509517817,True,[removed]
Compare means between overlapping groups,0,1,False,False,False,statistics,1509525391,True,[removed]
Programming in C,0,1,False,False,False,statistics,1509541142,True,[removed]
The Numbers Behind Content Marketing: Essential Statistics for 2017,1,1,False,False,False,statistics,1509542753,False,
Help on a question?,2,0,False,False,False,statistics,1509550000,True,"Out of 211 patients, 16 have a reaction to a medication used. Of the 53 patients actually used for the study, what is the probability that

a. Exactly 6 will have a reaction 
b.Fewer than 3 will have a reaction "
Stats masters programs that are good prep for Phd?,2,4,False,False,False,statistics,1509551934,True,"Hi everyone,
I'm currently applying to statistics masters programs and would really appreciate some guidance on which programs to apply to. I was an econ major in college and have a comparatively lighter math/stat background, thus my intention of pursuing a masters first. I would ultimately like to work as a data scientist, but am entertaining the thought of getting a Phd in stats after the masters.     However, most of the masters programs are advertised as terminal masters programs that are aimed at people getting industry jobs after. Does anyone have recommendations on masters programs that would serve as a good stepping stone to a Phd? Thanks!"
Help with medical research statistics?!,0,1,False,False,False,statistics,1509561726,True,[removed]
What did you/are you writing your Ph.D./masters thesis on? Aspiring graduate student.,36,13,False,False,False,statistics,1509563861,True,Current mathematics undergrad applying to graduate programs in statistics. I would like an idea of what you all studied while in grad school (other than the normal classes). Any advice on the app process is welcomed as well. Any ideas on what to make sure I have down before I start? 
How many people from Yorkshire live in London?,2,0,False,False,False,statistics,1509565263,True,Would anyone know where to find out how many people born in Yorkshire now live in London? Thank you.
Help me with these gambling odds please,2,3,False,False,False,statistics,1509577757,True,"Hello fine people who know about something that I have no idea about and thus am in your debt to know what I need to know, I was wondering if you could help me on something relating to stats. 

Basically on UK scratchcards they say that the ‘overall’ odds of winning are usually about 1 in 3.73 but when you check the odds for each individual win amount online, the lowest odds for just winning your money back are about 1 in 10 before you buy a card. 

So if I was to buy 3 cards and get my money back on one, did I do something with odds of 1 in 3.73 or 1 in 10 and thus in that little transaction ‘beat the odds’? 

Say that for different amounts there are different odds so the higher the win then the lower the odds, and I won some throughout the week and knew how many I had bought overall, how would I find out what the true odds of achieving what I did actually were? 

It may sound random but it’s really important to me that I understand this, I hope what I’m asking makes sense and trust me when I say that any help you can give me is MUCH appreciated. 

Thanks "
"Anybody have the text book ""Intro stats 4th ed."" By De Veaux ?",2,0,False,False,False,statistics,1509591611,True,[deleted]
CS Major looking for PhD programs in Statistics,19,8,False,False,False,statistics,1509609539,True,"Hi guy. I majored in Computer Science in college but now I am planning to apply to Stat programs. Could you please suggest which programs would be within my reach?

+ GPA: 96.22/100 (Rank 1 in the CS department of a top university in Korea)

+ TOEFL: 102. GRE:152V 169Q 4.0W 

+ Math classes taken: Calculus 1-3 (taken during high school), intro to linear algebra, probability and statistics, discrete math, intro to statistical analysis, intro to operation research. I got A in all of them except discrete math (B).

+ Research: I have one paper at a low-tier conference about data mining.

+ Experience: I did several internships in the industry at companies like Google and Adobe, working on machine learning / deep learning. One of the reasons why I want to go to Stat Phd program is because to me deep learning is like a black box and I want to understand the theory behind its success.

Thank you."
Process Development and Validation rest on the right Design of Experiments and Statistical Process Control,1,1,False,False,False,statistics,1509621731,False,
Is this study design a case control? Or what is it?,3,2,False,False,False,statistics,1509623978,True,"100 subjects with a disease have their blood pressure taken. 100 subjects without the disease have their blood pressure taken.  The average blood pressure is compared with a t-test.   

"
Quick question,0,2,False,False,False,statistics,1509628475,True,"I don’t know if there is a reference or something...

I’m a teacher and I wanted to get a hold of a bit of data. 

When I teach industrialization and show them the growth in population, they immediately think people had MORE sex as we progressed to now. 

The truth is more people had the same amount of sex. The more people part is due to advances in agriculture and medicine. 

So I have total population growth due to independent variables, but is there a population growth model for a population that was pre-industrial?  

Do you just use census data pre-1800 for US?  

Would say pre-industrial Britain be better because of the limited immigration?

Basically, is there a model that boils down population growth to: two children for every household per 20 yrs. 

"
Favourite statistics resources?,8,5,False,False,False,statistics,1509628718,True,"Hey, all. My favourite statistics teacher got arrested (damn) so all his resources were taken down. I’m a slacker so I never anticipated anything changing and lost all those resources and don’t have back ups. So, I’m a social science grad student and was hoping to see if anyone had any tips for beginner/intermediate statistics resources, especially those in SPSS programs. Cheers. "
Please help me understand this statistical discrepancy.,3,3,False,False,False,statistics,1509628735,True,"If I have a bucket with 100 marbles: 99 red and 1 blue.  I randomly choose two at the same time.  The chances are 2/100 or 1/50 (.02) that I have the blue one.  Now, if I start over and draw one marble once then draw one marble later, my odds of getting a blue one are 1/100 + 1/99 (.0201).  What's the difference?"
Probabilistic Graphical Models: a powerful framework to learn the models with dependency,3,6,False,False,False,statistics,1509631451,False,
I really like the Numberphile Youtube channel - are there any similar channels that focus on statistics?,11,89,False,False,False,statistics,1509637239,True, 
(Variational) EM algorithm: doubts,4,4,False,False,False,statistics,1509645063,True,"Here's my derivation for the EM algorithm: [picture](https://imgur.com/PLkmYYm) | [mirror](https://ibb.co/neounb).

Given that derivation, it's easy to prove the non-decreasing property of the EM algorithm. Also, EM is a Minorization Maximization (MM) method.

But is this enough to prove that EM makes real improvement towards the solution we want?

Let's say that, right after the E-step, we can't make any improvement in the M-step. Does that mean that we have found a local maximum for log p(x;theta)? Let's also assume that the E-step managed to bring the KL to 0. It's clear that the ELBO is equal to log p(x;theta) **but only in a particular theta**! That's not enough to assume that if we can't maximize the ELBO then we can't maximize log p(x;theta) either, right?

By taking the gradient of both the LHS and the RHS, we can see that, if KL = 0, the gradients, at theta, are equal (the KL is minimum at 0 so it's grad is 0).

**Question:** But by taking the gradient I'm assuming differentiability. Is there a more general method?

One might say, ""If you assume that the KL is 0 then it means q is as flexible as p so why use EM in the first instance?""

My answer to that would be that the sets of representable distributions of p and q might overlap and so we can still have KL = 0 (but not always) even if q is less expressive than p.

**Not a question, but a request: please let me know if anything doesn't add up!**"
Calculating power for a mediation model?,0,1,False,False,False,statistics,1509649266,True,"Howdy, y'all. I'm about to peel open a data set and there have been questions from my peers about calculating power for mediation analyses (I am using Mplus) with N = 300. There are seven variables in the model (one IV, four M, two DV) with two covariates. That being said, I've been told before that a general rule of thumb is have at N = 20 per variable in the model.

That was wordy, but I wonder if any of my e-peers in this sub could guide me toward either (1) literature regarding power analyses for mediation models and/or (2) a power calculator that has this function (which I've never heard of, but have been desiring since the first year of graduate school). "
I have assumed control of /r/datacareerquestions and want to get it running [x-post from /r/datascience],4,7,False,False,False,statistics,1509653526,True,"/r/datascience and /r/statistics have been getting a lot of ""Is this a good way to get into data science/statistics/machine learning/data engineering/etc.?"" questions.  I figured it would make sense to have a specialized sub just for data career questions where professionals in various data-related areas can answer questions and give advice to students or people curious about the field.  Turns out someone had already made one in the past and it didn’t have any mods, so [I’m the mod now.](https://imgflip.com/memetemplate/Im-The-Captain-Now)

For at the first month I'm not going to be enforcing any rules except ""No images"" and ""All threads must ask a question.""  Then we'll discuss rules.

Have fun :)"
Problems In Estimating GARCH Parameters in R,0,5,False,False,False,statistics,1509654993,False,
Regression with two DVs? But not really?,8,1,False,False,False,statistics,1509655855,True,"I'm currently running a 103 fever and can't think straight, but perhaps someone has a better operating brain than I do. 

I current hypothesize that years of participation in X activity (*activity*) predicts perfectionism profiles. I'm using a model of perfectionism that include two subscales, *Con-P* and *SE-P*, along with a global aggregate score (*perfect*) that is merely equal to (*Con-P* + *SE-P*). 

So while I could just regress *perfect* on *activity*, that doesn't give me an idea of the makeup of *perfect*. For example, someone who scores 20 on Con-P and 0 on SE-P would have the same *perfect* aggregate score as someone who scores 0 on Con-P and 20 on SE-P.

How should I handle this? Should I just treat Con-P and SE-P as two separate DVs? (All variables are interval and normal). I'm kind of at a loss for where to start haha."
Measuring attractiveness of people?,7,1,False,False,False,statistics,1509660519,True,"I'm building a survey project for my stats class, and one of the metrics I'd like to include is 'attractiveness'. However, the collection of data is going to be done by multiple people, and even if it wasn't, a single surveyor is likely going to be biased for such a subjective trait. Is it even possible to get an accurate measure of how attractive someone is, and if so, how?


Edit: Here is an example scenario of what I'm trying to accomplish:

Say I want to measure the attractiveness of customers of Starbucks vs Dunkin' Donuts. I send a single surveyor to five different locations of each, and have them record the attractiveness of each customer one a 1-10 scale. How can I ensure Surveyor A's ""7"" is relatively the same as Surveyor B's ""7""?"
"When I apply a linear transform to an input variable for a regression (and potentially an outcome variable), how do I reverse this to be able to interpret the coefficients on the original scale?",2,3,False,False,False,statistics,1509661396,True,"I am performing regressions using Bayesian parameter estimation. As per [Gelman et al. (2008)](http://www.stat.columbia.edu/~gelman/research/published/priors11.pdf) I am using a Cauchy distribution with a scale of 2.5. As part of this:

* In a logistic regression, I have rescaled continuous variables to have a mean of 0 and sd of 0.5 and categorical variables to have a mean of 0 with a 1 point difference between high and low (e.g., -.5 and .5).
* In a linear regression, I have rescaled the input variables in the same way as above, as well as now rescaling the output variable to have a mean of 0 and sd of 0.5.

My question is now: what do I do to the resulting coefficients and 95% CIs to be able to interpret them on the original scale? Does the answer differ based on whether both the input and output variables were rescaled vs. just the input variables?

Thanks!"
Machine Learning vs Statistics,3,0,False,False,False,statistics,1509676509,True,"I think most people who have taken some statistics are aware that there are 2 approaches to statistics: frequentist and Bayesian. The former is the significance testing, many-trials-in-the-long-run approach, while the latter is more concerned with how your beliefs about something (in a probabilistic sense) change with data.

But, machine learning doesn't really fall nicely in either field. A neural network generally doesn't use significance levels or frequentist methods, and most applications of Bayesian statistics to machine learning (that I've seen anyway) are shallow at best and extraneous at worst - there are things like naive bayes classifiers, but regression trees and neural networks generally don't use Bayesian methods.

But, it still seems to me that machine learning is a part of statistics. Fundamentally, machine learning is about the analysis of data, and most methods rely on some conception of regression, distributions, or some ""pure"" statistics topic.

How does machine learning relate to the field of statistics in general?"
Question about Poisson processes,12,1,False,False,False,statistics,1509683751,True,"I have a question about this. I'm an actuary and have a masters in Stats, but I never understood how one can identify a Poisson process.

I mean, per google definition, a Poisson Process is something where the number of events in an interval follows a Poisson random variable. But I want something more down to earth than that. Namely, in daily life, how do I determine if something follows a Poisson process with, say, if not 100%, but 99.99% confidence?

How does one confidently say that something follows a Poisson process in real life?"
Interpreting results of a survey I conducted,0,1,False,False,False,statistics,1509683857,True,"Hello,

This isn't exactly a 'homework' question, but it is part of a project I'm doing that's related to school. If this is the wrong subreddit for this, please tell me and I will repost it where appropriate.

TL;DR: Experiment about drug use, have subjects fill out form either anonymously or non-anonymously. How do I tease out results?

In my study, I'm trying to observe the effects that answering an anonymous vs. a non-anonymous survey had on reported drug use. I have now collected information from 230 students (though only 216 submitted a properly filled out survey). For my sampling design, I chose to block my sample by both grade in school and the difficulty of their class. I chose to visit English classes since it is required of all students, no students skip grades like they might in math, and for every year there is an advanced/AP class and a regular class. So, I visited 8 classes total, one for each grade level and level of class difficulty, and handed out roughly the same amount of anonymous v. non-anonymous surveys in each class. 

The meat of my question lies in analyzing the results. What I've done so far to see the cumulative effect of the anonymous vs. non-anonymous is to track the percentage of each classroom that answered yes to drug use in both scenarios. Then, I subtract the % of people who answered yes anonymously from the % of people who answered yes non-anonymously. If the number is negative, then people answered yes to drug use more often when they had to write down their names. If it is positive, then people answered yes to drug use more often when they were anonymous. Then, once I have these percentage changes, I am multiplying them by the proportion that each class is of the total population (the class sizes ranged from 21 to 33). If I'm doing this right, then I should add all the weighted %s, and that should be the net effect that answering anonymously has on reported drug use. 

My project teammate disagrees with the whole blocking by grade and class level thing, thinks we should focus on one of the two. Additionally, he thinks my last step, where I weight the % change in reported drug use according to the class size, counts as double weighting, and I should add up the percentages before that step to arrive at my net result (which turns out to be something crazy like -62.2%). 

Who is right? Is my blocking by grade + class level a statistically valid way of designing an experiment? Is my weighting step correct? I'm attaching my excel for reference, [here](https://www.dropbox.com/s/1zemnlboq4svkmf/Surveys%20Project%20Data.xlsx?dl=0) , it has all the analysis I've done so far. 

TL;DR: Experiment about drug use, have subjects fill out form either anonymously or non-anonymously. How do I tease out results?"
"Proposition that there is a Shannon information measure for an unbiased estimator data point, Gaussian error component.",0,1,False,False,False,statistics,1509688229,True,[removed]
Parameters for K-Means Clustering(opencv) or other algorithm suggestions.,0,1,False,False,False,statistics,1509689163,True,[removed]
Is a t-test with two data sets of sample size = 3 valid enough?,14,1,False,False,False,statistics,1509712734,True,"Let's assume I have two sets of data. One from a control group (3 people) and another from a treatment group (3 people). Let's say the numbers from control look like 0.905, 0.383, 0.568 and from treatment look like 0.891, 0.944, 0.920.

Is the amount of information that I have sufficient enough for me 
to perform a t-test? How accurate will this be if I were to present it somewhere?

PS- I apologize if the question isn't clear enough. Allow me to re-address if it is."
"Why is it called a ""moment"" in a moment generating function?",13,40,False,False,False,statistics,1509714550,True,"Most definitions say an MGF gives all moments for...etc etc. What exactly is a ""moment""? "
A slightly more difficult problem than 'How Not To Sort by Average Rating',0,1,False,False,False,statistics,1509730932,True,"The relatively popular [web page](http://www.evanmiller.org/how-not-to-sort-by-average-rating.html) article by Evan Miller outlines his proposed solution for dealing with positive vs negative votes for a specific product.

Is there a more general solution (or alternative idea) for dealing with arbitrary votes (for example, +52, -23.423, +12.53), provided they are bounded by some upper and lower limits? 

The problem is a bit easier for me in that I know that the mean of the entire set of votes (across all products of products) is 0, and that the number of positive and negative votes is the same. This removes the requirement to normalize the data (for example, if you were dealing with trolls where there's way more downvotes than upvotes).
"
Have MS in statistics thinking about MBA. Anyone gone this route? Any advice is appreciated.,8,2,False,False,False,statistics,1509732340,True,"I am thinking about going to a high quality local college to get an MBA. I have hopes to one day be a decent level executive, maybe something between data scientist and VP/C-level. Maybe you could talk me into or out of this idea?

* BS in Mathematics minor in Statistical Theory
* (never taken a business class)
* MS in Mathematical Statistics. 
* Researched and published for work in mathematical biology / ecology. 
* I work for a tech company doing data science type role. Develop reports, models, and prototype / requirement writer for software with heavy levels of statistics. 90% of the time work in Python.
* I have had this position for three years.
* I have had one promotion.
* I lead a team of two others. (Not officially a manager but I distribute tasks, approve work, etc)

Also, maybe make this general not just about my specifics. Since, I am sure others will have a similar question. 

Thanks ahead of time!"
"Probability Question... X &amp; Y are independent ~N(0,1) and Z=max(X,Y). What is the distribution of Z^2? I'm guessing some form of Chi-Sqr but how do I show it?",0,1,False,False,False,statistics,1509742352,True,[deleted]
Matching: Alternatives to Propensity Scores,0,6,False,False,False,statistics,1509744689,True,"Hey all,  
as a social scientist I used Propensity Score Matching (PSM) regularly, but in recent times, criticisms grows larger. If you dont know this very good explanation by King, check out these links:  
https://www.youtube.com/watch?v=rBv39pK1iEs  
https://gking.harvard.edu/files/gking/files/psnot.pdf  
It seems that PSM induces a lot of imbalance, which is quite the contrary of what we think we can get. I am now testing different options for Matching. King favours Coarsened exact matching (CEM), which seems easy to use and is already implemented in Stata and R (see https://gking.harvard.edu/cem). The main problem for me is now to find out which option is ""best"" in a real research setting with observational data at hand. I tried to replicate some of my older analyses using CEM instead of PSM. A challenge is to handle the number of covariates. In some models there are about 15 to 20, which can be used in PSM because of the reduction of dimensions. I could not get CEM working so far here, because I end up with no matches at all (duh).  
For me it seems most important to test whether a fully PSM model or a CEM model with fewer covariates yields the better results in the end. With better I mean closer to the real effects of course. What is your opinion about that, are there other solutions or ideas?  
Thank you!"
Profitable Strategies in Horse Race Betting Markets (pdf),1,2,False,False,False,statistics,1509749072,False,
How to verify validy of ROC,15,2,False,False,False,statistics,1509756970,True,"Hello community,

I am an engineer with no stats experience trying to figure out if the results of my research are inarguable.

I designed a diagnostic test that spits out a number for any given person. I performed the test on a group of normals and on a group of individuals known to have a particular disorder. I'm trying use the measurement as a binary classifier.

When I plot the ROC curve for my sample, it looks great and is very sensitive and specific. I used a Youden J-statistic to find the best threshold to draw between the groups (my understanding is that this is the threshold that equally weights the importance of sensitivity and specificity). It all makes sense and looks like what I expected it to look like. 

How might I go about showing that it's unlikely my ROC curve occurred by chance? 

A detail which may be important is that the number I expect to measure for the abnormal group is virtually 0 (well below the noise floor of the measurement system). The normal group has a reasonably normal looking histogram well above the measurement noise floor, but the abnormal essentially group tightly at the noise floor. 

I'm familiar with T-tests and p-values, but that doesn't really say anything about how good the classifier likely is.

I'd appreciate any help!"
How to determine which curve fits my non-linear data the best?,1,1,False,False,False,statistics,1509759543,True,I've got SigmaPlot and JMP. What's the best way to go about determining which model fits my non-linear data the best? What parameters should I be looking at besides R^2?
Online classes / self-study for a wildlife biologist?,2,1,False,False,False,statistics,1509762085,True,"I'm a field biologist (technician tbh) who wants to start a masters degree within the next year or so, and I'm looking for online courses or textbooks to get me better prepared in the meantime. I'd really like to get more into statistics and modelling, but I don't want to show up to grad school and find that I'm in over my head.

I took a semester of basic statistics in my undergrad and ran a lot of ANOVAs and chi-square tests during upper level classes but I never had a firm grasp on why we were using a particular test, or what it was actually doing. We would have a big data set to analyze and the professor or TA would just tell us what test to use and how to run it. I really want to understand the foundational concepts so I don't feel so lost this time. I just finished calculus 1 so my basic math skills are at least at a decent level (if that helps).

So far, I'm looking at the data science classes on Coursera, Discovering Stats using R by Andy Field, maybe even khan academy for a few things? Are there any good textbooks or online classes I'm missing? I've worked a little in R so I'd like to continue that where possible.

Any recommendations, experience, or advice would be appreciated!"
Did I do this correctly? [AP Statistics Probability Test Questions],1,1,False,False,False,statistics,1509762265,True,"Hey guys,

So I recently took an AP Statistics test (it was short, so technically a quiz) and there were 3 questions I was unsure of, so if you could help a homie out, that'd be great.

1. Tell me if this statement is false or not. ""If two events A and B are not mutually exclusive, their probability is P(A) + P(B).""

2. ""You roll 2 six-sided dice. What is the probability that you do not roll a sum of 4?"" 

This was the question I was least confident about on the test. I basically said 34/36 because 6^2 is 36, I thought that would be the different combinations you could have, and since rolling a 2,2 and 1,3 would be equivalent to rolling a 4, 34/36 would be the other options NOT equivalent to rolling a 46. IDK if I'm correct, though. because rolling 1 on the first dice and 3 on the second dice is not the same as rolling 3 on the first dice and 1 on the second dice. I'd appreciate if you could tell me what the right answer as I'm pretty sure I got this one wrong (or if I somehow miraculously got it correct, still let me know!). 

3. This was another slightly difficult (not as difficult as the one above) question. It had 3 parts and I had doubts about 1 of the parts.

""At a large company, 72% of workers are married. 44% are college graduates, and half of the college graduates are married. What is the probability that a worker is not a college graduate nor married?""

Please take a look at my work/thought process and let me know if I messed up.

_________________________________

First of all, I knew that being a college grad and being married were **independent events**. If you look at the question, you can tell that there are workers who are college grads but not married and workers who are married but not college grads.

The probability of both 2 independent events A and B are P(A and B) = P(A) + P(B) according to my textbook. (""and"" is replaced with upside down U in my book but they're the same thing).

So I basically did 0.28 (for the workers that aren't married) * 0.22 (for the college graduates that aren't married). 

I know it is probably wrong now that I think of it (I should have done 0.66 * 0.22 fml) , but that is what I did. When I was taking the test, I thought maybe I have to compare college grads in the context of those who were not married, but now I'm having second thoughts.

Anyways, I really appreciate the help! If I all of a sudden remember another question which stumped me, I might make another post, but I feel confident about everything else as of now.

Thanks!"
Questionnaire for Stats Class,0,0,False,False,False,statistics,1509762760,False,
Why 'Statistical Significance' Is Often Insignificant,5,30,False,False,False,statistics,1509772208,False,
[Shitpost] What did the statistician name his daughter?,1,0,False,False,False,statistics,1509775623,True,ANN
Elementary Statistics : Hypothesis Tests for a Population Mean : Classic...,0,0,False,False,False,statistics,1509787883,False,
The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant,4,6,False,False,False,statistics,1509798663,False,
"Google shows Doodle for Hirotugu Akaike, a Japanese statistician who formulated AIC",2,33,False,False,False,statistics,1509799192,False,
The scariest Halloween monster of them all!,30,256,False,False,False,statistics,1509804986,False,
Elementary Statistics Hypothesis Tests for a Population Mean : P Value...,0,0,False,False,False,statistics,1509806931,False,
Cumulative Sum of Failed Parts – Calculating number of failed parts with P(failure) per part,0,1,False,False,False,statistics,1509809302,True,[removed]
Using Earthquakes to Predict the Market,0,1,False,False,False,statistics,1509810757,False,
predicting lake freeze,0,1,False,False,False,statistics,1509814777,True,[removed]
Elementary Statistics : Hypothesis Tests for a Population Mean : P Value...,0,0,False,False,False,statistics,1509818305,False,
"The difference between prediction, estimation, explanation, and forecast",12,11,False,False,False,statistics,1509818504,True,"I recently got a job in industry and found that there is some confusion about the differences between these terms. This is a little information on what these things are. I hope this is useful to others. 

For a given x we have algorithms (means, GLMs, neural nets, boosting, posterior mean, ARIMA, mixed models) that return a value for y.

There are three possible uses for these algorithms.

**Estimation**: Great coverage of the difference between estimation and prediction is found in [this Cross Validated post](https://stats.stackexchange.com/questions/17773/what-is-the-difference-between-estimation-and-prediction). An estimator uses data to guess at a parameter while a predictor uses the data to guess at some random value that is not part of the dataset. 

*Natures parameter &lt;- Guesses about natures parameters (aka estimates)*

Estimation is a large part of the sciences. It is useful, for example, to have a function that describes the probability finding a galaxy given parameters such as brightness and distance from the earth. Prediction via a black-box algorithm does not aid knowing where not to look as efficiently as a function of parameters.

**Prediction**: Given an x, but not a corresponding y an algorithm to predict y, call it y_hat. For example, we use the number of bedrooms a house has to predict the value of a house. One way to predict is to use ordinary least squares OLS. Ordinary least squares predictions have two sources of uncertainty, the ""beta"" parameters (aka weights), and the uncertainty in the outcome. Predictions, in this case, are also one step removed from nature. 

*Natures parameter &lt;- Guesses about natures parameters (aka estimates) &lt;- Using parameters to guess the outcome.*

Note that the above is an example of using OLS for prediction. Neural nets, for example, are elaborate regression methods aimed solely at prediction - not estimation or explaination. These are maybe even better examples of algorithms aimed solely at guessing an outcome with no interest in the parameter of nature.

**Forecast (prediction)**: Forecasting is a type of prediction, but distinct in that the outcomes are autocorrelated and the desired y_hat is a future value, this means that if we were to use an algorithm, say multiple regression, for forecasting we would most likely see that our algorithm does not effectively use all available information in the data, while algorithms like ARIMA would. All forecasts are predictions, but not all predictions are forecasts.

**Explanation**. If estimation forms a regression surface then explanation is interested in the relative contribution of different parameters to that regression surface. Explanation asks, of all variables in a model which ones can safely be set to 0. Here a very parsimonious model is requested to understand the one two, or three variables contributing to the outcome. My personal favorite algorithm for setting variables to 0 is the lasso. See [Statistical Learning with Sparcity](https://web.stanford.edu/~hastie/StatLearnSparsity_files/SLS_corrected_1.4.16.pdf)

"
Software for MVA,6,1,False,False,False,statistics,1509820798,True,"Hi guys,

What software is most user-friendly to perform multivariate analysis?

I'm a clinician working with clinical trial data, if that matters.

Thank you.

Edit: By MVA I mean

- patients are given X drugs and we are looking at A, B, C outcomes

- there are baseline variables e.g. P, Q, R, S

- we want to find out which baseline variables are associated with the outcomes

- we usually report univariate and multivariate analyses

- I want to know what software lets you do it in the most user friendly manner"
Jobs with a statistics degree and no programming skills?,3,1,False,False,False,statistics,1509840053,True,"This is more of a hypothetical question than anything, because I'm confident that I'll graduate with some decent computer skills. But are there any quantitative jobs out there that don't require programming knowledge? I know statisticians use R and sql and stuff (obviously), but some job descriptions I've seen sound more appropriate for software engineering or computer science, rather than stats. It seems like statistics is getting more and more computerized now."
Anyone heard of the riser dummy variable method?,0,1,False,False,False,statistics,1509856469,True,[removed]
Excel/statistics problem.,6,4,False,False,False,statistics,1509872803,True,"Hi all, 

Can I calculate the variance of just EASTING coordinates? I am only looking at lateral change along a few x,y coordinate profiles"
Test your skills - Quick Quiz on Normal distribution,1,2,False,False,False,statistics,1509875768,False,
Formulating a market research question to get a useful estimate of how &lt;adjective&gt; an object is.,3,2,False,False,False,statistics,1509877203,True,"I've had to create a few market research questionnaires lately, and wonder if I could do a better job of formulating the questions. They are generally concerned with estimating how [good, fashionable, cool, useful, etc] a product is.

Specifically, I'm wondering about the criterion of measurement. Do I ask people to estimate a specific quality (*""How fashionable is it?""*), or a general quality (*""Give a general score""*), or even report a proportion by coercing responses to boolean (*""Is it fashionable? (yes/no)""*)? It would take about 5 years at the rate I can collect data to get enough to do PCA and pick out the most relevant dimensions and their weights, so I'm trying to make do in the meantime.

In the end, I want a sufficiently useful estimate of the quality in question. Useful means good enough to make a rational, profitable business decision. Anyone have any comments or links to literature or tutorials on this?"
Looking for the dataset behind the elephantgraph. Can you help me?,1,2,False,False,False,statistics,1509883129,True,"So im looking for the dataset behind the [elephantgraph](http://oxfamblogs.org/fp2p/whats-happening-on-global-inequality-putting-the-elephant-graph-to-sleep-with-a-hockey-stick/) 

Can you help me? "
Accountant interested in getting a masters in Statistics,22,8,False,False,False,statistics,1509893166,True,"Very interested in all aspects of data. Pretty obsessed with leaving accounting and getting into a more data intensive role. Know that I need 5 courses to begin the program: cal 1-3, linear algebra, stats/prob.

32 years old, probably will be 35/36 when I finish.

Program I am looking into is math based with a concentration in data science... courses are machine learning and python, etc.

Wanting to become any of the following (I know they are very different):

Data engineer
Data Analyst
Big Data Analyst/Engineer
Machine Learning Analyst/Engineer
SQL Engineer

"
Standard error definition in Radiotherapy Dictionary,0,1,False,False,False,statistics,1509898497,False,
Randomness and Entropy Online Tester,0,1,False,False,False,statistics,1509917402,False,
Can a Welch's T Test be performed on an average of an average?,1,1,False,False,False,statistics,1509917713,True,"A bit of an interesting situation I've got: I am trying to measure the average size of a restaurant order from visitors who use an in-store kiosk vs. those who shop at the counter. the hypothesis is that visitors who use the kiosk purchase more on average. 

At the moment, the only data I have available are the weekly sales figures, including the average weekly average order value (the average of an average). 

Because so few orders are placed on kiosk compared to the in-line order, the difference in variance is quite large, as is the sample size. 

With all the above in mind, is it possible to perform a Welch's T Test on the data and still get accurate results? Would there be any downside to looking at weekly averages as opposed to the individual samples?"
"How to better represent a standard deviation from a small sample size? (Or, help with manipulating data)",6,2,False,False,False,statistics,1509919038,True,"Hey everyone, 
I have very little experience with statistics, and have been assigned a task at work that I could use help with. 

See, my job is to package small parts into containers of 100, and to ensure that we do not under fill or over fill the containers by too much. I do this by weighing a sample of 30 parts on a counting scale, setting the scale so that it knows the average weight of a part, and then proceed to fill the containers of 100 by scooping parts from the bin and measuring them on the scale. Before I touch the parts, each bin is tumbled and mixed up to ensure that it is homogeneous. 

We recently had a customer come to us and ask how we know 100 parts measured on the scale really is 100 parts. I've done some testing, with tests of 33 parts each from 3 separate lots of parts, and then calculated the six sigma deviation to try and find the absolute lightest and heaviest values a part could be. I then figured out how many of our heaviest parts were needed in order to reach the same weight as 100 of our lightest parts, in order to figure out how many parts I needed to overfill by ( which turned out to be 84 heavy parts to 100 light parts ). However, my test yielded results showed that I needed to overfill each 100 part container by 16 percent in order to ensure 100 parts each time. This result is not what I was looking for, and is certainly not profitable to my company. 

SO, my question is, how can I manipulate my data to show that the chances of selecting a sample 30 of the lightest parts from a bin of about 18,000 parts, and then selecting 84 of the heaviest parts from the same bin is extremely unlikely? 

Do I need to do more tests with an expanded sample size in order to get a better representation of the standard deviation between each part and the average, or is there a way I can show with math that the majority of the 18,000 parts from a bin tend to be closer to the average part weight rather than the extreme ends of the data set?

I apologize if there is an easy answer to this question, I am a senior in high school, and have never taken a formal statistics class. "
Multiple Linear Regression Help,3,1,False,False,False,statistics,1509920708,True,"I've seen a few videos suggesting to omit an independent variable when doing multiple linear regression in excel.
Is there a reason behind this? And if so how do I complete an analysis for the omitted variable?"
Any significant benefits of a stats minor?,11,1,False,False,False,statistics,1509924654,True,"Hi all,

I'm trying to get up to speed since I recently switched out of engineering. So I declared as an Econ/Math Combined B.A. and Stats Minor recently. What significant benefits are there to a stats minor? I declared it because econ as a area of study uses stats in some capacity so I thought the additional classes would be useful for when I'm ready for grad school. 

Thinking about it though I don't see any significant impact in minoring in Stats beyond this. I would think this minor would compliment my joint major since it's missing some statstical background. 

I'll be taking the following courses for my minor:

Lower Divs:

Statistics

Upper Divs:

Intro to Probability Theory

Bayesian Inferences

Machine Learning and Data Mining (elective)

Economic Development OR Intro to Dynamical Systems (elective)"
t-Test: t estimate,2,1,False,False,False,statistics,1509926448,True,"During the calculation of the estimate of t, how do you know which one is the being subtracted and which one is doing the subtraction in the formula t=(mX2-mX1)/Smxpooled. Like, which is X1 and which is X2?"
"If something has a positive correlation, does it always mean that is more likely?",5,0,True,False,False,statistics,1509932882,True,[deleted]
"If it takes 4 years to get a master's, will your resume be trashed instantly?",0,1,False,False,False,statistics,1509933993,True,[removed]
"One year away from graduating, not the best credentials, looking for some career guidance",7,1,False,False,False,statistics,1509936088,True,"Hey everyone, sorry if this kind of question isn't allowed here (if not, I'd love it if someone could direct me to the appropriate thread or sub!)

I'm a fourth-year college student in my first semester as a transfer at a large research university after spending three years at a liberal arts college. My old school was very prestigious and name-recognized, and my new one is as well albeit a little less so. At my old school I was a pure math major because they didn't offer stats, and I opted to switch into stats once I got here. I am planning on graduating next December.

I'm having a very rough first semester, as evidenced by the 58 I just got on my midterm for Linear Models. Prior to this summer the only stats I had taken was AP in high school (got a 5), then this summer I took a course in probability (got an A), and this semester I'm taking linear models as well as the second semester of intro to stats (and two non-math courses). Next semester I plan on taking mathematical statistics and mathematical modeling for biology.

I've never had any sort of job or internship related to stats. In the summer of 2015 I had an internship at a startup related to SAT prep where I was mostly doing data entry for practice questions, producing solution videos, and a little bit of web market research. The following summer I worked a wanderlust-satisfying but overall unimpressive food service job at a national park, and this summer I tutored middle-school children in math. I was able to secure a (volunteer-based) undergraduate research position this semester, using R to perform statistical analyses on Likert-type survey data. So far it's going pretty slowly and I don't feel that I'm developing any useful skills that will help me in the workforce, but thankfully it looks good on paper.

I'm very, very worried about my future. With the failing grade on my most recent midterm, I'm probably gonna walk away from this semester with a B or a C in that 500-level math class. I can probably manage an A in my intro stats class but that barely matters. Of my other two classes I'll probably get a B or a C in one and an A or a B in the other. I should mention at my old school I got mostly As with a few Bs, I think my GPA was around a 3.6. One of my goals for this summer is to get a summer REU position, but multiple professors have told me that I can forget about it if I'm not getting As in all my classes. I'd also love to secure an internship, but I worry that my mediocre current grades and lack of experience will stop me from getting one, and I know not having had an internship will make it extremely hard to get a job after graduation. I'm not close with any professors here (I've only been at the research position about a month so I haven't gotten to know that prof super well, and since it's a super big school I don't know how to go about getting to know my math profs who have hundreds of students to worry about). I only knew one professor at my old school, my advisor.

I'm just looking for some general advice on where to go from here. Should I try to secure an internship for next semester? Is getting Cs in my classes going to really, *really* hurt me going forward? Am I shit out of luck for a research position (should I give up on it and try to find a summer internship instead)? What are some things I can do now to make sure I'm not unemployable this time next year? I understand a lot of the damage is already done and I'm freaking out about it. I would love some guidance. Thank you so much!"
"3rd year of a stats degree, should I take more comp sci or math?",12,5,False,False,False,statistics,1509937484,True,"Hi I'm currently in my third year of a stats degree and I'm wondering if it would be better to focus in my last year on taking more math or comp sci classes along with my stats classes. I was looking at it and because of the 8 math classes I will have done by the end of this year I would only be 4 away from a double major in math and stats. I was thinking of taking some mathematical finance classes. On the other hand I could take a couple classes in comp sci which might be more useful towards a job in data science. I was kind of wondering what would look better on a resume/would give me a better base of skills in the job market. For reference I'm decent in R, Python and java but its pretty much all self-taught as none of my stats classes have had a rigorous programming component to them and have mostly been theory."
lagging a time series,3,6,False,False,False,statistics,1509974589,True,I'm doing a linear regression on satellite insurance premiums with the success rate of the launches. There seems to be a 1 period lag between lots of failures and higher rates. What is the best way to model this? Is this something that ARMA could help with?
"Probably basic statistical question, could need some help.",0,1,False,False,False,statistics,1509974639,True,[removed]
Averaging stratified per capita statistics,0,1,False,False,False,statistics,1509981902,True,[removed]
"Can anyone help me understand the meaning of ""probability"" in logistic regression?",5,1,False,False,False,statistics,1509984663,True,"Regardless of everything I've read toward defining probability, I do not understand the relation between probability and the linear equation in regards to logistic regression. I believe I understand the use of e and the purpose of the ratio itself (1/1+e^-linearequation) but I'm lost in terms of relating beta0 +beta1x .... etc to probability. 

My instinct is to think of probability as likelihood of outcome/all outcomes where for instance a die would be side/allsides. Wikipedia generalizes to either saying it is the ""relative frequency of occurrence"" or the assignment of a number in light of a prior probability distribution (which I don't fully understand anyway). 

In any case, I don't see the relationship between probability and a linear equation.
Help? Thanks!"
What goodness of fit should test should I use?,7,2,False,False,False,statistics,1509994982,True,"Hi, I'm currently working on a project where I have to test data if their distribution is poisson, normal, lognormal, uniform, triangular, weibull, or negarive binomial. What goodbess of fit test do you recomend me?"
What variables to train Random Forest?,6,5,False,False,False,statistics,1509997752,True,"Hello, I'm hoping I can get some general advice, nothing too specific, about what kind of variables are good to train a Random Forest (regression not classification). The particular RF I'm using is EXTREMELY fast, so processing speed isn't a factor. 

For further reference, here is the library I'm using: 
http://haifengl.github.io/smile/api/java/index.html

And this is the website for the free + open source library:
http://haifengl.github.io/smile/

1) Is it best to put general raw statistics or should I normalize everything so that data from different ranges are easily comparable?

2) Should I put in raw variables or should I refine them to show more meaningful stats? Essentially should I do some analysis on the raw numbers before putting them into the training set or should I leave them as is and let the algorithm do its thing without me tampering with the original values?

3) Should I just add every piece of remotely meaningful data I have? Or will this add too much noise into the forest?

4) Some of my data comes in the form of a graph. I'm wondering if I can put the past hour of data (which will always be the same number of entries spaced the same time distance apart) as part of training data. I'm unsure because since it randomly decides which variables to train each tree with it almost always won't get the full picture so it might skew the analysis.

I'm relatively new to the world of machine learning, but I'm pretty clear on how for the most part things work functionally, I just don't have much experience. So think of me as a blank machine with no training data yet and every comment will serve as more training data :D"
Need advice in estimates interpretation,0,1,False,False,False,statistics,1510003923,True,[removed]
Comparing demographics the stats of race to race of mass murderers,14,1,False,False,False,statistics,1510005457,True,"So my girlfriend is on a tirade about how most mass murderers in America are white  and she quoted the figure of 51 out of 90 mass murderers are white, which fits her narrative that whites are bad. I studied statistics in undergrad and again for my masters so I know that stays can be abused for policies agendas. So 51 out of 90 were white, 15 were black, 7 Latino, 7 asian. So though ""murderers"" is not a random sample, it still more or less aligns with the distribution of races by populationin the US (63% white, 12% black 17%% Hispanic, 6% Asian)  compared to 56% white, 13% black, 13% Hispanic, 7% Asian, for murderers. My argument was that the racial distribution of murderers fits closely with the population distribution of the US. My analogy was that it wouldn't be surprising to state that 98% of all murderers in South Korea are of Korean heritage, so why would it be shocking that the US with its 63% white population would have a 56% rate of mass murderers being white?

My question is, does my argument hold some water at all, from a statistics standpoint? Or am I completely off-base with my assumptions? Thanks in advance and sorry for my seemingly political post. "
Finding the theoretical best performance of Machine Learning algorithms,5,0,False,False,False,statistics,1510011480,True,"This has been on my mind for several months now, as I reap the benefits of deep learning at my work everyday. I spend all day doing search routines for a better model and better parameters, and it makes me wonder: how difficult would it be to put an upper bound on the loss function's global minimum?"
Can inference in the data space be justified with bayesian probability?,0,1,False,False,False,statistics,1510011485,True,[removed]
"Changing from normal distribution to another distribution(not homework, theory based)",11,5,False,False,False,statistics,1510014409,True,"So this is for a homework problem but I don't understand how it works. 

For a signal and systems class, we are writing MATLAB code to create 2 arrays of  normal randomly distributed numbers(X and Y). Once we have those numbers, we rescale them from 1 to 30(X1 and Y1). THEN, we take each corresponding values of X1 and Y1 and convert their coordinates from cartesian to polar. When I was asking my instructor if he wanted theta in radians or degrees and he said he wasn't picky. He made a comment that it will change the distribution. 

My question is, how does taking something that is normally distributed, rescaling it, then converting it to polar coordinates change the distrbution from normal to something else?"
The James-Stein Estimator In R,1,1,False,False,False,statistics,1510015884,False,[deleted]
James-Stein estimator in R,2,0,False,False,False,statistics,1510016715,True,[deleted]
Resources to learn statistics,4,7,False,False,False,statistics,1510018300,True,"Hey everyone who is reading this,

Recently I competed in my first data competition at my school. I chose to go with a random team, only to find out that my teammates had no experience working with statistics, programming or data analysis. 

A little background, I am a math major with a strong love of the pure mathematics and I have never put much focus into statistics or data analysis because they felt real. I now find myself in the situation where my group performed poorly and I do not want to do this poorly anymore. I was hoping that the people of this subreddit might know where I can access some good resources for learning statistics and data analysis. YouTube channels, lecture series, or textbooks.

Thank you everyone for any guidance, it is very appreciated. "
Understanding linear regression,3,28,False,False,False,statistics,1510019478,False,
"When adding more IV's into a model (ANOVA, Regression, etc...), why does it change significance, F values, etc...?",1,1,False,False,False,statistics,1510019494,True,"I'm trying to understand this conceptually. Let's say I have a model with an IV let's call it IV1 and a DV and it comes out non-significant, but let's say I add IV2 and IV3 in the model and I get significant interactions and then the lower level main effect, IV1 becomes significant. Why does this happen? Thank you."
ELI5: P-values and power,14,2,False,False,False,statistics,1510022216,True,"I'm a statistics noob floundering my way through a doctoral statistic for behavioral sciences course. 
I do not understand how a p-value is not inherently related to the power of the study. I understand that p-values do not equal alpha levels, and that an alpha level is needed to predict the possibility for Type I or alpha errors. I also understand that power is indicative of prediction of Type II, beta errors, and should be calculated prior to the start of the study. That being said, from what I have read, many researchers assume that if p-values are listed as 0.05, then one is to automatically assume that they have an alpha level 0.05 as well, even if this is not explicitly stated. Technically then, beta levels should be set at 0.20 which would indicate a power of 0.80, due to Type I errors being four times a significant as Type II. Which makes p-values related to power in a roundabout way. 
I would greatly appreciate your help!!
"
Help with Higher-Order functions in R,0,1,False,False,False,statistics,1510022226,True,[removed]
Help with Higher Order Functions in R,4,2,False,False,False,statistics,1510022359,True,"I'm trying to use the Reduce function with the shift function (from binhf package).
When I do the following

    Reduce(shift, (1:5)[(1:5)%%2 == 0], 1:5, accumulate = TRUE)
I get a list of 3 vectors which are (1, 2, 3, 4, 5), (4, 5, 1, 2, 3), and (5, 1, 2, 3, 4).

The problem I am having is that I want to find the shifts in the opposite direction. The default direction to shift the vector in the shift function is right, but I want it to shift left. How can I accomplish this?

"
Confounding relationships,1,0,False,False,False,statistics,1510022475,True,Can someone explain how a confouding relationship is different from a common response or lurking variable?
Understanding MLE,8,6,False,False,False,statistics,1510028435,True,How can I understand this?? I can do the process but it feels useless because I don’t even know why it works...
What are the best schools for studying finance from a data science angle?,20,13,False,False,False,statistics,1510055027,True,
I have one year until GradSchool starts and I want to learn as much statistics as possible. Where do I start?,3,4,False,False,False,statistics,1510063006,True,"I have roughly one year until I start graduate school (MA) in philosophy. I will probably go into empirical/experimental philosophy, which is similar to social psychology/moral psychology in its methodology. I have some background in psychology (about 1/3 of a degree) but I never took a full statistics course, only ones where statistics was part of the content. 

I am sure that there are good online resources out there but I'd like to ask the experts first: Which online courses / books should I work through in order to get a solid understanding of statistics that will be useful to empirical/experimental (moral) philosophy / social psychology /moral psychology?

Thank you very much!"
"Kendall's tau - a measure of non-linear dependence: estimates, properties, plots",2,5,False,False,False,statistics,1510064540,False,
collecting convenience sample data,0,0,False,False,False,statistics,1510073247,False,
Why is there no standard algorithm for adaptive cluster sampling?,2,0,False,False,False,statistics,1510075411,True,"I've got a homework that requires doing adaptive cluster sampling in Excel. The professor claims nobody has programmed this yet. I've tried looking it up but didn't really get far. 

Adaptive cluster sampling must be used somewhere outside of academia, right? Someone must have automated this..."
Using Binary Logistic to predict results of unknowns in the dependent variable,0,1,False,False,False,statistics,1510077175,True,[removed]
What to do when only certain factors of a categorical variable is significant after regression?,5,3,False,False,False,statistics,1510080248,True,"Say you are running a linear regression model on profit of beer sales. Profit is your response. Among your predictors, there is a variable called ""brand"" that contains 60 brands of beers.

After running the regression, you see that only 20 or so of the 60 brands have low enough p-value.

How do one interpret this and what should be do next if we want to improve the model? Should we not use the brand variable? Or should we try to use only the significant brand (and represent them using dummy variable or something, like ""1"" if brand = A, ""0"" if brand !=A, where A is a significant brand)

I encounter this type of situations a lot and I need some advice on what to do and how to interpret it.

Thanks so much"
Review: PSPP - Software estadístico. Descargar Windows,0,1,False,False,False,statistics,1510084802,False,
Question on the kolmogorov smirnov and the anderson-darling tests,0,1,False,False,False,statistics,1510085107,True,[removed]
Coded Units vs Uncoded Units model in Minitab evaluate to different results,0,1,False,False,False,statistics,1510086133,True,"After a DOE factorial regression analysis in Minitab, I am able to get an output Regression Equation in Uncoded Units and a table of Coded Coefficients. If I evaluate the Regression Equation in Uncoded Units and compare the results with the Stat&gt;DOE&gt;Factorial&gt;Predict.. result, there is a 4% to 5% difference. I think this is too much of an error to be attributed to rounding errors from the coefficients displayed in the Regression Equation vs the actual coefficients used for evaluation. 

So, a couple of questions:
- Where is the difference coming from? 
- Is there a way to store the Uncoded Coefficients with more decimal places? I am able to store the coded coefficients, but not the uncoded ones. I want to evaluate the equation with more decimal places to rule out rounding error but I cannot get more precision on the uncoded coefficients than what is outputted in the Regression Equation in Uncoded Units.
- Is there an easy way to change the coded coefficients to uncoded?
Any help is greatly appreciated!
"
"Those of you who ARE statisticians, what's your job like?",46,49,False,False,False,statistics,1510089431,True,"What is your field of work? How did you get into it? And how tough is it to get the job you have now?

I have a BS in mechanical engineering, but I'm strongly considering getting an MS in statistics and going for a job that leans much more heavily on statistical analysis."
"Additional courses for versatile, well-rounded Stats Grad student?",0,1,False,False,False,statistics,1510102225,True,[removed]
"Job search starting point, PhD level",2,2,False,False,False,statistics,1510107245,True,"Hi all,

I'm graduating from my PhD program in May or August and starting to take a look at the job market now. I have applied to a few jobs, but it seems challenging to find jobs for statisticians outside of academia (?) A look through the job postings on amstat.org shows that nearly all posting are for faculty positions -- however I would like to transition into an applied stats role in government or private sector. Any tips for a job search noob?"
Median confusion,0,1,False,False,False,statistics,1510107668,True,[removed]
Fascinating Chaotic Sequences with Cool Applications,0,0,False,False,False,statistics,1510114304,False,
Road to getting an MS in Data Science/Stats without a relevant bachelors?,3,0,False,False,False,statistics,1510115432,True,"Psych major who has taken quite a few cs (intro, data structures), math (multivariable calc, linear algebra), and stats (intro, applied data analysis, models &amp; regression) classes but will graduate with just ""psychology"" on my degree. I did do some independent research using RShiny and have some work up on github. I'm hoping to get a stats/data science masters and possibly go into statistical genetics, genetic epidemiology, etc. What is the best way to make that switch? How can I gain admission into a asters program without a relevant degree? Do I need a bachelors in stats to get a statistics job for the next two years or so?"
How to describe a data set,3,0,False,False,False,statistics,1510121274,True,"Hey guys, in university and currently and taking a statistics subject. In many past exam questions ive seen it be asked to describe the data set (say for example a box and whisker plot) giving 2 or 3 lines. I am unsure how I should go about analyzing the data besides stating the obvious, mean median etc. Any ideas?
Thanks guys"
Is ther,0,1,False,False,False,statistics,1510148823,True,[deleted]
Is there a way to handle circular numeric systems?,7,6,False,False,False,statistics,1510149371,True,"OK, I'm building a mixed effects ANOVA model at the moment to predict sales at various outlets during different months of the year,

Obviously there's a pretty important seasonal effect, so I was thinking of just incorporating the month of sale as a dummy variable into the model. Then I got to thinking about customer models I've built where I used cubic splines to estimate the functional-form for age as an input.

So the advantage of using numeric inputs is that at a specific age (say 33), the model uses nearby ages to get extra information. But for months, the model would need to know that the nearest months to month 12 are months 11 and 1.

Has anyone seen anything cool for modelling something like this?


EDIT:
OK I know the usual method is to include a sin and cos function into your model right. But I guess I don't know how complicated the resultant functional form from these models can be. If we have a sharp peak in December sales, can these models capture these sort of asymmetrical functional forms?"
Best Path to Research for a Non-PHD,11,10,False,False,False,statistics,1510151307,True,"Hey r/statistics, any insight is welcome! Let me know if there is a better subreddit for this. Still consider myself a padawan of reddit...

For someone who has been out of school for a few years, what would be the best way to pivot into a research group doing impactful work? Specifically, I'd love to assist ongoing research projects as a statistical analyst/programmer, but I am not sure how to break into those networks without soliciting advice on reddit/linkedin/etc. or going into a grad program.

Has anyone made that transition from business to research? What roadblocks did you encounter?

About me: I've been working in large US company as a data analyst and then scientist. Typical activities include problem formulation, data asset identification, data cleansing, data enhancement (modeling), and then moving the processes into production. I have two master's degrees and while I have some pretty deep technical and business knowledge (transportation, marketing, distributed computing, and some popular stats software) I would not say that I have formal training in stats. In fact my undergrad was in a non-quant field. Before I started putting in applications or networking in person at the local university research groups, I wanted to get reddit's feedback : )
"
I used statistics to make an argument. I posted it on facebook. This is one of the mind mindbogglingly stupid responses I got. How do I respond?,20,0,False,False,False,statistics,1510155270,True,"To a statistics-based argument one of my facebook friends replied: ""100% of murderers drink water. We should ban water.""

I mean, what am I supposed to say to that?


Not really a ""statistics question"" per-say, but I didn't know how else to flair this."
"The values I want to use for a Chi Square 2x2 contingency table analysis are too big even when Yate's correction is applied. I'm assuming this I shouldn't do this but I'll ask anyways, would it be bad statistics if I made my values 10-fold smaller and THEN applied them to this formula?",4,2,False,False,False,statistics,1510156861,True,"
I preface this by saying that I'm not well versed in statistics and haven't taken a course on it in 6+ years. I work in disease research and we're trying to measure the efficacy of response in immune cells that are administered a certain antigen. A sample of the data is as follows:

 |Control|Antigen
:--|:--|:--
(+) response|26|324
(-)  response|180974|153676

I've been plugging the data into the following calculator:
https://www.graphpad.com/quickcalcs/contingency1.cfm

Since the data is too large, I was wondering if making the values smaller by a factor of 10 as shown below is a viable option or if that is just plain bad statistics:

 |Control|Antigen
:--|:--|:--
(+) response|3|32
(-)  response|18097|15368

If this isn't, GREAT. If not, are there any alternative formulas I could use to measure statistical significance (p-value)? 

Thanks. "
Using the Mean of a Confidence Interval,3,1,False,False,False,statistics,1510160755,True,"Rather than explaining my data using a confidence interval, can I take the average of all data points that fall within the lower and upper bounds? Would that misrepresent the data set?

Essentially, I'm looking at this as a way to exclude outliers on both ends. But I want to express the results as one number and not as an interval.

Thanks!"
What is data sampling?,7,5,False,False,False,statistics,1510163521,True,[deleted]
Writing a Resume as a Data Scientist or Statistician.,0,0,False,False,False,statistics,1510167086,False,
How do I️ do a simulation?,0,1,False,False,False,statistics,1510169838,True,[deleted]
Help with my research,0,1,False,False,False,statistics,1510171538,True,
Linear versus nonlinear regression? Linear regressions with a curved line of best fit? Different equations? Confused.,22,7,False,False,False,statistics,1510172236,True,"So, I'm working a lot with regression analyses and while I thought I had pretty good grasp of - what I thought - was a straight forward analysis, now I'm not so sure. 

Can someone clarify the difference between a linear and nonlinear regression? I had always assumed that a linear regression is just a regression that fits a straight line while a nonlinear regression is when were the line of best fit is a curve; but now I'm realizing that linear regressions can have curves. So what's the difference? When should I use a linear regression? When should I use a nonlinear regression? In my statistical software, I see a number of different equations, e.g., polynomial, peak, sigmoidal, exponential decay, hyperbola, wave, etc and then multiple subcategories within these equations. I'm assuming these are all related to the shape of the predicted curve. Which are linear and nonlinear though? How do I decide which equation to use?

Additionally, when I'm reporting my results...what statistics should I report? P-value, R^2, and S value?

Edit: Also, can anyone link a tutorial that delves into how to  best approach a regression data set? How to check for outliers, nonlinearity, heteroscedasticity, and nonnormality? And then how to remedy this problems if they are present? "
Job Title Suggestions?,9,0,False,False,False,statistics,1510172894,True,"I need some help... I do not have the most ~imaginative~ mind. I was asked to come up with my own title and I really don't know. I have a degree in biostatistics and work in healthcare economics. In terms of what I'm doing for work, I'm mainly looking at outcomes on both the patient and hospital level. There's a lot of descriptive analyses, so I'm not doing any fancy statistical tests at the current moment--but IDK if I would in the future. We present our findings in conferences.

Man, I really don't know :| the titles that my coworkers came up with sound so fancy."
What is a simulation,4,2,False,False,False,statistics,1510176960,True,"I️ am not a stats student and I️ have only ever taken one stats class, but I️ have a random question about it. I’m kind of interested in politics, and I️ know Nate Silver is sort of like a stats/political mash up. My question to you is what is a simulation and how does it work? For instance, if Candidate A has 52% in the polls and Candidate B has 48%, how does a simulation differ from that. To my non-statistical brain, it seems like running a sim would just have A win 52% of the time and B win 48%. How does the simulation differ from the polling numbers? 

Secondly, if I️ wanted to run a simulation. If I️ had a bunch of poll numbers and wanted to run a simulation myself to predict the outcome, is it possible for a layman to do it with a simple program like Excel? Or is there very specific knowledge you need with a very powerful program?

I’m just a very curious person. Anyone that could answer would be much appreciated, I️ think it would be fun to play around with this. Thanks."
How small is considered statistically impossible?,29,11,False,False,False,statistics,1510178410,True,"If you were to flip a fair coin, how many flips of the same side is considered ""near impossible to happen""?  I understand any amount of flips is *technically* possible, but if say, one were to place a bet at a certain number of coin tosses and say ""the coin will show both sides before this number of tosses, even if you were to flip constantly every day for a lifetime"", what would that number be?

For example, a statistician would say not to play the powerball, because your chances of winning are ""essentially impossible"" (1/175,000,000).  But where is the line drawn where they would start to say to play it?"
Expected value with floating point numbers in a range,1,1,False,False,False,statistics,1510180448,True,"Hi,

This is part of a thought provoking question for students who want to get ahead of the current topic the class is at.

I've decomposed the question into it's final part and I just need to solve this expected value question. My question is:

Given two random floating point numbers in the interval [0, 0.5], what is the expected value of the difference of the greater number and 0.5?

i.e., If we run this experiment once, and get 0.2 and 0.4 as our values, the difference should be 0.1 . If we get 0.1 and 0.1, then the difference is 0.4 .

Any help here would be greatly appreciated! I feel as though I am so close!

Thank you"
Problem of Non-Stationary in STATA help?,2,0,False,False,False,statistics,1510183943,True,"I want to do a regression where:
Yt= B1 + B2X1t + B3X2t + Yt-1 + Ut
All variables are stationary except Yt
To not create the problem on auto correlation I tried using the method to calculate rho but I did this by regressing
Yt= B1 + B2X1t + B3X2t + Ut
This gave results I'm skeptical of as the lag of Y (Unemployment rate) being put into the stationary regression only gave an R2 value of 0.09 and with just the lag regression an R2 of 0.01 which makes no economic sense. The unemployment rate in the last peroid should have significant explanatory power of the unemployment rate today.
Any advice please?"
Resource request for entropy,0,1,False,False,False,statistics,1510186239,True,[deleted]
Chi-square post hoc test?,3,3,False,False,False,statistics,1510186669,True,"So I've been doing some work with chi-squares and just want to make sure I'm Interpreting this right! 
I used chi square to see if a specific drug was related to cell death. I had 5 concentrations of the drug and set up the table with the number of dead cells vs the number of living cells for each concentration. My test showed significance, yay! So I'm hoping that means the drug is directly linked to cell death. 
One of my concentrations had almost double the amount of dead cells than any of the other groups -  i ran a pairwise chi square comparing each of the concentrations against one another and all concentrations showed significance with this sample that had the high number of dead cells - does this tell me that this sample had a higher proportion of dead cells? I want to prove that this sample does in fact have a statistically higher number of dead cells compared to the other groups to show that not only is the drug linked to cell death but that the higher the drug concentration the more death that occurs- does this chi square post hoc test prove that or is there a more suitable test? 
I hope this makes sense because I'm pretty clueless with statistics! Thanks :) "
"Statistical competition - $2,000 award, deadline December 31",1,3,False,False,False,statistics,1510187450,False,
Unsure of how to recode for Mann Whitney Test in SPSS,3,1,False,False,False,statistics,1510202631,True,I have two gender based scale variables and I'd like to organize them to create one gender dummy variable and one outcome variable. Any advice on how to do this would help a lot.
"How is biostatistics different from ""plain"" statistics and why does this difference exist?",10,14,False,False,False,statistics,1510207984,True,I'm considering grad schools now and I'm curious on what's the difference between a department of statistics vs a department of biostatistics? Isn't biostatistics just a subfield of statistics? I'm not sure I understand why biostatistics is often separated from regular statistics in different departments or even different schools (School of Public Health vs College of Arts&amp;Sciences)
Which statistical test for comparison of fetal growth charts?,11,6,False,False,False,statistics,1510229788,True,"I am currently undertaking research in the hospital where I work to compare a new system for fetal growth surveillance. New charts have been introduced in the way we measure the fetus using ultrasound. The basic comparison I am doing is: how many more babies do we find as small/large on the new charts that we would not previously have found on the old charts? To do this I will look at all the babies we have scanned with the new charts, and see where the measurement would have plotted on the old charts. However my statistical knowledge is poor! Any advice on which test to use to compare statistical significance would be much appreciated. I have been looking into a student t test, however am very unsure. Many thanks in advance!"
Your Year on Kaggle: Most Memorable Community Stats from 2016,0,5,False,False,False,statistics,1510233614,False,
Doubt about betting,5,1,False,False,False,statistics,1510234381,True,"I have a system that randomly chooses if it's number 1 or number 2 with both having the same winning percentage: 50%.

The guy X bets randomly.

The guy Y only bets when 3 results in a row are equal , and he bets on the number that is different. For example, if number 1 is the result 3 times in a row he betts in number 2.

Is the winning percentage of the two guys equal in both situation?"
Online Assignment Experts Australia,1,1,False,False,False,statistics,1510241847,False,
Question: How to measure modality/trends in time series?,1,2,False,False,False,statistics,1510245004,True,"I want to test a large data-set - consisting of about 200 sensors in different environments - describing concentration of a compound over the course of a day. I know that different sensors will be exposed to different conditions, and that there will likely be some pattern in the diurnal variation. 

However, rather than examine each individually, I was wondering how I might class the sensors - or, more precisely, the environments they're observing - by the pattern they exhibit: some will likely display two peaks, in the late morning and the early evening, others might have a sine curve with it's minima at around noon, while others will have a maxima around that time. Still others will fall over the course of the day, while others will rise.

How do I measure this? What methods should I use? I'm working in R, so any recommendations using that environment will be especially useful. Thank you in advance."
Is brain research underserved by statistics?,7,10,False,False,False,statistics,1510245955,False,
Help with sampling procedure,0,3,False,False,False,statistics,1510246015,True,"I want to test the hypothesis there is a difference in number of a particular tree species between closely related forest types. I've always had trouble trying to get my head around sampling. I'm happy to limit the scope of my test to forest reserves in one suburb, and have identified all examples of those communities. I understand in doing so, my results are likely to be relevant only to representatives of that community in that particular suburb.

How many sites of each community do I need to sample? How many subsamples from each site will I need?

I will be using a 1-way ANOVA to analyse the differences."
Month-year average meaning,2,2,False,False,False,statistics,1510247465,True,"Came across this concept in a paper and can't quite figure out what it means. It is a paper dealing with time series with hourly frequency that spans a few years.
The authors difference each variable with respect to its own month-year average
If month-year average means simply the average across each month, thus what would usually be called ""monthly average""? Is it the monthly average divided by 12? What would you interpret it as?

Edit: A bit of context. All variables are denominated in either USD per hour or in quantity per hour, and the results to be reported are also in the same denomination"
How can I reproduce this graphic? Can you suggest a software? Something noob-friendly would be appreciated! I really need it for my research project. Thanks,9,0,False,False,False,statistics,1510251363,False,
"Forecasting number of trucks, what's the best approach?",10,13,False,False,False,statistics,1510255097,True,"Hi,

I have a data set with the number of trucks sold in my country. I'd like to figure out how I can forecast the numbers for the next couple of years(or months even), but I'm unsure of the approach. 

I've downloaded af few macro economics data sets like GDP, industry production, Export etc that I thought might play a role. Conclusion(my multiple regression) seems like Industry production is the best indicator if I go from after the financial crisis in 2008, while industry production, stock prices and the interest rate is the best if I go back to 2000-2017. (Monthly data, so not just 17 data points). But the R Squared is low and in general the model is pretty useless. So I'm not really sure where to go from here, what would be a good approach to figuring this out? At this point I feel like just taking the overall average feels like the better way to do this.

Maybe I should take the average price of trucks and use that instead?

Graph looks like [this](https://i.imgur.com/BpuFWJE.png)

EDIT: As requested, here's a [link for the data set](https://www.dropbox.com/s/ta96bt9pwcat2qi/dataset_trucks.xlsx?dl=0)"
Why is stepwise regression criticized?,29,16,False,False,False,statistics,1510255769,True,"I want to know why stepwise regression is frowned upon. People say if you want to use automated variable selection, LASSO is better. Why is that?

I understand stepwise is a greedy approach. But I've been trying to find out why exactly it is not good, and here are the reasons I've found:

The p values are too low
The standard errors are too small

Why is that? Why can't you take a model selected stepwise and treat it as if it was fit using least square?

thanks"
Rules of thumb when choosing to using bootstrap CIs over standard CIs?,10,8,False,False,False,statistics,1510264876,True,"A reviewer recently gave me feedback on a manuscript, implying that our use of symmetrical confidence intervals was inappropriate, given that true CIs are rarely symmetrical. I recreated the plots we are using bootstrap CIs, which turned out to be slightly asymmetric, but not enough to change the interpretation of the data. 

This got me wondering: when/how do people choose to use bootstrap CIs over standard CIs? "
How can you characterise 2D spatial distributions?,0,1,False,False,False,statistics,1510265503,True,"I'm looking at the spread of tree diseases at the moment. I've tried looking into the fractal dimension to see if the spread can be described with percolation, but so far the data have suggested that otherwise (data only seems to count fresh infection sites, so no structure growth).

The data I have access to is USDA aerial survey data, which looks like blots and patches much smaller in scale than geographical features, so not much in the way of large swathes of infection. As I'm not exactly an expert in the field, I was wondering if I could get any advice on metrics that I can use to try and discriminate between different diseases and characterise their propagation. Any thoughts?

Thank you!"
Help with comparing length (time) of effects.,2,2,False,False,False,statistics,1510267057,True,"I am currently working on research that measures the amount of time Hypo-tension (low blood pressure) lasts after an intervention. Ie Hypo-tension lasted 80 minutes in group 1, 40 minutes in group 2 and 30 minutes in group 3. 

 In every study I have read so far they have done a 2-way ANOVA w/ repeated measures comparing the actual BP at each time, but when discussing the amount of time that Hypo tension lasted between the interventions they did not mention any statistical analysis. would  paired t-test be sufficient for comparing the groups? 

Thanks in advance"
Rules of thumb for top-coding highly skewed count variables that have a couple of extreme outliers?,1,2,False,False,False,statistics,1510270773,True,"I have a few variables that count the incidence of certain behaviors. Most do not engage in the behavior, and the ones that do, do so at a relatively low frequency. Of a little more than 1000 observations, about 85% of the distribution is within 10 events, while the remaining 15% are spread out from 10 to 100. There are then a couple of observations that are 100+, literally about five. 

To fit a poisson or negative binomial regression, I know top coding is not uncommon. But, are there any rules of thumb for where the distribution should be cut? My own research has suggested that the value that corresponds to 90% of the distribution is appropriate, but I am not sure that works for my data, so I am interested in second opinions. "
Random effects in mixed effects models versus covariates in linear models,9,2,False,False,False,statistics,1510283143,True,"I'm having the hardest time wrapping my head around the difference between using random effects and covariates in linear regression models.

Like, let's say I have a sample of children with data on height, age, and caloric intake. I want to know whether caloric intake (independent variable) is associated with height (response). Clearly, age is an important factor. I could use age as a random effect (for intercept) in a linear mixed effects model OR as a covariate in a normal linear regression model.

In either case, I get fairly similar answers on significance of association and effect (magnitude/direction). Conceptually, I like the random effects approach better since it says that, at each age, there is unique intercept in the relationship between caloric intake (x-axis) and height (y-axis), but there is a common slope? Does that make sense? 

On the other hand, using age as a covariate in a simple linear regression model:
    height ~ age + caloric_intake

is way simple computationally.

I'm asking this because the whole idea of random effects offers a brand new twist on adjusting for biases/heterogeneity in samples. It's something that just doesn't register with me."
YouTube channels to learn Statistics,15,50,False,False,False,statistics,1510283769,True,[removed]
Random forest training method + general advice,2,2,False,False,False,statistics,1510286334,True,"I'm training a random forest. My method involves having the forest observe the result of my own human actions in order to piece together how the scenario it's in works. The scenario is a digital economy inside an online multiplayer game. A common practice within this game is ""flipping"" items through an automated trading system by buying an item at a low price and then selling it back for a slightly higher price. A knowledgeable flipper can make a lot of in-game currency this way and I'm interested in seeing if I can train a random forest to predict the best prices based on the time it will take to buy and sell the item.

I have two random forests, one for predicting buy times, and another for sell times. For input, there's about 120 pieces of data that I won't discuss cause it's extremely boring, but just know I calculate nearly everything imaginable ranging from time of day to the current average price of the item (accurate within 10 minutes). The output is of course the time it takes for the offer to go through based on the input data.

The recording of stats is automatic, so all I need to do is perform a ""flip"" and the inputs that were known to me at the time of me creating the buy or sell offer are recorded, and the output is the time it took to complete that trade.

I was mainly wondering if my method for recording data was viable. It seems to be getting somewhat accurate results but I'm not able to record some trades because they just never go through.

Another thing to mention is that I'm currently iterating over all possible prices for an item (within a certain reasonable range) and outputting the buy and sell prices that produce the most profit in the shortest time frame. Should I be going about this differently? I could train another forest to predict the prices I would have predicted instead of iterating over all prices but my own logic may not be ideal and I would potentially miss out on profits.

One last factor is the idea of noise. I don't think this particular market system is very noisy when compared to let's say the stock market, but it still has some element of noise. Currently I'm not doing any sort of noise reduction, but since I have 120+ variables, I'm not sure how exactly to do this or if it's even necessary.

TLDR; I'm training a random forest based on my actions within a simulated economic system. I'm  predicting trade time based on offer price and other variables. Is this viable? How do I address the issue of noisy data?"
Need help analyzing multiple regression data,13,3,False,False,False,statistics,1510292321,False,
Conceptual question regarding regression model hypothesis testing?,18,5,False,False,False,statistics,1510298114,True,"Hi, 

I had a question regarding testing a regression models coefficients.

Say there is a regression model that has the form:

y = b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + e

For the sake of simplicity let:
e be the random error, x1 is age, x2 is severity, and x3 is anxiety. y is satisfaction.

Say I do a hypothesis test on the coefficient b1:

H0: b1 = 0

Ha: b1 is not equal to 0.

Say I get strong enough evidence to reject the null, and state that b1 is not equal to 0. 

Does this mean that  

 1. age has some ability to predict satisfaction level even after the effects of severity and anxiety on satisfaction level have been taken into account?

or that

 2. age has some ability to predict satisfaction level regardless of if the effects of severity or anxiety on satisfaction level have been taken into account.

I though that it was the second one, since this is testing the affect of one predictor, and since the null hypothesis was rejected, it means that it has some predictive power regardless of the other covariates. However, someone told me that it was the opposite.

I am not sure now, but if possible could someone please let me know which interpretation is correct and the reason it is correct?

Any help is appreciated.

Thank you for your reading
"
EC50 testing: linear vs. non-linear regression,2,3,False,False,False,statistics,1510331245,True,"Hello all! I'm hoping someone can give me an ELI5 of how someone would determine if they should use a linear vs. a non-linear regression to determine EC50 values. 

I grow fungi in agar plates amended with fungicides and calculate their relative growth to controls without fungicide amendments. It seems the standard practice is, and also what my PI does, is to use concentrations that can be log transformed and then calculate the EC50 using simple linear regression analysis. This I can wrap my head around. However, other people seem to use non-linear regression analysis and/or more complicated analyses that I am not brushed up on."
Do independent variables in regressions need to be normal and homoskedastic?,4,2,False,False,False,statistics,1510335507,True,"Hi. 

I know under simple linear models the response variable needs to fit parametric assumptions but i was wondering whether the predictor variables also need to be normal and homoskedastic? "
Need help with statista,0,1,False,False,False,statistics,1510337984,True,[removed]
Career Advice...? (Unhappy),0,1,False,False,False,statistics,1510343243,True,[removed]
"If Data Science job is considered the sexiest of this century, what are some reasons that make Statisticians not want to become Data Scientists (or perhaps should not become at all)?",37,11,False,False,False,statistics,1510347787,True,"Hi everyone, I am currently a Statistics major, and so far, my learning experience, from theory to application, both inside and outside of the classroom, has been great. Recently, I have talked to a lot of Stats master students at my schools, and almost all of them aim to a Data Science job eventually. That sometimes causes me to think the purpose of majoring or learning Statistics is solely for becoming a Data Scientists, and Statistician jobs seem to be uncool somehow. But, there are still a lot of people out there firmly differentiate themselves as Statisticians. Hence there also must be some reasons for that.

Please feel free to leave any comments or thoughts regarding my question. I strongly value all the inputs. Thank you so much   "
Is it possible to statistically determine whether two reddit accounts are being used by the same person?,7,3,False,False,False,statistics,1510350718,True,"So for a situation where someone is creating alternate accounts to get around a ban, is it possible to identify these alternate accounts?

I'm thinking that each user can be represented by the subreddits that they post two, then this high-dimensional data can be reduced and visualized using t-SNE, to create clusters which could suggest whether accounts belong to the same person.

However, I believe that this could only work well in certain situations, where the user has an extensive post history, and has created multiple fake accounts. So is there a better way to perform this analysis, or is it not possible?"
"How to Learn Statistics for Data Science, The Self-Starter Way",12,91,False,False,False,statistics,1510355998,False,
Uniform Distribution - How to test for non-randomness,18,1,False,False,False,statistics,1510362904,True,"Assuming I️ have a dice with value from 1-6 weighted on value 6. If I️ run 2 Million throws, and plot the results of my throws, I️ should see the percentage of 6s way higher than any other values on my graph.

Now, how would I️ go to test for non-randomness via calculation. I️ know I️ could compute the z-score, but I️ am not sure if 1) this test is appropriate for what I️ am trying to accomplish and 2) I️ can run this test on uniform distribution"
Please help,2,0,False,False,False,statistics,1510364100,True,[deleted]
Stats courses/resources that don't require Calc or linear algebra?,9,1,False,False,False,statistics,1510364302,True,"I am two semesters away from graduating with two very non-mathematical/quantitative bachelors degrees. After taking a basic statistics course, I really want to learn more because I thoroughly enjoy doing the work. I'm considering getting an Applied Stats graduate certificate, and if I'm not fond of the material, the certificate will still give me an advantage in the job market. If I do enjoy it, then I'll go on to complete the graduate degree.

I'm currently learning Calc 1 on Coursera so that I'll be prepared for the course when I take it at my university over the summer. (I'm wanting to knock at least Calc 1-2 and Linear Algebra out of the way before I apply for the certificate. If the program allows it, I'll take Calc 3 during).

However, I was wondering if any of you guys and gals can lead me to online statistics resources/courses that don't require Calc/linear algebra knowledge? I want to get a better feel on if I actually enjoy doing statistical work outside of the basics.

Thanks"
statistics misuse in publications,3,6,False,False,False,statistics,1510391118,True,"When evaluating different journals, we are often presented with things like ""Impact factor"" and other such evaluations of how prestigous a journal might be to publish in. Could we not create something similar for the quality of statistics usually seen in a journal ?

Examples:
- Ranking on a 1-5 scale by independent statistics regarding papers published use of statistical tools (eg. 1 for often seeing incorrect methods, 2 for only presenting P-values and no confidence intervals etc.)
- How often findings in a journal are found to be wrong in the following 2 years

Thoughts ? input ? discussion ?

"
What residual diagnostics are relevant for negative binomial modelling?,3,13,False,False,False,statistics,1510407590,True,"I'm not familiar with the residual assumptions of negative binomial modelling. Do I still visually check for linearity with fitted vs predicted values of residuals? How about leverage?  
I'm using stata and the postestimation help files give so many options, I don't know what's relevant or not. "
Variance calculation help,0,2,False,False,False,statistics,1510408185,True,[deleted]
Correct method to reduce response bias?,3,10,False,False,False,statistics,1510420514,True,"Hi there folks,
I'm currently reviewing a paper and the authors have mention that they tried to minimize non-response bias (which is great) using chained equations in STATA 14. Im unfamiliar with this and was hoping someone could enlighten me on whether this was a very good method of reducing non-response bias or if there is a better method.

Quick overview of study sample: Data collected from the English Longitudinal Study of Ageing involved 10,524 participants.
They were trying to reduce non-response bias on a memory test.
EDIT: Excerpt from paper:To minimise non-response bias, we imputed the missing values in both cognitive scores using chained equations in STATA 14. The imputed data were censored at the date of death. The analyses were based on 57,199 observations of recall score of which 14,652 were imputed and 48,791 oberservations of animal naming score of which 11,375 were imputed.""

For reference the paper I am looking at is ""Type 2 diabetes, depressive symptoms and trajectories of cognitive decline in a national sample of Community-dweller: A prospective cohort study"" - Demakakos, Muniz-Terrera, Nouwen 2017. 
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0175827 

Thanks in advance for anyone's advice

"
Issue with convergence with Stochastic Gradient Descent when approximating a sin curve with a polyonomial is not close to least squares solution,0,0,False,False,False,statistics,1510444017,False,
The Statisticians' Fallacy,18,48,False,False,False,statistics,1510450761,False,
Confused on confidence interval problem. Would love some help!,0,1,False,False,False,statistics,1510451434,True,[removed]
Trouble understanding specificity and sensitivity used in diagnostic testing,0,1,False,False,False,statistics,1510453005,True,[removed]
General GRE scores for admission to PhD programs?,11,4,False,False,False,statistics,1510453272,True,"I'm currently applying to Statistics PhD programs. I'm interested in top 20 programs (e.g., UNC Chapel Hill and University of Michigan). I think my application is fairly strong, but my general GRE scores are somewhat lackluster. What are the approximate scores that students should shoot for when applying to these schools? I looked on the departmental websites and could not find any information."
"Looking to advance my statistical knowledge, any books or advice?",0,1,False,False,False,statistics,1510462912,True,[removed]
"If it takes 4 years to get a stats master's, will your resume be tossed instantly?",15,0,False,False,False,statistics,1510464249,True,"Hi all, I had very limited math/stats skills going into the program aside from matrix algebra/calculus from years back, and my prior job for 4 years was a chemistry research lab technician/manager, so I went through the program slowly and it was very difficult for me.

First two years I purely focused on courses. For the next 1.5 years I did my thesis + a couple more courses + help my aging parents with their family business. So it would've been 3.5 years, but a medical emergency/surgery is going to make it 4 years.

What do I put on the resume? What do I say at interviews? 100% the truth is of course ideal, but I was thinking of stretching the truth a little bit, saying I would have graduated in 3 years, but that the medical emergency/surgery made it 4 years because the thesis can only be defended once per year (which is a bit of lie, the emergency is only delaying me 6 months, not 1 year). Or I could simply say I put off the thesis defense for a year so I could help my parents out. Both of these are true, but are not exactly THE reason I took 4 years. Any thoughts?

And am I in really deep trouble in terms of finding a job? "
"Can anyone tell me what SPSS tests I use for each question? The options being Related Dom t test/independent dom t test, anOVa, and Chi squared.",0,1,False,False,False,statistics,1510466851,False,[deleted]
Deconstructing Data Science,0,0,False,False,False,statistics,1510493106,True,[removed]
"How to count in R, the number of occurences for a particular value in each row?.",0,1,False,False,False,statistics,1510512854,True,[removed]
Biostatistics Masters in Canada,0,1,False,False,False,statistics,1510514368,True,[removed]
"How to find the percentage of outcomes that fall outside [0,1] in a linear probability model in Stata?",0,1,False,False,False,statistics,1510515464,True,[deleted]
Is measure-theoretic probability useful to know in industry? (Examples? Assume not publishing in journals.),11,14,False,False,False,statistics,1510521013,True,
Hypotheses Analysis help,1,2,False,False,False,statistics,1510523950,True,"I have two separate hypotheses for a paper I am writing. The first is 

Anxiety will be positively correlated with higher work place absences


And my second

Attendance will increase with anxiety support systems in the work place.


For my first I was thinking of running a bivariate correlation using Pearsons r. Second a T test. Would this be the right route to take, or am I missing something. We are not actually running the study."
Homework Help,0,1,False,False,False,statistics,1510540555,True,[removed]
Assigning numerical values to categorial attributes ... but why exactly?,15,14,False,False,False,statistics,1510542897,True,"Hi, sorry if it's a laughable no-brainer, but I ask it as legitimate question.

My handful of statistics courses (educational research undergrad, so rather entry level tbh) never touched the basic ***why?*** of assigning numbers to attributes which have nothing to do with numbers. Like 1 and 2 for sexes for example. They, and my textbooks as well, were always like ""well they just do that"" and jumped directly to levels of measurements.

I mean, I get it's easier for software and stuff. But it seems so counter-intuitive that it's gotta have some theoretical fundament, some person who came up with it first and had to defend this method, right?

So, my question would be, how did it start exactly to use numbers instead of, like, ""m"" and ""f""? Thanks!"
Looking to understand how consumer's brand engagement changes with exposed to video content. Its a short and anonymous survey. Thanks!,0,0,False,False,False,statistics,1510582154,False,
Analysis question: Measuring a trajectory of different response types in a psychology experiment,0,1,False,False,False,statistics,1510588281,True,
Does anyone know where to find historical polling data?,4,5,False,False,False,statistics,1510591622,True,"Edit: Sorry, I meant US elections in particular. Should have made that more clear."
Interpreting logit regression model coefficients and p-values,4,1,False,False,False,statistics,1510600501,True,"Is there an easy way to explain how to interpret the coefficients and non-statistically significant variables (i.e. variables with high p-values) of a logit regression model?

So in my intro to regression class in my poli sci PhD program, I'm running a logit regression to analyze the relationship between interstate conflict (measured in number of militarized disputes and degree of economic integration, while controlling for level of development.

My model is as such: logit(MIDDiff[t&gt;t+1]) = PTASum + MIDCountt0 + HDI

Where:

MIDDiff[t&gt;t+1] = 1 if # of disputes at time t is greater than at time t+1; else 0

PTASum = Additive count per state-year as PTAs come into force

MIDCountt0 = Number of disputes occurring at year t

HDI = HDI value at state-year at year t

My model results are below:
https://imgur.com/ogXiDgi

Any insight on how to interpret it would be greatly appreciated! Or any sources on how to interpret model coefficients would be appreciated also.




"
Statistics discord server,0,4,False,False,False,statistics,1510608715,False,
What statistics test do I need,2,2,False,False,False,statistics,1510610831,True,"There are 3 groups (participants remain in same group throughout experiment) measuring concentration of CO2 with distance from the city centre (different sites). So for each site I have three readings of CO2. There's a big range in the groups at each site, and quite a small average trend of decreasing CO2 with distance from city centre. I'm looking for a test that will tell me whether the average trend is significant given the large variation of group readings at each site.

Thanks"
Comparing population projections,2,1,False,False,False,statistics,1510616925,True,"This is surely a very simple question, but I've studied stats... never (though I'm now working on developing some skills). 

I have created a cohort-component population projection and want to compare it to some projections published by the govt. I.e. how close are my projections to the govt projections?

I had to create my own projections to look at future populations on a more local/community level, but would like compare the results of my method at the district level to those of the govt. 

This is probably a very elementary question, so I thank you for indulging me!"
What kind of statistical analysis is this? Variance analysis?,3,1,False,False,False,statistics,1510618480,True,"Lemme preface this that I have never taken a formal statistics course. I'm currently working as a mechanical engineer and I have some data I need to analyze but I'm not sure what exactly I am looking for. I am in healthcare, but I will use an automotive ""test"" example to explain what I'm talking about.



Let's say I'm supposed to test the force required to close a car door. I have 20 cars, and each car will have 2 doors. At the end I will have 40 data points. But every 2 will belong to the same car. My task is to figure out if there's some sort of correlation/variance/difference/ (??) between the forces. For example, if all 40 data points were between 20-25N, then I could just say that yes, all the doors require a similar force to close them. But how can I prove this statistically? And what test would I use? What kind of process is it? Does the fact that every 2 data points belongs to one car? Is there a way to kind of differentiate between the 20 cars?


Let's say that one car had: 20N, 22N. And the next car was 39N, 56N. And the rest of the car doors had very different force readings. What can I statistically say about these 40 data points? That they vary? That they are not correlated? What kind of test would I use to statistically prove this?



THANKS IN ADVANCE!"
I 'played' Snakes 'n Ladders 10 million times. Here's the results.,43,119,False,False,False,statistics,1510622018,True,"After playing Snakes 'n Ladders with my kids I became curious about the number of turns it could take to complete in the best and worst case scenarios. I posted here just in case anyone else is curious.

I wrote a python script to play the game as a single player and count how many turns it took to complete. I had it play 10 million times (which took 36 minutes, btw) and recorded the data for a [histogram](https://imgur.com/VCdi0Hx) as well as the results below.

    completed game 10000000 times
    max turns: 862
    min turns: 7
    average turns: 59.3
    median turns: 46
    mode turns: 24
    Run time: 2138.358 seconds

One curious result I noticed was that there were zero occurrences for both 129 and 251 turns. ~~Perhaps it just works out that these are impossible to attain with the particular snake/ladder configuration.~~ [Detail here](https://imgur.com/5KKtq1V) 

The board configuration I used for my code was [this one](http://codepumpkin.com/wp-content/uploads/2017/03/SnakeNLadder.jpg).

EDIT: 
My code is here:
https://pastebin.com/fy3tYB5t
perhaps someone can find a bug that caused my 'gaps'.
The rules I used were: No extra turns on a 6 roll. No bounce-back at 100, need exactly 100 to win, otherwise the turn is skipped.

EDIT 2:
It looks like my gap issue was just due to how I plotted my histogram. I set my number of bins to the max number of turns. But, many turn quantities were skipped, causing them to get out of sync with the bins.

Thanks to user avelaval!

Sorry for the false alarm!! The world is right again."
Latino/Hispanic Medicaid Birth Rates in New Mexico,0,1,False,False,False,statistics,1510625278,True,[deleted]
What are quantiles (percentiles)?,2,1,False,False,False,statistics,1510630902,True,"Also what does 0.5 , 0.95 and 0.99 quantiles represents in a data?

thanks"
What is the statistical model behind SVM algorithm?,2,1,False,False,False,statistics,1510637459,True,"I have learned that when dealing with data using model-based approach, first step is to modeling data procedure as a statistical model. Then develop effecient/fast inference/learning algorithm base on this statistical model. So i want to ask which statistical model behind SVM algorithm?
"
Best way to store and record changing data over rolling time periods?,0,1,False,False,False,statistics,1510644224,True,[deleted]
Cox-Regression: Clustered Standard Errors?,0,1,False,False,False,statistics,1510648832,True,"I'm using a coxph-model (in R) to model survival time on prepayment loans. My data consists of monthly panel (longnitude) data o.v.t. to adress time-varying covariates, f.e.: loan ID 1 last for 12 months, so I got 12 rows, loan ID 2 lasts for 3,5 years, so I have 30 rows etc.

Having already a suitable standard model cox model, I'm wondering if clustered standard issues could be an serious ""threat"".

In usual regression, clustered standard errors are the usual way to go. Specifically, in my case, due to the panel structure the i.i.d assumption does not hold. As far as I understood, the standard errors which is provided by the coxph() regression in R only delivers homoscedactic standard errors.

Did I get this right so far? If yes, is there any possibility to get heteroscedastic (""Sandwich"") Standard Errors. I read that the cluster() option might be an option, but I'm not sure if actually adresses heteroscedacity.

Note that I'm not talking about unobserved heteroscedacity, I'm concerned with the heteroscedacity of the standard errors.
"
Is there anything more interesting to do with survey data than multivariate regression analysis?,4,4,False,False,False,statistics,1510655781,True,"I work with alot of large public goverment survey datasets, think NHANES/NSDUH/DAWN, usually investigating mental health. Most are cross-sectional. In particular I like to look at how drug use impacts the use of other drugs (ie how does marijuana use effect opioid use?). But regardless of the dataset or hypothesis I always end up with a regression model including the independent, dependent variables of interest and a range of demographic covariates. 

Tests of association feel so limiting, what else is there?

Could predict... go the route of random forrests/boosted trees/nn. But then I have a model or classifier that can predict opioid use...how do I use that to explore the relationship to an independent variable (ex: MJ use, income, age)?"
Principle component analysis (PC1 and PC2),12,3,False,False,False,statistics,1510659878,True,"I searched for these in internet and I found that  principal component is a combination of the original variables after a linear transformation.

However I got an article which had PC 1 and 2 and I want to know the difference between them. What does it represents to have higher PC1 than PC2 or vice versa? (strongly associated with PC1 than PC2)

The article has done PCA of plant traits.(characteristics like height , leaf area etc) 

Thank you. 
"
Actuarial Credential Value in Grad School Applications,1,2,False,False,False,statistics,1510672075,True,"I graduated double major actuarial science and economics with a 3.5 gpa. I'm employed by a major insurer, completed 4 actuarial exams, and plan to get my ACAS. I'm seeing that statistics and data science are having more and more applications not only within the insurance industry but other industries as well. I am curious if any of you know if becoming an associate through either the SOA or CAS boosts your application to graduate statistics programs. Compared to the competition I had in undergrad (top 50 school), it seems like a 3.5 gpa isn't anything special and won't get you into a solid program alone. 

So essentially, how much - if any - value does an actuarial credential(ACAS in particular) have when applying to graduate statistics program?

Thanks!"
Can a scale of possible values be accounted for with Intraclass correlation coefficients?,1,1,False,False,False,statistics,1510681781,True,"I have a series questions analyzed by 5 raters and want to calculate an intraclass correlation coefficient. The questions are rated on a scale from 1-7, but all three questions were rated 5-7 by all raters. It would appear on the surface that there is high inter rater reliability, but the ICC is very low.

I suspect this is because the scale is not analyzed in the calculation, so the result shows 5 raters rating things between 5 and 7 with little agreement, rather than 5 raters rating things between 1 and 7 with high agreement.
Is there a way to account for the scale of values with the ICC? Should I be using a different measure of inter-rater reliability?

I was previously using the ICC() function from the Psych package in R."
How important is school name in terms of job prospect?,0,1,False,False,False,statistics,1510682448,True,
Statistics,3,0,False,False,False,statistics,1510687389,True,"Statistics - a set of concepts, rules, and procedures that help us to:
organize numerical information in the form of tables, graphs, and charts;
understand statistical techniques underlying decisions that affect our lives and well-being; and
make informed decisions."
How important is school name in terms of job prospect?,0,1,False,False,False,statistics,1510711583,True,[removed]
Is statistics seeing similar increase in enrolment like CS because of the potential for high salaries?,12,13,False,False,False,statistics,1510719161,True,So as many ppl know computer science has exploded enrolment in recent years and many depts are struggling to keep up with the demand. Is statistics seeing a similar increase in demand? Are many stats depts struggling to accommodate the large influx of new students into the stats major program?
Proc IML sas Matrix help (Probably very easy),1,1,False,False,False,statistics,1510719301,True,"I'm not sure what this is called but I need to do this for a box muller transform. Anyways, I have two identically sized matrixes and I need to multiply the two together, not the matrixes, but rather each corresponding case to each other to produce another 3rd matrix with the multiplied cases. Brain isn't really working tonight, so the google isn't helping so either the operator in proc IML that can do this or just what it is called and I'll take this rowboat from there. Thanks."
Coefficients of Standardized Predictors in Linear Regression,2,1,False,False,False,statistics,1510719715,True,"I think I missed something in the interpretation of the coefficients of standardized predictors in Linear Regression that I want to clear up. Let's say there are two predictors X_1 and X_2.

y = B_0 + B_1X_1 + B_2X_2

If X_2 is kept constant, then for every unit change in X_1 there is a B_1 change in y and vice versa. Now if I standardize X_1 and X_2 using (x - mu)/(sigma) to get x_1 and x_2.

y = b_0 + b_1x_1 + b_2x_2

I would say If x_2 is kept constant, then for every unit change in x_1 there is a b_1 change in y and vice versa. But recently I heard someone interpret these coefficients in terms of the standard deviation. (I don't remember the exact words.) Intuitively, it makes sense why that would be kosher, but I'm not sure I fully get it. "
Does it matter where you get your PhD from (stat or biostat?),5,5,False,False,False,statistics,1510750517,True,"Disclaimer: I do not want to be a professor or stay in academia.

I'm in an MPH in epi program right now. I definitely want more stats, and a PhD is required for a lot of the jobs I come across and want. My school is highly ranked in PH but much less so in stats-however admission to our biostat PhD program here would be much easier for me and cut 2-3 semesters off of graduation (due to overlap with electives I've taken).

I like my school, the city, the faculty, etc and would much prefer to stay here. I'm looking mostly at work in clinical research/pharma but am pretty open to interesting opportunities, tbh."
Applying Weights Across Multiple Surveys,1,1,False,False,False,statistics,1510759467,True,"I am currently working on data analysis from representative surveys conducted in three cities. For each site, we calculated survey weights to account for non-response (ie: the samples were skewed toward older, female respondents). Therefore, for each survey we have a column vector of weights for each individual respondent.

Applying the weights for each survey individually is simple. However, I want to perform an analysis across all three locations simultaneously within a hierarchical linear model. My question is: is it appropriate to apply the survey weights together in the pooled 3-city model? Do I need to re-estimate them instead?

Any help or suggestions is greatly appreciated."
Is this a research question I can use linear regression for?,9,1,False,False,False,statistics,1510759501,True,"I'm working on a group project in which we ""conduct"" a study and write an amateur journal article. Our topic is use of force in the Dallas PD. Our first research question is ""who is most likely to use force?"" And we examine that by looking at the race and sex of the officers. We have a public dataset of 2,137 use of force incidents, but because some officers are involved multiple times, there are only 1,001 separate officers actually described in this dataset.   
Can I even use linear regression to show a relationship here? Because the dataset lists officers specifically involved in UOF incidents, I don't feel like I really have a dependent variable. The answer to ""did they use force?"" is always yes - can ""used force"" still be my DV?   
Should I be looking into multiple regression instead since we're doing this by both race and sex? Our prof doesn't expect us to have super-involved analysis methods. As of right now, we're just using percentages to describe what we know. "
Apporximation from binomial to normal,2,0,False,False,False,statistics,1510764536,True,"Just need help solving this problem.
A sample of 1600 tires of the same type are taken from random from production process in which 8% are defective. What is probability that in such a sample 150 or fewer will be defective?"
Factor for rows without attribute and rows with attribute and additional characteristic,0,1,False,False,False,statistics,1510764641,True,"I have run an experiment in which I've measured some metric *X*, and collected an associated attribute `attr`'s value **if and only if** the value of *X* exceeds some threshold *t*. If a case's *X* value doesn't exceed *t*, then `attr` is not captured. Here's what my data look like:

    ID  attr_val  no_attr  outcome
     0       NaN        1        0
     1       NaN        1        1
     2       0.0        0        1
     3       4.0        0        1
     4       2.0        0        1
     5       1.0        0        0

Where `ID` is a unique identifier for each case, `no_attr` indicates whether the `attr` value was captured for that particular case (i.e. whether the `X` value exceeded *t*), and the binary outcome `outcome` is shown.

Now, I want to predict outcome based on the attribute value if a case's *X* exceeded *t*, and I also want to measure whether a case's *X* **not exceeding *t*** is predictive, as well.

In order to keep IDs 0 and 1 in the model, `attr_val` will need to be populated with some value, not just nulls. But I don't really feel comfortable filling in 0, for example, because IDs 0 and 1 didn't have a chance to give their `attr_val` because their *X* values didn't exceed *t*. However, this *X*-exceeding-*t* criterion is very important to my experiment, so I can't just take the `attr_val`s for rows 0 and 1 anyway.

Running a logistic regression of the form `outcome ~ attr_val + no_attr` currently would make the design matrix singular, as just zeroes are included for `no_attr` if I haven't filled in any nulls in `attr_val`. **Is the right approach here to augment `no_attr` by 1 so we're not multiplying by zeroes down the line?** Or is there a better way to encode this problem?"
What statistical procedures are used to evaluate trends in on population data over time?,9,12,False,False,False,statistics,1510768777,True,"Say I have the annual rate of deaths due to a specific disease for the entire US population over a ten year span.  There are some minor peaks and valleys, but overall it looks like there is a downward trend.  What statistical procedures are used to evaluate whether there is indeed a directional trend and how is the significance of that trend statistically evaluated?

Sorry for broad question, but general answers or links to relevant resources would be greatly appreciated.  "
Where to get started with Data Analytics beyond graduation?,23,7,False,False,False,statistics,1510770152,True,"Hi All,

I graduated with a degree in Actuarial Science from an accredited university in 2014.  At the time, I was working full time for a small IT company, managing their books and dipping my toe into Financial Analysis/Modeling.  After about a year beyond graduation, I transitioned to a Senior Financial Analyst role at a top 10 bank and  just recently made another move to a smaller bank in a very similar role.  I would say that I'm okay with my position but it is definitely not my passion and can feel my interest waning by the day.  I never envisioned myself working for a large bank, and just as well, after graduation I didn't pursue any of the actuarial exams with no real desire to work for an insurance company or pension consulting firm.

As of late, I've found my passion working with data, but I made a serious error during my education.  Already being proficient with Excel, I utilized it as my program of choice for all of my data analytics as opposed to R (the other choice in my curriculum).  My choice was further justified by working within finance where Excel really is king and have had no exposure to any statistical analytics software.  I really want to break this mold and start moving my career in a different direction, but short of going back to school to acquire a degree in data science, I'm not sure where to direct my studies to acquire this skill set that I'm seeing on the vast majority of analytics jobs postings.  Is there a place for me to learn these skills to make the leap into an entry level data analysis position outside of university training?"
Could I get some advice on how to understand GLMs?,12,2,False,False,False,statistics,1510774415,True,"I've been design a model matrices for the analysis of genomic data for some time now but I feel like my understanding of GLMs has yet to improve even slightly. 

If there's one variable I'm interested in, it's obviously quite simple. But when I include two experimental groups things start to fall apart.

E.g. 

* Genome 1 Cond1
* Genome 1 Cond2
* Genome 2 Cond1
* Genome 2 Cond2

Ideally, I'd like to compare the differences in expression of both genomes, while under a given stress. *However*, when I run them through software like R under a model matrix like (~genome+condition) I get unbelievably low p-values. I feel like this is wrong and I completely misunderstand GLMs.


Do any of you know a good resource to help a novice understand GLMs - especially in relation to R packages? I'd post on the package threat but I feel like the answers I'd receive would just bandaid a deeper misunderstanding. 

Thanks!"
Could an MS in stats hurt you? Could it ever be to someone's disadvantage to get one?,7,1,False,False,False,statistics,1510774747,True,"Let's say someone gets a masters in stats just because they have the time and money to do it; but this person has a general interest in anything related to data analysis such as working in finance, consulting, tech, actuary, whatever. Could it ever be to someone's disadvantage to get one? "
Is Shannon Diversity a measure of dispersion or centrality?,3,15,False,False,False,statistics,1510783962,True,
Z Test Vs. T-Test - Sample Size?,9,3,False,False,False,statistics,1510790495,True,"For some reason, I'm having trouble understanding the required sample size for either test. Obviously the presence or absence of the Population Standard Deviation marks which test to use, but what about the sample size? I'm aware it has something to do with &gt; or &lt; 30."
Bootstrapping for model validation,0,1,False,False,False,statistics,1510790764,True,[removed]
Help me with my HS stats... plz,2,0,False,False,False,statistics,1510791408,True,"Q1: You and your cousin decide to play a game. With a deck of 40 cards, half of which are red and the other half black, you take turns drawing. If your cousin draws a red card, she wins. If you draw a black card, you win. Your cousin, annoying as usual, insists she starts. The card drawn is replaced with shuffling. What is the probability that you win on your nth draw? What is the probability that your cousin wins on their nth draw? What is your cousins probability of winning relative to yours? What is the probability that no one wins in the first three draws?

A: So I figured to win on say my 2nd draw, my cousin would have to draw a black (0.5), I draw a red (0.5), she draws a black (0.5), and I draw a black (0.5). So the probability I win on my nth draw would 0.5^(n)... right? and her probability of winning on her nth draw would be 0.5^(2n-1)...?  Thus she has a 2:1 chance of winning, or in other words she is twice as likely to win... cause
2n - 1 - (2n) = -1 and 0.5^(-1) = 2... right???? Am I doing this totally wrong??"
Help me understand the CLT and distributions...,3,5,False,False,False,statistics,1510792075,True,"Q1: The CLT shows that with a large enough value of n (sample size), a Binomial Distribution(n, p) w/ p=0.4 will be approximately normal... True or False?

Q2: The CLT states that if we have a large enough sample size, individual outcomes of a Bernoulli trial will be approximately normally distributed... True or False?

Answers: I think both are true, however, I cannot put to words why I think this. I don't think the probability of success matters for the binomial distribution as a large enough n should bring it to an approx. normal distribution... right? As for the bernoulli trials, I took the ""individual outcomes will be approx. normal"" to mean a plot of the individual outcomes will be normal. I think I might not understand the question though..."
Help me with this combinatorics problem...,2,3,False,False,False,statistics,1510792747,True,"Q: There are ten departments at a school, with each department consisting of 8 different classes offered each semester. There are 5 open classrooms in a particular building for a particular time slot. What is the probability that in the first time slot, 3 of the courses among the 5 in the different classes are from the same department?

My answer: I figured it would play out like this...

C(10,1) * C(8, 3) * C(72, 2) / C(80, 5)

my reason being that there is one of ten departments selected, we need 3 of their 8 classes, and the remaining 2 classes are chosen from the other departments' courses. is this the correct way to approach the problem? "
Elementary Statistics Project (Undergraduate),4,3,False,False,False,statistics,1510794035,True,I have to do a project for my elementary statistics class. I want to find out if there is a correlation between education level and political party affiliation in the United States. My professor does not want us to do surveys. Does anyone know where I can find existing data on this? I’ve already tried citydata.com and couldn’t find anything very useful. 
Poisson Regression in SAS,2,3,False,False,False,statistics,1510799516,True,"Hi,

   I am currently working on a project where I am attempting to use poisson regression to measure the effects of heat waves on hospital admissions.
I currently have two separate excel files that look like this:
https://imgur.com/a/sYLz5

Where they have inpatient and emergency department hospital data. I am trying to run a poisson regression using this code:

proc genmod data=washoe ; where county in ('Clark', 'Washoe') and mdy(9,30,2015)&gt;date&gt;mdy(3,1,2009);
   class county year month;
   model totaladmissions =   month year month*year county dummyheat595/ dist = poisson TYPE3
     link = log
   offset=lnPop
   pscale;
run;

Here is a copy of the data. https://we.tl/vy0C8aLDoc

Now I just want to know if I am running the poisson regression analysis correctly in SAS, or if I should attempt to do this in another program, such as R or Minitab, which I can also do.

Appreciate any assistance/advice/support/nice thoughts that I can get!

Thanks for reading,
Boardgameguy123"
Can anyone recommend good study strategies for biostatistics?,1,0,False,False,False,statistics,1510803086,True,"My horrid disaster of a prof told us not to do practice problems because thats not how you study for statistics. Im lost, my exam is in a month and im going to fail it. "
Ok....wtf is a data scientist?,46,50,False,False,False,statistics,1510811149,True,"Been a handful of posts with this recently, hell for the last few years.  And in each one it seems like there's some frustration about the role.  But more than that, there's an important question lingering behind the scenes [that only occasionally gets asked](https://www.reddit.com/r/statistics/comments/7c3yzf/if_data_science_job_is_considered_the_sexiest_of/dpmzegt/).

What the hell even *is* a data scientist?  

Is it a programmer with some math and stats skills?  Is it a statistician with some programming skills?  Is it someone who knows SQL and Hadoop and took an intro stats class?  Is it a guy who knows how to use the slicer tool in Excel and can pivot?

Honestly this is my biggest beef with the term.  Wtf does it even mean?  I would be very curious to hear people's views on what it actually *is*."
Brilliant George Carlin being perhaps not so brilliant at statistics...,14,35,False,False,False,statistics,1510841509,False,
"Logistic regression: non-significant and negative intercept, how can I obtain / understand the baseline odds for my dependent variable?",1,2,False,False,False,statistics,1510842371,True,"Hi all; I think the question summarizes my question. I have a non significant intercept for my logistic regression model (fine), but it's also negative. How can I calculate the baseline odds of the event?"
Neural networks for beginners: popular types and applications in real life,0,66,False,False,False,statistics,1510842550,False,
Can you have a nonrandomized design with randomized sampling?,4,2,False,False,False,statistics,1510844911,True,I was reading a quantitative research article and this is what was said during it? If it’s a nonrandomized design it can’t be random sampling right? 
"If I have three independent events, A, B, and C, and given that the ordering of events A -&gt; B -&gt; C is considered ""failure"", but that I only control intervals between ""B"" events, how can I optimize these B-intervals to minimize the probability that any given C is like C in A -&gt; B -&gt; C?",0,3,False,False,False,statistics,1510845163,True,"For example, if I have data like:

A: 1.5,2.1,1.3,3,...

for A, B, and C, where the values above indicate, for each occurrence of event A, the time since the last occurrence of event A.

I have data like this for A, B, and C.

The events themselves are all considered independent.

I realize that this is a poisson process, and that the data I have is in some fashion related to an exponential distribution (in that the exponential distribution is generally used to model systems like this).

Also I understand that I'm sortof looking at ""what is the probability  that C happens at least once in the interval between the most recent B and the next A"" which is a poisson distributed thing - I'm just not sure how it should reasonably be modeled - I also realize that this is trivially solved by never allowing B to happen again, but I expect that if we could graph the probability of C occurring in the interval B -&gt; A on the y axis, and the possible intervals between B events on the X axis, we would see that the graph isn't entirely linear?

Anyway, thanks."
Multiplying and dividing datasets.,6,2,False,False,False,statistics,1510846224,True,"I'm trying to demonstrate a trend between two variables (the trend is definitely there) but one is quite small (max of 0.167 min of 0.0046) the other is quite large (max of 2984 min of 1505) will I compromise the data if I multiply the small set by 100 and divide the large set by 10 to bring them closer together so they'll make sense on the same graph (I'm already using a log scale). They're definitely proportional to each other, plot them on separate graphs and the line is the exact same shape, I just can't get that to show on any graph my computer can make, without the dividing and multiplying."
Corresponding standard scores in Excel,1,2,False,False,False,statistics,1510846991,True,"Hello,

I am trying to create a conversion chart converting various standardized scores. Specifically, I want to know the exact integer conversion for scaled scores (M=10, sd=3), standard scores (M=100,sd=3), t-scores (M=50, sd=10), and non-integer z scores and percentiles. What's the best way to do this? "
Fitting a straight line through a field of values,3,2,False,False,False,statistics,1510847786,True,"Hi,
I'm doing a parameter grid search, and so far I have something which looks like this (the results are a little noisy):
https://i.imgur.com/VDs0MKx.png

What methods are there to fit a straight line which best aligns towards the greater values?

I can of course just choose two pairs of coordinates and fit a line through them, but say if I wanted to do it programmatically?

Thanks,
MK"
Correlation when only sum of two variables is known?,3,2,False,False,False,statistics,1510849326,True,"Hopefully not breaking any rules by posting this, it is not homework but a project I'm doing with a professor in my free time. We have a data set [;S(t);] giving a strain at a given time. This dataset can be described by the function

[;S(t) = W(t) + n(t);]

Where only [;S(t);] is known. I've been given the hint that I should look at what happens when I square this equation.

[;S(t)^2 = \left(W(t)+n(t)\right)^2 = W(t)^2 + n(t)^2 + 2\cdot W(t) \cdot n(t);]

If I sum these and divide by the total amount of data, [;N;], I get the following

[;\bar{S(t)^2} = \bar{W(t)^2} + \bar{n(t)^2} + 2*E\left[W(t) \, n(t)\right];]

for the sample. I know that correlation can be given as [;corr(X,\, Y) = E\left[XY\right] - E\left[X\right]\cdot E\left[Y\right];], and can see that the last term is part of the equation for correlation. I just can't see what I can do from here. Any recommendations, tips or hints, especially when only [;S(t);] is known.
"
Is this the best statistical analysis to use for this particular data set?,4,1,False,False,False,statistics,1510855624,True,"Hello Folks, 
I have a quick query to which I am reviewing a paper that used data from the English Longitudinal Study of Ageing.
They mentioned that they conducted ""Linear mixed (random coefficient) model to look at the association between Exposure and 6 repeated measures of word recall over a 10 year follow up that was gradually adjusted for covariates"". Now in all my years of looking at papers and assessing their statistical analysis and design i've never seen such a complex model. Mind you it is a longitudinal study where they also a multistage stratified random probability design to recruit participants.

Would someone be able to help tease out for me in a simpler way what they did statistically to analyse their data and whether this was most appropriate (I believe linear mixed model- was appropriate but mainly because I think I might have a naiive approach to this and never conducted a study of this magnitude before).

I would love to know what every one else thinks.

The link to the paper is: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0175827# 
Title of paper: Type 2 diabetes, depressive symptoms and trajectories of cognitive decline in a national sample of community-dwellers: A prospective cohort study

Please feel free to message me to ask about anything I may have missed out.
Thanks in advanced"
Research Proposals?,4,1,False,False,False,statistics,1510861324,True,"Hello all,

I was wondering how a typical research proposal in non-applied statistics might look like (i.e., such that you are, for example, developing a test as opposed to using one). 
"
Heteroskedasticity Question,0,1,False,False,False,statistics,1510861331,True,"When using a software for Multiple regression (I Use gretl), can you determine whether or not heteroskedasticity is present from the results alone (i.e. no graph and no test within the software done for you such as White's Test)?

Thanks in advance!"
Which statistical tests to run for data and validity tests.,0,1,False,False,False,statistics,1510863078,True,[removed]
Question about transforming explanatory variables versus transforming depending variable in OLS.,0,1,False,False,False,statistics,1510863983,True,"I've run into an interesting problem that at first seemed really simple, but the more I think about it, the more confused I'm getting.

So I'm aware in simple linear regression (ie, one explanatory variable, one dependent variable) that if you see an exponential relationship between the explanatory variable and the response variable, you generally just take the natural log of the dependent variable to linearize the relationship and then proceed with OLS.  Simple enough.

But what about when you have two explanatory variables, where one appears to have a linear relationship and the other has an exponential relationship.  You can't just take the log of the dependent variable because that would screw up the linear relationship with the other explanatory variable.

At first, it seemed obvious that you should just take exp(x) for the explanatory variable that had an exponential relationship.  Intuitively, it seems like this should be the same things as taking the log of the explanatory variable.  But it isn't.

Functional form of taking log of dependent variable:
ln(y) = B0 + B1X1 + error

Functional form of exponentiating explanatory variable:
y = B0 + B1*exp(X1) + error

They are simply different functional forms, and there's no way to rework them to be the same.

So I guess my question is just... what is going on here?  Is it valid to exponentiate one explanatory variable when it displays an exponential relationship?  What should you do if you see both an exponential and a linear relationship with your explanatory variable?"
What are the advantages to treating a retrospective study as prospective?,0,2,False,False,False,statistics,1510864124,True,[deleted]
"Is a correlation coefficient of 0.25 ""significant""?",9,1,False,False,False,statistics,1510874016,True,"I'm leisurely reading a bunch of papers about the relationship between intelligence and chess skill. (Statistics is not my field.) I notice that practically all of these papers make claims about ""significant correlations"" for correlations as low as 0.25 and sample sizes between 15 and 100. Is this just hand waving, or is 25% considered significant in the context of ""expertise research""? Why is correlation the only measure that people cite, but they don't even bother including it in their abstracts? Is it just sloppiness or is there some reason p-values aren't used? (Do things like adjusting/accounting for other factors such as age make it impractical to calculate p-values?)"
What's an interaction term in SPSS (have to run an ANCOVA),2,1,False,False,False,statistics,1510877143,True,"I was told to run an ANCOVA using a certain variable as an interaction term. It's a physiological value obtained prior to the treatment. We want to see if it moderates the treatment response. So far I have the following which I'm sure I've done right. 

Fixed factor: treatment assignment
Covariate: pre-intervention behavioral assessment value
Dependent variable: post-intervention behavioral assessment value

What would this be? A covariate? "
Question regarding error calculations,1,1,False,False,False,statistics,1510877506,True,"

I have a question regarding error measurements. Say I want to generalize to the population from a sample (say, using random sampling, looking at specific groups within the population, or both).


To make the best inference about the error rate in the population, would it be better to sample from the population at random vs specific sub groups? I'm curious about the error rate for specific groups within the population, and I want to then compare that error to the general error rate, or to other groups. Would it be better to sample those specific groups directly, or to sample the entire population and then build groups from the larger sample?

It's easier to look at specific groups, but I reckon calculating confidence for those groups wouldn't generalize to the broader sample.

Any input is very much appreciated.


"
Looking at rape statistics in Sweden according to BRA study,0,1,False,False,False,statistics,1510884637,True,[removed]
Garch dcc standard errors,0,1,False,False,False,statistics,1510915431,True,[removed]
"Spearman's rho - a measure of non-linear dependence: estimates, properties, plots, references",6,13,False,False,False,statistics,1510917483,False,
Question about qualitative spatial point pattern analysis,2,1,False,False,False,statistics,1510920342,True,"First off, I'm sorry if my english sounds a bit off, but I'm not a native speaker and pretty new to the whole science part of the language. 

Right now I'm working on a biomedical project, where I immunostained  tumours for different antigens, scanned the slides and had my PC run an algorithm for cell detection, which also assigns numerical coordinates for the cell center and a few other measures.
After that I categorized the cells in four different categories. What I want to find out is, if there's a positive, a negative or no interaction between cells of different categories. Thanks to the assigned coordinates every single cell can be represented as a dot with multiple variables on a scatter plot. 

So now I want to do a quantitative and qualitative comparison of these data sets, including distribution pattern analysis. 

So my main question would be if there is some kind of test, that can give me a number for a potential interaction between the cells as talked about above.

I'd really appreciate someone taking the time.

Edit: Also not quite sure whether I chose the correct flair."
Discrete variable?,0,1,False,False,False,statistics,1510928981,True,
Why is it important to report what statistical software was used in research?,13,7,False,False,False,statistics,1510935055,True,"Hello folks quick question. 
When writing research articles I always wondered why it was necessary to report which statistical program was used to analyse the results. Surely you would not derive a different result by using R for example?

Thanks for you response in advanced."
"College STATS: please help, this is all I need to do",11,0,False,False,False,statistics,1510942674,False,[deleted]
How can you measure the accuracy of predictive models with continuous outcomes?,4,4,False,False,False,statistics,1510943547,True,"For a binary outcome model there are ROC curves and hit/miss statistics, which both provide a measure of accuracy.   
  
But how about continuous outcome models?   
All I can find is root mean squared error, which is more useful for model comparison rather than demonstrating individual model accuracy. "
What are some useful android apps to learn basics of R programming and other data techniques (analytics)?,8,2,False,False,False,statistics,1510951595,True,Title.
How would someone go about correlating multiple location data with geographical data?,1,1,False,False,False,statistics,1510953788,True,"So, I saw the [post about cardiac deaths and wafflehouse locations](https://www.reddit.com/r/dataisbeautiful/comments/7dnnng/cardiac_related_deaths_during_2013_in_the/) on r/dataisbeautiful. I thought it was really interesting how easy it was to visually correlate the relation between cardiac deaths and wafflehouses.

But, I really don't know how I would approach this mathematically.  Is there a method that would allow someone to use the raw data instead? Or would you have to create summary statistics for waffle houses and cardiac disease for each region and correlate that?
"
Bing results. Where stats and fashion meet.,5,250,False,False,False,statistics,1510959654,False,
Correcting for multiple comparisons in the following example when using Bayesian parameter estimation.,5,2,False,False,False,statistics,1510969737,True,"I am using two measures of some individual difference variable (call them Predictor1 and Predictor2) to predict a single construct, which is measured in two ways (call them Measure 1 and Measure 2).

This results in four analyses:

* Predictor1 predicting Measure1

* Predictor2 predicting Measure1

* Predictor1 predicting Measure2

* Predictor2 predicting Measure2

I am using Bayesian parameter estimation, fitting each model with the same generic weak prior. I am assessing significant using 95% CIs. 

If I were doing this with a frequentist approach I would correct for four tests of the same hypothesis (remember that the predictors are two ways of quantifying the same thing, just as the outcomes are two ways of quantifying the same thing). 

However, I'm not sure if this would even make sense with a Bayesian approach. If I understand correctly, a frequentist corrects for multiple comparisons because 5/100 times p &lt; .05 could be wrong. But a Bayesian 95% CI isn't based on ""how many times out of a 100"" it could be wrong, but rather the likelihood of that single estimate being wrong (i.e., 5% chance that the credible intervals don't contain the true parameter value).

So, 1) do I have to correct for multiple comparisons?, 2) if so, how would I do that?

Thanks!

Edit: I have looked at Gelman's paper (admittedly just the abstract) which recommends using nested models as a way around correcting, but that doesn't seem to apply here? I also wouldn't want to prioritize any one analysis and use its results to inform priors of the other three."
Power Analysis,6,1,False,False,False,statistics,1510976937,True,"I have a two part question. I will be conducting a mixed-model method experiment and I’m wondering how I should conduct my power analysis for this type of experiment. Specifically, I will have one between subjects variable with 3 levels and a within subjects (repeated measure) with 7 time points. I’m wondering if using the mixed-model test in G-power is the correct way to go about this, and if there are any better packages than G-power for conducting a power analysis? "
Best industries for Social Statistics jobs?,0,1,False,False,False,statistics,1510981244,True,[removed]
Am I the only one sick and tired of these gosh darn dashboards?,16,1,False,False,False,statistics,1510981892,True,"It's like I cannot escape them. The ones that I've made and seen, rarely have anything meaningful on them. It's just ""oh, pretty colors"" for non-stats people 99% of the time. Well, at least on my side. We're never asked to actually do any real analyzes on these data. Maybe they'll allow us to add some means, but that's it. The non-data people seem to totally ignore the possibility of outliers or whether any of this crap is important.

I know that I'm just ranting. But I'm tired of making these things!"
12 Powerful Stats of Customer Engagement,0,1,False,False,False,statistics,1510986626,False,
"Scrape the web : get data from amazon, airbnb, reddit, pexel, steam",0,0,False,False,False,statistics,1510994409,False,
Data Collection - Required,0,0,False,False,False,statistics,1511010827,True,[removed]
Understanding chi-square and P value,8,14,False,False,False,statistics,1511021479,True,"I'm trying to understand doing chi-square tests and converting that to a P value. My reference is this http://ccnmtl.columbia.edu/projects/qmss/the_chisquare_test/about_the_chisquare_test.html for finding chi-square and http://courses.atlas.illinois.edu/spring2016/STAT/STAT200/pchisq.html for converting to the P value

The above explains analyzing the results of a single die and I understand and can follow the math from data gathering to determining the chi-square value, then finding the relevant P value.

Suppose instead of a single die, I have 10 numbered balls from 1 to 10. I then pick 5 of them out of a bag WITHOUT REPLACEMENT like a lottery and record the results, replace them after all 5 have been draw, and repeat this process a large number of times. If I do 1000 trials like this, I will draw a total of 5000 balls and expect each ball to appear 500 times. What actually happens is that I get balls 1-9 appearing exactly 450 times each and ball 10 appearing 950 times.

Obviously this seems like there is a bias. What I want to determine is chi-square and P values for this data. Using a similar methodology as in the links above I end up calculating that chi-square = (2500/500) * 9 + 202500/500 = 450, and then I have 9 degrees of freedom so this is a P value of something like 2.868 * 10-91.

What I want to know is, is it correct to apply that same methodology as with a single die? Does the fact that the balls are drawn without replacement, or that there are multiple values in one set of draws mess anything up? I know that if I were to draw the balls one-by-one 5000 times, replacing them each time, then it would be the same situation as wih the single die. How does drawing them in groups of 5 change the analysis, or does it?

Is this something where I need to start looking at hypergeometric stuff and, if so, could you point me in the right direction?"
Applying a Normal Distribution Curve To Other Dataets,1,0,False,False,False,statistics,1511021672,True,"Morning, I have annual spending data for a few past projects. The annual spending fit a normal distribution pretty well (kurt = 2.79, skew = .2). I am comfortable using a normal distribution for these. I have to project ~50 projects going forward in which we have the total spend (estimate). The boss wants me to take the distribution for the past programs and extend/compress it to allocate annual spending. I am unsure how to do this. Should I take z scores and get the area under the curve and conver this to years?

Obviously, a deep dive into each is preferred. Due to timing and a few other reasons, we have to project it using historical data. 


At this point, I have the CDF, normal distribution made in @risk, and the mean and stdev. Any suggestions would be well appreciated  "
Entropy and Randomness Online Tester,0,0,False,False,False,statistics,1511027560,False,
Make sense using Cox-Prop model,0,1,False,False,False,statistics,1511035489,True,[removed]
Data visualization - software?,15,6,False,False,False,statistics,1511038739,True,"Hey guys. I teach statistics at a university and I’m preparing a “fun” lecture where I want to implement some visualization of (just as an example) what happens when one changes the values on the Y-axis to tell a different story. I want to make a video or animation where the data stays the same but I let the values on the Y change from small increments to very large and then observe how, as an example, trend lines can be manipulated. Anyone have any experience with this? What software do you use?"
Multidimensional Points (&gt; 3 values) &amp; Physical representation,1,0,False,False,False,statistics,1511045871,True,"While writing a k-means program with points having a dimensionality greater than 3, I was wondering how it's possible to develop theorems, models, etc. when it's physically impossible to represent (from my understanding) as we visualize elements in 3D (maximum). It seems very counter-intuitive that we are able to do Maths on objects we can represent.

* How is it possible to do develop concepts on points we can't physically represent?
* How do we know that the concepts, models, theorems, etc. are true (besides confirming results with calculations) when we can't create a physical representation."
serious question about statistics and business intelliegence,3,0,False,False,False,statistics,1511049815,True,"do you think in the world of business intelligence, a knowledge of basic statistics is missing? The reason I say this is because many times, not all the time, I see some kind of dashboard or chart that looks pretty but fails miserably to gain any insight. I'm not implying that people in BI need to have a background in stats, but I think knowing the basics wouldn't hurt. I feel like my team is good at pulling data and using all the BI tools, but I think we fall short in really understanding the data. By having a knowledge of stats, we could ask better questions about the data. Any thoughts?"
In what ways have you used your Statistics knowledge to your advantage in your personal life?,31,42,False,False,False,statistics,1511052099,True,"I had a professor tell me about a time he bought a ton of cereal from the store because the brand was doing a promotion where there was a guaranteed ""1/x"" chance to win a $20 dollar gift card on the inside of the box _(can't remember the probability atm)_. So he bought a ton of boxes of cereal to try and win as much as he could. There were other details that made the story more interesting, like the store put that cereal on clearance or something.

But it got me thinking, how have you guys used Stats in real life to your advantage? Did you go all out? Build scripts, gather data, etc.? or did you scratch some math on a note-pad and reap the rewards?"
Possible Internships for Undergraduates interested in Statistics?,6,4,False,False,False,statistics,1511066813,True,"I am not sure where to ask this question, but I'll ask it here because I'm doing currently doing a Math/Stat major. I am currently in my sophomore year in college and will have many of the ""computation"" math-courses finish by the end of this semester. I have completed Calculus I-III and Introduction to Statistics (Not Calculus Based), and will finish Introduction to Linear Algebra and Discrete Math when the semester ends. I currently have 3.93 GPA and tutor at my college which I believe might help get interviews. The real problem is that I'm lost about how to look for possible internships with my particular interest. I am aware that not having some proof-based classes or higher level classes will hurt my chances, but I'm personally looking for any kind of internship that will help me get started with working in the job industry for any type of Statistics-focused job. 

Another reason for posting here is that I am curious what internships you did during your Undergraduate, so I can address my current options.

Thank you for any advice."
Stats Project: about GPA,0,0,False,False,False,statistics,1511077590,False,
I figured out a way to use any probability to simulate any rational probability. Is this anything new?,0,1,False,False,False,statistics,1511104714,True,[removed]
"STAT's Problem CFI = 1, RMSEA = 0, Chi-Square less than DF",0,1,False,False,False,statistics,1511117698,True,[removed]
Tests to Reduce Number of Predictor Variables,0,1,False,False,False,statistics,1511123824,True,[removed]
Looking for fun/out-of-the-ordinary time series data.,5,13,False,False,False,statistics,1511125030,True,Gotta do a project to finish my graduate time series class. Want either the most epic or most boring time series data set. Looking for suggestions.  Anything with trend/seasonality that's out of the ordinary is welcome! Thanks in advance :)
"Biostats Project Survey (fill out if you can please, thank you)",0,0,False,False,False,statistics,1511133068,False,
Variable type and testing question?,0,1,False,False,False,statistics,1511150798,True,[removed]
Choosing a Masters in statistics program,4,4,False,False,False,statistics,1511152420,True,"I'm trying to get into a Masters program in Statistics, and below are a few that seemed interesting. I was wondering if anyone who has any feedback, experience, or heard anything on the programs. I""m already full time employed as an analyst,  so I'm not getting it to start a new career. Rather, I want to advance my analytical skills and  and step into a data science role. I don't plan to get a ph.D, but I would like to have a strong foundation and learn more than just interpreting results.

With that said, here's the schools I found, with some comments from me.

Penn state Masters in applied statistics:
https://www.worldcampus.psu.edu/degrees-and-certificates/applied-statistics-masters/overview

This one seems interesting.  My only concern is this part of the description
The master's program is designed to help you develop your data-analytic skills and explores the core areas of applied statistics (DOE, ANOVA, Analysis of Discrete Data, MANOVA, and many more) — **without delving too deeply into the foundations of mathematical statistics**.

Is this something to be worried about?  I want it to be rigorous enough that it is respectable.

Texas A&amp;M Masters in statistics
https://online.stat.tamu.edu/

I've heard good things about this one, but I also read reviews that it is very old school so you don't get to use software such as R and more of the 'modern' techniques.

Rochester Masters statistics
http://www.rit.edu/ritonline/program/APPSTAT-MS
I like this one because it has 15 hours of electives, so I get to specialize.

North carolina state masters statistics
https://online.stat.ncsu.edu/online-programs/online-masters-degree-program/#CourseReqs

Don't know much about

Idaho state MS in statistical science
https://www.uidaho.edu/sci/stat/academics/online-ms-degree

Kentucky state
https://stat.as.uky.edu/mas

Colorado state
https://www.online.colostate.edu/degrees/applied-statistics/

Does any one have any thoughts on any of these schools, whether it's through experience or through gossip?  If you had to choose a top 3, which one would it be and why?"
MS level stats student - is it possible to do research outside of the statistics department?,14,10,False,False,False,statistics,1511155964,True,"Lately I've been wondering if it's possible for someone in a statistics program to be able to do research under outside professors, such as biology, as I am more interested in the applied settings of statistics rather than the methods themselves. I've tried looking online for scenarios like this but it has been difficult to find any information. My first step was possibly cold e-mailing professors and asking if they are willing to take a stats student under their supervision.

Have you guys ever done research in a setting like the above scenario? Did you have to have lots of domain knowledge prior to getting in touch with professors? Any advice on how to go about it?"
Please Help Me Understand Paired Sample vs Independent Samples,1,0,False,False,False,statistics,1511156568,False,
SPC for automated data,13,2,False,False,False,statistics,1511163152,True,"Most of the examples i can find uses sampling. However, the data available are individual measurements which are automated. My task is to apply the western electric and nelson rules to detect data anomalies. 

1. Is there an advantage to the use of subgrouping when i have individual measurements? (shewarts individuals chart)

2. They've already defined the limits as to when a unit passes or fails. Do i use this as the control limits or stick to sigma/moving range?

3. I've never used a realtime spc software. How does it work? Am i supposed to recalculate the limits and mean every time a new data point comes in? Or do I have the 6 sigma fixed?"
Can somebody suggest an instrument to test working memory (has to be used in an academic article/journal),5,0,False,False,False,statistics,1511168594,True,
Exam Review help Please.,1,0,False,False,False,statistics,1511176948,True,"https://imgur.com/lQQ31qd

Hey I missed the class that my professor gave the answers to this review. These questions are confusing to me, because they seem unreal, or dumb. Like 10b will still be 20%, right? Or am I seriously not understanding this?"
Reporting follow up cronbach's alpha,7,5,False,False,False,statistics,1511182426,True,"Quick and simple question I could do with advise on.

When using a questionnaire that is many decades old, in a country that it was not designed in, is it important to re-assess the cronbach's alpha coefficients, even when they were stable at the time of factor construction (60s and 70s)?"
How do I compare the effect of an IV on a DV when accounting for gender?,4,3,False,False,False,statistics,1511189685,True,[deleted]
A Little Levity,7,214,False,False,False,statistics,1511190978,False,
"Looking at a series of two-way interactions: looking at them individually, they are significant, but together in the same model, only one is.",9,1,False,False,False,statistics,1511200847,True,"How would I go about understanding this? Would this imply the two that are not significant are spurious when accounting for the others? Or is it correct to only examine them individually?

Sorry, here is more information:

The model being estimated is a logistic regression predicting whether individuals reported a previous incarceration experience. I am interested in three terms: social status, whether the offense occurred at their job, and the type of offense. The equation looks as follows:

logit(prev. incarceration) = b0 + social status (categorical, 4 levels) + current offense (dichotmous) + job enabled (dichotomous) + controls 

The idea is to determine whether there is a relationship between these characteristics and a variety of outcomes (hence the bad temporal ordering).

And then when examining the interactions I add the proper terms (i.e., social status ## current offense). When examining each individually they each appear to be significant. When looking at them all in the same model, only the status by offense interaction is significant.  

Thanks!"
Time series analysis,4,0,False,False,False,statistics,1511208967,True,"Does anyone know good sources (i.e. easy to comprehend) to study time series based forecasting? I have searched online multiple times but couldn't find a good source. I even did the udacity course on time series analysis but didn't get the feel of it. So, it'd be really great if you could help me with it. Thank You"
Looking for spss help.,0,1,False,False,False,statistics,1511217969,True,Not sure how to do a mixed anova with a within subject factor. Have a scale of 5 social media platforms and the customers preference toward each. Also have a scale measure of household income. Would like to run a comparison measuring each bracket of income to it's preferred social media. Any help on this is greatly appreciated.
"So I created a Mokken Scale, now what?",0,2,False,False,False,statistics,1511229027,True,"So I’m trying to explore IRT and Mokken scales, I think I’m getting the hang of it. I get that some items can provide more information than other items and that there are rules to follow to examine this (unidimensionality, monotonocity, etc). What I don’t get is that after I get my H coefficients and AISP to run with Mokken, how do I evaluate the data in a meaningful way? Sure I created an ordered scale, but do the H coefficients determine item weights? How can I sum a participants score on my newly created Mokken scale?"
"Need help understanding a ""discrepancy"" in an academic study",0,1,False,False,False,statistics,1511233069,True,[deleted]
YouTube,0,1,False,False,False,statistics,1511233369,False,
SAS output,0,0,False,False,False,statistics,1511234182,True,"Hello all,

   I ran a proc genmod poisson regression and have output and just want to make sure that I interpret it correctly. 
How would one report the model, and where would you exponentiate. 

An example of output:
https://imgur.com/a/870YN

All help is appreciated.

Thank you,
Boardgameguy123"
Need help understanding a discrepancy in an academic study,0,1,False,False,False,statistics,1511234309,True,"Link: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3830676/#!po=42.3077

The general overview of this study is that it's looking at romantic relationship patterns among young adults (ages 18-25). 5 general patterns were identified.

However, I'm a little bit confused about the data presented in Table 3, specifically the Discriminant Validity Relationship Variables. For the average relationship length for the Later Involvement group, the mean was 14 months with a standard deviation of 20 months. However, my point of confusion stems from the fact that this looks like it conflicts with the number of waves this group reported being in a romantic relationship, which was 1.3 with a standard deviation of 0.69. My understanding is that a wave represents 1 year, with 7 total waves in the study as points of assessment (7 years longitudinally, age 18-25).

So for illustrative purposes, two standard deviations above the mean for length is 54 months (4.5 years), but two standard deviations above the mean for wave nomination is 2.6 waves (so 2.6 years?). I would think that these two variables would match more closely than this, which I guess is why I'm a bit confused.

Basically, I'm asking if anyone might have a better understanding than me as to why this ""discrepancy"" might exist. For example, if it has to do with the statistical procedure itself or something else. The authors talk about the discriminant validity variables on page 12.

I know this is probably a really weird and maybe dumb question, but I would actually very greatly appreciate help. Even just a best guess on your part would be greatly appreciated."
Including or excluding weekend data points for ARIMA model,1,2,False,False,False,statistics,1511242141,True,"I have a data set of daily balances I wish to forecast using an ARIMA model where balances exist for all week days (with the exceptions of holidays), but are *always* 0 for Saturdays and Sundays. I have dummy variables in place for Monday through Friday, but since balances are always 0 for the remaining two days, I'm wondering if I should simply exclude the data for these days altogether and simply fit the ARIMA model for activity between Monday and Friday. "
"Those of you with anxiety disorders, is it possible to find a job in this field that isn't stressful and just pays the bills?",18,9,False,False,False,statistics,1511243323,True,"I'm a stats major with OCD and anxiety, which I am working on treating. 

Recently I've been applying for a lot of finance internships (mostly trading) and I'm concerned that finance may end up being a lot more stressful than I can handle. It would honestly be awesome if I could find a job vaguely related to stats that can let me work consistently but also have the time to work on my stress levels and work-life balance? 

I'm also interested in actuary, which I heard has better hours in finance. I'm wondering what you all think? 

"
Metrics for validating multinomial models,1,1,False,False,False,statistics,1511246275,True,"Hi everyone,

Currently looking at a model that classifies things into one of 5 labels. 

Does anyone have suggestions of distribution free ways of measuring performance that ia also interpretable? Looking for something like Area Under ROC curve like in binary. I've come across the F-measure used in machine learning, but no one seems to be able to explain why you would want such a metric in the first place; aside from the fact that precision and recall are rates.

Thanks."
Quick stats brain teaser I’ve been mulling over,22,9,False,False,False,statistics,1511254779,True,"You have 100 cards numbered 1-100. You randomly pair all of the cards (all at once, not one by one). Whichever of the pair is a higher number is considered to be a ‘winner.’ On average, what percentage of cards from the upper half (51-100) will be considered to be ‘winners?’

I feel like I could have solved this pretty easily back in my college days but it’s just been too damn long! I would love to hear an answer to this and how you arrived at the solution. 

Thanks in advance!

&amp;nbsp;

Edit:

By doing (50/99+51/99+....+98/99+99/99) to get an EV then dividing it by 50, I've come up with 75.75% as the answer but it seems too damn simple and I get the feeling I'm doing something wrong.

"
State of Content Marketing: 2017 Blogging Statistics &amp; Data,0,1,False,False,False,statistics,1511258113,False,
SPSS Help,0,1,False,False,False,statistics,1511274667,True,[removed]
Superhuman Defenses Fight Fatigue,0,1,False,False,False,statistics,1511278669,False,
Sensitivity and Specificity,1,0,False,False,False,statistics,1511279413,True,"If a clinical test for a disease had the same sensitivity and specificity, what is the significance? (sensitivity=specificity=x)"
Issue with convergence with SGD with function approximation using polynomial linear regression,0,3,False,False,False,statistics,1511282877,False,
Doing linear regression the right way - The Data Scientist,0,1,False,False,False,statistics,1511287813,False,
"can you guys help me with this? I don't understand how to do this if my life depended on it! I am desperate for help, seriously",4,0,False,False,False,statistics,1511287984,False,
What is the tragedy of the commons?,1,0,False,False,False,statistics,1511288794,False,
"Trouble Understanding Notation in ""Intro to Statistical Learning"", keep going?",8,5,False,False,False,statistics,1511293258,True,"I'm trying to learn statistics and apply it with R. I've started reading chapters one through three of ISL and am having difficulty understanding what all of the notation represents. Conceptually I think I understood k-nearest neighbor and linear regression. I got all of the Stanford course quiz questions correct, but the notion is difficult for me to fully grasp. Should I start with a more beginner level book? Is understanding the notation critical, or should I try and do some of the R coursework?"
Need help with ANOVA assignment,0,1,False,False,False,statistics,1511294988,True,[removed]
Applicable test,0,1,False,False,False,statistics,1511299197,True,[removed]
One categorical IV with multiple categorical DVs,0,1,False,False,False,statistics,1511299951,True,[removed]
ELI5: Why do we use confidence intervals and p-values to draw inference (incorrectly) when we have Bayesian Statistics?,55,47,False,False,False,statistics,1511305539,True,"People attempt to draw conclusions from confidence intervals all of the time such as ""my confidence is small =&gt; my point estimate is precise"" and ""I have a 95% confidence interval =&gt; Pr( parameter \in CI) = 95%"".  So the reason these two statements are inaccurate is because CIs are really a frequentest a priori kind of argument, where the statements above are attempting to apply a Bayesian understanding to the world.

This phenomenon is really nicely described at length [here](https://link.springer.com/article/10.3758/s13423-015-0947-8).  The author even goes as far to say ""[So...] how does one then interpret the interval? The answer is quite straightforward: one does not"". So I read this paper and felt very intrigued by the idea, and definitely have bought it in full.  Yet it seems absurd to me that so many statisticians and laymen (this interpretation actually appears in some textbooks, see the above paper) would still use this interpretation if the theory behind it suggests pointedly that it's wrong.

So I ended up asking my econometrics professor about why we learn confidence intervals when they seem strictly inferior to Bayesian approaches to draw conclusions about data, and he told me that it has something to do with the Bernstein-Von Mises, and that the two are roughly the same thing.  

I don't really understand the theorem or the line of reasoning that he derived from it, so hence I came here to see if people can explain the topic in a simple to understand manner like the viewpoint presented in the paper linked above.

Thanks in advance!"
Domain Knowledge vs Stats Knowledge in Career,0,1,False,False,False,statistics,1511308544,True,[removed]
"I think this has to do with Conditional Probability, but help me articulate",2,0,False,False,False,statistics,1511311810,True,"Hello all, I am trying to explain the validity of my logic; Say we have 10 marbles. I need to filter out the bad marbles. I am currently using one criteria, is the marble round. This works pretty great, but if I add another filter step in I think I can bring up my yield with my roundness test. Lets add a step for identifying cracked marbles. Is there a good way to explain this to someone who doesn't have a background in stats/math? Is there a concept or wiki article I can reference so that I don't look like I made this up? Any ideas appreciated. See table below:


 | Start Step 1 | Not cracked, survived | Yield % |  | Start Step 2 | Round, survived | Yield %
---|---|----|----|----|----|----|----
No Step 1 yet | 10 | 10 | 100% |  | 10 | 4 | 40%
50% capture rate for Step 1 (50% of the not round marbles were actually cracked) | 10 | 8 | 80% |  | 8 | 4 | 50%
"
Help with eigenvalues,1,3,False,False,False,statistics,1511312943,True,"This is definitely not a homework question. I've been studying the acoustic properties of coins as a personal hobby related to coin collecting. I found this [fascinating article by a French professor](https://www.researchgate.net/publication/276297204_The_music_of_gold_Can_gold_counterfeited_coins_be_detected_by_ear) about how to determine if coins are genuine or counterfeit. This methodology is superb and I even tested it with the type of coin used in the paper and it works perfectly. However, it only relates to a specific type of coin of a specific composition. I'm creating a spreadsheet to allow me to enter any coin's properties to recreate this method. I'm almost done but I'm stumped by the last part which involves eigenvalues. If anyone has any background in that area of math, could you take a moment and look at Table 3 and explain how I could solve for λ 2,0 , λ 0,1  , and λ 3,0 and the corresponding frequencies? "
Confusion about regression and exponential smoothing,1,1,False,False,False,statistics,1511317984,True,[deleted]
Small cell sizes for dichotomous variable,0,2,False,False,False,statistics,1511321421,True,"I ran cross tabs on my categorical covariates by my ordinal outcome. I found that for one dichotomous 0/1 variable that I had ~100 1's and 2 0's. I'm assuming that inclusion of this covariate would reduce statistical power. 

I removed that dichotomous variable from my model since it wasn't my exposure of interest and not a confounder. Forward and backward elimination gave me more confidence in removing the variable. But am correct to remove that variable if I only based my rationale by small cell sizes? "
Moderated Mediation Hayes PROCESS Model 7 Questions,0,1,False,False,False,statistics,1511321439,True,[removed]
Testing for differences between two groups where the groups are not independent and not paired,3,3,False,False,False,statistics,1511325528,True,"Generally if I want to measure a group of interest from last year to this year and test for significant changes, usually my group changes - some from the previous year leave and aren't in this year's dataset, some new people arrive who weren't in last year's dataset.

As I see it my options are:
(1) paired t-test on the group members who were there for both years
(2) two sample t-test on the group members who weren't there for both years

Let's say ignoring the lack of independence in the two groups is not an option - that this year's group has x% of last year's group, where x is a high number.

Is there something that would combine the result of (1) and (2)? I feel like there should be literature on this but my searches are leaving me empty.

Thank you!"
"How relevant are log likelihood, wald and rao test in the age of k-fold cross-validation?",12,1,False,False,False,statistics,1511343664,True,"Is there any reason for using them, when we can simply use k-fold cross validation on couple on models and then select the best one?

I could imagine using them for getting some kind of feature importance and confidence intervals, but I still can't make up solid argument, why CV would not be more reliable."
"PCA following EFA, purposes and interpretation?",0,4,False,False,False,statistics,1511367877,True,"I'm reading a paper [(He, Van de Vivjer, Espinosa &amp; Mui)](http://journals.sagepub.com/doi/abs/10.1177/1470595814541424) and the way they describe some of their statistical methodology is a little unclear. I was hoping I could get some help on whether or not I'm interpreting things correctly. 

This paper is trying to identify a general factor for three of the primary ""response styles,"" acquiescence, extreme, and middle responding. They use data from 8 surveys across 30 countries. There is overlap of the countries represented in the surveys. In each survey they evaluate they have a number of items used to measure each response style. Response styles are measured individually and then averaged for a country level index for each style. These index scores are not reported (no country level data is reported).

They describe the EFA with a single sentence. ""The identification of the GRS was addressed in an exploratory factor analysis with the ARS, ERS, and MRS indexes per survey."" They then go on to describe how they followed up with a PCA. I've never followed up an EFA with a PCA. Is that a common approach anywhere?

The second question I have concerns their PCA. They say they conduct a PCA and it reveals a single factor *in each survey*. They argue this is the GRS factor. Since each survey is designed to measure just one thing, and they extracted a single component, wouldn't the parsimonious explanation be that they are still measuring the substantive construct, and not the GRS factor? Their support for that is that the three response styles all load in the same direction on all surveys: ERS loaded positively, MRS and ARS loaded negatively.

I feel like the explanation of what they did is lacking here because it is lacking in the article. Please let me know if I can provide more information. "
"If I've never learn to code before, should I stick with SPSS or should I invest my time with R?",65,23,False,False,False,statistics,1511369948,True,"Thanks for clicking. I'm currently a Dermatology Research Fellow and my role is to publish as many papers during my time here. I didn't pay too much attention to stats or biostats during undergrad and grad school, respectively. 

I'm currently looking at these two options and was wondering which is the best choice to go with. I've never coded in my life before, so wondering if that'll be a disadvantage for me with R. Many of the scientific publications which I see on PubMed use either SPSS or Minitab for their stats. "
how do i derive two original distributions if i have a distribution of the differences between the original distributions?,2,0,False,False,False,statistics,1511372076,True,"If I have the mean and the std for the distribution of the differences between two distributions, how would i reverse engineer the mean and std for the original two distributions, assuming that the means of these two distributions are symmetric about zero?

For example, say that I have a difference distribution with a mean of .20 and a std of 0.10 and I know that the mean of one of the original distributions is -mu and the mean of the other original distribution is +mu with equal std.

* Could I derive the mean of these two distributions by simply dividing the difference distribution mean, .20, by 2 and assigning the appropriate sign?
* Similarly, could I derive the common std of these two distributions by dividing the difference distribution std, 0.10, by 2?"
What test do I need for correlation/statistical significance for these two?,7,1,False,False,False,statistics,1511374025,True,I have measurements of methane (g m^-2 d^-1) vs their depth (m). What statistical test do I need to do to find the significance between them?
How statistic lies?!?,7,0,False,False,False,statistics,1511374646,True,[deleted]
Issue while importing data from Excel TO SPSS,1,0,False,False,False,statistics,1511383690,True,"I'm trying to import data of a poll from excel to spss but some holes appear in the data, like some questions weren't answered but they were in the excel, it's just when passing it to spss that they dissappear. Any help? "
"What does it mean if school donations have a mean of $400, an average of $1500 and a standard deviation of $3500? I am not understanding why the std.-dev. is bigger than the average?",14,0,False,False,False,statistics,1511383824,True,"median, not mean"
Help! Is using a repeated measures ANOVA okay here?,7,11,False,False,False,statistics,1511393314,True,"tl;dr What statistical test do I use to compare two treatment groups that are measured every year?

I'm trying to understand what types of ANOVA test to use in different situations.

I'm using a data set from a 14-year manipulation of an experimental site in Arizona. The researchers set up 24 separate plots, and applied different treatments to them: some had all ants removed, some had kangaroo rats removed, some had all rodents removed, and some had combination of two of the treatments. Plant abundances for both winter and summer annual plants were measured for each plot using 16 random quadrats. All plots had the same treatment for all 14 years of the study.

I'm interested in determining the effects of ant and rodent absence or presence on annual plant total abundances. I'm hypothesizing that the presence of granivores will lead to lower abundances of annual plants.

I first thought that I just wanted to use a repeated measures ANOVA. My between-subject effects would be ant and rodent presence/absence. And then I would need to account for annual variation (hence the repeated measures ANOVA) and maybe the effects of plot too.

But something I read online that said this: ""However, the fundamental difference is that in a mixed ANOVA, the subjects that undergo each condition (e.g., a control and treatment) are different, whereas in a two-way repeated measures ANOVA, the subjects undergo both conditions (e.g., they undergo the control and the treatment).""

To me, that makes it sound like I need to use a mixed ANOVA, since in the experimental setup I'm using, any individual plot either gets the treatment or the control for the entire 14 year period, but doesn't get both.

So what kind of model do I use?

This is all done in R."
Assistance with Econometric Regression,0,6,False,False,False,statistics,1511400195,False,
"When do Shapiro-Wilk test, what is minimum sample size?",10,10,False,False,False,statistics,1511404882,True,"I've read that n=3 is all you need to do the test. But does such a small sample allow you to be confident in the results of the test? I'm applying the test to a dataset where n=5 and getting p=0.9338. With such a small sample size, how confident can I be that the dataset really does have a normal distribution?"
Histograms for Discrete Probability Distributions,0,1,False,False,False,statistics,1511405246,True,[removed]
Instagram Story Viewers Order Analysis,0,1,False,False,False,statistics,1511408564,True,"Hi everyone, I have created a spreadsheet to analyse on how a user's interaction with another user will affect the list of viewers order of a story.
Below is the link to the spreadsheet, any inquiries or questions regarding this analysis or the methodology of it you can reply below.
https://docs.google.com/spreadsheets/d/158Dx_oeaEm_dpyYCyv_Tgohey2yM8eBE0FFTTcNDF3Y/edit?usp=sharing"
Using Standard Deviation to measure efficiency of software bug fixes?,6,4,False,False,False,statistics,1511412073,True,"Hi all,

I am tasked recently with creating a model to monitor the efficiency of fixing software bugs using time.

What I want to measure is:
- How long did we take to realise there was a bug in a software (i.e. time between bug introduction to discovery)
- How long did the dev team take to fix the bug?
- How long did the ops team take to deploy the fix?

In this model I am more concerned with consistency of the bug fixing process i.e. how consistent are we at fixing certain bugs. If there was a particular fix that longer than usual, we should be able to single it out and find out why it took so long. Perhaps, there is a particular bug that we are bad at fixing etc. The statistical model will be a live model and we'll continue to feed it data (i.e. time) the more bugs we continue to fix.


Ultimately, I want to be able to show a chart or diagram to my customers to provide assurance that we are indeed very competent at fixing bugs in our software and that we have a good process and can do it consistently, regardless of the bug.

What is a statistical method I can use to measure this time taken in the bug fixing process? I was thinking of using Standard Deviation. Is standard deviation applicable to such a scenario?
"
[D] How to prove my handmade formula.,0,1,False,False,False,statistics,1511412475,True,[removed]
When is correlation(qualitative) between independent variables not important?,0,1,False,False,False,statistics,1511438114,True,[removed]
Finding the right sampling type,3,2,False,False,False,statistics,1511439453,True,"After sending out surveys to specific companies’ employees where I want to say something about the whole population (job market) I have a hard time telling if probability sampling or non-probability sampling was used. All employees could participate in the online survey, but it was limited to 72. Not a lot of prior work to finding the companies was done.

Thanks for your help."
Earth mover's distance,0,1,False,False,False,statistics,1511439515,True,[removed]
Statistical test to compare 2 proportions?,2,0,False,False,False,statistics,1511441410,True,[deleted]
Can you safely ignore multicollinearity between IV(nominal) on categorical data analysis?,4,1,False,False,False,statistics,1511443927,True,"im doing a research right now, and i have dependent variables(fatal, nonfatal injury, no injury) and i just want to check if the independent variables(nominal) i found are siginificant with the dependent variable.  "
Filtering out noise/insignificant data when testing millions of contingency tables for association,0,1,False,False,False,statistics,1511451166,True,"I have a large dataset of around 20 million 2x2 contingency tables, as below:

         Y=1  Y=0
    E=1   a    b
    E=0   c    d

I want to measure the effect that exposure (E) has on the expression of a trait (Y). The trait under test is always unique for each exposure. For this I'm calculating the absolute risk (AR) for each contingency table:

    AR = a / (a+b) - c / (c+d)

Here's a sample of the data:

    group     a     b     c     d    AR
      A       3     0     1 55559  .999
      B     566  1799  1683 51515  .208
      C      33    55    85 55390  .373
      D       9     5    13 55534  .643
      E    1155  4282  3596 46540  .141
      F       1     0     1 55561  .999

The problem I'm having can be clearly seen in samples from groups A and F. These groups obtain an extremely high AR-value, with only marginally small sample sizes. Manual checking shows that these samples should be considered noise, and obtained their very high AR by accident. Of course, this is bound to happen with ~20 million samples.

To resolve this issue, I've tried using Fisher's Exact Test and Pearson's/Neyman's Chi Squared Test to obtain p-values for the null hypothesis that there is no association between exposure and expression, the reasoning being that with such a small sample size of the exposed population, we could not conclude with a very large probability that there is an association. I subsequently apply the Benjamini–Hochberg procedure to control for false discovery rates (FDR) to obtain q-values, and filter out all groups with a q-value &lt; 0.05.

Unfortunately, this does not yet resolve my issue. I would have to set my alpha much lower than 0.05 to control for the 'noise' I'm getting, which seems like an invalid practice. I do know the distributions of a, b, c and d but I'm at a loss about whether I can use these, and if so, how.

So, the question is: how do I control for groups which have a low exposed fraction, and make sure they either don't obtain a high AR by accident, or are filtered out.

Thanks!"
Probabilistic graphical models: parameter estimation and inference algorithms,0,31,False,False,False,statistics,1511454227,False,
Stats Project Using SCF Data? (Grad),0,1,False,False,False,statistics,1511455032,True,"Hello!
I'm a graduate student in a public policy statistics course. Our final project for my statistics course is a bit open ended, and we have this codebook which contains Survey of Consumer Finances from last month. My policy focus is in International Trade &amp; Finance policy, so I'm looking at the various financial variables available but I'm not sure what to look for. This project is just going to use simple multiple regression analysis. Any advice would be helpful! Thanks!"
"Entry-level/101 question: Testing out-of-sample prediction of a linear model, need help interpreting r-squared/slope of regression lines.",1,0,False,False,False,statistics,1511458432,True,"My dataset is four years of chess games from around 500 players, and I'm testing to see if the first three years of time spent between moves is predictive of time spent between moves in the fourth year.

Fourth year ~ first three years has an r-squared of .24, which, as I understand it, is low, but not necessarily meaningless when dealing with humans/human variables, and the slope perhaps indicative of moderate correlation (?), coming in at .5995.  

I would like to know if there's utility to using the past numbers to attempt to predict the future and then investigate where variances lie -- both for improvement, for potentially detecting cheaters, for learning opportunities, for who might be better suited in a different time control. Is an r-squared of .24 enough to use it as a ""prior"" in a regression?"
Manufacturing statistics questions,0,1,False,False,False,statistics,1511458549,True,[removed]
Excel or SPSS,12,2,False,False,False,statistics,1511460424,True,"Hi,

I will begin to learn statistics again. I used SPSS in the college. But I found it very bad to use because of its user-friendly and user-interface. 

I'm an advanced user of Excel 2016. And found some add-ins such as [this](http://www.excel-easy.com/data-analysis/analysis-toolpak.html).

So, I can do anything on Excel without using SPSS such as ANOVA tables, chi-square test, F-tests, regressions etc. Graphics and charts of Excel 2016 are awesome compare to SPSS charts and graphics.

To sum up, I'm thinking Excel is better and user-friendly. Also, Excel 2019 will release in the mid of 2018 with powerful tools. But I'm not sure yet. First I need your opinions. What do you think guys? Which one is better? Excel or SPSS?"
How to calculate Chi-Squared distribution and Student's t-distribution in C,0,1,False,False,False,statistics,1511460502,True,[removed]
How can I estimate negative binomial parameters based on sample data??,0,1,False,False,False,statistics,1511462130,True,[removed]
Can anyone help?,3,0,False,False,False,statistics,1511473399,True,"I just need the prevalence rates for diabetes in the U.K. For 2015-2016 and 2010 by general population, age, gender, race and region. You think this would be easy like looking it up in the CDC web site for the U.S.  Any help would be welcome. Need it for a collage class. Thank you!!"
What is a good book that I can use to learn about Matrix Calculus?,7,2,False,False,False,statistics,1511475323,True,"I've taken calculus up through Calc III, I also took an undergraduate Linear Algebra class. However I never learned how to do things like take derivatives of matrices. I'm specifically trying to work through the Elements of Statistical Learning and some of the notation that they use, especially with taking the derivatives of matrices, is a little over my head.

Is there some book that I can read that will help me fill in some of the gaps?"
"I'd assumed the best measure of a coefficient's ""veracity"" was whether its 95% credible interval included 0. Is that an acceptable measure? Are there others?",10,5,False,False,False,statistics,1511486674,True,"Coming from a frequentist background (which still dominates my field), I am looking for some kind of measure as to whether the coefficients in a model fit using Bayesian parameter estimation are (forgive the term) significantly different than zero. In other words, how to tell if the results indicate that whatever effect a coefficient is associated with has a genuine effect on the outcome.

I had thought that looking at the 95% credible intervals for that coefficient was the way to do this, and whether or not they include 0. Is this acceptable? Is there another/better way?"
Survey Data Over Multiple Time Periods,2,2,False,False,False,statistics,1511488480,True,"A survey is given each time period that an educational program is run.  The survey tracks Independent Variables as well as a Dependent Variable. Sometimes a student returns in subsequent time period and takes the survey again.  Sometimes a student doesn't.  

So if we wanted to do a logistic regression...

1. Can we treat each participation event as a separate case, pool all survey data together, and control for number of time periods the student participated in program?

2. Or do we have to exclude the surveys of returning students?

Can you point me to some stat literature or keywords that can help figure out how to compose a sample?

"
Learning the Mode under Bandit Feedback,0,1,False,False,False,statistics,1511491159,False,
Correlation coefficient and the slope of the least-squares regression line,2,1,False,False,False,statistics,1511502971,True,"A topic I'm having massive confusion with. So generally slope is rise over run, which for regression would be the units of the response variable y over the units of the explanatory variable x; but the units for the slope of the least-squares line is calculated using standard deviations. Why did we start using standard deviation? And also, the fact that the change in our predicted response must always be less than the change in the explanatory variable with less-than-perfect correlation, I don't understand that. The formula for the slope is r(standard deviation of our sample data for y / standard deviation of our sample data x); so what if you just had a really large standard deviation for y? How can you still guarantee that the change in the predicted response will be less than the change in the explanatory variable?"
What are the odds of getting at least 4 sixes when rolling 5 die before The Devil Went Down to Georgia stops playing?,12,4,False,False,False,statistics,1511517280,True,"Yeah. So 3 of my cousins and I made up a dumb game where we have to beat 3 sixes (Satan)  before the song ends. We got obsessed with the math behind what our odds were once we started joking about Satan taking our souls if we didn’t win. We started calculating the odds of our rolls but then we realized we had to factor our time limit. That’s where we’re stuck. Anyone who’s bored want to help us out? Hahaha

Edit: The Devil Went Down to Georgia is 215 seconds long."
Statistical Significance and the Dichotomization of Evidence,17,55,False,False,False,statistics,1511522209,False,
"If I put a message in a bottle in a random place in the ocean, what's the chances of another person actually finding it? How long would it take?",8,0,False,False,False,statistics,1511549661,True,
"It seems like a histogram and an empirical characteristic function are very closely related, is it just that the characteristic function more or less interpolates the medians of the histogram bins, or something?",4,1,False,False,False,statistics,1511553633,True,"Sorry if my question is a bit confusing, I've not really taken any maths"
SAS output to LaTeX table,4,0,False,False,False,statistics,1511554177,True,"Does anyone know how turn output in SAS into a LaTeX table? It's not too hard at all in Stata, but I'm having a lot of trouble figuring out how to do it in SAS. Any help would be greatly appreciated - I'd highly prefer not to transcribe every single number!"
Granger causality testing. Real step by step walkthrough?,1,3,False,False,False,statistics,1511555933,True,"Hi,

In the frame of my bachelor thesis I have to test two sets of data regarding granger causality. Unfortunately I dont find any good explanation regarding how to actually do this online.

There are a lot of theoretical explanations but they never show what I actually have to do with the data I have?

So what I need to know:

Lets say I have 5 data points for x and y

x: 2, 4, 10, 8, 4

y: 2, 2, 6, 24, 16

I already included what I think is meant by lag if y is caused by x.

But what do i do from here? what kind of math do I actually do? 
I am really lost here and would be very thankful for some help :).

Cheers and thank you very much!"
Appropriate type of weighted model for work units?,0,0,False,False,False,statistics,1511560250,True,"I am trying to find resources so I can model and compare different types of work units based on a combination of complexity, effort, and urgency. Each work unit could have multiple parameters that increase complexity and effort within the model. I'm found a lot of weighted models online, but most are a reference to weighting and comparing business requirements so you can determine the appropriate trade offs. Thanks for the help!"
Wait for it stats fans,0,0,False,False,False,statistics,1511562004,False,
How can I explain this probability puzzle,11,6,False,False,False,statistics,1511562580,True,[deleted]
Loss rate at convergence - Gradient descent,4,7,False,False,False,statistics,1511603004,True,"Hello guys,
   I'm implementing the gradient descent. One thing come at me but I couldn't find the definition of it. I don't have luck with google so far. 

What is the loss rate at convergence and how to compute it? Thanks. Sorry if it's not appropriate to this sub."
Deciding on pursuing stats or applied math PhD.,0,1,False,False,False,statistics,1511604196,True,[removed]
"Statistical Anomaly, Joan of Arc and the number 333",0,1,False,False,False,statistics,1511613821,True,
Which statistical test for this hypothesis?,0,0,False,False,False,statistics,1511625021,True,"Hey. So I have an mixed experimental design with four conditions, 3 independent variables (including pre and post) and one main dependent variable. 

Hypotheses: 
1) There will be a difference in -dependent variable- when -independent variable- are specified. 
2) There will be a difference in -dependent variable- when -other independent variable- are specified. 
3) There will be an additive effect of hypothesis 1 and 2. 
4) The differences specified in hypotheses 1, 2, and 3, will be moderated by scores on a personality questionnaire. 

I know the first three (ANOVA), but would the fourth hypothesis (with a moderating effect) be an ANCOVA/regression? 

I haven't covered moderator analyses before.

Thanks!"
How to combine 2 or more categorical variables into one for frequencies purposes. [Novice] SPSS,10,5,False,False,False,statistics,1511625617,True,"Hello,
I have 4 categorical variables (disease diagnosis) who run over the span of 11 years, yes/no.
They make up a sum of about 2 million cases.
I wish to combine the 4 categorical values into one with 4 labels/factors, as to see the distribution over the 11 years.

Ive tried different approaches, e.g. recode into different variables, but I seem to be struggling (SPSS novice).

Any suggestions as to how to approach this?

Kind thanks for any help!"
Ecology statistics question,0,1,False,False,False,statistics,1511635377,True,
Can someone explain this statistics book?,12,0,False,False,False,statistics,1511639301,False,
Python: Graphing 10 Dice with 1 MILLION ROLLS EACH!,3,0,False,False,False,statistics,1511640130,False,
What do with only median and coef of variation?,0,1,False,False,False,statistics,1511654282,True,[removed]
"Im doing my final statistics project in my intro stats 1210 class, can anyone give me any ideas on topics in earth science?",13,2,False,False,False,statistics,1511660316,False,
I think my logistic model is overfitted even with Lasso? R,34,10,False,False,False,statistics,1511663733,True,"I have a survey with 25 variables. Most are categorical, some are continuous. The variable of interest is categorical (binomial). 120 observations. 

If I do:

model &lt;- glm(Y~.,family=binomial(link='logit'),data=...)

I get an error:
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 

I'm pretty sure there's no perfect separation, so the problem is probably overfitting right? 

1) I tried to do two things. The first thing I did was avoid any variable that gave me that warning message, this left me with 7 variables. I performed an AIC test and got it down to three:

model_noerror &lt;- glm(Y~sex+race+shoes)

Obviously I shouldn't be avoiding variables that give me a separation problem. So I looked it up and people suggested overfitting, which can be handled with LASSO.

2) So I used LASSO:

set.seed(999)
cv.lasso &lt;- cv.glmnet(x, y, family='binomial')
penalty &lt;- cv.lasso$lambda.min
fit.lasso &lt;- glmnet(x,y,family='binomial',alpha=1,lambda=penalty)
coef(fit.lasso)

LASSO told me 10 variables were worth using. But if I include them, **again** I get this ""0 or 1 occured"" error message. 

What could I do? 

*****

Edit: I've tried running a Principal Component Analysis. The results are not promising: https://i.imgur.com/Dt0uBHU.png It seems I can only remove a few variables

Here's what I did to get this PCA in R:

&gt;new_data &lt;- subset(data, select =-c(Y))

&gt;library(dummies)

&gt;new_data &lt;- dummy.data.frame(data)

&gt;pca.train &lt;- new_data[1:120,]

&gt;pca.test &lt;- new_data[121:161,]

&gt;pca.train &lt;- pca.train[,!apply(pca.train, MARGIN = 2, function(x) max(x, na.rm = TRUE) == min(x, na.rm = TRUE))]

&gt;prin_comp &lt;- prcomp(pca.train,scale.=T)


&gt;std_dev &lt;- prin_comp$sdev

&gt;pr_var &lt;- std_dev^2

&gt;prop_varex &lt;- pr_var/sum(pr_var)

&gt;plot(prop_varex,xlab=""Principal Component"",ylab=""Proportion of Variance Explained"",type=""b"")

I got an ""cannot rescale a constant/zero column to unit variance"" error without the third line, so I added that in to remove constant/zero columns. "
Hearts (cards) Probability Problem,0,1,False,False,False,statistics,1511670193,True,[removed]
[Undergraduate Statistics] Comparing two groups of frequencies.,0,1,False,False,False,statistics,1511674439,False,
"PKB in Europe, 1938. 100$ 2017 - 12.5 $ 1938. Włochy-Italia",0,1,False,False,False,statistics,1511697976,False,[deleted]
GDP in 1938,3,1,False,False,False,statistics,1511698601,False,
I made a test for an entry level course on SPSS. How would I best feed the data?,2,8,False,False,False,statistics,1511716063,True,"Greetings,

an university course invited me to construct a (online) test and to gather data, the next step would be feeding said data into SPSS for further analysis. Being new to the software, I don't really know what I'm doing, and am asking for some direction. English is also not my first language, please excuse my possibly unprecise terms and descriptions.

Let me describe the test first:

* It's about intuitive estimation of quantities (white dots on black surface). 
* There are 53 items (pictures of dots) in total. 
* Simple procedure: the participant sees every item individually in sequence, types his estimation into a text box, presses enter, next item.
* The order of the items is fixed and carefully chosen.
* The first 3 are examples: fixed presentation for 1600ms each and with feedback on the correct answers, just to get a feel for it and streamline the following estimations. 
* The latter 50 are each randomly presented either 1200ms, 1600ms or 2000ms. No feedback.
* The latter 50 are roughly sorted in subsets of ten in their appearance, but this isn't explicitly communicated (e.g. different sizes of the dots between sets, one set has different sizes in each picture, generel geometric tendencies etc)
* Within these sets there are four pairs with the same correct value: 1 random- &amp; 1 structured appearance (e.g. symetry, groups, other visual cues). And two ""strange items"" which feel out of place, e.g. suddenly very easy, or totally different appearance, to help with repetitiveness, habituation effects and to hide the structure. A picture should make it more clearly: https://i.imgur.com/A1eINCA.png 
* Every item has therefore a correct quantity (the amount of dots).
* In the end I'm asking about age and sex.

So in the end, every complete set (of 1 participant) has 53x the estimated answer, the correct answer, therefore the difference of these two, 53x the duration (first three are fixed, the rest varies), age, sex.

What I'm interested in is:

* The progress of the divergences ( x^correct - x^estimation ) as a whole, i.e. if someone becomes more precise or if the deviation stays constant etc

* The divergences of x^estimation between random dots and structured dots.

* The influence of the duration on everything.

* As well as interrelations between individual subsets and questions, this one is quite open and explorative.

My problem is, the course is focused on surveys with questions and rating scales which are very easy to code, I'm the only one who felt to be a unique snowflake. :D And I don't want to bother my prof everytime with my special construction.

So, dear people, how would I start, how would I feed the data into SPSS to not hinder myself later? How would I define a correct value, a kind of open answer format (1-99) with an absolute value, and can SPSS calculate differences itself like Excel? And how would I best connect a divergence *( x^correct - x^estimation )* with the size of the correct value because undershooting with 10 is far more significant when the correct value is 20 than when it's 85. 

As I said, I'm a rookie, this is the first time I even opened SPSS and I feel like I'm way over my head. Thank you so much.

^**Edited** **^some ^bits ^for ^clarity.**
"
Please suggest most user friendly free package to run vector auto regression model. Perhaps there is some free excel add in?,12,2,False,False,False,statistics,1511718516,True,
Strange Porto Seguro data on kaggle,2,4,False,False,False,statistics,1511725854,False,
Stata or R?,41,16,False,False,False,statistics,1511727051,True,"hey guys, I am a sociology student and which one would you suggest to study and why? "
Help with SPSS and Lending Data Analysis,0,1,False,False,False,statistics,1511727662,True,[deleted]
What's a pirates favorite stats software,5,0,False,False,False,statistics,1511733311,True,
GARCH DCC,0,1,False,False,False,statistics,1511739230,True,[removed]
Mann-Whitney U critical value,1,1,False,False,False,statistics,1511752627,True,[removed]
"How to find the distribution of E(X|Y)? X, Y are r.v's that are functions of an r.v. uniformly distributed on (0,1).",3,0,False,False,False,statistics,1511765908,True,"**X and Y are random variables such that:**

X= 

1 if *u* is an element of (0, 1/4)

-2 if *u* is an element of (1/4, 3/4)

-1 if *u* is an element of (3/4, 1)

--------------------------------------------------------

Y=

12 if *u* is an element of (0, 1/3)

u if *u* is an element of (1/3, 1)

-----------------------------------------------------------
And lastly U ~ **U(0,1)**

Any hint and advice on how to approach the problem would be appreciated! 

I'm a bit lost because Y is partially continuous and partially discrete... I think? 

Then combining Y into X|Y and it's the expected value of that .... "
Statics Assignment Help online for UAE students at best rates,0,0,False,False,False,statistics,1511775374,False,
Noob question-what is the best transformation approach towards a monthly dependant variable which is to be regressed against weekly exogenous variables?,4,5,False,False,False,statistics,1511794418,True,
Question - How do I account for multiple responses from the same individual?,6,6,False,False,False,statistics,1511797100,True,"I have a survey which is open to all individuals in a particular environment. Some participants are using it more than others. Overall, I would like to be able to show a percentage breakdown of scores using a percent bar graph. 

The problem is, some users are using this more than others. Is there a simple way to get the mean of their scores to show the data accurately? Or should I not be averaging their scores as they are taking the survey at different times?

Note: The survey is a psychological one which asks about distress.

Thank you!"
Quick Question: Calculating Average Percentage Change with Baseline,0,1,False,False,False,statistics,1511804246,True,[removed]
Raw moments vs. central moments,2,6,False,False,False,statistics,1511804320,True,"Correct me if I'm wrong, raw moments are distributed around 0 and central moments are distributed around the mean. Suppose we have a Pareto distribution and we want to find the moments. How would I go about calculating the moment generating function so that I can get the central moments? Not a homework question, just curious since I see that most resources give the moments in terms of raw moments and not central moments? Why is this? "
What kind of graph is this? What can I search to learn more?,9,13,False,False,False,statistics,1511810337,False,
"mean percent diff vs means themselves, losing significance",1,3,False,False,False,statistics,1511810710,True,How do I explain the loss in statistical significance with mean percent differences if there was significance with a paired t-test with means themselves?
"N00b Question: How do I check if two groups - one is length and has 10 values, another is also length, and has 200+ values, are correlated?",3,3,False,False,False,statistics,1511812998,True,"I'm really at a loss for what should be simple statistics. Which test should I be using? What does the resulting p value mean in simple terms? What should my null hypothesis be. I've tried several crash course statistics sets and they have not helped.

The language used is bizarre and the triple negatives of regecting the hypothesis when the hypothesis is already a double negative are doing my head in."
Best way to analyze change over time of a distribution when having a control group is not possible.,4,1,False,False,False,statistics,1511814957,True,"Assumptions: 

* Distribution remains a normal distribution over time

* There is 0 historical data to compare against


Thoughts were just to look at how mean decreases.  An example would be force a group of 30 students to study for 8 hours on a test, then force them to study 7, 6, etc.etc. to 0 but be unable to run a control group.  Look at decrease percentage of mean after removing any possible outliers using the IQRx1.5 equation.

I'd prefer simple &gt; complex, unless there is a considerable difference."
Why do Statisticians have so low salary?,64,16,False,False,False,statistics,1511815258,True,"Makes me wonder why people even wanna become a statistician, or if there is an underlying reason why the salaries are so low? I mean you aren't a statistician before you have a master's degree right? I just don't understand how the salaries are so low. Are there a lot of ""fake"" statistician or whatever you call it? statisticians without a proper STEM background that are reducing the salaries or what is it? 

---------------------------------------------

So I checked some salaries on glassdoor: Here is the national averages from each one: 

Statistician: $87k ($105k in SF). 

Actuary: $123k ($131k in SF)

Data scientist: $129k ($152k in SF).

Machine learning engineer /scientist: $128k ($151k in SF)

Quantiative analyst: $109k

Quantitative researcher: $161k ($170k in NY).

And all of the ones above here, you can do with masters-phd in statistics, right?

But this is the one that triggered me: Software engineer: $110k (130k in SF)

Not gonna lie, but I'd be pretty sad if I'm getting a master's in statistics, and getting myself through real analysis, measure theory and functional analysis, etc and not even be able to match national average for software engineering. That really makes me wonder if I chose the wrong thing. I'd preferably like to atleast match their SF salary...
"
Calculating Standard Error of Predicted Value,4,1,False,False,False,statistics,1511817688,True,"I have some regression output (variables with coefficients and standard errors) that I want to use to create a ""calculator"" -- in other words, you can enter in someone's attributes on each of the variables and it will create a prediction.

I also want to create a confidence interval around that prediction. How can I use the standard errors of each of the coefficients to also create a standard error of the predicted value? I am particularly looking for a formula, or another workaround that doesn't involve having all of the source data, only the output."
Stats Question Urgent Please Help Sampling Distribution on the Mean,0,1,False,False,False,statistics,1511819741,True,[removed]
"Where can I go to learn the very basics of ELO, and it's variants such as Glicko, and TrueSkill?",3,3,False,False,False,statistics,1511820573,True,
"Interpreting change scores, PLEASE HELP!",6,1,False,False,False,statistics,1511821013,True,"I had to calculate some change scores for my dissertation. I didn't even know what they were until my prof told me to do that in addition to my other statistics. I tested 2 groups, 1 got supplements the others a placebo and tested them before and after taking them. They filled in mood and stress questionnaires and I got only very slightly significant results for stress everything else is not significant but goes into a certain direction. So my professor told me to calculate the change scores to look more indepth at what is happening.

I was unable to find anything about how to interpret them online or in my stats book and my prof is not replying my email. I for example analyzed the change score for stress between the two groups and got significant results. But what does that mean now? That the supplements probably had a positive effect? Or that there is a difference between placebo and supplements? or should I look at the change score for the supplement group individually and for the placebo group and see if each one of those is significant or not? But if it is, what does it actually tell me?"
"[College Stats] T Distributions, No Work Shown Needed",0,1,False,False,False,statistics,1511825241,True,[removed]
Comparing best-fit values from different data sets in Prism,0,1,False,False,False,statistics,1511826205,True,"I've been trying to compare best-fit values (IC50s for the curious) between different treatment conditions (e.g., experimental compound A and control compound B). I'm able to take my data (log-transformed compound concentration vs normalized viability) and run a nonlinear regression using Prism to generate curves and an estimate for IC50s - no issues there.

The problem I run into is when I want to compare the calculated IC50s between compounds A and B to see if they're statistically significant. [I found this helpsheet from Prism that outlines how to do this](https://www.graphpad.com/support/faqid/144/) using ANOVA, but I am running into an issue I'm not understanding.

When I make a new data table using my antilog-transformed results (e.g., I take the actual IC50, rather than the logIC50 and use an [error propagation formula](https://chem.libretexts.org/Core/Analytical_Chemistry/Quantifying_Nature/Significant_Digits/Propagation_of_Error) to determine the antilog standard error sigma^antilog = IC50 * 2.303 * sigma^logIC50 ) as instructed in the Prism help document, I get statistically significant differences between my experimental and control compounds. 

However, when I repeat this using the log-transformed data that comes right out of the nonlinear regression, all of my comparisons become statistically insignificant. 

I'm trying to understand why that might be. My only guess right now is that in one of these scenarios I'm violating the ANOVA's assumption on equal variances, but I thought this method was relatively robust to that issue. Any ideas out there?"
"Hypothesis test, known standard deviation question",3,1,False,False,False,statistics,1511826282,True,"hey doing a hypothesis for the mean. I have a sample of 35 and I know the standard deviation of the original population. In the Z statistic, do I still use the sample standard deviation or the population? thanks"
Question: Visualizing the Probability of winning a sports match,3,4,False,False,False,statistics,1511827950,True,"Yesterday, the Canadian Football League just finished their finials and the game went down to the final play. If it wasn't for a forced fumble, the winning team was dead in the water. 

Long story short, I really want to visualize the probability of both teams winning throughout the game. I understand this is a complex ask, but what kind of data would be relevant to do something like this?

My initial thoughts would be using data from the full 2017 season:
- Avg. Field goal range by special teams
- Avg. Completed field goal percentage
- Avg. Yardage per play
- Avg. Turnovers per game
- Avg. Scores at the end of each quarter
- Avg. Red Zone Conversions
"
What can you say about the overall average and standard deviation of a set of averages,5,2,False,False,False,statistics,1511837162,True,"Lets say I have this set 



each player has played finitely many games but the amount is unknown and may be a different number than any of the other players. Is there anything I can say about the average gamer score or the standard deviation of the set? My goal would be to determine which players have an average score of more than 1 standard deviation above or below the mean. If I can't get to that, what can I say about the set?

Player| Average Score per game
------|-----------------
1|15
2|17
3|14
4|22
5|19
6|21
7|22


edit: if it matters, a game has a minimum score of 0 and no max"
"If you apply a non-parametric test when you have a normal distribution, are you making conservative estimates or inaccurate estimates?",6,3,False,False,False,statistics,1511838186,True,I'm reading a paper that used parametric tests when I think they shouldn't have. Is this problematic? Or is it a conservative move?
What are the consequences of treating a latent factor as observed?,3,2,False,False,False,statistics,1511850630,True,"Say you have three general factors derived from separate PCAs and believe they are part of a greater, general factor. What are the consequences of entering the factor scores (which represent latent constructs) as observed variables in a CFA? 

Edit: three, not two."
No Hesitations: Mostly Harmless Econometrics?,0,3,False,False,False,statistics,1511866341,False,
Do my statistics homework,2,0,False,False,False,statistics,1511871701,False,
What are you go-to statistics resources?,21,28,False,False,False,statistics,1511875668,True,"I am in grad school studying psychology and I am having a really difficult time understanding the approach to statistics that my professor has. I have tried using older textbooks (which have been helpful, so Gravetter &amp; Walnau if you see this, thanks), the recommended textbook (good book, just not for this class), and online resources.

The difficulty that i am having is that the professor is VERY theoretical. I took statistics in my undergrad and did very well, got the top mark in the class in my advanced stats course. Now I am barely (if at all) getting my in my graduate course. So I am wondering what resources you folks use when you have a hard time understanding concepts in statistics.

I am looking for some explanations of concepts such as ANOVA (mainly two-way/factorial), interactions, and moderation/mediation/direct/indirect effects."
Root Analysis Case Study help,1,1,False,False,False,statistics,1511883778,True,"So I've got a case study where I have to analyse the raw customer satisfaction data from a customer satisfaction survey. 

I've calculated the NPS so far for the 236 claims made shown in the spreadsheet given. However I've unsure how to analyse the key factors driving customer satisfaction (NPS). 

Due to the fact that there is one overall NPS and a number of factors to correlate with.

Does anyone have experience that could possible help?

Thanks a lot! &lt;3
"
From Data Visualization to Interactive Data Analysis,0,1,False,False,False,statistics,1511900180,False,
Good Probability &amp; Statistics Book To Refer To?,21,5,False,False,False,statistics,1511903882,True,"Hi all, I'm just about done with my Probability &amp; Stats course for my Masters in Statistics. However, I feel as I didn't learn as much as I would like from the class(I got by just by pure memorization and Youtube videos). I wasn't crazy about the professor's teaching method and the textbook was a heaping pile of trash. For something I can use as future reference for other courses or even some statistical work is there a book(s) someone would recommend that I can refer back to along the lines of what's discussed in your typical Prob &amp; Stats class. "
Sample covariance for unpaired data?,2,1,False,False,False,statistics,1511904252,True,"We have the following problem. There are two sets of samples, one from a normally distributed random variable X, and another one from a normally distributed random variable Y. Let say that we have 100 samples in each set (200 in total). We know that random variables X and Y are correlated and we have to estimate this correlation. For paired data we could use sample covariance. However there is no natural pairing for the samples since both sets of samples are just sets without any additional structure. Is there any way to approximate the correlation/covariance based on the unpaired samples? Is this theoretically possible? "
Quick stats question for a noob,0,1,False,False,False,statistics,1511905314,True,[removed]
Online courses or resources.,1,2,False,False,False,statistics,1511906770,True,"I am thinking of applying to a masters in Psychosocial epidemiology that has a stats prerequisite. There's an exam I must take to gain entry to the course but this won't be until May or June. When I asked the admissions board about the content of the exam they said that anyone who has a bachelors in Sociology or Psychology should pass.  What's the best way to get myself to this level in 6 months?  

My undergraduate was in Neuroscience, I don't have an extensive background in Stats. I've completed one Biostats module which I scored an A in so I think I have some aptitude for stats. The masters program is in the Netherlands by the way."
Biostastitics certificate through UCSD Extension,2,2,False,False,False,statistics,1511911608,True,"Hello! 
I am a grad student in biological science, and I really want to learn more, and become comfortable, with statistics. I took a class at my school, but it just wasn't helpful... i tried googling things to learn on my own, but I found myself needing more organization. Has anyone had experience with the biostat. classes or the certification program at UCSD extension? It's all online. If anyone has recommendation for other online courses, please let me know too! Any information would be helpful.

Thank you in advance!"
Question about election polls,4,1,False,False,False,statistics,1511913701,True,"Why do election polls report one margin of error for both major candidates if two percentages don't add up to 100%?

Is it because both margins of error round to the same number?"
Principal components and multicollinearity,2,1,False,False,False,statistics,1511916200,True,"Say I have parameters:
X1 = a, X2 = a+b, and X3 = a+b+c

Would it be better to transform those parameters to the following before using PCA:
Y1 = a, Y2 = b, and Y3 = c

I’m worried the PCA components formed from the X variables will overstate the impact of “a” during the subsequent cluster analysis. "
What statistics jobs don't include being at a desk all day?,36,31,False,False,False,statistics,1511916277,True,"I'm finishing up a statistics class at uni and I really enjoyed it. I'd like to take more classes or possibly take stats as my major. As for jobs after I graduate I can't see myself pinned to a desk all day, but stats and to lend itself to that type of work. What options do I have that would require just sitting at a desk all day? I'm still open to being in an office environment, just not stuck in a cubicle. "
How would you go about this problem? And what are the names of these actual functions?,0,1,False,False,False,statistics,1511935599,True,[deleted]
Dependent and independent samples,0,1,False,False,False,statistics,1511943221,True,[removed]
Overthinking this simple question #5. Help would be appreciated.,0,0,False,False,False,statistics,1511949228,False,[deleted]
SEM: Confirmatory Factor Analysis Help,0,1,False,False,False,statistics,1511950958,True,[removed]
What test do I use here?,2,1,False,False,False,statistics,1511951759,True,"So I've designed an experiment to measure how one factor (let's say overeating) is affected by future self similarity. So my hypothesis would be that the higher future self similarity a person has, the lower score on overeating behaviours would be shown. So this part could be shown by a simple correlation but I also want to see how anxiety levels affect this. I'm not sure what test to do for this?"
Seasonal adjustment in EViews 9: How do I know which method to use?,2,5,False,False,False,statistics,1511964330,True,"I have some time series data over the household savings ratio from OECD that is seasonally adjusted according to Datastream. However, this data was discontinued in 2007 so I need to link this together with some other unadjusted data that I have. There are several methods to use, TRAMO/SEATS, Census-X12, Census-X13, and moving average methods. How do I know which one to use? Thanks in advance."
Comparing negative and positive log likelihood values,18,5,False,False,False,statistics,1511966557,True,"Hi,

When comparing linear mixed models (SPSS in my case) I'm used to ""smaller is better"". How does this work when some models yield negative and some yield positive values?

Does smaller is better indicate that models yielding negative -2 log likelihood values (if significant when tested with df) are always better fitting than those yielding positive values or does ""smaller"" refer to the absolute number, as in ""closer to 0"" (e.g. 200 would be better than -300)?"
"I’m conducting a regression with GDP as my dependent variable and other independent variable’s (unemployment, CPI, etc) predictive value. How do I set up the null hypothesis? Ho: GDP is not affected by these independent variables? Or Ho: GDP is affected by these independent variables?",0,1,False,False,False,statistics,1511970009,True,[removed]
An alternate to 2 x 2 x 2 repeated design,0,1,False,False,False,statistics,1511975163,True,[removed]
"Need help!!! Can someone help me with number two, I don’t understand hypothesis testing. On the bottom is an example of how my professor wants us to do it, the top is the problem I’m struggling with.",0,0,False,False,False,statistics,1511976454,False,
Getting a masters with little math background. Is it possible?,11,14,False,False,False,statistics,1511983699,True,"Hi there

I was wondering I could get the input of people here on a plan that I have in hopes of getting a masters in statistics. I always enjoyed math and statistics but was always unsure what to study in college so I just decided to graduate from a good university with a bachelors in Communications and English. 

After getting my first job, I began to work with a bit of math and statistics and it is something that I have fallen in love with again and I am hoping that I could be accepted into a statistics masters program. 

I know this will require a lot of self study and taking prerequisite classes but I am determined. I was wondering what steps I should take? I took precalc in college and know that I will need Calc I,II, III, linear algebra, and a intro to statistics class to just even be considered for admission. I know I will also probably need some programming background. What else can I do to further improve myself and give myself a better chance at admission?"
Need SPSS Statistics advice!,2,0,False,False,False,statistics,1511987570,True,"Dear Redditors,

I am currently working on a statistics project by measuring memory. I am trying to find a way to compare 2 variables (Whether people feel more productive in the morning or evening, and when (morning or evening) they completed a certain memory test. I want to compare both of those variables to 3 different test results my participants have filled out.

What would be the best method in SPSS to compare this data?

Thank you very much!"
Does PhD Rank Matter?,5,0,False,False,False,statistics,1511987620,True,"Hey y’all,

I’m sure you’ve been asked this question before, but I’m a junior in college looking towards applying to Statistics doctoral programs.  I have a good GPA (3.75) in a Math/CompSci double major, with no grade lower than a B+ in any of my classes, and I go to a top 50 school in the U.S., however not one necessarily known for their math department.  Because of this, I am worried that I will be unable to gain admission to top programs.  

My question is, does ranking matter for PhD programs? If so, should I aim for top 20? Top 30? Top 50?

Thanks! "
Mann-Whitney U question,0,1,False,False,False,statistics,1511991335,True,[removed]
Calculating Log Odds of Y = 0 in Logistic Regression?,1,1,False,False,False,statistics,1511999986,True,[deleted]
What statistical test should I run?,3,2,False,False,False,statistics,1512008228,True,"Okay, I'll try to give a brief description of my project:
Basically, I am trying to determine if someone's perception of social mobility can affect how people feel about low and high-skilled immigrants. There were two conditions: a moderate social mobility condition and a low social mobility condition. In each condition, participants read a false report detailing social mobility in America. Then, they answered likert sales on low and high-skilled immigrants: 8 questions on low-skilled immigrants and 8 questions on high-skilled immigrants. The order in which participants answered the scales was counter-balanced to prevent order effects. Basically, how do I determine, on SPSS, if those in the low-social mobility condition and the moderate social-mobility condition differed on their opinions of immigrants? Do I first make a composite score for the low-skilled and high-skilled questionnaires? I'm just lost on where to start. I could ask my supervisor, who is wonderful, but I just feel like I am constantly bothering him so I figured I would give this a shot first. Thank you in advance xx"
Question about group equivalence between treatment and control groups,0,1,False,False,False,statistics,1512010025,True,
Is median survival using K-M analysis and simply the median value the same?,3,3,False,False,False,statistics,1512011600,True,"So if I have survival duration for a list of patients, is the median survival the same figure that I will get through Kaplan Meier analysis versus what I would get if simply calculated the median from an excel table?

If not, why not?

Thanks"
How should I calculate the standard deviation?,1,2,False,False,False,statistics,1512021593,True,"I am conducting an experiment where:

* z-scores are calculated for several different populations

* These z-scores are categorized into several categories

* A confidence interval is constructed for the average z-score in each category

What equation should I use to calculate the standard deviation of z-scores?"
"I was contacted about minoring in Applied Statistics (as a Marketing Major). The thing is, my first stats class is horribly boring to me and I'm worried about what it means for my future success.",13,10,False,False,False,statistics,1512023575,True,"Long story short; I am seriously considering a stats minor due to how useful it is in marketing. From the people I've talked to, statistics is a very attractive thing for employers to see. 

I am good at the material in my intro to statistics class (grade-wise), however, I find it very bland and difficult to comprehend without re-teaching myself from Google. My professor speaks quietly with a heavy accent, lectures from notes the entire time without involving the class, and just runs flies information like its a chore. I am able to use notes on tests, and I find ways to ""cut corners"" on homework so that I don't spent all night teaching myself (it's not like I cheat though). I dislike my current professor and her ways of teaching, but I am worried that maybe I'm just not actually cut out to handle a full minor in stats. 

I'm very intruiged by this opportunity, but I don't want to dive in head first and realize I'm only good at *passing* stats - not learning it and developing. Is there anyone who can shed some light on why I should stick with statistics, or provide a better perspective from their own experiences?

Note: I am unsure of how to ""flair"" reddit posts on mobile, but this I obviously a college question/ advice. Thank you!"
Handling big data in R (ff),0,1,False,False,False,statistics,1512024003,True,[removed]
"Personal notes about statistics and swift language, first post.",0,0,False,False,False,statistics,1512030479,False,
Is knoema.com a reliable source for German economic statistics?,1,1,False,False,False,statistics,1512035318,True,"Im looking for data for the Real GDP Growth in Germany. 
When looking at official sites like OECD (https://data.oecd.org/gdp/real-gdp-forecast.htm#indicator-chart) i am unable to find data from before 1992. 
On other websites like Knoema (https://knoema.com/atlas/Germany/Real-GDP-growth) they claim to have this data.
Is Knoema a reliable source of statistics? 
Or are there alternative sources from which i could find what im looking for?

Thanks to anyone who takes the time to reply"
Building a Brand Value Proposition through Statistically Designed Experimentation,0,0,False,False,False,statistics,1512040612,False,
Online intro to stats courses,8,1,False,False,False,statistics,1512056148,True,"Hello,

I am heading for a amateurs degree in financial engineering soon and would like to brush up on my stats. I would like to take an online course, a Coursera type, but am open to other things as well! I'd appreciate any suggestions. 

Thanks! "
How to rationalize my dispersion?,2,1,False,False,False,statistics,1512058230,True,"I need to split up my life expectancy group, which is numerical into categorical variables of low, medium, and high. How would I do this assuming that the mean life expectancy of the world is around 71 years old. How could I split this into 3 groups?"
Deciding whether grad school for stats is a good choice?,2,15,False,False,False,statistics,1512061273,True,"I am having a little bit of trouble figuring out which path I should be heading down. I am approaching my last year of undergrad and am currently a pure math major. Reading about machine learning and data science seems like a very interesting field and one I would really enioy. I’ve read that a PhD in stats is a degree that many employers look for when hiring.

The thing is that I really don’t know what stats is all about. As a pure math major I only took the two basic stats courses that are required for math students and engineers. It was essentially plug and chug formula classes. I’ve taken a basic probability course and really enjoyed it, I’ve also taken analysis 1 and 2 to set me up for a future course in measure theory. By graduation I will have completed two semesters of linear algebra, topology, algebra, and a handful of programming classes. Is graduate level stats work similar to analysis? I really enjoy analysis and and probability, and hear that measure theory is the backbone of probability. While I love math, I really enjoy the applied side, but I want to make sure I understand the theory behind the tools that I’ll be using. 

I am unsure on what programs to apply for, and if my background is under prepared for graduate programs in statistics."
How To Understand Projected Old-Age Dependency Ratio,0,0,False,False,False,statistics,1512061516,True,"Hi! I'm not very good at statistics, but I need to explain the Projected Old-Age Dependency Ratio for my economics class. 

If the Ratio is 52.3 for example, what does that mean? Main question is: How much working population supports the retired population?

 (Here is the whole table: http://ec.europa.eu/eurostat/tgm/table.do?tab=table&amp;plugin=1&amp;language=en&amp;pcode=tsdde511)

Projected old-age dependency ratio
Per 100 persons
This indicator is the ratio between the projected number of persons aged 65 and over (age when they are generally economically inactive) and the projected number of persons aged between 15 and 64. The value is expressed per 100 persons of working age (15-64).


P.S.: Very sorry for probably not being clear. I am really confused. If you need more info, I'm happy to supply it. THANKS!"
"(Get to Top, please!) I'm from Small Academy of Science and I'm conducting a research about txting. Please patisipate in questionnarie and help me to build statistics. (You can answer in comments by putting number and answer ( 1.A/B for example) (Some answers are possible)",0,0,False,False,False,statistics,1512067418,False,[deleted]
Need help with what test to conduct,0,1,False,False,False,statistics,1512072755,True,[removed]
Question about Bayesian terminology.,7,8,False,False,False,statistics,1512076561,True,"When reporting p values, I would say ""there was a statistically significant effect of ___"".

What is the shortest equivalent phrase for when a Bayesian analysis finds the 95% HDIs of a parameter don't include zero? Do people still use ""statistically significant""? Is there something else I should be saying that is only one or two words?"
Researchers Find Oddities in High-Profile Gender Studies,16,88,False,False,False,statistics,1512082642,False,
Plotting and Saving Histograms in Python,0,1,False,False,False,statistics,1512083492,False,
Question about R value,3,2,False,False,False,statistics,1512084144,True,So i’m in a business stats class and I have a question- can the R value =0? It is nowhere in our notes and my friends thinks it is impossible for it to equal 0.
Understanding Turkey-Kramer Post-Hoc Results?,6,1,False,False,False,statistics,1512087336,True,"[Here](https://i.imgur.com/SRW6HVG.png) is a picture of my results. 

I understand that test tells me the Ethanol and Isopropyl are similar to each other but different from the Glycerin and Formalin (which are similar to each other) but I don't understand the B's. "
"What is a standard way to convince somebody that two groups of data have ""no significant difference"".",4,1,False,False,False,statistics,1512095901,True,"Say I have implemented a program and after several days I refactored the code, the refactored code is expected to have the same performance with the old one. However, as some randomness is involved in the program and the randomness is hard to control(for example, by using the same seed), each run of the program would give a slightly different result. 

So, how can I convince myself that the new program has the same performance? I know that the p-value is often used to show that there is some significant difference, can I use p-value as an evidence of ""no difference""? For example, should I perform some repeated experiments, do the two-sample t-test, and expect the p-value to be above some threshold?
"
Can someone help interpret this stat density map?,10,0,False,False,False,statistics,1512099223,False,
Visualising SSH attacks with R,0,1,False,False,False,statistics,1512125092,False,
Data Analysis Help,0,1,False,False,False,statistics,1512127412,True,[removed]
"Which tools to use to establish equivalency between 3 measurement instruments, each instrument has 96 channels, each channel can measure an independent sample? How would I design an experiment for this?",8,5,False,False,False,statistics,1512135975,True,
Experiencing Data Analysis for Employment,0,1,False,False,False,statistics,1512136253,True,[removed]
Granger causality and size effect,6,7,False,False,False,statistics,1512144876,True,"In the frame of my bachelorthesis I am researching the causality between terrorists and media coverage. I am doing this as an extension work of an already published paper, including newest data. The main statistic principle conducted in this study is Granger causality (You can find the study here if you want to read more: https://www.jstor.org/stable/27698175?seq=1#page_scan_tab_contents)

Within this study they do granger causality test from 1998 - 2005. For the one nullhypothesis thes define a level of confidence of 95% and obtain a p-value of 0.018, showing you can reject the null hypothesis For the second nullhypothesis they choose a level of confidence of 99% and get a p-value of 2.3e-12, meaning this hypothesis can also be rejected.

Now I wanted to extend the analysis using the time frame 98 - 2016. I calculated granger causalities and now obtained

Nullhypothesis 1: p = 0.016 Nullhypothesis 2: p = 4.2e-4

So with the given levels of confidence I can still reject both hypothesis and therefore talk about a casuality between both factors.

Then, I thought that the changes in p-values also give me a statement about the strength of the causality, but then I heard that this isnt the case. By Granger you can only determine if there is a causality or not.

This is a little strange to me, because its hard to believe that there is no difference when you obtain p-values of 0.0499999 or 0.000000001 (with a level of confidence of 95%). You can reject null hypothesis in both cases, but is there really no other difference? I was told that I need to calculate the effect size in order to determine the strength of the causality.

When I read the paper, I found this passage:

We find, that the F-statistics for both direction of granger causality are even higher for fatalities in Western Europe and the US than for fatalities worldwide. The granger causality relation is significant in both directions at a level of confidence of 99%. Each terror victim in western democracies receives more media attention from the NYT than a similar death in a developing country.

From this, I understand that they actually assign a higher causality just based on the p-values and the usable levels of confidence. If they can go to 99% for both null hypothesis (before they used 95% and 99%) then the causality is higher. Since this is a published study, I find it hard to believe that this is wrong, but this is leaving me completely confused.

It would be awesome if you could help me out here.

Thank you all very much

"
Why do you study statistics?,25,8,False,False,False,statistics,1512150834,True,
What program should i use to make this funnel plot,7,1,False,False,False,statistics,1512155528,True,[deleted]
Lecture 11. The chi-square test for goodness of fit. - ppt download,0,1,False,False,False,statistics,1512156180,False,[deleted]
Prereqs before going through Statistical Inference by Casella and practice problems?,6,5,False,False,False,statistics,1512156829,True,"I've been teaching myself math for a bit and am looking for a rigorous treatment of statistics and I've heard great things about Statistical Inference. The last statistics class I've taken was in high school, so I know a little bit but you can assume that I'm new for the most part. What topics should I be familiar with (multivariable calc, proofs, etc.) before beginning this book? Should I learn statistics from a less rigorous book first (if so any suggestions) or is this fine as a first text for statistics assuming I am familiar with the other kinds of math in it.

And lastly does the book contain problem sets or are there any out there that correspond with the book?"
A Failure to Heal - The New York Times,6,28,False,False,False,statistics,1512158420,False,
Five ways to fix statistics - a Nature comment,3,16,False,False,False,statistics,1512166286,False,
What program should I use to make this plot?,10,0,False,False,False,statistics,1512170044,False,
SAS/SPSS vs python/R,11,6,False,False,False,statistics,1512172152,True,Is there anything besides support that one gets with these paid statistical languages/frameworks? I'm having to learn SAS for my program and as a long lover of R (and its rich frameworks and community support) I'm feeling a bit confused. Certainly not blown away by it at its price tag and some of the syntax is giving me the eyetwitch. So I'm just wondering if I haven't gotten to the real juicy stuff yet or what.
"help with it, someone, is for my thesis",0,1,False,False,False,statistics,1512178236,True,
The trump problem,35,408,False,False,False,statistics,1512212741,False,
Grad school help,5,1,False,False,False,statistics,1512213899,True,"I am a stats grad student, I just finished my last masters semester and am starting my doctoral courses.I have a stats question related to my job (I am a part time student, perform stats at a hospital, self-taught prior to school) and I was hoping you could offer an opinion on whether I'm on the right track.

I am looking at lymph node biopsy data. One of two radioactive dyes is being injected into patients and I am wanting to know which dye gets to the nodes quicker. My IV is which dye (x = 1, 2), my DV is how long until the lymph node is illuminated (Y = y) (Do you see how I’m trying to use the notation?), Which analysis would you use for this? I thought it would be smart to use a Kaplan-Meier time-to-event type, but my advisor says that’s not appropriate. Can you tell me why it’s wrong and which analysis would be best?

Edit: I should add that there are 4 time points, for a sequence of: injection, localization, removal of first node, removal of final node. I didn’t think it was possible to analyze this in one model, I thought I would have to break the analysis into 3 parts, one for each time interval."
Reporting non-parametric results in research papers,10,1,False,False,False,statistics,1512221420,True,I'm writing a lab report/research paper and I'm not sure how to report non-parametric results. When reporting parametric tests you can say A (mean +/- SD or SEM) is statistically significant (P value). Should I also use means when comparing non-parametric results? Because the non-parametric results in PRISM are given in ranks and differences between them (multiple comparisons).
Having trouble finding critical value,1,1,False,False,False,statistics,1512223943,True,"I just can't seem to be able to find critical value, I watch videos but I still can't find it, my z table is different as well, and if you do know the level of significance, how do you use it to find the critical value, it's driving me insane I'm not sure what I'm missing"
Door 2 - Data Science Advent Calendar: Shark Attacks EDA,0,1,False,False,False,statistics,1512225988,False,
"Aspiring statistician but my c1ollege has no statistics degree, help!",0,1,False,False,False,statistics,1512236511,True,[deleted]
"Aspiring statistician but my college has no statistics degree, help!",4,1,False,False,False,statistics,1512237169,True,"I'm currently on the B.A. Math track for secondary education but recently had a change in heart. I go to school where they offer a B.A. in Mathematics and B.S. in Mathematics but no degree for statistics, not even a minor. I've been goofing around on glassdoor for statistician (data scientist? not sure of the difference) jobs but a lot of them say you need a degree in statistics, mathematics, or computer science. If I cannot relocate to a school with as statistics program, what would be my best course of action to ensure my employability? Would I need to switch to the B.S. and then minor in computer science? Or should I bite the bullet and even double major? I know I want to learn data mining, and I'm not sure if the minor will cover it. I work full time so I can only take like two classes a semester at most plus I pay for my own schooling (not eligible for financial aid for reasons other than need; I definitely need it though). Thanks in advance!"
"Multiple regression with interaction; chart, centering dummy variable, R squared change",0,1,False,False,False,statistics,1512239696,True,"Hi,

I am having some doubts when performing multiple linear regression with interactions. In my regression analysis I am testing if two continues variables (independent variables) have different effect on another continues variable (dependent variable) and also testing if those two variables have different effects when another phenomenon (binary, dummy variable) is present or not. So there are two interactions in my regression; first one is first independent variable + dummy phenomenon, the second one is second independent variable + dummy phenomenon .Some control variables are also included in regression analysis, but never centered. 

My first question is if I can or should center dummy variable that is also used in interaction. My main goal of centring is to reduce multicollinearity. Interpretation is a bit harder, but that is not really important.

My second question is how to show interaction effect on a graph/chart. I have seen some examples, but I have not found a good tutorial where it is shown how to do it properly. I am mostly interested to know which values to use for all main points on the chart. Example is shown here

https://www.google.es/search?biw=1536&amp;bih=759&amp;tbm=isch&amp;sa=1&amp;ei=SPEiWu-RF8m8UdaIqJgB&amp;q=regression+with+interaction+graph&amp;oq=regression+with+interaction+graph&amp;gs_l=psy-ab.3...3198.4307.0.4386.5.5.0.0.0.0.147.464.4j1.5.0....0...1c.1.64.psy-ab..0.0.0....0.XxYk9mD04w8#imgrc=_sWeUpoBgAt7nM: 

and here 
https://www.google.es/search?biw=1536&amp;bih=759&amp;tbm=isch&amp;sa=1&amp;ei=SPEiWu-RF8m8UdaIqJgB&amp;q=regression+with+interaction+graph&amp;oq=regression+with+interaction+graph&amp;gs_l=psy-ab.3...3198.4307.0.4386.5.5.0.0.0.0.147.464.4j1.5.0....0...1c.1.64.psy-ab..0.0.0....0.XxYk9mD04w8#imgrc=FpwBhD_eIAUJwM:) 

Can maybe somebody provide a good article or preferably video?

My last question is about R squared change. Can results of last level (where there are interactions in the regression) be used if R Square is not significant? I use SPSS and put control variables on the first level, all centered variables on the second level and interactions on the last, third level.

Any help would be appreciated.
"
M.stat degree,0,1,False,False,False,statistics,1512241856,True,[deleted]
Please Help cannot find solutions to some questions please have a look at the Highlighted Ones. Any help would be appreciated I have EXAM and couldn't find answers to this!,1,0,False,False,False,statistics,1512244069,False,
Am I getting in over my head?,5,2,False,False,False,statistics,1512250284,True,"Hello everyone,

I am a medical lab scientist in hematology who is interested in potentially making a career switch to statistics. I currently have two bachelors degrees, one in microbiology and the other in clinical lab science. We get to use basic stats in our job (Levey-Jennings charts, Westgard Rules, simple univariate linear regression, correlation and precision studies...), but nothing too fancy. Mostly it's clinical (microscopy etc). 

I have been in the process of applying to a masters in applied statistics and analytics offered by my state university. According to them, my As in calculus I/II and elementary statistics with my R programming course is sufficient prerequisite material. But I definitely feel a bit intimidated looking into some of the crazy graduate level probability questions and so forth that I have seen.

I know this sort of been asked before and I apologize if this post seems redundant, but I just wanted to get a bit more info on what I should expect/look into. Has anyone here used a masters degree in stats as a means to change careers into it? Thanks in advance."
Nonparametric ANOVA Question...,2,1,False,False,False,statistics,1512260391,True,"I am trying to compare likert scale survey data between many subgroups that make up one whole data set (for example there are 30 ""organizations (org)"" that make up the entire data set, I am trying to compare each org to the grand mean of every org to see what orgs are performing below or above the grand mean). The data is not normally distributed and the sample sizes are unequal so I am running nonparametric tests (Kruskal H in SPSS). When I do this, the adjusted means (bonferroni correction) is throwing up all 1.0 p values. I feel I am missing something, any suggestions?"
Is there a standardized way to determine the point at which a distribution begins to increase?,1,4,False,False,False,statistics,1512261086,True,"For example, I'm doing an experiment that relies on the detection of current as dependent on a variable I adjust. I get a constant (albeit noisy) read on the current until I reach a threshold point; after which, I get a bell curve-like distribution.

My goal is the determine the least value of my independent variable that corresponds to a significant increase in current (significant here meaning that the increase deviates past the usual baseline noise).

Note: curve fitting is feasible if that helps for derivatives."
Did I do this probability question correctly?,7,0,False,False,False,statistics,1512269678,False,
Does anyone know of a good place to look for an internship?,3,5,False,False,False,statistics,1512271435,True,I am really really struggling to find an internship for data science or machine learning. 
What does “typical percentage” mean in statistics?,0,1,False,False,False,statistics,1512284425,True,[removed]
Looking for multiple regression data set,0,1,False,False,False,statistics,1512284734,True,[removed]
Help part f,2,0,False,False,False,statistics,1512289431,False,
Why can you make the exact same event seem likely or unlikely just by describing it differently?,11,6,False,False,False,statistics,1512318203,True,"Let's say you have a lottery drum filled with balls numbered 1-1,000,000. You roll the drum and ball #5 comes out. This event can be described in many different ways:

**Description A:** Most of the balls had numbers with more than one digit. Therefore, our event was very unlikely.

**Description B:** Most of the balls had numbers that were less than 999,999. Therefore, our event was very likely.

**Description C:** Half of the balls had even numbers, and the other half had odd numbers. Our event was just as likely to occur as not occur.

**Description D:** The number 5 was improbable, but it was just as likely as any other particular number. So there's nothing to be surprised about.

All these descriptions of the event are true, but they make it seem either very surprising or totally expected.  How are we supposed to describe the event in order to assess its probability?"
Good undergraduate programs for statistics / similar,5,1,False,False,False,statistics,1512332083,True,"My current life plan is to get a phd in statistics and become a quant. I was under the impression that school prestige was important when it came to becoming a quant, so I was applying to a good amount of top schools. Recently, however, I realized that 1) I have a lower chance of getting into these schools than I thought I did and 2) undergrad prestige doesn't really matter because I'll be going to grad school anyway

I have 99.6 weighted gpa and a 1530 and 33 on the sat and act so I'm confident I could get into most schools, but I'm just not up to par with top school requirements (I did poorly on the subject tests, not many ECs or service hours, shit like that). 

So, now I'm looking for really good undergrad stat / similar programs at schools I would likely be admitted to. Here's what I have so far:

Rochester Institute of Technology

Penn State

NYU Stern 

University of Buffalo

Any input on my current options? Any suggestions (preferably in the northeast)?  I really just want the best stats education I can get.  I may minor or double major in computer science as well. Thanks!"
Need Help Solving a Statistics Homework Question,2,0,False,False,False,statistics,1512344606,True,[removed]
"In my old Cognitive Science Lab, I got a lot of questions about MANOVAs, so I wrote about it, for the non/ new-statistician",12,54,False,False,False,statistics,1512345257,False,
Could use help with asking questions of data set(s) for research!,0,1,False,False,False,statistics,1512348665,True,[removed]
How does one use Hermite polynomials with Stochastic Gradient Descent (SGD)?,0,2,False,False,False,statistics,1512354187,False,
Question regarding the chi-square statistic.,1,1,False,False,False,statistics,1512360110,True,"Is it possible to reject both the null hypothesis and alternative hypothesis?

Is it possible not to reject either the Null nor the alternative hypothesis?

My guess is that if the chi-square statistic yields a p-value of 50%, then you can't reject either. In that case, the second question would be ""yes"". Or if the p-value is 50%, does this mean we reject both? "
"For a two tailed test, I think it's better to divide alpha by 2 instead of doubling the p value. Am I wrong?",1,1,False,False,False,statistics,1512362174,True,"To my understanding, the results will be equivalent. It makes more sense to me because we're still saying that the probability of observing such an extreme value or more extreme is the p value. In both cases we're splitting the rejection region denoted by alpha into both tails."
Help with Log linear analysis or independence model,0,1,False,False,False,statistics,1512365001,True,[removed]
"When you use inverse probability weighting for estimation, what are the weights actually doing?",0,2,False,False,False,statistics,1512370231,False,
How do I choose the appropriate test statistics,0,1,False,False,False,statistics,1512372300,False,[deleted]
How do I determine the appropriate test statistic?,2,0,False,False,False,statistics,1512376043,False,
Applying for a Statistics Masters,6,1,False,False,False,statistics,1512377410,True,"I'd like to know if I have a decent chance of being accepted into a mid tier statistics master's program.
I have a mathematics undergrad degree with a 3.85.
40th percentile on the math gre subject test.
I've taken Multivariable Calc and ODEs and linear algebra (though it's been a hot minute so I'd need to refresh myself). 

I've also taken upper undergrad advanced calculus (proofy calculus) and probability, and some most likely irrelevant abstract algebra.

My main concern is that essentially all of my classes were more on the pure math than applied math side of things (excluding the probability). I'm also 2 years out of school. (though I've still been doing math stuff essentially every day because I like it, so I haven't forgotten too much)"
Orthogonal Contrast trouble: 1-way ANOVA; what to do when a factor has zero variation?,9,2,False,False,False,statistics,1512384264,True,"This is a situation I haven't encountered before, I'm not sure how to best handle it:  
My client wants a post-hoc pair-wise comparison of sites, if appropriate, and an ad-hoc reference vs. non-reference (orthogonal contrast).  
I have 5 sites, 1 designated as a reference site. Each site has 2 replicates. I'm looking at metal concentration between sites, with a contrast of reference vs. non-reference.  
  
One of my sites has the same value, so no variance. I initially thought I would simply omit it from the analysis... but now I'm wondering if I should include it in the orthogonal contrast ANOVA. Since it's lumped in with the other non-reference stations, the variance is getting pooled anyways correct?  
  
How might you handle these situations??  

Thank you. "
Why is this number not in the 99th percentile?,4,6,False,False,False,statistics,1512402637,True,[removed]
What type of orthogonal polynomials does R use?,6,2,False,False,False,statistics,1512406289,False,
Standard Deviation for FG%,0,2,False,False,False,statistics,1512407313,False,
ttest/mann whitney and anovas-please help me out here,1,1,False,False,False,statistics,1512409385,True,"hey guys i am having some issues evaluating data and i am hoping some of you can help. so i have data with multiple factors and a response. the factors are some physical attributes (age, sex, fitness) and the response is a score based on some performed task.  now i am kinda lost here and not sure what to do. my innitial though was to just do an annova (my results are distribuited normaly) and then maybe do a mann-whitney u test to compare individual attributes to the final score, but this does not seem to get me anywhere... what tests should i use here to make sense of my data? thank you"
Making a board game a need help with the statistics of rolling d6's,7,0,False,False,False,statistics,1512410244,True,"Basically what I am trying to figure out is the probability you would succeed on rolling multiple six sided dice (d6). So in order to ""succeed"" you would need x amount of successes. Rolling a 5 on a d6 would get you one success and rolling a 6 would get you two successes. So is there a formula I could follow to figure this out? 
ie: If you would 3d6, then what is the chance you would get 4 successes?
ie: If you would roll 5d6, then what will your chance be now?"
What goes into an effective introductory statistics recitation?,4,7,False,False,False,statistics,1512410321,True,"I was recently asked to help TA an introductory statistics course for graduate students in a school of public health. The course is meant for students who are, in the professor's words, ""quantitatively challenged,"" i.e. have trouble with basic algebra. It will cover topics from basic probability through linear regression.

I will be responsible for teaching a weekly hour-long recitation for about thirty or so students. This will be my first time doing anything like this. The recitation structure will be up to me. In your experience, what has made a recitation effective, either as a student or an instructor, especially for students without a strong math background?

I'm thinking of splitting up the hour between basic math review, lecture content review, practice problems, and question/answer.

Thanks in advance for the feedback!"
[College Stats] Intervals,0,1,False,False,False,statistics,1512416503,True,[removed]
What would be the average score for guessing on every single question on an exam?,12,3,False,False,False,statistics,1512417753,True,"Say you had a large class, and each student was instructed to just completely guess on each question. Not an educated guess, a completely random guess. 

Would you be able to calculate the class average for this hypothetical situation?"
Logistic regression + machine learning for inferences,38,18,False,False,False,statistics,1512420169,True,"My goal is to make inferences on a set of features x1...xp on a binary response variable Y. It's very likely there to be lots of interactions and higher order terms of the features that are in the relationship with Y.

Inferences are essential for this classification problem in which case something like logistic regression would be ideal in making those valid inferences but requires model specification and so I need to go through a variable selection process with potentially hundreds of different predictors. When all said and done, I am not sure if I'll even be confident in the choice of model.

Would it be weird to use a machine learning classification algorithm like neutral networks or random forests to gauge a target on a maximum prediction performance then attempt to build a logistic regression model to meet that prediction performance? The tuning parameters of a machine learning algorithm can give a good balance on whether or not the data was overfitted if they were selected to be minimize cv error.

If my logistic regression model is not performing near as well as the machine learning, could I say my logistic regression model is missing terms? Possibly also if I overfit the model too.

I understand if I manage to meet the performances, it's not indicative that I have chosen a correct model."
Trying to start my hand at machine learning and AI and wanted to try making a video game AI to make it fun. What games would you suggest for a beginner?,6,1,False,False,False,statistics,1512420202,True,"Little bit of background, I have basic knowledge in terms of probability and statistics, Bayes Theorem, etc. I also know Python, C++, Java, and MatLab for programming languages, though I could learn a new one if necessary "
Best way to directly compare different regression methods?,2,6,False,False,False,statistics,1512420527,True,"I'm fitting some data with a standard regression and a tobit regression, and would like to determine which model better describes the data. I believe the tobit fit is better, but I want to be sure. Is it statistically valid (or advisable) to use AIC to compare them? "
Questions about sampling methods!,0,1,False,False,False,statistics,1512431433,True,[removed]
Statistical Analysis Question,0,1,False,False,False,statistics,1512439299,True,[removed]
Urgent help needed for statistics exam,3,0,False,False,False,statistics,1512440536,True,"https://i.imgur.com/CnpQliD.jpg

Please explain I have an exam soon"
Good video series where i can learn a solid foundation of Basic Statistics?,7,17,False,False,False,statistics,1512441084,True,"I got interested in database related programming languages like Python, SQL and R and delved into Data wrangling/science. Problem is i have s shitty stat foundation and i wanna re-learn it.

Any recommendations? Book/Text based is not advisable for me. I prefer an interactive learning experience

Thanks!"
Is my thinking here logical?,4,2,False,False,False,statistics,1512448431,True,"Hi. I'm looking to analyse the data I got from a recent experiment. I'll just describe the data and then what I'm thinking would be best for it.

My data is based on counting the instances per minute of a certain behaviour in birds at ""rest"" states and when certain stimuli are played. So for example, the bird might do this specific action 0 times per min at rest, 4 times per min when a predatory call is played, 12 times per min if another sound is played ... etc. (all the data was paired, if that's important)

There are two separate questions being asked here: is there any observable difference between the behaviour per minute when at the rest state and after being exposed to the various sounds? and also because the test was done in rural and urban areas, I'm kind of interested in the difference between the two locations as well.

summary of data:

Rural Area   Resting     Hawkcall     Starling Song   Blue Tit Call
Bird 1              0               1               2                     0
Bird 2              1               2               2                     0
etc
Urban Area
Bird 1
etc

(not the real data)

issues
-there are a *lot* of zeros in the data
- graphing the (non-transformed) results show no normal distribution, between resting data or anything else.

One thought I had would be making the data categorical (so rather than measuring the rate, measuring whether or not rate per minute, I could just record ""Yes"" and ""No"", whether there was any response at all. and then compare them with ANOVA?

Is there a better way?"
HELPPPP,1,0,False,False,False,statistics,1512451455,False,
comparison between simulating in R and Python,0,1,False,False,False,statistics,1512464720,False,
"Colorado launches sophisticated post-election audits ahead of 2018 midterms, becomes first in the nation to secure election system with paper ballot voting system and systematic ‘risk limiting audits’ of statewide election results.",0,93,False,False,False,statistics,1512472775,False,
Checklist of diagnostic and validity checks for linear regression model?,13,3,False,False,False,statistics,1512486203,True,"I am using linear regression to model a continuous outcome with four input variables. What is a good “checklist” of diagnostics to run?

For example, I know that checking residual plots to ensure they are randomly scattered is important. It would be helpful to have a list of general diagnostics to check for future reference. If possible, having some possible fixes for the model (e.g. transforming the data) would be nice though not necessary.

Thank you! "
Understanding common misconceptions about p-values,0,1,False,False,False,statistics,1512490479,False,
$40 Paypal if the image I posted is Easy to you. Need help,3,0,False,False,False,statistics,1512496645,True,[deleted]
The Birthday Paradox: 9 alternative versions you’ve never heard,1,0,False,False,False,statistics,1512497205,False,
Taking analysis into account when doing power analysis.,4,2,False,False,False,statistics,1512500158,True,"I have two cases. 

In one I have an effect size based off of a previous study analyzed with logistic mixed effects regression. In my follow up study, the data will be analyzed using Bayesian parameter estimation of logistic mixed effects models and 95% CIs. It seems that using G* Power might not be appropriate because that does not allow for the previous study to have been run using mixed effects nor the current study to be using Bayesian paramter estimation. How would I conduct a power analysis in this case?

In the other case I only have a best guess at the effect size, and the data will be analyzed using multivariate Bayesian parameter estimation. Again, G* Power doesn't seem to be able to take these specifics into account.

Is there an alternative to G* Power that I should be using for power analyses in this case? Or, should I simply use G* Power?

(As a PS, I'm aware that a power analysis might contradict the fundamental ideas of Bayesian statistics. However, as I am using 95% CIs to determine which effects are interpretable, I have one foot in the frequentist pool and would still like to do them. I'm aware of the downsides of this from a purely Bayesian point of view.)"
Big noob to statistics looking for help!,10,0,False,False,False,statistics,1512509720,False,
Paired t-test vs. independent samples?,4,2,False,False,False,statistics,1512518550,True,"I usually understand when to use paired vs. independent, but I can't find anything online for this. I am looking at average precipitation between two years, from the same 6 weather stations. Should this be paired (pairs are year 1 vs. 2, between each weather station)?"
Econometrics project help (longitudinal model),0,1,False,False,False,statistics,1512527707,True,[removed]
What is an interesting simulation I could do to show the power of bootstrapping?,8,16,False,False,False,statistics,1512529822,True,"In MatLab I'm trying to think of a simulation I could do that would demonstrate how useful bootstrapping can be and how it can give you different, improved results. It's used for several things of course, so I'd like to show how it could be useful in more than one way or perhaps just one ""advanced"" application of it. I'm not really good at thinking of this kind of stuff though, does anyone have any suggestions off the top of their head? "
"Is there a relationship between a student’s GPA and where he or she sits in class – towards the front, middle, or towards the back?",7,1,False,False,False,statistics,1512530172,True,"Is there a relationship between a student’s GPA and where he or she sits in class – towards the front, middle, or towards the back? 

I have a class presentation on this topic and was unable to describe what kind of formula we used to come up with our conclusion. At the moment we're just going with the notion that the graphs are our evidence in our conclusion. 

If anyone can help, please and thanks!
"
Looking for Statistics help,0,1,False,False,False,statistics,1512537283,True,[removed]
Population proportion inference: what statistical error am I committing here?,1,2,False,False,False,statistics,1512540350,False,
DFE for t-test in multiple linear regression,1,2,False,False,False,statistics,1512547470,True,"When we're determining a final model for MLR, after finding the F-stat, we go on to perform t-tests for each explanatory variable. I have in my notes that the degrees of freedom for each t-test is DFE, or sample size minus the number of explanatory variables minus one. Is the sample size and number of explanatory variables for all explanatory variables in the MLR, or just the one we're running through the t-test? In other words, will the degrees of freedom for the test always be n-2?

Sorry, I know this is wordy. I'm writing on mobile but want to be precise as possible with my question."
Confidence Intervals when Reshaping Data,0,2,False,False,False,statistics,1512563389,True,"Let's say I was running an A/B test and I collected 'n' samples for each group, and I calculated that n independent samples was sufficient for the confidence intervals (and confidence level) I needed.

Suppose I then reshaped my data into two time-series, such that each row contained the mean value for each group for a given week (or some other unit of time). I might do this so that I'm working with logical pairs and my hypothesis test better matches my application.

My question is: after reshaping my data, did my confidence interval change? And if so, why?

Thanks!"
Avoiding recompiling of Stan code in Multiple Imputation,1,5,False,False,False,statistics,1512569921,True,"I currently am fitting a set of models to a data set with missing data. I imputed the missing values across 20 datasets - easy so far.

I want to fit my model to each of the imputed datasets, then pool the posterior distributions to get my final analysis. However, compiling the Stan code 20 times is making this take a lot longer than it should.

Is it appropriate to fit a single model, then update it for each imputation - saving me the time of recompilation?"
I have an exam coming up that involves writing 2 essays. The professor gave us 11 questions ahead of time. 8 of these questions will be on our exam and we have to answer two of them. What is the minimum number of essay questions that I can prepare ahead of time?,0,1,False,False,False,statistics,1512580042,True,[removed]
Questions about internship,0,1,False,False,False,statistics,1512582007,True,[removed]
3 way repeated measures anova help,0,1,False,False,False,statistics,1512584968,True,[removed]
Which correlation method is best for testing the correlations of stock market closing prices [time series data],4,5,False,False,False,statistics,1512587479,True,"I've currently done it with PearsonR. Although, I've read that the data should be normally distributed, which of course it isn't. Obviously if did delta price, it'd be normally distributed. But this isn't an option with the data I'm using due to missing data."
Should I use bootstrap for my paired samples -t-test?,4,1,False,False,False,statistics,1512587548,True,I have done all my statistical analysis. One of them is a paired sample t-test. I have 2 groups who were measured at baseline and after 3 months again. I calculated and analyzed everything but I just had a look into my stats book to see if I reported everything right and saw that it used a bootstrap in their paired samples t-test analysis. Should I redo my analysis also using a bootstrap? Or is this only fo specific cases?  
What kind of knowledge in the statistics-domain do business intelligence/data analysts have?,6,1,False,False,False,statistics,1512587752,True,"I'm currently a data engineer but I'd like to try more of the analysis part of crunching numbers without getting into the 'data scientist' realm. I understand that it sounds like I'm asking how to float in a pool without learning how to fully swim, but from what I've seen in my consulting gig, there's a need for engineers who know how to also analyze data at a mid level.

Anywho, my Python and sql skills are solid, and I'm not concerned about not knowing R; I can ramp up on it fairly quickly. So technical skills are fine imo at least. It's the analytical material where I have close to zero knowledge. I've worked with some data scientists with grad degrees on a project who did a lot of modeling obviously, but they mentioned that advanced degrees aren't required for a few things. Doing preliminary research for what to learn hasn't given me much guidance, just a lot of people suggesting to learn sql, which I already know

So I'm trying to get a better understanding of what to learn and be a decent analyst and end-to-end engineer. I know 'data analyst' is a very very very generic job title, but maybe a Business Intelligence analyst would be a better fit for what I'm aiming for. What have you guys seen data analysts do in regards to statistics? Should I learn linear and non-linear equations? Predictive? Maybe not even any of that?"
Econometrics class. What is the point?,19,4,False,False,False,statistics,1512591717,True,"Looking at the syllabus for an advanced econometrics class, a recommended course for our MS in Statistics. Nearly all the topics look like stuff that we cover as first year MS statistics students, except for instrumental variables (IVs), IV regression, exogeneity and endogeneity, which are things I don't think I've ever heard of. But everything else looks like stuff we do in our other core stats courses like time series, GLM, linear regression, fundamental statistics, etc. 

So I'm just wondering...why does econometrics exist, why not just call it statistics? 

Also, just reading the Wikipedia, I can't tell whether those things I mentioned are actually new. Like isn't an instrumental variable simply ""a variable that we have data for, but we didn't include it in our first model, which shows omitted variable bias, so now we are going to include it to see if it helps"" ?"
Calculating error term in regression,1,5,False,False,False,statistics,1512592507,True,"Hi, I am currently writing my dissertation on whether the Efficient Market Hypothesis holds more robustly in different sectors of the stock market.

I thought the best way to do this would be to formulate an expression for the returns of the stock portfolios and then the error term generated through the regression would be the abnormal returns, therefore would be expected to be equal to 0 for EMH to hold.

I am not that skilled at regressions and was hoping someone could shed some light as to the best way to do this?"
Career advice needed,3,4,False,False,False,statistics,1512594919,True,"Background...I'll be graduating in May with a BS in Stats and a BA in Sociology. I definitely want to do something more quantitative. Problem is, I have no idea what I want to do. My GPA is 3.52. I passed two of the actuarial exams but have, for now, decided to look for something else (that field seems extremely crowded). I was seriously considering enlisting in the military, but for now have backed off and am trying to start a career. I just hate feeling like I don't know what my path is. 

What sort of jobs/titles should I be looking for? It seems like most jobs that I look at require an MS in Stats. Is an MS in Stats going to be necessary for even entry-level jobs? I realize I may need to do things to increase my marketability (better resume, better interviewing skills, etc.) but am I employable as is?"
Intuition on why the F-test for means works?,3,3,False,False,False,statistics,1512596089,True,"Hey,

I have this question:

Explain the intuition behind the F test to assess the global null 

H0 : β1 = β2 = β3 = 0

versus its alternative
 
Ha : (β1=β2=β3) != 0 

and provide details on how the F test is constructed using the residual sum of squares for the full model and that for the reduced
model.

I know the formula for the F-test is 

F = ((TSS - RSS)/p)/(RSS/(n-p-1)

TSS = Total sum of squares
RSS = Residual sum of squares
n = number of observations
p = number of predictors

I understand the formula but am not sure if I fully understand the intuition.

Any advice is appreciated.

Thank you for your time"
How do you decide which interaction terms to include in a GLM?,10,8,False,False,False,statistics,1512596603,True,Running a negative binomial GLM. How do you decide what interaction terms to include in the model? Should you include all of them? Only the ones of interest?
Rate Smoothing in Spatial Data,0,7,False,False,False,statistics,1512598322,True,"Working on some spatial epidemiology research a while back, that has resurfaced, I was told that I should [smooth the incidence rates](http://www.dpi.inpe.br/gilberto/tutorials/software/geoda/tutorials/w6_rates_slides.pdf) to account for areas with small population/high variance causing outliers (low population with 1 case may have a huge rate). I am unable to contact the professor who gave me the advice at the moment.
 
What I don't quite understand is:

* Even if the rate is inflated by the low population, isn't it still valid? That's what is present in the ""real world.""

* Should I use the smoothed rates in calculations further down the road? If I want to calculate spatial autocorrelation (Moran's I), should I use the raw or smoothed rates? Is rate smoothing primarily for visualization or for analytical purposes?"
How do I conduct a power analysis based off of a mixed effects logistic regression?,0,4,False,False,False,statistics,1512599856,True,"As far as I know G* power only allows a power analysis based off of a logistic regression without random effects. 

Can I just plug in the odds ratio from the random effects model and treat it as though it were from a regular logistic regression?

Is there a better solution?

Edit: As a bonus question... In another analysis, have multiple measures of a variable and am putting them in the same multivariate regression model. I am assuming a general effect size of .02. If I calculate the power in G*Power for a univariate regression with this effect size, do I have to make any adjustments given the fact that I have multiple outcome variables?"
"What's the best way to deal with ""chaotic"" intervals in your data series?",7,5,False,False,False,statistics,1512609932,True," I am looking at the y-on-y change in monthly land purchases in China. The data seems to be nice and fitting the narrative in 2015-2017. However, in 2013/2014 the year on year change rate for every other month is just going through the roof and we are talking about number and above percent points. I am not comfortable to use this date series in ARIMA model. What is the best way to proceed? should I ignore and go ahead, limit the time period assume a structural shift rendering dataseries less relevant in terms of accuracy projection capabilities (which is not too useful for result accuracy), treating those extreme values as outliers?. any other suggestions? I use Eviews.

Edit:here is the chart https://imgur.com/a/tC3q0"
Can you declare neutral cases as missing on SPSS?,7,3,False,False,False,statistics,1512614367,True,"I am grouping together variables so that I have ""agree"" and ""disagree."" Neutral is getting in the way. Can I just declare this as a missing value in my data set? Or is this not allowed?

I still have a significant count without it"
Large Sample vs Plus Four Confidence interval?,0,1,False,False,False,statistics,1512615076,True,"studying for exam at the moment and am confused on when to use a large sample confidence interval for difference in proportions vs a plus-four confidence interval. The notes state that a large sample can be used can be used when the # of successes and failures in each sample are at least 10. For a plus-four, it says that it can be used when n1&gt;=5, n2&gt;=5. So lets say if I have 2 proportions with n1=11, n2=15, which one would I use? They seem to have almost the same conditions..."
"Difference between Z and T tests, Z and T intervals.",3,7,False,False,False,statistics,1512619639,True,"I'm in an entry level stat200 class (last of my gen eds) and I'm kind of terrible at math. Can someone please just explain in easy terms exactly what the difference is between Z tests and Z interval tests, t tests and t interval tests, t tests and z tests, and when to use each? PLEASE AND THANK YOU.

I'm almost at the end of this dang class, I usually understand the work as I'm doing it, but studying for finals, and reviewing everything from the beginning of the semester has my brain all mixed it. Thanks in advance for any help!"
Five ways to fix statistics,3,29,False,False,False,statistics,1512644283,False,
Using Pilot Studies in Clinical Research,0,1,False,False,False,statistics,1512655190,False,
How to know one is a real statistician in his/her heart,4,0,False,False,False,statistics,1512655290,False,
Learning Multilevel Modeling,6,4,False,False,False,statistics,1512666729,True,"short background, i'm an undergraduate psychology major graduating in 2 weeks with an end goal of a PhD. i'm going to have all the free time in the world this spring/summer, and i've decided to re-teach myself statistics from the ground up and furthermore work all those concepts into learning how to do them in R.

my main goal is to be able to learn the fundamental of Multilevel modeling and eventually how to perform this analysis in R.

Questions i have:
1. Do any of you all have a suggested path of learning to work my way up to the level of MLM? 
2. What online resources have you all found to be most helpful?

Currently plowing through a fundamental stats refresher course on UDEMY, working my way through all the Swirl Packages in R, and have ""fundamental statistics for the behavioral sciences"" and ""discovering statistics using R""."
Name of a certain type of bias in clinical studies.,5,8,False,False,False,statistics,1512667291,True,"Hello everyone, I really don't know if I'm right here, but it was the best place I could think of.

I've tried googleing for quite a while but couldn't find the name of the bias I am looking for.

I'll explain it shortly:

Basically, patients will usually start taking medication when their symptoms are at their worst. However, even without any kind of intervention the symptoms will usually better in a periodic disease when the symptoms are at their worst.

Thus the effect of medication will be overestimated.

Does anyone recognize that kind of bias? Would be really glad if you could help me out."
Little help with M&amp;M's problem,0,1,False,False,False,statistics,1512668349,True,[removed]
Could “random sampling” be used to reliably estimate the ranking of large numbers of alternatives ranked via pairwise comparisons?,0,1,False,False,False,statistics,1512672391,True,"Using 5/5 averaging seems easier but some things are too specific or time-consuming to reasonably expect people to learn what “average” means. In such cases I think pairwise comparisons could work but just 30 alternatives would create 435 possible comparisons and using random sampling for each comparison would require 435*300=130,500 comparisons. Could statistically robust estimates of ranking be obtained with less? If so, what numbers would be required? Could they be determined mathematically/statistically beforehand or would they require experimentation? Would preexisting comparison datasets do? If so, does anyone know how to get them?

On a separate note, matters would be further complicated by the variables in pairwise comparisons methodologies. For the method, there's: round-robin comparisons of all alternatives against all other alternatives, Swiss tournament comparisons of winners against winners and losers against losers, regular elimination tournaments, or “randomized tournaments” where alternatives could be compared to one or more alternatives randomly, (with “to-be-determined” methodology/numbers that could also potentially vary when necessary). As for variables, there's whether/when to utilize short ranked lists where #1 wins against #2, #3, #4, etc, and #2 wins against #3, #4, etc. There's whether to use winner-take-all, win/loss ratios, additive wins, win-minus-lose, mean, geometric mean, etc, methods. There's whether to add the scores of beaten alternatives to the winners score. There's whether ties should be allowed and, if so, how the math should score them.

As an aside, does anyone know if ranked pairwise comparisons align with averaged ratings?
"
undergrad looking for advice on grad school prereqs,4,1,False,False,False,statistics,1512674689,True,"Hello, I posted something similar in r/datascience but figured I'd try here as well.

I'm current a junior at UC Davis. I'm taking a regression class right now and working in a psych research lab and I'm realizing that I really like working with data. I'm starting to look into fields like data science but I wanted some advice on majors. I started out as a psych major (hence the psych lab) and I can change my major to stats and still graduate on time but only if I do the applied stats track. This means I'll have take calc 1-3 (the series for bio majors though), linear algebra, regression/ANOVA, time series, analysis of categorical data, and math stats among some other stats and CS electives. I want to do a masters in stats but I was wondering if I'm missing prereqs. My school recommends taking real analysis but that requires a different calc series than the one i took. Any advice?"
Study Design - am I wrong? (Feeling adrift right now),2,1,False,False,False,statistics,1512676219,True,"Hi all - social science student here, doing a study on compensation methods and impact on life satisfaction (to be very brief) with a control group (no compensation methods used). Cross-sectional data using a survey, not experiment.

I ran a power estimate and will have roughly 350 participants in my survey.

Problems: I am worried about unbalanced sample sizes and if I'm even on the right track for study design.

I will have two big groups to compare on my DV (life satisfaction): those that use compensation methods and those that don't (control group). However, my compensation methods groups will be very small- I will have about 7 variables of compensation methods and will compare with the control group (no compensation) to find which are associated with higher rates of life satisfaction (DV).

I am contemplating the statistical design and am feeling like I'm not seeing things clearly.

Question 1) Since I'm paying to run this study myself, I am worried that I will have to exclude data if I have really small sizes in each cell for the compensation methods (each variable). One method may have n=50, and another n=3. I don't want to combine groups because they're all different and I want to analyze each separately. Apart from paying more for a larger sample, what can I do?

2) Which seems to make more sense for this study design?

Primary Independent Variable: Question: Do you use compensation method(s) for research issue? - Dichotomous variable (yes/no)
If no, control group (continuous variable from scale score)
If yes, they will choose one or more of 7 dichotomous variables (yes/no)
DV: Life Satisfaction Scale score (continuous variable)

a) Multiple linear regression: Since the DV is continuous, using the compensation method as a predictor to find which methods are the most successful in predicting satisfaction? Not sure if I'm approaching that wrong. Especially since the control group is there, and I don't know how that factors in to a regression equation as it's not the constant.

b) Multi-factor ANOVA: Comparing each group: No compensation, each individual method (7 groups) with DV? I'm fuzzy on this approach but feel something along the lines of a group comparison make the most sense versus prediction (like in a regression model).

c) DFA: Doing MANOVA backwards: Creating group memberships via the scale results (of life satisfaction): using their data from the compensation methods (or none) to predict membership. This - this I feel isn't properly thought out nor the right approach.

Long long long story short, I'm a student which is why I'm really fuzzy, and I'm away from campus without resources to obviously get help. Just hoping someone can understand where I may be going and offer suggestions/advice. THANK YOU!!"
Why do a lot of people think R is untrustworthy?,21,0,False,False,False,statistics,1512679357,True,"I am in a biostats department, and most professors insist on SAS/STATA because they think that those tools are more trustworthy.

Is that the consensus in other places?  R seems to be pretty popular among data scientists and people in proper stats departments.  Why all this hate for R?"
Samples size considerations and Beta Risk when either rejecting or failing to reject a null hypothesis for a two-sample dependent t-test. Any help?,8,1,False,False,False,statistics,1512683722,True,"When I compare some normal distributions (n&gt;250 for all samples), why might I get .90 power or more when the null hypothesis is rejected (p-value &gt; .05), but I only get .14 power or less when I fail to reject the null hypothesis?  

Note: In this case my null hypothesis is that mean difference = 0. 

Reject Null Hypothesis Example:

Sample 1-&gt; n=388 mean=3.478277 var=0.6377024

Sample 2-&gt; n=428 mean=3.218875 var=0.5702508

*Two Sample Test-&gt; p-value = 4.619e-06 power= 0.9964*

Fail to Reject Null Hypothesis Example:

Sample 1-&gt; n=388 mean=3.478277 var=0.6377024

Sample 2-&gt; n=328 mean=2.883003 var=0.6348134

*Two Sample Test-&gt; p-value = 0.9702 power=.02726*



"
What kind of scatterplot is this?,5,1,False,False,False,statistics,1512698719,True,"Trying to understand what[ this graph](https://imgur.com/a/l6cWK) is telling me.  Can anyone point me in the right direction?  I know it's a scatterplot, and I know that shape means _something_, but I'm unsure what it is.  "
How can I compare proportions when my chi square value is very high?,0,1,False,False,False,statistics,1512698805,True,[removed]
My chi square is very high - how else can I interpret these percentage differences?,0,1,False,False,False,statistics,1512699430,True,[removed]
My chi square value is very high - how else can I interpret these percentages?,3,0,False,False,False,statistics,1512700348,False,
"How to Correct Outliers in Regression Models: An example with race, education, and the uninsured on Trump’s vote",11,20,False,False,False,statistics,1512739168,False,
Need help with analysing experiment results using SPSS for university assignment,2,1,False,False,False,statistics,1512740521,True,[deleted]
Central Limit Theorem Simulation. Feedback would be appreciated!,10,7,False,False,False,statistics,1512751866,False,
Statistics homework help?,0,1,False,False,False,statistics,1512758282,True,[removed]
Anyone know any good articles or have tips on picking a health insurance plan?,0,1,False,False,False,statistics,1512767540,True,[removed]
Predictive Modeling for a Descriptive Goal,5,4,False,False,False,statistics,1512767871,True,"I've been having some interesting conversations with colleagues recently - all surrounding a project in which we want to accurately describe the marginal impact of a set of IVs on a DV. Normally, we work in the realm of predictive analytics, but this project is more closely related to description of historical phenomena.

I suggested that, in a logistic regression framework, we should run a model that contains all of the hypothesized relationships regardless of sign or significance level (I don't really want to get into the utility [or lack thereof] of p-values!). This would allow us to address any specific questions regarding the IVs within the context of each other (i.e., a percentage of contribution to the DV).

So, my questions to you, r/statistics, are: 

Are predictive models that contain non-significant IVs an accurate way of describing phenomena, given that the description will only apply to the data set on which it was trained (i.e., no extrapolation to other data sets)?

And does the inclusion of the non-significant IVs negatively affect the accuracy of the historical description of relationships between significant IVs and the DV?

Thanks in advance. Cheers!
"
Calculating/propagating the uncertainty associated with a mean of computed values?,3,3,False,False,False,statistics,1512770776,True,"Hi all, I'm having some trouble wrapping my head around uncertainty propagation as it relates to my work, and I was hoping someone could help me out. I've experimentally measured the values of two physiological quantities for each of 5 subjects. I know the uncertainty associated with both of these quantities. I then use the two measured quantities to compute the value of a property of interest for each subject, and I'm able to determine the uncertainty associated with this property using the formula for propagation of errors (https://en.wikipedia.org/wiki/Propagation_of_uncertainty). I would like to report the mean value of this property across my 5 subjects, as well as the associated uncertainty; however, I'm not sure what the appropriate metric for uncertainty of the mean would be in this case. 

I can either (a) take the standard deviation/SEM/variance of the property's value across my 5 subjects, or (b) again use the error propagation formula, treating each subject as a separate variable, to determine how uncertainty associated with each subject propagates through the averaging equation. 

I've never heard of anyone propagating error through an average before, but simply calculating the SD across the 5 subject values seems like it ignores the uncertainty associated with each of those values. If anyone knows what is more statistically meaningful or appropriate in biomedical research, I'd love to hear your thoughts!"
help me about normality test,0,1,False,False,False,statistics,1512787536,True,
Help! Normality Test,0,1,False,False,False,statistics,1512788301,True,[removed]
Help with Regression Project in R,5,1,False,False,False,statistics,1512791169,True,"How would I make R output a regression lines in this form in a plot? I am doing regression analysis on an auto mpg dataset.



Y = 33 -.005*X1 -.04X2 + .76*X3 - .0009*X1*X3


Edit: Y = MPG, X1 = Model Year, X2 = Weight, X3 = country of origin"
Contractor versus full-time worker? Thoughts?,3,4,False,False,False,statistics,1512792224,True,"I'm currently a FTE, but I am considering switching over to being a contractor. I am bored with my career and I'm bored of doing the same ole thing everyday, every week, every month. It is driving me to absolute madness. I've been talking to some recruiters and they seem to suggest that maybe a contract position would fit me better. I was at first, resistant to this until one of them explained what it's like to be a contractor. I am also leaning more towards this because I am primarily interested in clinical research and a lot of these positions are for studies or trials that aren't going to last forever. These studies do not run indefinitely. Most seem to run for a maximum of 3 to 5 years.

I always seem to get bored of my job after a year. But I think being more involved in research and knowing that I'm only signed up for a year or 2 would stop me from wanting to shoot my brains out.

Anyone go down this route? The pay is more as typical. But there's no insurance--but the recruiter told me the insurance they offer is pretty much equal to the shit I'm paying at my job RN!

Please share your stories."
Interpreting results in linear/non-linear regressions,3,0,False,False,False,statistics,1512798044,True,"Hello /r/statistics. I jsut had a couple of questions about different regressions.

Q1: If I run a regression of y = x + x2, does the significance level matter, or just the positive, negative signs?

Q2: If I run another regression y = x*z, where x and z are strongly related, waht does the output mean?"
Here Comes the Rooster! *Imitates Layne Staley*,2,2,False,False,False,statistics,1512806866,False,
Normality of residuals of two-way ANOVA is significant,20,2,False,False,False,statistics,1512817871,True,"Hi. I have trouble with the statistics of my exam. I have two categorical variables (gender (male/female) and age group (three groups)). I am measuring the interaction of those two IV's on emotion regulation effectivity, DV.

If I understand it correctly, I have to use a two-way independent ANOVA. I have already done this. My problem is, however, after calculating the residuals for each score using SPSS, I did a normality test of the residuals and found it to be significant. What should I do now? Is there a non-parametric alternative to the two-way ANOVA in SPSS?

I am sorry for any missing information. Feel free to ask I have forgotten something."
How Not to Be Fooled by Randomness,1,3,False,False,False,statistics,1512818070,False,
question about chi-square test interpretation,0,1,False,False,False,statistics,1512830355,True,[removed]
My Favorite Statistical Methods...,3,43,False,False,False,statistics,1512834507,True,"I made some videos a while back to help friends and colleagues understand some important concepts in statistics. I have expanded these videos to include the topics I find most fascinating in the field.

The first topic you may find fun is biased parameter estimation, and the role it plays in dealing with multicollinearity in regression. This is also an important concept in machine learning, though it is often introduced slightly differently by ML practitioners than my given presentation. You can find this video [here]( https://www.youtube.com/watch?v=nQ4HpOksgPM).

The second discusses the Receiver Operator Characteristic, a method that was originally developed in World War II, where it was used to determine the likelihood that a radar blip was an enemy and if this object should be shot at. Interestingly, it now serves a wide range of research purposes in medicine and epidemiology. You can find my video on it [here](https://youtu.be/pYIIZR_XIyY).

If you enjoy this content, I have a variety of videos on my channel that aim to help people understand statistical topics and their applications, which you can find [here](https://www.youtube.com/channel/UCaBNiBH92Qr7Sr4t4tuf7UA).

I hope you enjoy, happy data modelling. "
Can i compare two different survey questionnaires (with 2 different groups of respondents) on spss?,4,1,False,False,False,statistics,1512842216,True,Im having a study on the industrial relations between employers and employees. i had an instrument for the employers and a different instrument for the employees. is there any way to compare/correlate them on spss? what is needed to be done in order to do so?
"How to ""clean"" your data (e.g. check for monotonous answers)",0,1,False,False,False,statistics,1512853183,True,[removed]
How should I refer to opposite slopes that have no statistical significance? Does the lack of relationship make it spurious?,0,0,False,False,False,statistics,1512855368,True,[deleted]
Need help with what statistical test to use! Never done statistics..,1,0,False,False,False,statistics,1512855739,True,"Hey!

I am writing a small essay for which I need to calculate the significance of my data. Problem is I don't know the first thing about statistics. The data is taken from a corpus (database of words).

The following is a representation of the actual study, which means the concepts are true, but I'm using other keywords etc.

So, lets say I want to compare how many times the words *happy* and *sad* occur together with the word *person*. I also want to see how the frequency of these have changed over time.

Search 1:
I search for the word *person* (restricted to the use in year 1990), and the result is a population(?) of *1453*. From these results, I generate a random sample of *200*.
From the sample in which *person* occurs in every instance, I find *sad* **8** times, and *happy* **5** times.

Then I do the same search again but now restricted to the year 2015. The frequency of *person* returned is *1800*. Again, a sample size of *200* is generated. In this sample, I find *sad* **3** times, and *happy* **12** times.

Now I want to compare how *happy* and *sad* have trended over time, and I want to know if these results are significant with the significance level at .05.

If I understand correctly, the way to go is a chi-square test. But I can't seem to understand the differences between a test of independence, contingency test and goodness of fit. Which one is correct?

Furthermore, As the population (?) sizes are different, would I need to account for this in some way when calculating the significance? ..And lord have mercy, what is the degree of freedom?

I would very much appreciate any help. I would also appreciate if you could try to explain as if I was a child =)

Again, thank you!"
ASA Issued a Statement Clarifying Proper Interpretation of P-Values... Dr. Stephen Nawara Explains What Motivated It,6,25,False,False,False,statistics,1512859308,False,
I have some questions regarding the assumptions of Linear regression with higher order terms.,7,1,False,False,False,statistics,1512865712,True,"So when we started learning linear regression. One of the assumption is that Y and X's have a linear relationship. And one way we would check for this is to plot the residuals against Yhat and X's. 

So my question is would we still need to worry about linearity with higher order terms in the model? Because technically Y and X are no longer just related linearly. "
What's the motivation to define Fano factor?,0,1,False,False,False,statistics,1512881256,True,[deleted]
What's the motivation behind Fano factor?,0,3,False,False,False,statistics,1512881853,True,What's the motivation behind the definition of fano factor ;[ \sigma^2 /\mu]; . How does it related to the SNR (signal noise ratio) of the random process? 
Residuals of Poisson regression - variance stabilizing transformation and Pearson/deviance resids.,10,10,False,False,False,statistics,1512885292,True,"I'm trying to teach myself everything related to Poisson regression. I've fitted model on DoctorVisits datasets from AER package, in R. 

    library(AER)
    poiss &lt;- glm(formula = visits ~ ., family = poisson(link = ""log""), data = DoctorVisits)

I would like now to display residuals on a scale, on which they should, in theory, appear iid. I recall from stats theory, that there is something like variance stabilizing transform, and for Poisson distribution we have to divide by value, square rooted. However, I'm not quite sure what to do here - am I supposed to divide prediction? Residual?

Another option I've tried are Pearson residuals and deviance, however I residuals are clearly not iid - but I don't really know if they should be? My guess would be that they should appear normal from N(0, 1). 

For reference, I've created simulated data and fitted Poisson model.

    x &lt;- rnorm(300)
    x &lt;- matrix(x, nrow = 100, ncol = 3)
    y &lt;- rpois(n=100, lambda=rowSums(x)-min(rowSums(x))+1)
    simul.fit &lt;- glm(y~x, family=poisson())

Plots are here:
https://imgur.com/a/9ZWXM

They look iid, so that would suggest that analogous plots for DoctorVisits model are not OK.

So, two questions remain:
1) How do I do variance stabilizing and is it just Pearson residual?
2) Should Pearson and deviance residuals be normal with zero mean?"
Covariance Matrix Comparison - How?,4,5,False,False,False,statistics,1512898895,True,"Hello everyone,
I need to ""compare"" two covariance matrices - one is based on an estimator and the other has the theoretical value it estimates. What would be a good way to compare them? I want to see how the estimator affects the covariance matrix, and have no idea how to compare them.
Thanks in advance"
What (simple) test is this study using?,10,2,False,False,False,statistics,1512914847,True,"Hi everyone,

I am reading this paper (http://www.jstor.org/stable/350880) that has the following table in it: https://i.imgur.com/aqWZv3O.png

As in any good sociology paper they do not explain what what was computed exactly but they do give a chi-squared value and a hint at a p-value (&gt;.05 lol).

In the paper they write 

&gt; ""The first finding of substantive interest is the manner in which subjects responded to the items ""Does it make sense to say a woman (man) has lost her (his) virginity?"" (The results in Table 1 are cross-tabulated by the sex of the respondent.) In short, Table 1 indicates that there is no significant difference between males and females as to the ""zero-sum""conception of ""loss"" of virginity. Thus, the component of sexual ideology relating to the meaningfulness of virginity does not appear to be subject to differential adherence by sex.

It's probably stupid but I'm missing what kind of test they are using. Can anyone figure it out?

Thanks in advance

edit: Of course I tried to apply the chisq-test but I get 1.2389 instead of 1.5059…

**edit2: The were indeed doing a chisq test, but I was comping it wrong (with a correction whereas they didn't use it). Have a nice day everyone.**"
Need help with correlation and about quantifying occupation and place of residence.,0,1,False,False,False,statistics,1512916428,True,[removed]
I just got a B in my 3rd year Mathematical Statistics course. How will this affect my chances of getting into a masters program?,5,0,False,False,False,statistics,1512922862,True,"I unfortunately got terrible marks on both the first midterm as well as the final. Both times largely due to errors on my part (the first test I didn't bring a calculator, the exam I simply got flustered on). Otherwise, my grades are strong, I have a 3.7 GPA overall. I am planning on going into 400 level stats courses next term, so I'm hoping success in those classes can make up for my math stats mark. Any advice?"
Question about transforming a skewed distribution in the context of process control and setting pass/fail specification limits,1,1,False,False,False,statistics,1512928348,True,"Hello!

In the application of statistical process control on hardware, sometimes a measured distribution will turn out to have a very skewed distribution rather than a normal one. In this case, setting specification limits based on standard deviation (to guarantee a CPK value) in my understanding essentially assumes a gaussian distribution, and a skewed distribution will hence either under or overestimate the probability of a part occurring at the tail, depending on the direction of skew.


In this case, would it be fair to use a transformation (log, sqrt, box-cot etc) in order to move the distribution back to normality, and set standard deviation based limits in the transformed space? 

For example, let's say we have [this situation](https://i0.wp.com/www.r-statistics.com/wp-content/uploads/2013/05/zumel_fig_1.png?ssl=1)

 If I had set 4x standard deviation-based limits on the top plot for upper-side limit, that might put me somewhere around $300,000 but a gaussian fit on the data would actually show that the model over-predicts density at that point, since the data is lognormal. 

Would it make sense here to to transform the data to the lower plot and set 4*standard deviation limits in that space, as it is much closer to a normal distribution? 

Thanks for any advice. 
"
Can someone explain is Table for me ?,2,0,False,False,False,statistics,1512929728,False,
Can my research question be tested statistically?,5,2,False,False,False,statistics,1512931345,True,"Hello Folks, So I am not super statistics savvy but I have done enough research to get myself through the basics. I am conducting new research but I feel like I'm confusing myself on the realm of what is possible to statistically test and HOW I would test it.
I would appreciate all the guidance anyone has to offer.

The study:
My study will involve participants conducting trait measures of mindfulness (along with perceived stress, and depressive symptoms) and then be put in to either a 
1) Mindful meditation condition
2)Mindful colouring condition
3) Control condition
Following the completion of the condition they will fill out some questions measuring their state-mindfulness (as well as stress and mood). END STUDY

My first research question is whether the different levels (high or low) of trait mindfulness predicts levels (high or low) of state mindfulness following their mindful condition. 

But I also want to see whether any predictive value from the trait is influenced by the condition and if so how much. So is the level of state-mindfulness merely just a consequence of the condition that they were in rather than their level of trait-mindfulness. 

Im sorry if I have explained this poorly, I will clarify any queries. 
I am very appreciative of anyone who takes time to help me with this problem as I believe I would be right in conducting a some sort of regression but unsure which one or how. 

Thank you very much in advanced
"
Studying Statistics After Graduating with Math Degree,21,20,False,False,False,statistics,1512935715,True,"Hey guys,

I graduated with my BS in mathematics but hardly studied any statistics. 

What's a good place to go online to begin learning the basics I would need to know to conduct and interpret research studies? 

Bonus points if it is focused around explaining the concepts + showing how to implement them in Python (I am aware that R is more easily suited for this but I'm honed in on python right now). 

Edit: 

Fine, I suppose R is acceptable as long  as the course explains the statistical concepts. "
Basic Question On Probability/Bayes' Theorem,1,3,False,False,False,statistics,1512938787,True,"I'm quickly revising some basic probability for my machine learning module, apparently I've completely forgotten even the basics.

From Bishop's book on Pattern Recognition and Machine Learning he gives a simple example (~p39) of a scenario where, during a cancer diagnosis, X-rays are taken of a patient, stored as a vector x, and the cancer diagnosis is a discrete value Ck, 0 for no cancer, 1 for cancer.

So... p(x, Ck) is just the joint distribution which comes from the data.

p(Ck) is the prior probability of having cancer.

P(Ck|x) is the posterior probability of having cancer given the image data, found using Bayes' Theorem.

I can't wrap my head around what p(x) or p(x|Ck) would be conceptually.

Is it just a kind-of-meaningless average probability of pixel intensity at some point on line of pixels? And p(x|Ck) is the probability of that average intensity pattern given a cancer diagnosis?"
Question about dependent distributions I don't know how to answer,0,1,False,False,False,statistics,1512950191,True,[deleted]
"When to report ""I Don't Know"" responses?",3,3,False,False,False,statistics,1512955891,True,"Hello all.

I was having a discussion with a few friends about this, and I wanted to hear others' opinions.

Should non-responses (""I don't know"", ""I can't answer"") always be reported in analyses? Or is it appropriate to make it clear that you are only reporting the results from participants who assessed your question, say as true or false, and then report only those participants who answered ""True"" or ""False""?

Thanks."
"I’m not the brightest at math but I feel like this question has to have an answer. If I were to give you an answer - in number format - then give you all the variables - in number format - would you be able to reverse engineer a formula? Better yet, could you reverse engineer a formula with 100’s",8,2,False,False,False,statistics,1512958927,True,"Of sets of these answers and formulas?

So if I said the answer is 61 and 

A=85
B=15
C=20
X=24
Y=12 
Z=50

And then gave you 99 more of these answers and variables.

Would you be able to create a formula that would work for each but the formula would be the same? 

What I am looking for is a formula where I can plug in a,b,c,x,y,z and it would pop out an answer +/- a minimal amount... 
"
"Aside from a meta-analysis, is this a no-no?",1,2,False,False,False,statistics,1512964786,True,"I conducted a study using secondary data. There are other datasets where I can use the same type of data to answer my same RQ. My goal on using multiple datasets: to get a good picture nationally (nationally representative samples) of my RQ. 

Note: Though unlikely, some of the same people may have participated in more than 1 survey. 

Is this frowned upon? I would NOT be appending or merging datasets, but rather reporting: That A dataset had this result, B dataset had this, etc.

Would it be better to publish A dataset results, then publish B dataset results, etc.?"
What’s Wrong With 538?,14,6,False,False,False,statistics,1512971144,False,
How to resolve violating i.i.d. for Logistic Regression?,2,1,False,False,False,statistics,1512983516,True,"I have a dataset that looks like the following

id | Person1 | Person2 | Response | Var1 | Var2 
:--|--:|--:|--:|--:|--:|--:|
1| Alice | Bob | 1 | 1000 | 1000
2| Alice | Charlie | 1 | 1050 | 1000 
3| Charlie | David | 1 | 950 | 1000
4| Bob | David | 1 | 950 | 950
1| Bob | Alice | 0 | 1000 | 1000
2| Charlie | Alice | 0 | 1000 | 1050
3| David | Charlie | 0 | 1000 | 950
4| David | Bob | 0 | 950 | 950

I am trying to fit a logistic regression model. The idea is I want to predict who will win a match (Response = 1) based on Person1's rating (Var1) and Person2's rating (Var2): Response ~ Var1 + Var2

Originally I just had rows 1 through 4, but then all my responses are 1. So I duplicated the entire dataset, flipped all the variables, then attached it. As you can see rows 1-4 and rows 5-8 are data from the same matches (id). 

I'm assuming that I will need to do a logistic regression, but I am also certainly violating the principle of i.i.d. (since the residual for row 1 will be the opposite sign of the residual of row 2, and so on). What should I be doing instead?"
How to lie with Statistics?,13,4,False,False,False,statistics,1512994876,False,
Still don't understand why the p-value distribution is uniform when H0 is true.,29,23,False,False,False,statistics,1512999165,True,"I've read topics on it here, I read papers on it and Googled it, and I still don't get it. 

As I understand it, this  means that if we assume there is no effect (H0), the chance of finding a p-value of 0.01 is just as great as finding a p-value of 0.76 or any other value. Intuitively it seems to make sense (to me) that if we assume no effect, we are more likely to find data in support of this. In other words, the distribution of p-values would then be skewed towards 1.00.

When the p-value distribution is uniform, simplified I guess you could say that in 50% of cases we are tending more towards refuting H0 on the basis of 'unexpected' data, and in the other 50% of cases we are tending more towards retaining (.00 &lt; .50 &lt; 1.00) on the basis of expected data under H0.

Where is my understanding off track?"
Books for working with time series data/time series analysis,4,5,False,False,False,statistics,1513013753,True,"First year PhD student in political science. Can anybody recommend any good books on getting started with time series analysis? FWIW, I'm interested in working with time series data in the context of explanatory/causal modeling, not so much forecasting."
Unclear on what test to use,14,1,False,False,False,statistics,1513018127,True,"Hi all,

I am trying to analyze the data but struggling on which test to use.
I have been assessing children's preferences for food and leisure items. I've scored them on the percentage of trials they chose to consume each class of item. Each assessment has 4 food items, and 4 leisure items.

At the end, I have a mean percentage of consumption for 4 items in each class. I'm trying to determine if the difference of consumption between the two classes is significant. 

I started using a paired sample t-test: Take the 4 percentages of consumption for the edible items and compare them to the 4 percentages of consumption for the leisure items. 

I'm not sure if a one-sample t-test is more appropriate, or how I would calculate it. since the one-sample t-test relies on a difference score, I was planning on subtracting the mean consumption of edible items from mean consumption of leisure items, but that will only leave me with 1 score (and no standard dev.)."
How important is Linear algebra for Statistics?,5,2,False,False,False,statistics,1513020041,True,"Basically how good should I be at linear algebra for statistics? I have forgotten quite a bit, the main ideas that I currently still remember  are linear independence, solving system of linear equations, basic matrix operations (inverse, multiplication, determinants etc). eigenvalues. 

Is it really necessary to know about bases, orthogonal projections, vector spaces and sub spaces, linear transformation,  etc? "
What is the power relationship between multivariate and univariate regression?,13,4,False,False,False,statistics,1513030317,True,"Lets assume I have three possible outcome variables (Z1, Z2, and Z3). I have one predict (X).

If I run a multivariate regression with X predicting Z1, Z2 and Z3, does this have more or less power to detect an effect of X on Z1 compared a univariate regression with X predicting Z1?

Edit:

Suppose I have a multivariate model like this: X predicting Z1, Z2 and Z3.

I also have a univariate model like this: X predicting Z1.

Which model has more power to detect an effect of X on Z1?"
I am working on a paper and I'd like to have someone read a 1-2 page part involving order statistics to see whether I've made any mistakes or written anything that doesn't make sense.,2,0,False,False,False,statistics,1513038090,True,I am working on a paper and I'd like to have someone read a 1-2 page part involving order statistics to see whether I've made any mistakes or written anything that doesn't make sense.  I can pay remotely.  
"My method for guessing on multiple choice exams, does it work?",11,0,False,False,False,statistics,1513039558,True,"So this semester I have employed a new strategy when I arrive at questions for which I am clueless and need to take a random guess:

I choose option A. Every time. My reasoning behind this is that by going straight down the line and filling in the same bubble for each question that I leave blank until the end, I am keeping the probability of choosing correctly at a consistent 1/5 instead of changing the bubble each time. So instead of going ""A, C, D, A, B"" i pick ""A, A, A, A, A"".

Since each question is independent of the last I am not sure if doing this benefits me, but if anybody has knowledge or an opinion on this I'd be happy to hear. My guessing game feels like it's gotten better but it could simply be luck or coincidence.
"
Which is independent and which is dependent in the question “Are there gender differences in dating violence victimization”?,0,0,False,False,False,statistics,1513040930,True,[removed]
finding critical values of a negative t,4,1,False,False,False,statistics,1513054557,True,"First year stats student here. I've just completed an online assessment for uni and came across a question that had me stumped. 

I'm fine using the t table for questions like:

Find t_(0.25,14) 
The answer being 0.6924

But the next question was (with a - before the t):

Find -t_(0.25,14)

I figured it can't be a typo, but I can't find anything in the textbook and I'm sure we didn't cover it. It's too late now, but I'd like to know for final exams."
Uniformly sample a fraction of data for a given time window.,0,1,False,False,False,statistics,1513064570,True,[removed]
Chi Square Test and Odds Ratio,1,3,False,False,False,statistics,1513068540,True,"Hi, first post here. I am doing this paper on whether a hockey player's age affects his point production. I did my chi square test and found that these categorical variables are independent of each other. Now I want to do an odds ratio test to see how independent these varaibles are. When constructing my odds ratio contigency table, do I use my observed values or expected values from Chi Square? Lastly, how do logs come into play in the odds ratio. The website I am looking at speaks only on taking the ratios. Here is said website, http://teaching.sociology.ul.ie/SSS/lugano/node16.html

Thanks in advance, apologies if these questions don't make sense or are plain stupid, honestly quite lost."
p-value in correlation,16,1,False,False,False,statistics,1513073065,True,"I was reading Andy Field's text (Discovering Statistics using R) and in it he mentioned: ""Correlation coefficients are effect sizes, so we can interpret these values without really needing to worry about p-values."" The rationale is that p-values are related to sample size, so there is no need to worry about them for evaluating correlation. My very limited knowledge of statistics has led to me thinking that p-values is the chance of obtaining the observed (or more extreme) results. Does this not matter in interpreting correlation?"
Using Fisher’s Exact Test for Small Sample Contingency Tables,1,10,False,False,False,statistics,1513074933,False,
Which statistical test?,0,1,False,False,False,statistics,1513083136,True,[removed]
Which stats test?,0,1,False,False,False,statistics,1513084020,True,[removed]
Is this calculation correct? [chi-square test],9,0,False,False,False,statistics,1513086584,False,
SEM,0,1,False,False,False,statistics,1513097867,True,[removed]
Type I Error ‘not that bad’?,0,1,False,False,False,statistics,1513099165,True,[deleted]
Type I Error overhyped?,3,1,False,False,False,statistics,1513100060,True,"I just read a paper by Rothman (1990) arguing that “no adjustments are needed for multiple comparisons” (=title). Does anyone have a similar opinion on this subject? I feel like there is not much room for ignoring the dreaded alpha accumulation that comes with repeated testing of the same variables. Any thoughts about his bold opinion? 💭 
"
ANOVA with multiple response set in SPSS,0,1,False,False,False,statistics,1513103168,True,[removed]
Nate Silver Methodology,3,12,False,False,False,statistics,1513105187,True,"Can anybody tell me the formula/techniques Nate Silver used in this article? Any help would be greatly appreciated! 

https://fivethirtyeight.com/features/the-u-k-snap-election-is-riskier-than-it-seems/"
How does one design regularizers such that they match the parameters that generated the data when one has strong priors about the parameters?,2,1,False,False,False,statistics,1513112796,False,
Help with win % of a betting strategy given independent parameters,6,1,False,False,False,statistics,1513113591,True,"I've back-tested some parameters and am curious if there is a way to figure out expected win % between two teams?

Say team A has a 70% chance of winning given it's current parameters over 20 total bets of data with the same set of parameters.

team B has an 83% chance of winning given it's current set of parameters over 7 bets of data with the same set of parameters. 

So team a and team b have different parameters giving different win %...making them indepedent. 

If we can't combine win %...is there a way to use amount of bets instead? I would imagine that 20 bets of data with a 70% is better than going with 83% over just 7 bets of data. "
statistics tattoo ideas?,14,4,False,False,False,statistics,1513116160,True,anyone have any ideas for tattoos regarding statistics-- specifically related to the big data/ data mining side of stats
How to find the margin of error without the sample size?,12,1,False,False,False,statistics,1513136281,True,"Finals week is slowly killing me. 

Edit: the actual question I'm working on states:

""An IQ test is designed so that the mean is 100 and the standard deviation is 23 for the population of normal adults. Find the sample size necessary to estimate the mean IQ score of statistics students such that it can be said with 90% confidence that the sample mean is within 5 IQ points of the true mean. Assume that standard deviation = 23 and determine the required sample size using technology.""

Edit #2: Solved! Thank you all so much!"
Pointers on modelling horse races?,0,2,False,False,False,statistics,1513140198,False,[deleted]
How do polling agencies select a truly random sample of respondents for their surveys?,8,10,False,False,False,statistics,1513141372,True,"Thanks to statistics, we can learn a lot about population characteristics from a random sample. But how do polling agencies (or market research firms) come up with samples that satisfy the need for ""a random sample""? What techniques do they use?

Edit: Not sure I used the most appropriate flair for this question. Apologies if I didn't.
Edit2: Changed the flair to ""Statistics Question""."
Online Homework Help - Tutorspoint.com,0,1,False,False,False,statistics,1513151817,False,
Help with non-linear regression: Check my modelling methodology!,5,4,False,False,False,statistics,1513177580,True,"Hello,

For my research, I am attempting to estimate two parameters from some time-course data. There is a physiological phenomena that theory predicts to behave in a non-linear way. The expected amplitude of measurement can be modelled as a function of time, as well as two other parameters. With my measurement apparatus, I can collect amplitude values at 9 different time points. I then fit this time course data to our physiological model and determine the optimal values of the two free parameters. **My question** is, what is the best way to get a ""goodness of fit"" value that I can use to accept / reject the model fit.

My best attempt so far is as follows. Following the formulation seen here (https://en.wikipedia.org/wiki/F-test#Regression_problems), I calculate the residual sum of squares (RSS1), and the total sum of squares (RSS2), set p2=2 (parameters estimated by model), p1=0 (parameters of null model), n=9 (number of observations from which the parameters are estimated). This then gives me an F-statistic which I can then check against a look up table (but I'm not sure what my degrees of freedom are here?) and exclude based on some threshold from there.

Does this look right? What are me degrees of freedom? Thanks in advance."
How to setup parallel computing for R using RStudio in the aws cloud,2,20,False,False,False,statistics,1513179713,False,
Online graduate regression course?,4,3,False,False,False,statistics,1513190798,True,"Hi all,

Looking for an online graduate level course in regression that I can take. Anyone know of a such thing?

Thanks!"
Pearson's Correlation VS Cramer's V,1,1,False,False,False,statistics,1513194949,True,[deleted]
Advice for soon-to-graduate grad students?,2,0,False,False,False,statistics,1513202525,True,"My apologies if there have been similar threads already.

I'm currently doing my masters in Statistics (I did my undergrad in Stats as well) - I'll be finishing my degree this time next year. I've worked over a summer with a professor during my undergrad as a research assistant, and of course I'll be graduating having done a dissertation and taken on a few small programming projects as part of coursework (using R and Python). But other than that, I've no relevant experience. My GPA should be fairly competitive, but not perfect by any means (~3.7ish). And of course, I was a fairly immature person in my late teens/early twenties, and only picked it up academically in my last two years (and it took me longer than it should to graduate having switched majors twice).

My question: as someone with not much experience, what are some things I can start doing already to improve my chances of getting a good job once I graduate? I live in a city where quite a few finance and tech companies have their European offices (Dublin), so that's an advantage as well.

Appreciate all the comments!

EDIT: Also, what are the opportunities to move into management after a few years of experience? "
Is there a relationship between variance in a dependent variable and the R2 of a model predicting it?,1,1,False,False,False,statistics,1513203482,True,"I can imagine this both ways. Larger variance would mean more variance to explain, so larger R2? Or, larger variance means the dependent variable is less predictable so lower R2. 

Is there a tendency for these two things to be related?"
Appropriate use of a binomial distribution,3,0,False,False,False,statistics,1513209168,True,"I want to double check my thoughts regarding a use of a binomial distribution model I haven't seen before.  

I have a situation where land cover could change in a given area - this area is comprised of a lot of little areas, so I'm looking at change in terms of percentages (from which comes my question).

The number of trials (years) is known.  Say there are 52 years in which a change is possible.  The probability of a change in any given year (assume independence) is 3% (0.03).  

I run this in R (1 - pbinom(1, 52, .03) and get 0.46.  Can I interpret that to say 46% of my landscape is likely to change?

It doesn't seem quite right.  But, if I do that, I get quite good matches with actual ground data:  46% predicted vs. 42% changed in one area, 85% predicted vs. 80% change in another (where p=.06 and trials = 55).


"
Correct test for drug study,0,2,False,False,False,statistics,1513210006,True,"I have a drug study I am reading through. Although the outcomes look great, I am unsure if the proper statistical test was used. Can somebody please message me so I am able to send you more specific information?"
How to determine which factors/interactions to remove from a model?,2,3,False,False,False,statistics,1513211718,True,"How do you determine which factors/interactions to remove from a model? Should I be dropping higher-order interactions from my model if they aren't significant and aren't increasing my R-square/F-ratio? Asking specifically for an ANCOVA that I'm running (two categorical IV's and one covariate) but am curious about for other models as well (for example a generalized linear model). Can we run a number of models and decide what to include based on on AICc? In the case of GLM's, can we simply rely on selection methods (i.e., Lasso, elastic net, etc.) to remove factors/interactions? Or are we going to have to use our background knowledge of the system to determine what is important (and should be included) and what might simply be ""nuisances"" (and can be removed if they're not adding anything to the model)?"
NBA - significant trend?,1,2,False,False,False,statistics,1513211881,True,"Okay so I have a player. In 3 point shooting LeBron has, before this season, made 1467 shots on 4295 attempts. 1467/4295 is 34.156%. A shot ends with either a make or a miss (1 or 0).  

This season, LeBron has made 57 shots on 135 attempts, which is 42.222%. What I want to know is if his increased percentage this season is statistically significant?

I've looked online and done a t-test with one tail, a t-test with two tails, and a binomial test. I get slightly varying p-values and tbh I'm a little bit out of my depth here, and rusty because I haven't done any statistics in years. I think the binomial test sounds right because it's a 0 or 1 type of thing. **What test should I use and what outcome seems right?**

I used http://vassarstats.net/binomialX.html with n=135, k=57, p=(1467/4295), and it gave me a p value of 0.0310. **Is that statistically significant?** 

That p value is quite similar to the 0.0304 I got from using a pre-made excel workbook for a 1 tail t-test"
"Quick Question: Assume data pairs (Xi, Yi) for i = 1...n have been observed under the SLR Model. If y tends to do nothing as x increases, then the variable are (blank) linearly correlated.",1,0,False,False,False,statistics,1513212023,True,"A. Positive 
B. Negatively 
C. Not
D. None of the above"
How Stitch Fix uses Item Response Theory and random effects to model latent client clothing sizes,3,52,False,False,False,statistics,1513214439,False,
"How do I handle proportion data? When is it appropriate to use each method (e.g., beta regression, logistic regression, transformations, etc)?",1,2,False,False,False,statistics,1513222076,True,"It seems like there are so many ways to handle response variables that are proportions. I've seen some sources support arcsine and logit transformations while other sources have touted beta and logistic regressions. It's become really overwhelming and I'm not sure when I should be using each one. Can anyone shed some light on when I should use each method when dealing with response variables that are proportions? What do you do when your response has 0's?

Bonus question: how do I deal with proportional data where each replicate that the proportion is calculated from has a different sample size? For example, one replicate has 6 individuals while another replicate has 10 individuals that I'm calculating the proportion from."
"Statistics, we have a problem. – Kristian Lum – Medium",36,6,False,False,False,statistics,1513227801,False,
Please Help With This Stats Problem..I can't figure it out!,4,0,False,False,False,statistics,1513233089,True,"Using the data set attached to the email in which this exam is also attached, conduct a multivariate linear regression analysis that attempts to explain the variance in mass trust in government (the variable name is trustgovnever). Observe and interpret the RELATIVE predictive capacity of race (white vs. nonwhite), gender, age, income, education, party identification, and the degree to which one follows public affairs. 

First, evaluate the STATISTICAL significance of each independent variable. 
Second, compare the SUBSTANTIVE significance of each variable, determining the relative levels of trust for different categories of these variables, by using the constant/intercept and the regression coefficients in the linear algebraic formula we discussed in class (y=a +b1x1+b2x2, etc.). Draw conclusions about the strongest and weakest predictors (independent variables).  

Pay very close attention to the direction of the relationship, and how these variables are coded (the direction of whether higher values correspond to more or less trust, and the number of “unit changes” in the variable), by using the frequency command before you run the regression.

https://ibb.co/c4Rj4m for the data"
Confidence Interval,3,2,False,False,False,statistics,1513245833,True,[deleted]
A Primer on the General Linear Model,4,20,False,False,False,statistics,1513258142,False,
Calculating when to stop advertising using confidence intervals,18,8,False,False,False,statistics,1513260530,True,"I run adverts, which are triggered based on keywords. e.g. I have keyword1, keyword2, keyword3 and so on. The process is advert&gt;landing page&gt;sale.

The data I get back from my reporting is: sales quantity, sales revenue($), Click Through Rate on the advert (5% means 5% of people click an advert when seen), impressions (number of times an ad is seen), conversion rate (% of people who buy once visiting my page).

I would like to apply science to the decision making behind my advertising. Specifically, when should I stop paying for adverts for a specific keyword? I was thinking perhaps I need to calculate when - with 90% confidence - a specific keyword is not going to lead to a profit [Edit: not sale as previously written].

Is this the right approach?

If this is the right approach, how do I calculate how many clicks are needed before I should prune that keyword from my marketing?

Thank you,

Jon"
What would be the most powerful test (ie leading to the smallest required sample) for comparing rare event rates?,2,5,False,False,False,statistics,1513264131,True,"Background: We have two widgets: WidgetA and WidgetB. Both widgets test to see if some process failed to meet standards. WidgetA is ol'reliable and has a false positive rate of somewhere near 1/100,000. WidgetB is thought to be a newer, quicker improvement on WidgetA but they suspect that the false positive rate is 1/1,000. 

We can generate paired data, WidgetA and WidgetB can both assess the same instance of the process in question. McNemar's test, Fisher's exact test, Chi-sq test of independence, score tests for proportions all give equivalent results in terms of power calculations, ie &gt;6000 per group to attain 80% at 0.05.

Is there some test that I've overlooked? Or am I SOL given how rare the events are?"
"I have a categorical predictor, and a binary response, I need help",0,1,False,False,False,statistics,1513268171,True,[removed]
Unable to replicate results for multiple regression across various platforms: what am I doing wrong?,2,4,False,False,False,statistics,1513268288,True,"I am new to statistics and trying to implement multiple regression for the first time. Here is my dataset: https://drive.google.com/file/d/11-qBrRT9Ar7UW2tugvwSdXx-NZf0OMiD/view?usp=sharing

Here are my methods and results:
https://docs.google.com/document/d/1cdPI7A5rSDX8R_qzLcDwj_oWD7hJ129UPC2Bhl75qOY/edit?usp=sharing

Just looking at R^2, adjusted R^2 and coefficients: R^2 matches across all three results, but coefficients are different across results. Adjusted R^2 matches for python and R but not for excel.

I have not looked at the other results because I'm not terribly sure how to interpret them in the first place. So... why would R^2 match but all coefficients be different? Why am I getting different results in the first place? Are these methods running different ""versions"" of multiple regression somehow? Thanks!"
Blog explaining why you should think of statistical power as a curve rather than a static,3,2,False,False,False,statistics,1513269577,False,
Confidence Interval,10,0,False,False,False,statistics,1513276168,True,"I have a data set, it’s not normally distributed and is made up nine observations. 

The observations are estimates of a true population value. On average these estimates are discovered to be incorrect by six units. The standard deviation is three units.

How do I calculated a 95% Confidence Interval for reference on future estimates? "
Are percent cover values already standardized?,1,1,False,False,False,statistics,1513279588,True,"If I'm using percent cover (i.e., coral cover, or algae cover) as an input into multivariate analyses, do I need to standardize/transform the data sets, or are they already ""standardized?"""
Acceptable timeline of job offers?,0,1,False,False,False,statistics,1513285510,True,[removed]
Dataset for discriminant analysis,1,1,False,False,False,statistics,1513286712,True,"Hey everyone,

I'm looking for some kind of repositories for data I can do a discriminant analysis on. Indeed I have an assignmment to do on complex statistics methods. I'd be very grateful if anyone could direct me somewhere I can find various datasets. In case someone has some data at hand it would be even better. The analysis will be done in SPSS.

Thanks in advance!

"
"Quick question of dealing with percentage data, ANOVA? Chi?",3,1,False,False,False,statistics,1513288539,True,"Hello and thank you for helping me out,

So I have collected data, about the usage of a drug for a disease across a region. I want to know if certain health centres (16 in total), tend to prescribe this drug more often than other. So i have the numbers of patients taking/ not taking this drug in each health centre. What is the best way to analyse this data? I have put them into a contingency table in prism and chi squared reveals significance. Is there a better way to do it?"
The Derivative of a Lowess Function,2,1,False,False,False,statistics,1513288773,True,"Hey all,

I posted this:
https://www.reddit.com/r/rstats/comments/7g02j0/fitting_data_with_a_logistic_function_or_taking/
on /r/Rstats a little while back and didn't get a good response, so I'm hoping I'll have better luck over here.

I'm looking at fan behavior as it relates to the success of a baseball team. As such I have this graph: https://i.imgur.com/Z0F6eTZ.png of attendance as a portion of the stadium vs the team's winning percentage. The red line is a lowess regression line.

I would really like to know the marginal attendance rate by winning percentage by just taking the derivative of this curve, but I really haven't found much on how to do that.

When I've tried estimating this with ""S"" shaped functions like the Logistic or Gompertz functions I don't get anything that looks close to what I need.
(side note: I think a lot of my problem is that I get an error when I try to limit the data in R between [W%&gt;0 &amp; W%&lt;1] so I use all the data and there's a lot of bias in the data as a result of high opening day attendances. If anyone has input on how to do this in an nls without error it'd be appreciated)

So I really have 2 questions: 
1) Is there a way to take the derivative of a Lowess function?
2) Is there some easier way to estimate this with a regular function that I doesn't require me to have to take the derivative of the Lowess?

Thanks"
Online tools to construct probability distributions?,6,1,False,False,False,statistics,1513289466,True,"Are there tools online to help you construct probability distributions?

I've seen [plotly](https://plot.ly/create/?fid=PythonPlotBot:4) has an area curve creator, but the problem is that it doesn't force the area under the curve to be 1 so that it's a proper probability distribution.

I think having such a tool would be really useful in helping you think probabilistically. Does such a tool exist?"
Why statistics?,32,27,False,False,False,statistics,1513303952,True,I'm currently a high school senior and I was wondering why you guys chose to become statisticians? What differentiates this field from other fields and are there any experiences that drove you towards this field. Thank you :)
"Statistical question in /todayilearned; when comparing the rate of tourist ""accidental death"", and CDC US population accidental death; what would the proper methodology be",3,4,False,False,False,statistics,1513326855,False,
"Are the percentages of, say, favorable outcomes for discrete events (per unit time) continuous or discrete?",4,1,False,False,False,statistics,1513331442,True,Looking at objectives completed out of a total (in the 1000's) per month. I am not a stats man but as the outcome could be any number between 0 and 100% I suggested it was continuous.  I am trying to give a friend a hand with her work but fear I may have given her bad advice.
Advantages and disadvantages of slicing and dicing A/B test results to sub-populations?,31,9,False,False,False,statistics,1513335599,True,"We ran an A/B test at work on out website users and found a statistically significant difference in conversion rates from x to y in one of the treatments.

Since we have more data (e.g. user attributes like gender, work title, location...) I was asked to slice and dice the results further, to see for example if there's a difference between males and females.

I think an obvious disadvantage (or issue) with doing that is that we might not have enough observations in each of the sub-populations to draw a valid conclusion.

A possible advantage is to get more insights on how our users were affected and maybe offer them different experiences.

Are there any other disadvantages / pitfalls / things to watch out for?

Thank you for your insights"
What field is better to pick for my master studies: Content based retrieval or Applied Statistics? I want to hear more about these two fields and make a comparison.,0,1,False,False,False,statistics,1513337481,True,[removed]
Super beginner question - test to determine if race affects acceptance rate,9,0,False,False,False,statistics,1513349268,True,"Hi, everyone!

It's been a REALLY long time since I've taken any sort of statistics class, and my Google-Fu isn't that great, so I have a simple question for you folks.

I have a data set that identifies a person's race (6 possible values), zip code, and whether their application was accepted (simple binary yes or no). What test do I use to determine if there's a statistically significant difference in acceptance rates of different races within a zip code?

I thought about just doing some binomial distribution math - suppose we have a zip-wide acceptance rate of 80%, but only 2 out of 10 African Americans were accepted - I could easily figure out, given an 80% acceptance rate, that the probability of getting 2 or fewer successes out of 10 is (10 nCr 2) * 0.8^2 * 0.2^8 + 10 * 0.8 * 0.2^9 + 0.2^10 = 0.000078 (around 1 in 12800), but that doesn't seem quite rigorous enough, especially if the zip-wide sample size isn't large enough.
 
Thanks in advance!"
"If I run a regression on an entire population (not a random sample), should I care about the P value?",0,1,False,False,False,statistics,1513352039,True,[removed]
I wrote a blog post about using settlers of catan and loaded dice to explain p-values and their flaws,46,57,False,False,False,statistics,1513356090,False,
Could wage statistics potentially be biased because society places a higher pressure on men to make lots of money?,6,2,False,False,False,statistics,1513357583,True,[deleted]
Why is it so important to have principled and mathematical theories for Machine Learning?,2,1,False,False,False,statistics,1513365709,False,
how difficult is it to get into masters programs?,3,11,False,False,False,statistics,1513367955,True,"Basically, I had a rough start to college and received B's in all of my calculus class (and had to retake one of them) and my intro stats course. I'm now taking regression analysis and I think I'm either going to barely pass or have to retake it. This is because I bombed my first midterm, and then had a few mental health issues throughout the quarter (not an excuse but sometimes life really does make things hard). These are the only classes related to the major I've taken so far but I am a junior. Will I have a shot at mid ranked schools? I'm just starting the major so there are a lot more classes I'll be taking but I'm feeling a little down about this. I'm also a bit confused because in other fields I've looked at, grad programs are very competitive but when looking at stats forums it seems as though typically those with low gpas do masters to get into phds, so is the bar lower for masters programs?"
Need Multivariate Datasets for analysis,0,1,False,False,False,statistics,1513373332,True,[removed]
Number of draws to reduce draw size to zero,8,6,False,False,False,statistics,1513390071,True,"I am making a card game and looking for some statistics help. basically, players have a hand of 5 cards and a deck size of X. When a player takes damage N wound cards are shuffled into their deck which increases their deck size by N. At the start of every turn, a player discards all non wound cards and then draws back up to 5. so if a player had three wounds in hand they would only draw two cards. What I am looking for is an equation of on average how many draws would it take before all five cards in your hand are wounds, given a deck size of X and N wounds in the deck.

Discarded cards are not shuffled back into the deck "
"How would you simulate a draft lottery number that influences a binary ""military veteran"" variable?",5,0,False,False,False,statistics,1513393251,True,"I want to simulate some variables - such as whether someone is a veteran (""Vet"", 0 or 1), which is influenced by L, their lottery number. How can I simulate L in a logical way? Lower L values should make it more likely someone is a veteran (since lower numbers means more likely to be drafted). However, vet still needs to be influenced by randomness as well as education level.

I was thinking L could simply be a series of numbers from 1...n. Then I could ""create"" Veteran in the following manner, and invent some sort of cut-off where anything above a specific value is 1 and anything below that is 0.

generate Vet = e (error term taken from a normal distribution) - Education - .10(draft lottery number)

Then invent an arbitrary cut-off point. This obviously makes it so whether or not you're a veteran highly depends on where you appear in the dataset (since lower n means more likely to be a veteran), but I figured for the simulation purpose that makes sense (since that's sort of what the draft lottery number is doing to someone anyway).

"
"Depressed, did bad in statistical theory class. Need advice, etc.",0,1,False,False,False,statistics,1513396242,True,[removed]
(Simple) How do I solve for the number of iterations this program will go through?,5,1,False,False,False,statistics,1513418670,True,"Basically, as kind of a meme based off a reddit thread I wrote [this](http://prntscr.com/ho8db7) short python program.

The list has 24 items, of which there are two 2s, two 3s, two 6s, two 47s, three 345s, and the rest of them are unique numbers. 
 My best attempt at a solution right now is something along the lines of (1/24)\*(2/23)\*(2/22)\*(1/21)\*(1/20) etc 

but really I have no idea if that would be accurate.
"
can anyone help me out by giving a sample report of descriptive statistics using SPSS???,2,0,False,False,False,statistics,1513424484,True,
Google statistics director Steve Scott accused of sexual harassment at conferences is suspended pending investigation,0,2,False,False,False,statistics,1513433476,False,[deleted]
"Google statistics director Steve Scott, accused of sexual harassment at conferences, is suspended",79,132,False,False,False,statistics,1513433683,False,
Wilcox Test to compare two non-normal rates of movement in weird dataframe?,13,0,False,False,False,statistics,1513434202,True,"I have two very positively skewed data sets that I can't transform to make even sort of normal.

They are two separate data frames that show rate of movement between two different families of travelling icebergs. n=20 for both sets.

Is it appropriate to use the Wilcox test to compare them to see if their speed is different from zero? I was thinking of doing a difference of means test.

A lot of what I'm reading is saying that Wilcoxon is only good if you have a categorical x variable. My problem is that I am using a dataframe that was made on R that has my single x variable stored as the 'edge' between vertices. So technically, two columns constitute my X variable, which makes levels difficult to work with in my scenario.

Any advice?

Thanks!"
Math for a Masters in Statistics?,4,2,False,False,False,statistics,1513444564,True,"I know this has been asked a ton before, and I have searched through posts on this subreddit for good math to know before doing a Msc in stats. I see Real Analysis and linear algebra as musts, ODE's as essentially useless, and mixed reviews on complex variables/analysis.

How relevant do you think the following courses would be for a Msc in stats?

Groups and Symmetries

Mathematical Logic

Number Theory

Introduction to Topology

Graph Theory

Complex Variables

Non-linear Dynamics and Chaos

Combinatorics

Non-Linear Optimization


"
MINITAB ANOVA,0,0,False,False,False,statistics,1513448994,True,"What ANOVA analysis should I run on the dataset in Table 1 at the end of the PDF? (e.g. ANCOVA, Repeat measures ANOVA etc).

I would be very grateful for any insight."
Missing data definition,0,1,False,False,False,statistics,1513452115,True,[removed]
Need advice on selecting the right nonparametric analysis - is Skillings-Mack test appropriate for this dataset?,0,0,False,False,False,statistics,1513461155,False,
Fantasy Football Problem!!!,3,1,False,False,False,statistics,1513463468,True,"I’m in the same 10 man fantasy football league as my 2 brothers. If my brothers come in first or second I get my money back. Before the season started, what was the probability that I would get my money back? (assuming fantasy is all luck)

Edit: if either brother comes in 1st or 2nd I get my money back."
KNN posterior probabilities,0,1,False,False,False,statistics,1513471029,True,"A friend of mine shared this problem with me and i'm totally stumped:

We have determined that the optimal value for KNN is k=7, in a sample where we want to classify two levels: A and B.  a) How would you calculate the posterior probability if the sample data is representative of the population?  b) Same question, but now the data is not representative of the population.

I'm pretty sure the response is https://stats.stackexchange.com/questions/157502/how-to-explain-knn-in-bayesian-probability but the distinction of not representative is driving me nuts. Is this a trick question? Like its not parametric so it should be more robust right? Am i missing something?"
Logistic Regression Models: Only Using Entries that the outcome I am trying to attempt has occured,0,1,False,False,False,statistics,1513472136,True,[deleted]
Logistical Regression: what is the issue with using an identical instance of the training and test dataset when doing a logistical regression?,12,1,False,False,False,statistics,1513472735,True,"Hi everyone,

I'd like to use a logistical regression model to predict terminations on an employee dataset. I wanted to know what would I effectively be doing/what would happen if I used the same dataset for training as I did in testing?


I'm curious - what is wrong with this approach? I make this assumption that it is 'bad' to do this as it isn't a something that has been highlighted in the online materials I've reviewed.  "
What concepts have you had the most trouble with incorporating into your intuition ?,17,15,False,False,False,statistics,1513485879,True,
Opportunity Costs and the Parable of broken windows.,0,0,False,False,False,statistics,1513519999,False,
Can anyone name this equation?,0,1,False,False,False,statistics,1513532478,True,[removed]
How to tell which exponential distribution formula to use in light of PDF or CDF?,9,8,False,False,False,statistics,1513536403,True,"Hi,

So the exponential and poisson distributions have been puzzling me for over a day now.

I keep seeing a few different formulas and not knowing when/how to use them. For example, I typically see:

λe^−λx

1 - λe^−λx

e^-λx

e^a/β

1/β * e^-x/β

I don't understand when to use which formula. I keep reading that one is for the PDF and one is for the CDF and I can't really figure out the difference between the two.

Help is appreciated a lot. Thank you."
How to calculate df for a multifactor CFA?,5,7,False,False,False,statistics,1513540712,True,"Hi everyone. I'm reviewing for a journal, and the reported df's are very suspicious. Can anyone tell me how to calculate df for a model containing three latent factors? All of my training were based on one factor solutions. The 3 factors are informed by 10, 5, and 5 items each."
Random forest: overshadowing important variables,2,7,False,False,False,statistics,1513548616,True,"I'm training a random forest and was wondering if it were possible for me to overshadow an important variable with unimportant ones. I've got let's say 20 very important variables, and around 200 slightly important variables. I'm wondering if the importance of the 20 variables will be diminished by the sheer number of less important variables and if I should cut down on those 200 to prevent the overshadowing effect."
Beta Regression Model Interpretation Help.,2,0,False,False,False,statistics,1513556009,True,"Hello I'm not sure how to interpret Beta coefficients of each predictor. Yes it's for an exam and homework. \&gt;___&lt;

[I hear it's just like logistic regression] (https://stats.stackexchange.com/questions/63350/how-to-interpret-the-coefficients-from-a-beta-regression).

Here's one of exercise problem output via SAS:

https://i.imgur.com/Gr4sDlkl.png


mass_kg (numerical not categorical):
 
(exp(-0.01502) – 1)*100 = -1.49078 represents the estimated percent change in the number of successfully migrated birds when mass is increased by one kg, while all the other predictors are held fixed. 

Is this correct?

The response is the number of successfully migrated birds.

Thanks for your time.
"
Help with a normal approximation to give confidence interval in excel.,1,2,False,False,False,statistics,1513559798,True,"Here is the prompt: 
&gt; A Las Vegas casino is testing new, less expensive dice.  It rolls an experimental die 1200 times to understand the distribution of values.  The results of these rolls are given in column A.  Compile in D2:D7 the frequencies of the values 1 through 6.

&gt; Given that each of the possible values 1 through 6 should occur 1/6 of the time, use a normal approximation to give in D10:D11 a 90% confidence interval symmetric about 200 for the number of occurrences of each value.  In E2:E7 indicate whether the appropriate value occurs within the confidence interval.  In D13 indicate (“FAIR” or “NOT FAIR”) whether the die should be considered as fair or not, depending on whether or not all values fall within the 90% confidence interval.


https://docs.google.com/spreadsheets/d/13szMZBro2fwj_pd1cLk9ZMTN3GMDTWAdiFLG0Gl-HaQ/edit?usp=sharing
"
What is the result of arithmetic operations with two different normal distributions?,5,1,False,False,False,statistics,1513572804,True,"Let's suppose we have two different normal distributions N1(mean1, variance1) and N2(mean2, variance2). What can be said (if anything) about N1 + N2, N1 - N2, N1 * N2, N1 / N2?"
PhD-level mathematical statistics,0,1,False,False,False,statistics,1513580854,True,[removed]
Fractality,0,1,False,False,False,statistics,1513586842,True,[removed]
What's a reasonable simulation for the residual variance of a simulated linear regression?,0,1,False,False,False,statistics,1513590864,True,[deleted]
Generated dice rolls using 2 dice (d6),14,4,False,False,False,statistics,1513591010,True,"I play an online game in which a dice (2 d6) are rolled. I began to suspect that the game may not be truly accurate in how it generates this number. I've collated over 700 dice rolls. 

My question is, is this a big enough sample size and what type of analysis would I do in order to determine how the die rolls are skewed. "
Tangential skills,14,3,False,False,False,statistics,1513597626,True,"I want to become a professional statistician. Aside from statistics coursework, what are skills which are needed in order to find a job as a statistician. I've heard that I should learn SAS. Should I learn other computer science related things? Should I know what's in a data structures and algorithms class?"
How do I divide a long survey of many questions over multiple users?,2,0,False,False,False,statistics,1513604046,True,"Hi Reddit, not sure if this question is appropriate here or if it should go in /r/AskStatistics.

I am developing a survey website. The surveys are quite long (75 questions) so I want to divide the questions over multiple users. Example numbers:

* Survey length: 75 questions
* Users: 100
* Questions per user: 15
* Answers per question: 20

So, I want to ask every user 15 questions which will get me 20 answers per survey question.

My question: How do I divide the questions over the users properly? I have multiple properties per user that I would like to take into account (e.g. ""team"" and ""gender""). I know the make-up of the full population and I know that my 100 selected users accurately reflect the full population (I used stratified randomization to select the 100 users).

Ideally, I would also like to take previous answers into account. Say I run this type of survey every month. I would prefer it if users do not get too many duplicate questions. I.e. if Bob answered question 10 last month, I would prefer his list of 15 questions to not include question 10.

Is there any good statistical or mathematical way to distribute the questions over the users so that the answers will reflect the total population?

Thanks in advance!
"
Lasso versus adaptive lasso for variable selection,8,12,False,False,False,statistics,1513612560,True,"I was wondering what may be some differences between lasso and adaptive lasso for variable selection for regression, and whether one may be more suitable than the other in certain circumstances. What does everyone think?"
Specification error,0,0,False,False,False,statistics,1513625194,True,[deleted]
Research proposal help. Economic Statistics,0,1,False,False,False,statistics,1513628385,True,
Relative Risk or Odds Ratio? Help with preliminary research.,0,1,False,False,False,statistics,1513631361,True,[deleted]
ICC Cricket Ranking Formulas,0,1,False,False,False,statistics,1513634376,True,
How do you track your workflow?,1,16,False,False,False,statistics,1513634772,True,"I understand this isn't directly about statistics, but more about the practice of statistics. I'm genuinely interested to know how as statisticians you track and manage your time. Relevant questions might be:

- How do you track each project you work on? 
- Are there particular milestones that you make note of?
- Do you track the time you spend on projects? How?
- Are you required to track your time?
- Are you required to report to somebody about your time?

I'm wondering because I've only recently started to work as a statistician and thought it might be sensible to devise a consistent and thorough way of tracking my time, as I do indeed report to people and am required to track my hours spent on projects (and no, there isn't an existing system in place for this!)"
Best type of average for tracking employee stats?,2,2,False,False,False,statistics,1513635863,True,"I need help designing a Restaurant Bonus Structure for 30+ stores, specifically with the COG% averaging. The standard measurable span of time is going to be a month, or roughly 4 weeks. 
Most of our managers are straight shooters, run their stores really well. The problem will arise at some point, where they try and rig the system and intentionally drop their COG% for the final week before the ""finish line"" to meet their bonus obligations.

Is there a better way of taking an average, so that these fluctuations are discouraged or so that the outliers are minimized?"
"History of theoretical concepts of statistics, a recommendation of a book",2,3,False,False,False,statistics,1513636697,False,
Raffle odds,1,0,False,False,False,statistics,1513639884,True,"The gym I work out at is doing a raffle for the next 12 weeks, where whenever you get a personal record you put a ticket in the box. There are 2 rolls of tickets to choose from, one has heavier duty tickets that are closer to card stock, the other roll is lightweight tickets that are similar to normal printer paper. Which roll should I choose from to raise my chances of winning? (Also what little tips can raise my chances. For example, crumpling up your tickets gives them a larger surface area)"
Probability of a baseball team not winning championship for 100 years,16,15,False,False,False,statistics,1513645401,True,"Hi all,

Here's the question:


There are 30 baseball teams. Each team has an equal likelihood of winning the world series every year. What is the probability that, for a given 100-year period, at least one team never wins the world series?

I thought this problem was pretty easy, but when I wrote a Python script to test it, I'm getting a different result than I expected. I thought it worked like this:


1-(1-(29/30)^100)^30  =  64.247%


Simulating this 100,000 times in Python is giving me something much closer to 66.5%, which leads me to believe I'm thinking about this the wrong way. Can anyone help me?


Thank you"
Crowdsourced Projects to work on,3,6,False,False,False,statistics,1513653523,True,"Hello Everyone,

Anyone know of any crowdsourced projects requiring stat skills? I'm a recent MS grad looking for a challenge and to donate a little time. 


Thanks!"
"Hi everyone, does anyone know what this graph is called or how I can make one?",9,7,False,False,False,statistics,1513654797,False,
A couple of noob questions about running a simple simulation of variables,13,2,False,False,False,statistics,1513657213,True,"I'm wanting to show a few concepts of OLS but I'm actually new to simulating data.

I have y=hourly wage and a few regressors x1 - x3.

Simulating regressors x1 and x2 as N(0,1) makes life much easier, meanwhile x3 is binary. The problem is, I don't want negative hourly wages.

So what I did was standardize x1 and x2 as coming from N(1,1) instead of having mean 0. I also brought in intercept bo=5. With these variables and invented coefficients, y ends up being ~6 which is about the mean hourly wage of the time period I'm looking at.

So is this valid at all? Or is it completely stupid? Even though the variables are just simulations, x1 is still representing something (education level) - does pretending it's a real variable that I standardized to N(1,1) look really dumb?"
Something more general than a conjugate prior...,6,1,False,False,False,statistics,1513660762,True,"I am rarely at a loss for what to use as a search keyword, but this one has me stumped...

Consider some probability distribution g(x).

I'd like to know all of (or at least some of) the other distributions f(x) such that the integral of f(x)g(x) from 0 to infinity (or whatever the relevant domain is) has a tidy closed form.

If the integral of f(x)g(x) is in the same family as g(x), we say that f is a conjugate prior.

But this is something more general - I just want the posterior to be something I can work with easily, not necessarily something similar in form to g and not necessarily a named distribution.

Is there a name for these? ""Generalized conjugate prior"" and ""integrable non-conjugate distribution"" didn't get me much of anywhere."
Social Science PhD dropout -&gt; Statistics Masters?,3,3,False,False,False,statistics,1513680666,True,"Hi all,

I was hoping I could get some insight into my current situation. 

Due to a variety of personal reasons, I recently dropped out of an Organizational Psychology PhD program 1.5 years into it. I am now working full time in an Analyst role at a small consulting firm where I do a lot of ""data science"" work (e.g. develop online field experiments, code machine learning models in Python, etc.). I am hoping to go back to get a Master's in Operations Research or Statistics in the next year or two and was wondering how competitive I would be for either Georgia Tech (for the former) or Emory (for the latter, specifically biostatistics).

Here are my stats: 

* GRE V: 160 / Q: 162

* Publications: I have multiple conference symposium presentations (3 first authors) from my time in my PhD program and I am second-author on a methodological simulation study looking at mixed effect models.

* TAed 5 sessions of an intro to psych lab over the 1.5 years being there

* 4 internships from during my undergraduate doing stats / research for a few non-profits

* 3.71 Undergrad GPA (Psychology major, minor in Stats), 3.92 PhD GPA (including graduate courses in GLM, Psychometrics, and Bayesian stats)

* I am enrolling as a non-degree student for multivariate calc and linear algebra in the Spring at one of the aforementioned schools.

* All of the faculty at my PhD program were supportive of me and offered to be references for the future.

I wanted to see what else I am missing on my application and if there is anything I can do to fill in any gaps in my application.

Thank you so much in advance.



"
"All About that Bayes: Probability, Statistics, and the Quest to Quantify Uncertainty",11,64,False,False,False,statistics,1513689872,False,
Data Analyst Interview Questions,9,9,False,False,False,statistics,1513693050,True,I'm a senior graduating with a B.A. in statistics. I'm looking for a full-time analyst position - preferably in the media industry as I have experience in videography and content production. How technical should I expect Interviews? I'm good on my feet with general Interviews but I'm worried about my ability to answer technical statistics questions on the spot in a high pressure environment.
Politics Moves Fast. Peer Review Moves Slow. What’s A Political Scientist To Do?,0,2,False,False,False,statistics,1513697143,False,
Confused about setting up my 2x2 table to calculate odds ratio.,0,0,False,False,False,statistics,1513706120,True,"Thanks for clicking. I'm currently working on a manuscript which is looking to compare complication risks in patients that have hypothyroidism and those patients who do not have hypothyroidism. 

I received my data, but confused about how to set it up. 

I know on the left side it will be hypo and nonhypo. The top of the 2x2 will be complication 1 (ie UTI) and control. 

Patients that had hypothyroidism and a UTI the numbers were 2,448 = a

Patients that did not have hypo and had a UTI were 1,396 = c

Question is what are my ""b"" and ""d"" values?

Edit1: So I looked into this more, it seems like ""b"" and ""d"" are the total number of cases that did not have UTI in either the hypo and non-hypo group, respectively. 

Looking at my figures, here is the breakdown of the important numbers:

# number of patients with hypo = 4,811
# number of patients with hypo and UTI = 2,448 = a
# number of patients with hypo that didn't have UTI = 2,363 = b (?) 4811 = 2448

# number of patients without hypo = 4,511
# number of patients without hypo and UTI = 1,396
# number of patients without hypo that didn't have UTI = 3,115 = d(?) 4,511 - 1,396 = 3,115?

Setting up the 2x2 table where (AD)/(BC) = 2.31. 

Edit2: Sorry, no idea why the above shows up in bold. 




"
How many results are important in analysis?,0,1,False,False,False,statistics,1513711782,True,[removed]
Is Differential Equations needed?,14,4,False,False,False,statistics,1513718838,True,"Is Differential Equations required to be successful in a Masters program in Statistics? I am currently enrolled in Linear Algreba because I was short that course before applying to a masters program in Statistics but I've always been kind of interested in Diff Eq. However, if the course isn't really necessary then I will just take the course via EdX and save myself the money. "
Can I use my powers for good?,21,6,False,False,False,statistics,1513727820,True,"I am a junior stats major. Lately the idea of being an actuary has sounded...boring to me. A lot of people in my major become analysts in finance or marketing, or go to grad school. Most of the masters stats students in my college become data analysts in NYC or something.

This will sound weird but something that I like to do to pass the time is fantasize about how to make the world a better place. Like, I read articles about the possible impacts of climate change, changes in the tax bill, advancements in medical science. I like to think about urban policy, and how supply and demand affects housing and infrastructure prices, stuff like that.  

I guess I'm wondering if any of you know any career or company that seeks to help society in some way and could use the help of a statistician or someone that likes numbers?  "
what are the odds of this,1,0,False,False,False,statistics,1513730002,True,what are the odds of dating 6 straight women who smell bad? 
"Can I state that given a choice of x random numbers from a distribution and a choice of y random numbers from the same distribution, where x&gt;y, the expected ith highest (e.g., highest, 2nd highest) numbers out of the x choices is higher than the expected ith highest number out of the y choices?",24,2,False,False,False,statistics,1513731259,True,"This is an idea that I believe to be true that I'd like to apply in a biology paper.  Is this self-evidently true or do I need to say something about it to give it support?  Is this true for any distribution?  If not, what distributions is this true for that would be useful for biology?  

I'd appreciate it if someone could answer some questions along these lines.  If it's too much for a Reddit post, I can pay over freelancer for someone to answer questions and also cite that person in the paper's acknowledgements.  Thank you.  
"
"""Slopes as Outcomes"" Model: Is this the same?",0,1,False,False,False,statistics,1513736892,True,"I've been asked to run a ""slopes as models"" analysis. While I have used Linear Mixed Models before, I struggle with interpreting notation used in a lot of tutorials. I think I've figured out how to get at the same thing, but would like to double check.

The data:

N=61
Observations per N: 8900
4 Events (Dummy Coded) for everyone at matching timepoints.
Two grouping variables, each with 2 levels.

I am using JMP Pro 12, because I know it the best. I have been told this is based on SAS.

I've attached screenshots of my script, dialogue, and output. Does this look right for slopes as outcomes?

[Screenshots](https://imgur.com/a/5LMMO)"
What test can I use to compare two significant differences? Explanation in text.,3,1,False,False,False,statistics,1513738170,True,"I have an experiment with 4 groups that required a two-way ANOVA with Sidak's post-hoc analysis. This is a biological experiment. There are 4 groups total (2 independent variables) - animals with genetic manipulation and then surgery or sham:

- Non-transgenic animals (NTG) undergoing a sham procedure
- NTG actually undergoing the surgery
- Transgenic (TG) undergoing sham
- TG undergoing surgery

There is a quantifiable outcome, lets call it is weight. The surgery induces greater weight in both the NTG and TG groups. The NTG surgery vs NTG sham has a body weight increase of about 50% (the post-hoc p&lt;0.001) and the TG surgery vs TG sham has a body weight increase of about 30% (post-hoc p&lt;0.05). The increase in body weight, albeit still significant in the TG surgery group, was quite a bit less than the increase in body weight in the NTG surgery group. What test can I use to detect or demonstrate that the difference between two groups is less than the difference between the other two groups. Thanks for any help. Let me know if I should clarify more."
'My first Mathematical Statistics course: a tale about my LOVE-HATE relationship with statistics',10,6,False,False,False,statistics,1513767359,True,"Prologue: I just gave the final exam for this course, and I am sooooo fucking pissed. 

Introduction: This is going to be a rant about my love-hate relationship with the a course I took last semester. The title of the course was 'Statistical Inference'. It was a junior year proof based inferential statistics course. I'm a physics major, and I took this course out of my growing interest in statistics.

Chapter 1: We are reviewing some results from the previous course from this series, which was titled Probability. A deceptively simple name for a tricky course that I somehow managed to get a B- in. The content was extremely interesting. My high point was when we proved the Central Limit Theorem in class using the MGF approach. I couldn't believe I finally had a superficial understanding of one of the most important theorems in the field of statistics. We ended this chapter by reviewing covering some multivariate normal theorems, some order statistics, some confidence interval stuff. I was having fun, knowing the machinery and logic behind the vague tests we see these days is all I ever wanted.

Chapter 2: In the second month we started off with some point estimation. Method of moments est., Likelihood est., cramer rao lower bound. I liked the stuff a lot, it was also a lot of fun to sometimes dive deeper than what was discussed in class. 
We had a midterm exam that month, I thought I was prepared since I had done the homework diligently and was scoring fairly well in the weekly tests. 
The midterm exam was poorly designed. I fucking hated it. It gave us three lengthy but simple problems, and 1.3 hours to solve them. Given 2 hours, I could've solved them all easily. So I got 33%, mean was 35%, but I was hoping to fair well in the finals. 
Chapter 2 ended with the Blackwell Rao theorem, Sufficiency, Completeness, UMVUEs. I spent a lot of time trying to  understand the proofs and the connections between these concepts since in class we were given only the easy proofs just taught how to apply the theorems..

Chapter 3: Hypothesis testing. Fairly confusing because of the way it was taught, but I managed to get a handle on it. Main topics were: Neyman Pearson's theorem, its extension to the exponential family, UMP tests, likelihood ratio tests and its approximation for multinomial distributions: Pearson's goodness of fit test. 

Climax: I have three days before the final exam. I've done all homeworks, understood all concepts. Day one, I revise point estimation do some practice problems. Day two, revise hypothesis testing, do one-two practice problems. Day three, I fall sick. Got a terrible cold, didn't do anything.

This Is It: Still feeling a little unwell, the evaporation of my cold sweat slowly decreasing my body temperature, I step into the exam hall. Haven't done many practice questions, but still confident. 

Last thoughts before exam: I enjoyed this course a lot, did some unnecessary extra studying of proofs, will see if that comes in handy.

First thoughts after exam: FML. It felt like trying to mow my lawn, with a pair of scissors. I know I'm perfectly capable of doing it, as long as I'm given enough time.

Grades: Got mean marks and a C+

My take-away: Don't do ANYTHING more than what's taught in class. No one gives a shit about your depth of understanding. Just learn to solve problems as quickly as possible, learn small tricks that can speed up problem solving.

The End

TLDR: I loved what was taught, did some extra stuff during self study sessions, but failed to score well due to lack of practice and slow problem solving speed."
Predicting diseases,14,10,False,False,False,statistics,1513767723,True,"Hi guys, 
I'm working as the biostatistician in a medical research group but am just out of university and there's no one here that knows enough about stats to give me advice. 
I'm currently using logistic regression to predict disease development. Sample size is about 120 and I am getting different results every time depending on what seed I set before splitting the training and test set. I've used repeated 10 fold cross validation and am wondering if I'm not doing it correctly because of the variance in the results. I'm quite tempted to just use the results that suit me best but I'd much rather be sure I've all of this do right. 
Any advice would be absolutely great!
Thanks for reading.
Edit: Thanks for the help everyone, really appreciate it, will be sure to keep an eye on new posts so I can return the favor. "
"I did the math, fairness in dice roll in online game",26,13,False,False,False,statistics,1513769715,True,"So recently I asked a question in regards to what is the best way to determine if the dice rolls generated by an online game were fair or not (uses 2 x d6). I suspected they weren't but decided to actually do some maths to find out if that was the case. 

The test suggested to me in my original thread was the Chi-square test which I did below. 

My null hypothesis was that the ""dice"" are fair. 

dice roll|obs|expected|(O-E)^2|
:--|--:|:--|--:|
2|24|22|3.46|
3|58|44|188.30|
4|67|66|0.34|
5|102|89|180.75|
6|102|111|75.59|
7|125|199(wrong)|5513.06(wrong)|
8|94|111|278.70|
9|94|89|29.64|
10|64|66|5.84|
11|37|44|52.97|
12|30|22|61.80|
|n||x^2|
|797||41.74 (wrong)|



~~So from my understanding at a 5% confidence interval given x^2 is less than 49.8 (acquired from a table) we accept the null hypothesis.~~

~~i.e. the dice rolls are fair~~

~~Am I correct in my methods, calculations and conclusion? Because it just doesn't feel/look fair~~


Edit: I miscalculated the probability of rolling a 7, copy and paste error in excel. Also, I used 35 as my df (36 (number of possible dice rolls) - 1) when I should have been using 10.

dice roll|obs|expected|(O-E)^2|
:--|--:|:--|--:|
2|24|22|3.46|
3|58|44|188.30|
4|67|66|0.34|
5|102|89|180.75|
6|102|111|75.59|
7|125|133|0.46|
8|94|111|278.70|
9|94|89|29.64|
10|64|66|5.84|
11|37|44|52.97|
12|30|22|61.80|
|n||x^2|
|797||14.53|

So my X^2 is in fact 14.53 which is less than 18.31. So we can't reject H0 based on this data, but it still just feels/looks like the numbers are ever so slightly skewed towards the lower end (i.e. 3, 5 and 6 are rolled more often than 11, 9 and 8 respectively, even though they have the same probability of being rolled) "
Probably not a good place to ask.. ill try anyways,7,0,False,False,False,statistics,1513771619,True,How hard is a statistic major compared to something like engineering or finance
Criminal statistics,2,2,False,False,False,statistics,1513774187,True,"Hi I am a criminal justice major who interested in becoming a criminal analyst or intelligence analyst as the two terms can be interchangeable. I was wondering if anyone here is familiar with the field. I have just finished taking my intro level statistics, however I want to learn more. 

I have visited both the iaca (international association of criminal analysts) along with local associations, however as I proceed I want to field all the possible information I can in my endeavor. Any resources and tips would be greatly appreciated. "
Develop reliability score based off of success rate and number of attempts,1,1,False,False,False,statistics,1513784347,True,"Suppose we're looking to rate NBA players in terms of their reliability at making free-throws. We want to account for 1) each player's made free-throw percentage and 2) the number of free-throws attempted.

 

We want to account for both of these metrics because, I think, most would agree that a player with a 100% FT% and only 5 attempts should be deemed less reliable than a player at 90% FT% and 100 attempts.

 

The ""reliability"" metric I've developed follows.

 

First, rank in a descending and dense fashion the FT% and number of attempts. Then calculate the reliability score as `1 / (n_attempts_rank + ft_perc_rank)` and sort descendingly to get the most reliable free-throw shooters.

 

player|attempts|made|ft_perc|attempts_rank|ft_perc_rank|reliability_add

:--|:--|:--|:--|:--|:--|:--

A|50|48|96%|3|2|0.20

B|60|48|80%|2|3|0.20

D|500|350|70%|1|4|0.20

C|5|5|100%|5|1|0.17

E|20|16                |80%|4|3|0.14

 

This addition metric seems to work well for player A, but by the metric's logic, player D is just as ""reliable."" Pretty clearly, a 70% free-throw shooter on that many attempts is not better than a 96% on not-an-insignificant amount of attempts.

 

What suggestions might you have for producing a reliability metric based on this line of thinking, using only FT% and FT attempted? Feel free to experiment with different distributions/skewness/etc.

Edit: sorry if the table above is poorly formatted. I’m on mobile and tried my best..."
Can anyone help with this extra credit??,3,0,False,False,False,statistics,1513786831,False,[deleted]
How to calculate margin of error for a survey question with multiple choices of different weights?,0,1,False,False,False,statistics,1513788007,True,[removed]
US NIST Uncertainty Machine: online comparison of Monte Carlo and Gaussian linear approximation methods of error propagation for any function,1,4,False,False,False,statistics,1513789123,False,
Stranger Things: Five lessons for analyzing and communicating data,1,0,False,False,False,statistics,1513793219,False,[deleted]
I used an app to log my daily activities for the year of 2017. What cool stuff can I do with this data?,8,1,False,False,False,statistics,1513794670,True,"Hello everyone! I'm a junior in high school right now, and although I'm taking a statistics class at the moment, I'm relatively new to the subject.

So I used the app NowThen to log my activities this year to pretty much see what I was doing in my free time. The app itself is very simple, allowing you to start a timer when you are engaging in the activity, and then puts all the data into a pie chart and a table-y thing to let you better visualize how you spent your day and what you did at specific times of the day.

This data is all very interesting, but I'd like to get an even more in-depth analysis of my year. So far, I have amalgamated the data into an excel sheet and figured out how I spent my time on a ""typical 2017 day."" I'd love to do something else with my data, but I'm not sure how to proceed. Any ideas?"
Why would a non-significant covariate increase R-Squared by .11?,22,3,False,False,False,statistics,1513795079,True,"The adjusted R-Squared value of my model was .74 before I included it, and .85 after I included it. It also dropped the AIC of the model from 1728 to 902. 

Any reason why this would happen, in general? "
"Can anyone find the normal cdf (0.8077, 1000)?",2,1,False,False,False,statistics,1513795396,True,[deleted]
Any [R] packages that can perform generalized linear mixed model with Firth correction?,4,1,False,False,False,statistics,1513798525,True,"Hi all,

I am performing a genetic association analysis with binary outcomes (case-control study) and would like to include a genetic relatedness matrix (GRM or kinship matrix) as a covariate along with others. One issue that arises is the case of perfect separation, where all or no individuals with a genotype has outcome of interest. These scenarios could be potentially interesting and I was wondering if there are any known packages that can handle GRMs and have the ability to perform Firth correction. All packages that I am aware of (GCTA, Fast-LMM, plink2, etc.) cannot perform this procedure.

Or, alternatively, are there any relevant methods that could avoid the perfect separation issue? One alternative I am thinking of pursuing is LEAP: https://github.com/omerwe/LEAP/.

Thank you very much in advance"
Assistance with Stats,0,1,False,False,False,statistics,1513803139,True,[removed]
Logistic Regression where one class is over represented?,8,3,False,False,False,statistics,1513807349,True,"Hey!

I have a data set which I would like to apply logistic regression to. What I am wondering is if there will be any issue if I have many more observations for one of the classes. Right now I have 13 observations in 0 and 370 with 1. Will this cause any issue and is there something I should watch out for or can I just go ahead as normal with linear logistic regression?"
Binomial Distribution help,2,0,False,False,False,statistics,1513810625,True,What is the difference between a variable being independent and the probability of successes being the same for each trial. My teacher thought of a scenario where it was independent but the probability of success changed with each trial but I don’t remember it. Thank you for your help.
"How to ""rank"" teams when the length of participation varies?",17,7,False,False,False,statistics,1513813039,True,"I'm not sure if this is really a statistics question, so if not, please direct me where I should be.

The quick version of what I'm trying to do is rank a list of teams in order of their performance over the course of 7 years. Normally I could just take an average and be done with that. The problem I'm having is that not all the teams have been competing for 7 years i.e. a few have been here for 6, some 3, and one just once. 

Is there a method to use in order to come up with a single ranking system for all the teams? I though of using percentiles, but don't entirely know how to implement that.

I'm including an image of my table just to give a clearer idea on what I'm trying to accomplish. Thank you!

[Table!](https://imgur.com/a/5k1vs)"
Estimating Sample Size for Error Reporting,2,7,False,False,False,statistics,1513816443,True,"I know the basic equation for sample size estimation. However, I am not sure why I cant wrap my mind around this request. 

Context: We have a comment labeling system based on keyword search

Question 1: I want to estimated the accuracy of the labeling system. What is the sample size needed for this?
What I know:
I know that the alpha is 1.96 but I cant seem to figure out how I can get the standard deviation from just a accurate or no accurate population. Also in this scenario, how could I figure out the margin of error. 

Any help will be greatly appreciated. "
Understanding CV,2,2,False,False,False,statistics,1513816488,True," 
If a measured value is 6 and the CV is 50%, what is the anticipated range of results?
 
Is it between 3 and 9 (i.e., 50% of the measured value added to both sides)  or is it 4.5 to 7.5 (50% of the measured value spread equally on both sides of the measured value)
 
Not sure if any of these interpretations are accurate...
"
...,1,0,False,False,False,statistics,1513841081,False,
Bayesian modeling of 2x2 factorial design. Am I doing it right?,4,1,False,False,False,statistics,1513867944,True,"I've posted this question on cross validated but nobody replied. I'm hoping to have more luck here. I'll paste it in here but to see the full mathematical formulas go [here](https://stats.stackexchange.com/questions/319247/bayesian-modeling-of-2x2-factorial-design-am-i-doing-it-right)

I have a 2x2 factorial design with factors task (a, b) and stimulus type (c, d). I'm looking at behavioral data and was wondering how to test the main effect of task. To be more specific, I want to know if a&gt;b

. The data I have are counts (correct button presses), so using a binomial regression would make sense here (correct me if I'm wrong).

Essentially I thought of doing it in two different ways. One is to use a contrast to test the difference in task. For this I model each possible combination of factors, giving me four different Bionmial regression models, such as this one:

nhits,i∼Binomial(ni,pi)
logit(pi)=α
α∼N(0,10).

I then end up with four posterior distributions for α

with these combinations: ac, ad, bc, bd. I then proceed in calculating the contrast using a deterministic distribution:

diff=(αab+αac)−(αbc+αbd).

The second way I thought of modeling it is to use a linear regression:

nhits,i∼Binomial(ni,pi)
logit(pi)=α+βab⋅xab+βcd⋅xcd
α∼N(0,10)
βab=βcd∼N(0,10).

Now I also think there is an interaction, but before going there I want to know if I'm modeling this the right way. If looking at the main effect of task, which parameter estimates do I look at, βab
? Going further, I need to calculate the difference in probability between a and b. I took the traces from the NUTS sampler and calculated the difference in probability between α+βab and α. Or should I also include the second level: α+βab+βcd vs α+βcd

?

The two different approaches give me different results. The contrast approach gives me a distribution (median with 95% probability interval):

2.5%     0.026857
50%      0.045664
97.5%    0.064639

While the second approach gives me for the first difference:

2.5%    -0.000202
50%      0.025890
97.5%    0.051782

and for the second difference:

2.5%    -0.000140
50%      0.018837
97.5%    0.037980

Why do I get two different answers? Which one is right?

I'm implementing all this using pymc3, if anybody cares."
R-squared for Bayesian regression models(Gelman er al. 2017),0,11,False,False,False,statistics,1513868500,False,
Great Deep Learning Achievements Over the Past Year,0,32,False,False,False,statistics,1513868707,False,
Meta-analysis of survival data,0,1,False,False,False,statistics,1513869646,True,"What is the recommended method (preferably in R) for performing meta-analysis of survival data? Would we require individual patient-level data to conduct this analysis, or can it be conducted using information from published studies (e.g., study that gives as much info as a typical New England Journal or Lancet RCT)?"
How to determine which Councillors vote the same way most often?,7,2,False,False,False,statistics,1513873207,True,"I have voting records for a few hundred local council votes. There are 30 councillors and for each vote, three possible options: “yes”, “no”, or “absent”. 

I am trying to quantitatively determine how similar any two councillors voting records are. A contingency table might work, but I am unsure how to go about creating that."
Does sample size affect sampling distribution model?,0,1,False,False,False,statistics,1513874684,True,[deleted]
"Writing Prompts, Quizzes, Lessons and Graphs: Our Most Popular Teaching and Learning Resources in 2017",0,1,False,False,False,statistics,1513879472,False,
A non-comprehensive list of awesome things other people did in 2017 · Simply Statistics,1,25,False,False,False,statistics,1513884933,False,
How do you handle missing variable in multiple regression?,2,0,False,False,False,statistics,1513887606,True,"If I have y=b+mx1+mx2+mx3 for some number of x's, where each sample has of course values for x1, x2 and x3... what happens if one of the samples is missing a value and only has, say, x1 and x3 but not x2? ...or.. i guess sample isn't the right word. What is the right word? Data point? Idk.

Do you discard that entire sample? Thanks!"
I have a choice to study abroad or not and i can't seem to figure out how it'll effect my GPA.,6,0,False,False,False,statistics,1513892298,True,"My current GPA is a 2.7. most of my courses I've achieved grades all over the place from 1.0 to 4.0 so the standard deviation is quite high. 



So heres the deal. I have three semester left. I have the choice to study abroad one semester and by doing so I won't have my grades for those courses counted so my GPA won't be effected in a negative way. Plus if I come back and do well in the remaining two semesters I'll have those courses possessing a higher weightage on my overall grade thereby say if I get 3.5s in those next two semesters they will hold relatively higher weight.



The other choice is to not go abroad and study in my current uni and have those grades count. I'm expecting getting higher then my current cumulative GPA (2.7) so around 3.0-3.5. But here's the catch. I did some back off the hand calculations and it turns out it'll make a huge difference depending on if I get 3.0 or 3.5. If I get a 3.5 next semester and get a 3.5 the next two semesters I'll do wonders for my gpa. However if I get a 3.0 then my concurrent 3.5 for the next two semesters will hold relatively less weight and my GPA will not go up as much.




So I've got an interesting dillema. I'm 100% confident I can at least get a 3.0 but not too confident I can get a 3.5 next semester. This is the largest variable effecting everything and makes a huge difference. Should I be going abroad or not to save my GPA?"
Should I stick with Biostatistics or head towards a “Database” position?,0,1,False,False,False,statistics,1513897923,True,[removed]
Best open source R software for beginners?,4,11,False,False,False,statistics,1513899227,True,I am a Decision Sciences major with upcoming classes that focus on R. I want to be more comfortable with this language before starting the classes and was wondering if anyone could steer me in the right direction software-wise? Any help is greatly appreciated!
How exactly do probability distributions help?,6,3,False,False,False,statistics,1513914027,True,"While I feel like I have a fairly good grasp on how one interprets probability distributions (z, t, f, etc) for hypothesis testing... one thing that has escaped my understanding is why they are so common. 

What exactly encourages the use of these standards. Is it difficult to calculate the probabilities without these standards? Is it difficult to compare different distributions if they weren't ""converted"" to some standard?

From calculus, which I took ages ago... I feel like calculating the area under a bell curve is something that can be done... but the one graduate statistical class, which I have taken, didn't even touch this subject. And as I am planning to take make more statistical classes... I feel like I'm missing something important. Thus, I want to learn more. 

Thank you =)"
How to iteratively estimate factor analysis loadings?,0,1,False,False,False,statistics,1513954636,True,[removed]
Dave Ramsey tries to make a point about mutual funds; he instead makes a case for better statistical education.,56,49,False,False,False,statistics,1513956291,False,
Lowered drop rate from Loot Boxes... suspicious or not?,0,1,False,False,False,statistics,1513961057,True,[removed]
Possible_Career__Option_in_Statistics,4,0,False,False,False,statistics,1513963276,True,Can I pursue MS in Statistics or MS in Financial Engg or MS in Theoretical Physics after B.Tech in Mechanical Engg?
Specific question regarding test selection for paired data and multiple comparisons,2,1,False,False,False,statistics,1513968075,True,"Was wondering what test to select when comparing percentages in paired data. My example would be with drug clearances; most drugs are cleared to different degrees by the liver and by the kidneys, and so we might say drug X is 70% cleared by the liver. If I were to have samples of clearances of the same several drugs in several patients, and wanted to compare these clearances to a reference drug, what test might I use? I know the Dunnett's test could be used for non-paired data. I'm not sure if it applies here. And, I'm not sure what the alternative would be if the data aren't normally distributed. Any help would be appreciated."
Cheapest laptop for running AMOS?,5,1,False,False,False,statistics,1513971320,True,"I am a mac user. I am attempting to buy a windows laptop, as inexpensively as possible, exclusively for running AMOS. I find the world of PC laptops to be confusing. I find the least expensive options to include chromebooks - but I know those won't work. Then I see (Lenova ideapads)[https://www.bestbuy.com/site/lenovo-ideapad-11-6-laptop-intel-celeron-2gb-memory-32gb-emmc-flash-memory-mineral-gray/6061800.p?skuId=6061800]. I'm not sure if this is similar to a chromebook and therefore also won't work.

Here are some others that I found for around $200:

https://www.bestbuy.com/site/lenovo-14-laptop-intel-celeron-2gb-memory-32gb-emmc-flash-memory-mineral-gray/6061700.p?skuId=6061700

https://www.bestbuy.com/site/dell-inspiron-11-6-laptop-intel-celeron-4gb-memory-32gb-emmc-flash-memory-bali-blue/5618116.p?skuId=5618116

https://www.bestbuy.com/site/lenovo-flex-4-1130-2-in-1-11-6-touch-screen-laptop-intel-celeron-2gb-memory-64gb-emmc-flash-memory/5579833.p?skuId=5579833

https://www.bestbuy.com/site/hp-stream-14-laptop-intel-celeron-4gb-memory-32gb-emmc-flash-memory-aqua-blue/5623611.p?skuId=5623611

Should any of these work? Any other suggestions for something as inexpensive as possible that can be used for running AMOS?"
Best college-level online stats courses,3,1,False,False,False,statistics,1513987476,True,"Hi, I’m looking for recommendations for online stats courses that meet these criteria:

- have an actual instructor
- have graded assignments
- as rigorous as a university-level stats class
- ideally, worth college credit
"
"How do you calculate % chance of winning of a texas gold em hand, before the flop?",3,0,False,False,False,statistics,1513998291,True,"For example, [THIS](http://www.natesholdem.com/pre-flop-odds.php) site outlines the % chance of you winning, based on your hand before the flop (when three other cards are shown). How do they calculate this?

I've searched everywhere and have no Idea how I'm supposed to do this. "
Will you please recommend me one or two solid intro-to-statistics books to read before I delve into academic stats classes?,23,8,False,False,False,statistics,1514001446,True,"I’m a writer and editor by trade, but I’ve decided I’m going to pursue some formal stats training because I find stats fascinating. I’ll be taking an entry-level stats course starting Jan. 8, but I want to read a few well-regarded entry-level books on the topic before the class begins. Any recommendations? Thanks!"
"So I'm looking at a variety of statistics/data related masters degrees for grad school and there's alot of variety out there. Between a MS in Stat, MS in Applied Stat, MS in Statistical Practice, MA in Stat, and a MS in Data Science, will it make a difference ingetting a job given its a good school?",6,1,False,False,False,statistics,1514003928,True,
Regression apps for iPad?,8,4,False,False,False,statistics,1514052873,True,"I teach statistics at a high school. The only tech we have are iPads, not PCs. If anyone here has any sort of idea about which apps are best for analyzing data with an iPad, that would be great."
2x2 ANCOVA no interaction effect but significant interaction after post hoc. How?,4,5,False,False,False,statistics,1514061393,True,"I ran a 2x2 ANCOVA with both variables having two levels. One of the variables had significant main effects but there is no interaction effect. I ran a post-hoc test (accidentally actually, b/c I had no interaction, running a posthoc for the interaction was unnecessary actually) but I saw a significant interaction effect. 
I am linking the between subjects tests [results](https://imgur.com/Nt7NXD1)
and the pairwise comparison [results](https://imgur.com/a/1vTXB)

Why is this happening? I used bonferroni correction. Am I supposed to report this as a significant interaction or not? "
Average holiday weight gain is just 1 lb. BUT: the average middle-aged adult is overweight due to average weight gain of ... 1 lb every year.,2,73,False,False,False,statistics,1514068439,False,
Help me explain to my family the percentage chance when they are rolling dice in a dice game.,8,4,False,False,False,statistics,1514078502,True,"I was unable to explain to some family members about the statistics involved with rolling dice. The scenario was a Yahtzee game and they were rolling for a full House (3 of on die, 2 of the other).  My cousin rolled 2 two’s and 2 6’s on her first roll. I said that song she had two more turns to roll either a 2 or a 6 that she had a 66% chance of rolling one of those numbers between the two turns she had remaining. She very vehemently disagreed and said that she only had a 33% chance of rolling either a 2 or a 6 between both of her  remaining turns because the chance of rolling them doesn’t stack. 

Please, someone, help me explain this to her. "
Deriving a Statistical Model,19,0,False,False,False,statistics,1514122766,True,"Dear all,
is there any way to represnt continuous break line into a statistical model, and derive a mathematical equation to solve the same?
Plzzz refer to the image..
https://imgur.com/a/WNULo
Regards"
Why do we need statistics at all?,11,0,False,False,False,statistics,1514131767,True,"It all started during the last war to convert capitalist America into planned war economy with great success that proves the best that capitalism is a major hindrance for American economic development.

Having said that, there is zero need for statistics now with no input/output matrices currently used anywhere.

We don’t really need any data. we already know what we want to know and all people just use data to confirm their priors.

It is all very expensive and completely not necessary since we do not fight any wars today.

"
Computer Reccomendations,21,8,False,False,False,statistics,1514133809,True,"So I'm about to finish my undergrad with a Bachelors in Stats, with an emphasis on statistical computing and a minor in poly sci. I'm looking ahead to grad school and eventually working with large data sets and realized that my current computer isnt going to cut it. Especially after working on my senior project and dealing with crashing

I'm looking for suggestions about what I need to be versatile and successful when doing analysis. The basics of my potential build are as follows

* Budget: I'm trying to stay below $1500. Lower would be nice

* Software: I'm most familiar with R, but also use SAS and the office suite. Additionally, I'm also using various compilers for python and c based languages

* I want to have this be a desktop. I've had a few different laptops and I'm not sold on them

* I also have built a computer before but I'm not super comfortable doing that, but if pointed towards instructions I would do this. 

Thanks for any advice!"
Logistic Linear Regression,19,0,False,False,False,statistics,1514204265,True,Where are concepts like Linear Regression and Logistic Linear Regression used in Trading? How do they help predicting the Stock's movement?
Can we get a sidebar link to r/pystats,2,1,False,False,False,statistics,1514224820,True,[removed]
Good statistics book for someone with fair mathematical maturity?,19,10,False,False,False,statistics,1514238641,True,"I've google for this, but most people who ask this question are beginning from scratch, so they're recommended beginner books that assume basically no knowledge (even about probability). 

I've gone through Introduction to Probability by Bertsekas &amp; Tsitsiklis and bits of Probability in EECS by Walrand. I have some basic knowledge of stats (MLE, MAP, Binary Hypo Testing, LLSE, MMSE, etc). Any suggestions?"
Help needed ! Logistic regression in JMP (adjusted for clustering),0,1,False,False,False,statistics,1514261292,True,[removed]
Class bias and the chi squared test,3,2,False,False,False,statistics,1514265124,True,"According to [this article](http://r-statistics.co/Logistic-Regression-With-R.html), for logistic regression I need to have a proportional number of events and non-events. Does this also apply to the chi-squared tests, or can I use the total number?"
Skills Tree for learning Probability and Risk Analysis,0,1,False,False,False,statistics,1514268379,True,[removed]
Public Health Data analytics?,7,1,False,False,False,statistics,1514276651,True,"Merry Christmas ya'll

I'm looking to establish a data analytics program for a public health campaign I am involved in. We are taking in information like needs assessments, services provided, and basic demographic data. Not too complex. We are currently using Excel for collection and analysis but while it is fine for data entry and collection, it is less useful for analytics. I would like to use a more powerful program that is better for descriptive statistics and correlations. What program should I start with to help our non-profit do better?"
Is it ok to use OLS for 2 proportions? ie: rate of homicide vs unemployment rate by state?,17,1,False,False,False,statistics,1514276692,True,I heard there are problems if you're using 2 rates. 
Math Says You're Driving Wrong and It's Slowing Us All Down,8,74,False,False,False,statistics,1514292711,False,
Describe statistics,0,1,False,False,False,statistics,1514302656,True,[removed]
Mathematics intern with Question,0,1,False,False,False,statistics,1514307974,True,
"Looking for cheap online courses, with rigour, difficulty, and instant feedback",2,1,False,False,False,statistics,1514308460,True,"I want to learn some statistics at all levels.  Where are the good courses hiding?


I have a mild preference for those 4 course series that build off one another."
Best book for forecasting and maybe time series,0,1,False,False,False,statistics,1514308543,True,[removed]
Confused about handling count data,0,1,False,False,False,statistics,1514309119,True,[removed]
"How do you define ""Statistics""?",8,0,False,False,False,statistics,1514312170,True,
First difference for entity-invariant effects,0,0,False,False,False,statistics,1514312834,True,"So I'm currently doing a project where I'd like to do an FD or FE on panel data. I'd like to compare the two but am thinking FD is better because looking at change in variables makes intuitive sense in my problem and also because the data doesn't seem to be stationary so taking a difference seems to mitigate that slightly.  

I know dealing with both time and entity-invariant unobserved heterogeneity can be done through a within transformation with time and entity-demeaned variables, but am confused about how this works with first difference. Don't we end up with the difference lamba_t - lamba_t-1 still in the equation that gets captured in the error?

EDIT: sorry i just realized this might violate community guidelines but am on mobile so not sure how to delete"
I made a video on how to draw samples from a multivariate normal using numpy and scipy,0,4,False,False,False,statistics,1514319988,False,
"I do a survey annually with a group of people about their satisfaction levels in a program. The survey is done with Leikert scale q's. The average score 5 years ago was 4.38/5, and each year it's gone up. This year it was 4.70. Would you consider the increase to be statistically significant?",14,0,False,False,False,statistics,1514342317,True, 
How would you go about modeling a card game as a stochastic process?,2,0,False,False,False,statistics,1514353319,True,[deleted]
Sampling Distribution of Stochastic Dependent Data (Mean of Quantile-Differences of Two Groups),2,2,False,False,False,statistics,1514368401,True,"Reddit, I need your help as I'm a psychology student and my statistics-knowledge is limited.

I'm trying to develop a significance-test using the quantiles (percentiles) of the two groups to be compared:
* 1) standardizing
* 2) calculating differences of each percentile (Q1x-Q1y, Q2x-Q2y,... Q100x-Q100y) and the quantile standard-error
* 3) these differences would be t-distributed, but I'm squaring them so all of them are positive --&gt; F-distribution with df1=1, df2=(nx+ny-2)/2

Now it works perfectly fine to do 100 F-tests, but I would like to find a single test-statistic that expresses the mean/maximal/whatever difference, which can be compared with a critical value for significance.

I already found a numeric value by doing bootstrapping, but I can't find any distribution that fits the sampling distribution of the mean difference.
I think the problem is, that the quantiles are not stochastic independent, so the sampling distribution of the mean is not a random distribution.

QUESTION: is there any distribution that fits stochastic dependent data?

I would be really grateful for your help! Thanks!

PS: I also tried using the sum of signficant F-tests of all the percentiles, hoping to arrive at a poisson distribution (under H0: ""how often do 1,2,3,..100 quantiles differ significantly?"") but could not make it fit. Maybe my knowledge about Poisson and my R-skills are insufficient."
"Recommended textbooks/books that explain in detail on how to use convergence cross mapping for causal inference, preferably one with a lot of examples to aid in explanation?",1,9,False,False,False,statistics,1514374683,True,"Which book would you recommend to learn convergence cross mapping? I found a paper on Wikipedia which I feel was a great paper but even that paper I still find confusing. This is the paper: 

http://www.uvm.edu/~cdanfort/csc-reading-group/sugihara-causality-science-2012.pdf

Is there anything that would explain it even better for dummies like me, preferably with examples?"
A curated list of awesome Shiny Apps for statistics (ASAS),2,35,False,False,False,statistics,1514395063,True,"I made [a curated list of awesome Shiny Apps for statistics \(ASAS\)](https://github.com/huyingjie/Awesome-shiny-apps-for-statistics). 

The goal is to 
1. help teachers teach basic statistics to their students.
2. help self-learners to visualize statistics concepts.

I hope that teachers can use this page as a reference in the coming spring semester.

 Any feedback would be appreciated!"
"Simple statistics question comparing 2 audits, internal and external for Tricare vs private Healthcare",0,2,False,False,False,statistics,1514397618,True,"Hello,

Our base has 2 audits to track if we are asking patients for ""other"" health insurance. This allows the military to bill Tricare dependents with these other health insurances and save the govt money.

We have Internal and External audits.

Patients fill out an insurance form electronically, 86% of the time it saves correctly. 14% of the time we use a paper forms.

Internally we account for those 14% paper forms. Example 9/10 filled it out electronically, 1 was paper, so we know we asked 100% of the patients about other health insurance.

Externally, they do not see the paper forms. So at best it looks like 14% of the patients were not asked to fill out the form.

Over a period of the month, what would be a reasonable standard error between the internal audit that accounts for the 14% paper forms with the external audit that does not account for the paper forms which skews our numbers.

I know this seems simple, sorry. Thanks. Much appreciated!!"
Overview of High Level Statistical Methods,2,6,False,False,False,statistics,1514414595,False,
Trying to understand how to do the following using SAS University,0,1,False,False,False,statistics,1514419177,True,
Question about significance of choice differences based on marital status,2,1,False,False,False,statistics,1514419902,True,"I am looking at a population and trying to determine if marital status is a significant indicator on preference.  

I'd like to make sure I can support a claim that 
&gt; ""non-married people are much more likely than married people to choose this product""

OR

&gt; ""non-married people are more likely than average to choose this product""


Here's the data:

Marital Status | Yes  | Study Population| Yes  | No
---|---|----|----|----
Married |	1,171 | 	10,227 | 	11.5% | 	88.5% | 
Non-Married |	949 | 	6,487 | 	14.6% | 	85.4% 
Total |	2,120 | 	16,714 |	12.7% |	87.3% 

p.s. I have several of these so some help on the setup is appreciated so I can test the rest of them. 
"
Do you use Bayesian statistics and do you think it will outpace frequentist statistics eventually?,32,47,False,False,False,statistics,1514421407,True,"I had a guest speaker in a grad-level stats class present some of his research (he does ecology stuff for the government of Canada) a few weeks back. I'd never really looked into Bayesian stats before. The guy seemed elated about it, and his explanation of some of his procedures left me more confused than when I knew literally nothing about it.

Just curious how other people view this approach? His presentation almost made it seem like there would be some inevitable shift towards Bayesian approaches in the distant future. Maybe I'm wrong. I don't know. I'm still a student. Let's have a discussion?"
A Simple Introduction to Complex Stochastic Processes,0,4,False,False,False,statistics,1514432315,False,
"Data scientists of reddit, how come data science is a field that's basically just math and math and math, and doesn't emphasize critical thinking skills?",0,1,False,False,False,statistics,1514440632,True,
Sign test for small samples,0,0,False,False,False,statistics,1514442754,False,
How to tackle a question with type II error,1,0,False,False,False,statistics,1514444369,False,
Easily conduct simple linear regression model using minitab and interpret,0,2,False,False,False,statistics,1514471093,False,
Exploratory data analysis made simple with Qlik Sense. This video shows you how to quickly do an effective and telling exploratory data analysis with any data set.,5,0,False,False,False,statistics,1514474137,False,
Is there anything at all P values are used for which couldn't be better achieved by an alternative statistical tool?,6,2,False,False,False,statistics,1514483390,True,
Correlation Analysis Made Easy With R and Rattle - this makes it super easy and there is no need for a bunch of lengthy formulas or any coding.,3,33,False,False,False,statistics,1514485114,False,
just talking about probability without payoff is BS: Nassim Nicholas Taleb,10,2,False,False,False,statistics,1514489080,True,"Taleb and Tetlock had an argument on twitter about this yesterday.
The argument started from a [Joyplot of what people mean when they say 'likely' and such words](https://twitter.com/PTetlock/status/945978679154434053).
Taleb's side is [here](https://twitter.com/nntaleb/status/946037441118527488) and [here](https://twitter.com/nntaleb/status/946355998444408832).

Tetlocks is [here](https://twitter.com/PTetlock/status/946346459640795136)

What do you think of the argument? Is it true that you cannot [separate utility from probability?](https://twitter.com/nntaleb/status/946355998444408832) when there is a nonlinear payoff? How does Bayesian statistics deal with this?"
"Given games played and won, what's the best way to weight the win percentage for predicting the outcome of the next game?",7,3,False,False,False,statistics,1514498527,True,[deleted]
"In response to a recent question, here is my Bayesian approach to a coin flipping puzzle",4,17,False,False,False,statistics,1514500887,False,
The Power of Bias in Economics Research,0,22,False,False,False,statistics,1514501617,False,
My chances of grad school in Europe?,2,4,False,False,False,statistics,1514508651,True,[deleted]
ELI5:Can you explain to me the difference between terms in statistics (association vs. causation; sample vs. population; quality vs. power),2,2,False,False,False,statistics,1514510388,False,
"How to quickly build, test and improve upon a predictive model with R and Rattle.",0,11,False,False,False,statistics,1514515071,False,
How to evaluate a conditional expectation generated from martingale?,2,12,False,False,False,statistics,1514516009,True,"Lets say we have a martingale x(t) adapted to a filtration F(t). Suppose we know x(T). Our best estimate of x(t) is therefore E[x(T) | F(t)]. (Yes this is exactly like applied in asset pricing where x(t) here is an asset price divided by the numerair).

To evaluate E[x(T)\F(t)], I have seen pdes being used, but sometimes E[x(T)\F(t)] is evaluated directly. What are the ways to proceed from here to evaluate E[x(T)\F(t)]? And what are the assumptions for each method?


"
Career Change to Statistics,9,6,False,False,False,statistics,1514517301,True,"I am a college student at a university in the US and want to eventually work in the field of statistics. However, I am currently studying IR. What are some viable options to make this a possibility besides taking classes in the subject, which I am already doing? "
Can you calculate a p-value without doing a hypothesis test?,4,0,False,False,False,statistics,1514525659,True,"Many of the articles I've read have calculated p-values when performing a hypothesis test. But, can you just calculate the p-value to understand how likely your correlation is without doing a test? Sorry, I'm new to stats!"
Relationship between number of predictors and flexible/inflexible model?,0,2,False,False,False,statistics,1514526985,True,"TLDR:
How does the number of predictors influence whether you'd think a parametric or nonparametric model would work better?

Hey Guys,

I'm currently working through ""Statistical Learning with Applications in R"" and one of the exercises asks to indicate whether you'd generally expect the performance of a flexible statistical learning method to be better or worst than an inflexible method. By flexible I'm assuming they mean some type of nonparametric model and by inflexible, a parametric model. The specific question is if you have a large sample size n and the number of predictors is small. I understand how a large sample size works well for flexible models, since you have more training data it becomes easier to make sure you're not overfitting, but how does the number of parameters make a flexible model better or worst? Thanks. "
Send Help: Find the right model for causal effect,0,1,False,False,False,statistics,1514544913,True,[removed]
How to find Pearson correlation for grouped data on a Casio calculator?,0,0,False,False,False,statistics,1514558869,True,"And if it is not possible, can anyone explain it to me plain. Because the explanation I got my hands on is complicated.


A student who has an exam tomorrow."
Gaussian Processes,0,35,False,False,False,statistics,1514560714,False,
Quantile regression vs OLS for fitting moderation models,8,3,False,False,False,statistics,1514568713,True,"I'm fitting a model of the type:
y ~ a + b*u + c*v + d*(u*v)
where y is dependent variable and (u,v) are independent variables. There is a moderation effect modeled with ""d"" its parameter.

y is close to normally distributed, ""u"" is uniformly distributed over an interval, and ""v"" has few outliers on left tail. 

I have few questions:
(1) Does it make sense to have moderation effects modeled in quantile regression (say, median regression)?

(2) Is it odd if there is greater significance for the parameter ""d"" in median regression versus OLS? Standard errors in the median regression were evaluated via bootstrap. I had thought median regression has generally less power than OLS.

(3) Does quantile regression place assumptions on distribution of res

(4) Which regression is better to report for analyses --- especially if quantile regression yields better significance and is supposedly more robust, is that better? "
Item analysis in SPSS?,0,1,False,False,False,statistics,1514576411,True,"Hi, please, could someone give me an advice on item analysis? I have a dataset which includes responses to a questionnaire. It also contains 12 subscale scores and 1 final score. I would like to investigate further the relation between the final score and the subscales (for example which of them contribute most to the final score etc.) so I could use only those in the follow-up study. I have never done this but I suppose the item analysis in SPSS could be the right choice? Thank you very much in advance."
Implementing the elliptical slice sampler using numpy (Bayesian inference with multivariate normal prior),0,4,False,False,False,statistics,1514578975,False,
Piecewise model in polar coordinates,0,1,False,False,False,statistics,1514590674,True,[deleted]
Do I have a chance to get into a good statistics master in North America?,4,0,False,False,False,statistics,1514609472,True,"I am at a fairly prestigious undergrad program in Canada doing statistics. I have interned at several companies with 2/5 of them being very statistics heavy roles (stochastic modelling, statistical consulting). I also do a lot of statistics related side projects and have made two contributions thus far to an undergraduate statistics conference. However, my culmulative GPA is a 3.3. My statistics average is 3.9, but my other math marks are not as good (with a few being &lt; 3). I also have 2 failed courses in my third year due to a run in with depression. I have a decent (but non-working) relationship with professors. My academic advisor recommended me to apply to a bottom tier Canadian school, but I have not heard great things about them. I have only recently been exploring U.S. master programs as well. I noticed that most of the U.S. schools tend to have lower admission cut offs than Canadian schools, but I'm guessing most successful applicants have much higher grades. 

Do I still have a chance to get into a good statistics master in North America? Thanks in advance. "
"How to Quickly and Easily Evaluate Predictive Models with R and Rattle! In this video 4 models are compared (Linear, Random Forest, Neural Net and Decision Tree).",1,43,False,False,False,statistics,1514647444,False,
Advise sought - Weekly prices aggregation to monthly,0,0,False,False,False,statistics,1514655069,True,"Hi All,

Merry Christmas and Happy new Year everyone.

A bit of primitive question, BUT... 

I need to aggregate daily/weekly (prices on Friday each week) prices to monthly. The first thought is to go for the monthly average average. I am to use monthly average with the monthly reported data with the multiple regression model.

Could you confirm if that is the most optimal aggregation approach or suggest other more relevant please?"
Any advice for someone trying to get an analyst position in education with only a BS in statistics?,0,1,False,False,False,statistics,1514674990,True,[deleted]
The evidence-based medicine problem: US doctors cling to procedures that don’t work,17,89,False,False,False,statistics,1514684800,False,
"In ARIMA what values can p, d, q have?",4,1,False,False,False,statistics,1514694083,True,"In the arima function in R what values can be assigned to p, d, q , because for some reason in every example that I see out there (like this one) tests values of 0 and 1 to get optimal model, but can't I test with values like 2, 4, 5?.

I don't mind leaving the computer for several hours executing the process, but I want to get the best model."
Check my math - how un-lucky / non-random was lottery scratcher?,4,1,False,False,False,statistics,1514730083,True,"Hi all,

  It's been quite a while since my stats classes and I was never really good with picking the if this, then subtract this probability.  So can you can check my math on this lottery ticket? (And yes, I know EV of lottery is negative; not going to run second trial on this one).

Game set-up:  Pick six numbers 1 to 40 (observed, not certain, but this is most favorable to game, right?).  Replace numbers and draw 20 times (replacement is uncertain).  

Question 1 - simple odds of no matches, with replacement:  So my odds of losing each individual scratch is 34 losing numbers / 40 total numbers = 0.85.  If we assume the 20 trials are independent, i get 0.85 ^ 20 = 3.9% of losing all scratches.

Question 2 - No replacement - I did observe that all 26 numbers revealed on the card were unique: So on my first scratch I start with the 0.85 chance of losing and this decrease to 15 / 21 on final scratch (40 - 6 winners - 19 previously revealed) / (40 - 19 previously revealed) = 0.71.  When I multiply all those down, I get 1.0%.

That seems intuitively high.  To have 26 random numbers and no repeats.  Did I do anything wrong? "
How to become a Psychometrician? Do I go for a masters in statistics? What should my undergrad major be?,7,6,False,False,False,statistics,1514739399,True,Are there any internship opportunities for such a field? 
What's the best statistical methods for not seasonal data?,1,1,False,False,False,statistics,1514745475,False,[deleted]
What's the best statistical methods to forecast non seasonal data?,5,11,False,False,False,statistics,1514746141,False,
Preparing for a post-bacc certificate,3,3,False,False,False,statistics,1514746928,True,"The last [relevant discussion](https://www.reddit.com/r/statistics/comments/6pzo9a/anyone_have_experience_with_statistics/) about this was 7 months ago, which is why I am posting on a new thread. Career-changer with a History BA. Just finished Calc I (integrals were limited to Chapter 4 of Larson Calculus). I am considering [this program](http://bulletin.vcu.edu/undergraduate/college-humanities-sciences/statistical-sciences-operations-research/statistics-certificate/), which is local and in-person. Among other things, I desire employment as an entry-level business analyst.

Two questions:

- can a right-brained person master the math needed to complete this curriculum? I am taking my calc classes (3 semesters) at a community college, and I am concerned that my success (A in Calc I) thus far will be undone by 300-level coursework

- what else can I do to prepare for Probability Theory?

Thanks and Happy New Year's Eve!"
Statistic Homework Help,0,0,False,False,False,statistics,1514754134,True,[deleted]
